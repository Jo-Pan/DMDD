{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6187f3d-3d63-467c-b22f-7d99a0331c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "pwc_base = \"https://production-media.paperswithcode.com/about/\"\n",
    "pwc_file_names = [\"datasets.json.gz\", \"papers-with-abstracts.json.gz\", \"links-between-papers-and-code.json.gz\",\"evaluation-tables.json.gz\",\n",
    "                  \"methods.json.gz\", ]\n",
    "\n",
    "pwc_files = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "556f8c43-9cad-40b0-94c6-6904b8433f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"links-between-papers-and-code.json.gz\"\n",
    "url = pwc_base + name\n",
    "# headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'}\n",
    "# req = urllib.request.Request(url, headers=headers)\n",
    "# with urllib.request.urlopen(req) as response:\n",
    "#     with gzip.GzipFile(fileobj=response) as uncompressed:\n",
    "#         file_content = uncompressed.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e141021-7e05-42a1-9541-63e81f10cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89e6a26e-09ab-47d0-bb5b-9304fe19278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = subprocess.Popen([\"wget\", url, \"-O\", f\"./input/{name}\"])\n",
    "_ = subprocess.Popen([\"gzip\", \"-d\",  f\"./input/{name}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55b93406-8605-42d4-9a8a-c575d1ced6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(f\"./input/{name.rstrip('.gz')}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f3751e9-a3f5-46b5-8111-fa151a82ce58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5863"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25696eac-3baf-4fbd-a12d-7c2374d05f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url, headers=headers)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "511e9919-9919-4e38-abe1-2c1618ea6cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_papers =  json.load(open('./input/papers-with-abstracts-datasets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c4f4b54-eb07-4625-a1a2-d524ab529d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paper_url': 'https://paperswithcode.com/paper/dynamic-network-model-from-partial',\n",
       "  'arxiv_id': '1805.10616',\n",
       "  'title': 'Dynamic Network Model from Partial Observations',\n",
       "  'abstract': 'Can evolving networks be inferred and modeled without directly observing\\ntheir nodes and edges? In many applications, the edges of a dynamic network\\nmight not be observed, but one can observe the dynamics of stochastic cascading\\nprocesses (e.g., information diffusion, virus propagation) occurring over the\\nunobserved network. While there have been efforts to infer networks based on\\nsuch data, providing a generative probabilistic model that is able to identify\\nthe underlying time-varying network remains an open question. Here we consider\\nthe problem of inferring generative dynamic network models based on network\\ncascade diffusion data. We propose a novel framework for providing a\\nnon-parametric dynamic network model--based on a mixture of coupled\\nhierarchical Dirichlet processes-- based on data capturing cascade node\\ninfection times. Our approach allows us to infer the evolving community\\nstructure in networks and to obtain an explicit predictive distribution over\\nthe edges of the underlying network--including those that were not involved in\\ntransmission of any cascade, or are likely to appear in the future. We show the\\neffectiveness of our approach using extensive experiments on synthetic as well\\nas real-world networks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.10616v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.10616v4.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Elahe Ghalebi',\n",
       "   'Baharan Mirzasoleiman',\n",
       "   'Radu Grosu',\n",
       "   'Jure Leskovec'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-27',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/pac-bayes-bounds-for-stable-algorithms-with',\n",
       "  'arxiv_id': '1806.06827',\n",
       "  'title': 'PAC-Bayes bounds for stable algorithms with instance-dependent priors',\n",
       "  'abstract': 'PAC-Bayes bounds have been proposed to get risk estimates based on a training\\nsample. In this paper the PAC-Bayes approach is combined with stability of the\\nhypothesis learned by a Hilbert space valued algorithm. The PAC-Bayes setting\\nis used with a Gaussian prior centered at the expected output. Thus a novelty\\nof our paper is using priors defined in terms of the data-generating\\ndistribution. Our main result estimates the risk of the randomized algorithm in\\nterms of the hypothesis stability coefficients. We also provide a new bound for\\nthe SVM classifier, which is compared to other known bounds experimentally.\\nOurs appears to be the first stability-based bound that evaluates to\\nnon-trivial values.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06827v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06827v2.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Omar Rivasplata',\n",
       "   'Emilio Parrado-Hernandez',\n",
       "   'John Shawe-Taylor',\n",
       "   'Shiliang Sun',\n",
       "   'Csaba Szepesvari'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [{'name': 'SVM',\n",
       "    'full_name': 'Support Vector Machine',\n",
       "    'description': 'A **Support Vector Machine**, or **SVM**, is a non-parametric supervised learning model. For non-linear classification and regression, they utilise the kernel trick to map inputs to high-dimensional feature spaces. SVMs construct a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure to the right shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called “support vectors”. \\r\\n\\r\\nSource: [scikit-learn](https://scikit-learn.org/stable/modules/svm.html)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Non-Parametric Classification',\n",
       "     'description': '**Non-Parametric Classification** methods perform classification where we use non-parametric methods to approximate the functional form of the relationship. Below you can find a continuously updating list of non-parametric classification methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/automated-bridge-component-recognition-using',\n",
       "  'arxiv_id': '1806.06820',\n",
       "  'title': 'Automated Bridge Component Recognition using Video Data',\n",
       "  'abstract': 'This paper investigates the automated recognition of structural bridge\\ncomponents using video data. Although understanding video data for structural\\ninspections is straightforward for human inspectors, the implementation of the\\nsame task using machine learning methods has not been fully realized. In\\nparticular, single-frame image processing techniques, such as convolutional\\nneural networks (CNNs), are not expected to identify structural components\\naccurately when the image is a close-up view, lacking contextual information\\nregarding where on the structure the image originates. Inspired by the\\nsignificant progress in video processing techniques, this study investigates\\nautomated bridge component recognition using video data, where the information\\nfrom the past frames is used to augment the understanding of the current frame.\\nA new simulated video dataset is created to train the machine learning\\nalgorithms. Then, convolutional Neural Networks (CNNs) with recurrent\\narchitectures are designed and applied to implement the automated bridge\\ncomponent recognition task. Results are presented for simulated video data, as\\nwell as video collected in the field.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06820v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06820v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yasutaka Narazaki',\n",
       "   'Vedhus Hoskere',\n",
       "   'Tu A. Hoang',\n",
       "   'Billie F. Spencer Jr'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/gradient-descent-with-identity-initialization-1',\n",
       "  'arxiv_id': '1802.06093',\n",
       "  'title': 'Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks',\n",
       "  'abstract': 'We analyze algorithms for approximating a function $f(x) = \\\\Phi x$ mapping\\n$\\\\Re^d$ to $\\\\Re^d$ using deep linear neural networks, i.e. that learn a\\nfunction $h$ parameterized by matrices $\\\\Theta_1,...,\\\\Theta_L$ and defined by\\n$h(x) = \\\\Theta_L \\\\Theta_{L-1} ... \\\\Theta_1 x$. We focus on algorithms that\\nlearn through gradient descent on the population quadratic loss in the case\\nthat the distribution over the inputs is isotropic.\\n  We provide polynomial bounds on the number of iterations for gradient descent\\nto approximate the least squares matrix $\\\\Phi$, in the case where the initial\\nhypothesis $\\\\Theta_1 = ... = \\\\Theta_L = I$ has excess loss bounded by a small\\nenough constant. On the other hand, we show that gradient descent fails to\\nconverge for $\\\\Phi$ whose distance from the identity is a larger constant, and\\nwe show that some forms of regularization toward the identity in each layer do\\nnot help.\\n  If $\\\\Phi$ is symmetric positive definite, we show that an algorithm that\\ninitializes $\\\\Theta_i = I$ learns an $\\\\epsilon$-approximation of $f$ using a\\nnumber of updates polynomial in $L$, the condition number of $\\\\Phi$, and\\n$\\\\log(d/\\\\epsilon)$. In contrast, we show that if the least squares matrix\\n$\\\\Phi$ is symmetric and has a negative eigenvalue, then all members of a class\\nof algorithms that perform gradient descent with identity initialization, and\\noptionally regularize toward the identity in each layer, fail to converge.\\n  We analyze an algorithm for the case that $\\\\Phi$ satisfies $u^{\\\\top} \\\\Phi u >\\n0$ for all $u$, but may not be symmetric. This algorithm uses two regularizers:\\none that maintains the invariant $u^{\\\\top} \\\\Theta_L \\\\Theta_{L-1} ... \\\\Theta_1 u\\n> 0$ for all $u$, and another that \"balances\" $\\\\Theta_1, ..., \\\\Theta_L$ so that\\nthey have the same singular values.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.06093v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.06093v4.pdf',\n",
       "  'proceeding': 'ICML 2018',\n",
       "  'authors': ['Peter L. Bartlett', 'David P. Helmbold', 'Philip M. Long'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/temporal-coherence-based-self-supervised',\n",
       "  'arxiv_id': '1806.06811',\n",
       "  'title': 'Temporal coherence-based self-supervised learning for laparoscopic workflow analysis',\n",
       "  'abstract': 'In order to provide the right type of assistance at the right time,\\ncomputer-assisted surgery systems need context awareness. To achieve this,\\nmethods for surgical workflow analysis are crucial. Currently, convolutional\\nneural networks provide the best performance for video-based workflow analysis\\ntasks. For training such networks, large amounts of annotated data are\\nnecessary. However, collecting a sufficient amount of data is often costly,\\ntime-consuming, and not always feasible. In this paper, we address this problem\\nby presenting and comparing different approaches for self-supervised\\npretraining of neural networks on unlabeled laparoscopic videos using temporal\\ncoherence. We evaluate our pretrained networks on Cholec80, a publicly\\navailable dataset for surgical phase segmentation, on which a maximum F1 score\\nof 84.6 was reached. Furthermore, we were able to achieve an increase of the F1\\nscore of up to 10 points when compared to a non-pretrained neural network.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06811v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06811v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Isabel Funke',\n",
       "   'Alexander Jenke',\n",
       "   'Sören Torge Mees',\n",
       "   'Jürgen Weitz',\n",
       "   'Stefanie Speidel',\n",
       "   'Sebastian Bodenstedt'],\n",
       "  'tasks': ['Self-Supervised Learning'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet', 'cholec80'],\n",
       "  'datasets_used_full': ['ImageNet', 'Cholec80'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/better-runtime-guarantees-via-stochastic',\n",
       "  'arxiv_id': '1801.04487',\n",
       "  'title': 'Better Runtime Guarantees Via Stochastic Domination',\n",
       "  'abstract': \"Apart from few exceptions, the mathematical runtime analysis of evolutionary\\nalgorithms is mostly concerned with expected runtimes. In this work, we argue\\nthat stochastic domination is a notion that should be used more frequently in\\nthis area. Stochastic domination allows to formulate much more informative\\nperformance guarantees, it allows to decouple the algorithm analysis into the\\ntrue algorithmic part of detecting a domination statement and the\\nprobability-theoretical part of deriving the desired probabilistic guarantees\\nfrom this statement, and it helps finding simpler and more natural proofs.\\n  As particular results, we prove a fitness level theorem which shows that the\\nruntime is dominated by a sum of independent geometric random variables, we\\nprove the first tail bounds for several classic runtime problems, and we give a\\nshort and natural proof for Witt's result that the runtime of any $(\\\\mu,p)$\\nmutation-based algorithm on any function with unique optimum is subdominated by\\nthe runtime of a variant of the \\\\oea on the \\\\onemax function.\\n  As side-products, we determine the fastest unbiased (1+1) algorithm for the\\n\\\\leadingones benchmark problem, both in the general case and when restricted to\\nstatic mutation operators, and we prove a Chernoff-type tail bound for sums of\\nindependent coupon collector distributions.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1801.04487v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1801.04487v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Benjamin Doerr'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-01-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/scaling-neural-machine-translation',\n",
       "  'arxiv_id': '1806.00187',\n",
       "  'title': 'Scaling Neural Machine Translation',\n",
       "  'abstract': \"Sequence to sequence learning models still require several days to reach\\nstate of the art performance on large benchmark datasets using a single\\nmachine. This paper shows that reduced precision and large batch training can\\nspeedup training by nearly 5x on a single 8-GPU machine with careful tuning and\\nimplementation. On WMT'14 English-German translation, we match the accuracy of\\nVaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a\\nnew state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We\\nfurther improve these results to 29.8 BLEU by training on the much larger\\nParacrawl dataset. On the WMT'14 English-French task, we obtain a\\nstate-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.00187v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.00187v3.pdf',\n",
       "  'proceeding': 'WS 2018 10',\n",
       "  'authors': ['Myle Ott', 'Sergey Edunov', 'David Grangier', 'Michael Auli'],\n",
       "  'tasks': ['Machine Translation', 'Question Answering', 'Translation'],\n",
       "  'date': '2018-06-01',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['wmt-2014'],\n",
       "  'datasets_used_full': ['WMT 2014'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/almost-exact-matching-with-replacement-for',\n",
       "  'arxiv_id': '1806.06802',\n",
       "  'title': 'Interpretable Almost Matching Exactly for Causal Inference',\n",
       "  'abstract': 'We aim to create the highest possible quality of treatment-control matches for categorical data in the potential outcomes framework. Matching methods are heavily used in the social sciences due to their interpretability, but most matching methods do not pass basic sanity checks: they fail when irrelevant variables are introduced, and tend to be either computationally slow or produce low-quality matches. The method proposed in this work aims to match units on a weighted Hamming distance, taking into account the relative importance of the covariates; the algorithm aims to match units on as many relevant variables as possible. To do this, the algorithm creates a hierarchy of covariate combinations on which to match (similar to downward closure), in the process solving an optimization problem for each unit in order to construct the optimal matches. The algorithm uses a single dynamic program to solve all of the optimization problems simultaneously. Notable advantages of our method over existing matching procedures are its high-quality matches, versatility in handling different data distributions that may have irrelevant variables, and ability to handle missing data by matching on as many available covariates as possible.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06802v6',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06802v6.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yameng Liu',\n",
       "   'Aw Dieng',\n",
       "   'Sudeepa Roy',\n",
       "   'Cynthia Rudin',\n",
       "   'Alexander Volfovsky'],\n",
       "  'tasks': ['Causal Inference'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-spatiotemporal-representation-of-the',\n",
       "  'arxiv_id': '1806.06793',\n",
       "  'title': 'Deep Spatiotemporal Representation of the Face for Automatic Pain Intensity Estimation',\n",
       "  'abstract': 'Automatic pain intensity assessment has a high value in disease diagnosis\\napplications. Inspired by the fact that many diseases and brain disorders can\\ninterrupt normal facial expression formation, we aim to develop a computational\\nmodel for automatic pain intensity assessment from spontaneous and micro facial\\nvariations. For this purpose, we propose a 3D deep architecture for dynamic\\nfacial video representation. The proposed model is built by stacking several\\nconvolutional modules where each module encompasses a 3D convolution kernel\\nwith a fixed temporal depth, several parallel 3D convolutional kernels with\\ndifferent temporal depths, and an average pooling layer. Deploying variable\\ntemporal depths in the proposed architecture allows the model to effectively\\ncapture a wide range of spatiotemporal variations on the faces. Extensive\\nexperiments on the UNBC-McMaster Shoulder Pain Expression Archive database show\\nthat our proposed model yields in a promising performance compared to the\\nstate-of-the-art in automatic pain intensity estimation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06793v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06793v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mohammad Tavakolian', 'Abdenour Hadid'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [{'name': '3D Convolution',\n",
       "    'full_name': '3D Convolution',\n",
       "    'description': 'A **3D Convolution** is a type of [convolution](https://paperswithcode.com/method/convolution) where the kernel slides in 3 dimensions as opposed to 2 dimensions with 2D convolutions. One example use case is medical imaging where a model is constructed using 3D image slices. Additionally video based data has an additional temporal dimension over images making it suitable for this module. \\r\\n\\r\\nImage: Lung nodule detection based on 3D convolutional neural networks, Fan et al',\n",
       "    'introduced_year': 2015,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/73642d9425a358b51a683cf6f95852d06cba1096/torch/nn/modules/conv.py#L421',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Average Pooling',\n",
       "    'full_name': 'Average Pooling',\n",
       "    'description': '**Average Pooling** is a pooling operation that calculates the average value for patches of a feature map, and uses it to create a downsampled (pooled) feature map. It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs. It extracts features more smoothly than [Max Pooling](https://paperswithcode.com/method/max-pooling), whereas max pooling extracts more pronounced features like edges.\\r\\n\\r\\nImage Source: [here](https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/flexible-collaborative-estimation-of-the',\n",
       "  'arxiv_id': '1806.06784',\n",
       "  'title': 'Robust inference on the average treatment effect using the outcome highly adaptive lasso',\n",
       "  'abstract': 'Many estimators of the average effect of a treatment on an outcome require estimation of the propensity score, the outcome regression, or both. It is often beneficial to utilize flexible techniques such as semiparametric regression or machine learning to estimate these quantities. However, optimal estimation of these regressions does not necessarily lead to optimal estimation of the average treatment effect, particularly in settings with strong instrumental variables. A recent proposal addressed these issues via the outcome-adaptive lasso, a penalized regression technique for estimating the propensity score that seeks to minimize the impact of instrumental variables on treatment effect estimators. However, a notable limitation of this approach is that its application is restricted to parametric models. We propose a more flexible alternative that we call the outcome highly adaptive lasso. We discuss large sample theory for this estimator and propose closed form confidence intervals based on the proposed estimator. We show via simulation that our method offers benefits over several popular approaches.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06784v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06784v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Cheng Ju', 'David Benkeser', 'Mark J. Van Der Laan'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/consistent-individualized-feature-attribution',\n",
       "  'arxiv_id': '1802.03888',\n",
       "  'title': 'Consistent Individualized Feature Attribution for Tree Ensembles',\n",
       "  'abstract': 'A unified approach to explain the output of any machine learning model.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.03888v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.03888v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Scott M. Lundberg', 'Gabriel G. Erion', 'Su-In Lee'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/bingan-learning-compact-binary-descriptors',\n",
       "  'arxiv_id': '1806.06778',\n",
       "  'title': 'BinGAN: Learning Compact Binary Descriptors with a Regularized GAN',\n",
       "  'abstract': 'In this paper, we propose a novel regularization method for Generative\\nAdversarial Networks, which allows the model to learn discriminative yet\\ncompact binary representations of image patches (image descriptors). We employ\\nthe dimensionality reduction that takes place in the intermediate layers of the\\ndiscriminator network and train binarized low-dimensional representation of the\\npenultimate layer to mimic the distribution of the higher-dimensional preceding\\nlayers. To achieve this, we introduce two loss terms that aim at: (i) reducing\\nthe correlation between the dimensions of the binarized low-dimensional\\nrepresentation of the penultimate layer i. e. maximizing joint entropy) and\\n(ii) propagating the relations between the dimensions in the high-dimensional\\nspace to the low-dimensional space. We evaluate the resulting binary image\\ndescriptors on two challenging applications, image matching and retrieval, and\\nachieve state-of-the-art results.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06778v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06778v5.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Maciej Zieba',\n",
       "   'Piotr Semberecki',\n",
       "   'Tarek El-Gaaly',\n",
       "   'Tomasz Trzcinski'],\n",
       "  'tasks': ['Dimensionality Reduction'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10'],\n",
       "  'datasets_used_full': ['CIFAR-10'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multifit-a-multivariate-multiscale-framework',\n",
       "  'arxiv_id': '1806.06777',\n",
       "  'title': \"Multiscale Fisher's Independence Test for Multivariate Dependence\",\n",
       "  'abstract': 'Identifying dependency in multivariate data is a common inference task that arises in numerous applications. However, existing nonparametric independence tests typically require computation that scales at least quadratically with the sample size, making it difficult to apply them to massive data. Moreover, resampling is usually necessary to evaluate the statistical significance of the resulting test statistics at finite sample sizes, further worsening the computational burden. We introduce a scalable, resampling-free approach to testing the independence between two random vectors by breaking down the task into simple univariate tests of independence on a collection of 2x2 contingency tables constructed through sequential coarse-to-fine discretization of the sample space, transforming the inference task into a multiple testing problem that can be completed with almost linear complexity with respect to the sample size. To address increasing dimensionality, we introduce a coarse-to-fine sequential adaptive procedure that exploits the spatial features of dependency structures to more effectively examine the sample space. We derive a finite-sample theory that guarantees the inferential validity of our adaptive procedure at any given sample size. In particular, we show that our approach can achieve strong control of the family-wise error rate without resampling or large-sample approximation. We demonstrate the substantial computational advantage of the procedure in comparison to existing approaches as well as its decent statistical power under various dependency scenarios through an extensive simulation study, and illustrate how the divide-and-conquer nature of the procedure can be exploited to not just test independence but to learn the nature of the underlying dependency. Finally, we demonstrate the use of our method through analyzing a large data set from a flow cytometry experiment.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06777v7',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06777v7.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shai Gorsky', 'Li Ma'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/kernel-based-outlier-detection-using-the',\n",
       "  'arxiv_id': '1806.06775',\n",
       "  'title': 'Kernel-based Outlier Detection using the Inverse Christoffel Function',\n",
       "  'abstract': 'Outlier detection methods have become increasingly relevant in recent years\\ndue to increased security concerns and because of its vast application to\\ndifferent fields. Recently, Pauwels and Lasserre (2016) noticed that the\\nsublevel sets of the inverse Christoffel function accurately depict the shape\\nof a cloud of data using a sum-of-squares polynomial and can be used to perform\\noutlier detection. In this work, we propose a kernelized variant of the inverse\\nChristoffel function that makes it computationally tractable for data sets with\\na large number of features. We compare our approach to current methods on 15\\ndifferent data sets and achieve the best average area under the precision\\nrecall curve (AUPRC) score, the best average rank and the lowest root mean\\nsquare deviation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06775v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06775v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Armin Askari', 'Forest Yang', 'Laurent El Ghaoui'],\n",
       "  'tasks': ['Outlier Detection'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/kid-net-convolution-networks-for-kidney',\n",
       "  'arxiv_id': '1806.06769',\n",
       "  'title': 'Kid-Net: Convolution Networks for Kidney Vessels Segmentation from CT-Volumes',\n",
       "  'abstract': 'Semantic image segmentation plays an important role in modeling\\npatient-specific anatomy. We propose a convolution neural network, called\\nKid-Net, along with a training schema to segment kidney vessels: artery, vein\\nand collecting system. Such segmentation is vital during the surgical planning\\nphase in which medical decisions are made before surgical incision. Our main\\ncontribution is developing a training schema that handles unbalanced data,\\nreduces false positives and enables high-resolution segmentation with a limited\\nmemory budget. These objectives are attained using dynamic weighting, random\\nsampling and 3D patch segmentation. Manual medical image annotation is both\\ntime-consuming and expensive. Kid-Net reduces kidney vessels segmentation time\\nfrom matter of hours to minutes. It is trained end-to-end using 3D patches from\\nvolumetric CT-images. A complete segmentation for a 512x512x512 CT-volume is\\nobtained within a few minutes (1-2 mins) by stitching the output 3D patches\\ntogether. Feature down-sampling and up-sampling are utilized to achieve higher\\nclassification and localization accuracies. Quantitative and qualitative\\nevaluation results on a challenging testing dataset show Kid-Net competence.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06769v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06769v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ahmed Taha', 'Pechin Lo', 'Junning Li', 'Tao Zhao'],\n",
       "  'tasks': ['Semantic Segmentation'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/modularity-matters-learning-invariant',\n",
       "  'arxiv_id': '1806.06765',\n",
       "  'title': 'Modularity Matters: Learning Invariant Relational Reasoning Tasks',\n",
       "  'abstract': 'We focus on two supervised visual reasoning tasks whose labels encode a\\nsemantic relational rule between two or more objects in an image: the MNIST\\nParity task and the colorized Pentomino task. The objects in the images undergo\\nrandom translation, scaling, rotation and coloring transformations. Thus these\\ntasks involve invariant relational reasoning. We report uneven performance of\\nvarious deep CNN models on these two tasks. For the MNIST Parity task, we\\nreport that the VGG19 model soundly outperforms a family of ResNet models.\\nMoreover, the family of ResNet models exhibits a general sensitivity to random\\ninitialization for the MNIST Parity task. For the colorized Pentomino task, now\\nboth the VGG19 and ResNet models exhibit sluggish optimization and very poor\\ntest generalization, hovering around 30% test error. The CNN we tested all\\nlearn hierarchies of fully distributed features and thus encode the distributed\\nrepresentation prior. We are motivated by a hypothesis from cognitive\\nneuroscience which posits that the human visual cortex is modularized, and this\\nallows the visual cortex to learn higher order invariances. To this end, we\\nconsider a modularized variant of the ResNet model, referred to as a Residual\\nMixture Network (ResMixNet) which employs a mixture-of-experts architecture to\\ninterleave distributed representations with more specialized, modular\\nrepresentations. We show that very shallow ResMixNets are capable of learning\\neach of the two tasks well, attaining less than 2% and 1% test error on the\\nMNIST Parity and the colorized Pentomino tasks respectively. Most importantly,\\nthe ResMixNet models are extremely parameter efficient: generalizing better\\nthan various non-modular CNNs that have over 10x the number of parameters.\\nThese experimental results support the hypothesis that modularity is a robust\\nprior for learning invariant relational reasoning.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06765v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06765v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jason Jo', 'Vikas Verma', 'Yoshua Bengio'],\n",
       "  'tasks': ['Relational Reasoning', 'Visual Reasoning'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [{'name': 'Average Pooling',\n",
       "    'full_name': 'Average Pooling',\n",
       "    'description': '**Average Pooling** is a pooling operation that calculates the average value for patches of a feature map, and uses it to create a downsampled (pooled) feature map. It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs. It extracts features more smoothly than [Max Pooling](https://paperswithcode.com/method/max-pooling), whereas max pooling extracts more pronounced features like edges.\\r\\n\\r\\nImage Source: [here](https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': '1x1 Convolution',\n",
       "    'full_name': '1x1 Convolution',\n",
       "    'description': 'A **1 x 1 Convolution** is a [convolution](https://paperswithcode.com/method/convolution) with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an [MLP](https://paperswithcode.com/method/feedforward-network) looking at a particular pixel location.\\r\\n\\r\\nImage Credit: [http://deeplearning.ai](http://deeplearning.ai)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://www.healthnutra.org/es/maxup/',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Batch Normalization',\n",
       "    'full_name': 'Batch Normalization',\n",
       "    'description': '**Batch Normalization** aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for [Dropout](https://paperswithcode.com/method/dropout).\\r\\n\\r\\nWe apply a batch normalization layer as follows for a minibatch $\\\\mathcal{B}$:\\r\\n\\r\\n$$ \\\\mu\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}x\\\\_{i} $$\\r\\n\\r\\n$$ \\\\sigma^{2}\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}\\\\left(x\\\\_{i}-\\\\mu\\\\_{\\\\mathcal{B}}\\\\right)^{2} $$\\r\\n\\r\\n$$ \\\\hat{x}\\\\_{i} = \\\\frac{x\\\\_{i} - \\\\mu\\\\_{\\\\mathcal{B}}}{\\\\sqrt{\\\\sigma^{2}\\\\_{\\\\mathcal{B}}+\\\\epsilon}} $$\\r\\n\\r\\n$$ y\\\\_{i} = \\\\gamma\\\\hat{x}\\\\_{i} + \\\\beta = \\\\text{BN}\\\\_{\\\\gamma, \\\\beta}\\\\left(x\\\\_{i}\\\\right) $$\\r\\n\\r\\nWhere $\\\\gamma$ and $\\\\beta$ are learnable parameters.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.03167v3',\n",
       "    'source_title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/36f91261099b00194922bd93ed1286fe1c199724/jax/experimental/stax.py#L116',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Bottleneck Residual Block',\n",
       "    'full_name': 'Bottleneck Residual Block',\n",
       "    'description': 'A **Bottleneck Residual Block** is a variant of the [residual block](https://paperswithcode.com/method/residual-block) that utilises 1x1 convolutions to create a bottleneck. The use of a bottleneck reduces the number of parameters and matrix multiplications. The idea is to make residual blocks as thin as possible to increase depth and have less parameters. They were introduced as part of the [ResNet](https://paperswithcode.com/method/resnet) architecture, and are used as part of deeper ResNets such as ResNet-50 and ResNet-101.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L75',\n",
       "    'main_collection': {'name': 'Skip Connection Blocks',\n",
       "     'description': \"**Skip Connection Blocks** are building blocks for neural networks that feature skip connections. These skip connections 'skip' some layers allowing gradients to better flow through the network. Below you will find a continuously updating list of skip connection blocks:\",\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Global Average Pooling',\n",
       "    'full_name': 'Global Average Pooling',\n",
       "    'description': '**Global Average Pooling** is a pooling operation designed to replace fully connected layers in classical CNNs. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the [softmax](https://paperswithcode.com/method/softmax) layer. \\r\\n\\r\\nOne advantage of global [average pooling](https://paperswithcode.com/method/average-pooling) over the fully connected layers is that it is more native to the [convolution](https://paperswithcode.com/method/convolution) structure by enforcing correspondences between feature maps and categories. Thus the feature maps can be easily interpreted as categories confidence maps. Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. Furthermore, global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/baa592b215804927e28638f6a7f3318cbc411d49/torchvision/models/resnet.py#L157',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Residual Block',\n",
       "    'full_name': 'Residual Block',\n",
       "    'description': \"**Residual Blocks** are skip-connection blocks that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. They were introduced as part of the [ResNet](https://paperswithcode.com/method/resnet) architecture.\\r\\n \\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$. The $\\\\mathcal{F}({x})$ acts like a residual, hence the name 'residual block'.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. Having skip connections allows the network to more easily learn identity-like mappings.\\r\\n\\r\\nNote that in practice, [Bottleneck Residual Blocks](https://paperswithcode.com/method/bottleneck-residual-block) are used for deeper ResNets, such as ResNet-50 and ResNet-101, as these bottleneck blocks are less computationally intensive.\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L35',\n",
       "    'main_collection': {'name': 'Skip Connection Blocks',\n",
       "     'description': \"**Skip Connection Blocks** are building blocks for neural networks that feature skip connections. These skip connections 'skip' some layers allowing gradients to better flow through the network. Below you will find a continuously updating list of skip connection blocks:\",\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Kaiming Initialization',\n",
       "    'full_name': 'Kaiming Initialization',\n",
       "    'description': '**Kaiming Initialization**, or **He Initialization**, is an initialization method for neural networks that takes into account the non-linearity of activation functions, such as [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nA proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. Using a derivation they work out that the condition to stop this happening is:\\r\\n\\r\\n$$\\\\frac{1}{2}n\\\\_{l}\\\\text{Var}\\\\left[w\\\\_{l}\\\\right] = 1 $$\\r\\n\\r\\nThis implies an initialization scheme of:\\r\\n\\r\\n$$ w\\\\_{l} \\\\sim \\\\mathcal{N}\\\\left(0,  2/n\\\\_{l}\\\\right)$$\\r\\n\\r\\nThat is, a zero-centered Gaussian with standard deviation of $\\\\sqrt{2/{n}\\\\_{l}}$ (variance shown in equation above). Biases are initialized at $0$.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.01852v1',\n",
       "    'source_title': 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/0adb5843766092fba584791af76383125fd0d01c/torch/nn/init.py#L389',\n",
       "    'main_collection': {'name': 'Initialization',\n",
       "     'description': '**Initialization** methods are used to initialize the weights in a neural network. Below can you find a continuously updating list of initialization methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Residual Connection',\n",
       "    'full_name': 'Residual Connection',\n",
       "    'description': '**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'ResNet',\n",
       "    'full_name': 'Residual Network',\n",
       "    'description': '**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack [residual blocks](https://paperswithcode.com/method/residual-block) ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}(x)$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}(x):=\\\\mathcal{H}(x)-x$. The original mapping is recast into $\\\\mathcal{F}(x)+x$.\\r\\n\\r\\nThere is empirical evidence that these types of network are easier to optimize, and can gain accuracy from considerably increased depth.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/resnet.py#L124',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['mnist', 'shapes-1'],\n",
       "  'datasets_used_full': ['MNIST', 'SHAPES'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/closing-the-generalization-gap-of-adaptive',\n",
       "  'arxiv_id': '1806.06763',\n",
       "  'title': 'Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks',\n",
       "  'abstract': 'Adaptive gradient methods, which adopt historical gradient information to automatically adjust the learning rate, despite the nice property of fast convergence, have been observed to generalize worse than stochastic gradient descent (SGD) with momentum in training deep neural networks. This leaves how to close the generalization gap of adaptive gradient methods an open problem. In this work, we show that adaptive gradient methods such as Adam, Amsgrad, are sometimes \"over adapted\". We design a new algorithm, called Partially adaptive momentum estimation method, which unifies the Adam/Amsgrad with SGD by introducing a partial adaptive parameter $p$, to achieve the best from both worlds. We also prove the convergence rate of our proposed algorithm to a stationary point in the stochastic nonconvex optimization setting. Experiments on standard benchmarks show that our proposed algorithm can maintain a fast convergence rate as Adam/Amsgrad while generalizing as well as SGD in training deep neural networks. These results would suggest practitioners pick up adaptive gradient methods once again for faster training of deep neural networks.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06763v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06763v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jinghui Chen',\n",
       "   'Dongruo Zhou',\n",
       "   'Yiqi Tang',\n",
       "   'Ziyan Yang',\n",
       "   'Yuan Cao',\n",
       "   'Quanquan Gu'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [{'name': 'Affine Coupling',\n",
       "    'full_name': 'Affine Coupling',\n",
       "    'description': '**Affine Coupling** is a method for implementing a normalizing flow (where we stack a sequence of invertible bijective transformation functions). Affine coupling is one of these bijective transformation functions. Specifically, it is an example of a reversible transformation where the forward function, the reverse function and the log-determinant are computationally efficient. For the forward function, we split the input dimension into two parts:\\r\\n\\r\\n$$ \\\\mathbf{x}\\\\_{a}, \\\\mathbf{x}\\\\_{b} = \\\\text{split}\\\\left(\\\\mathbf{x}\\\\right) $$\\r\\n\\r\\nThe second part stays the same $\\\\mathbf{x}\\\\_{b} = \\\\mathbf{y}\\\\_{b}$, while the first part  $\\\\mathbf{x}\\\\_{a}$ undergoes an affine transformation, where the parameters for this transformation are learnt using the second part $\\\\mathbf{x}\\\\_{b}$ being put through a neural network. Together we have:\\r\\n\\r\\n$$ \\\\left(\\\\log{\\\\mathbf{s}, \\\\mathbf{t}}\\\\right) = \\\\text{NN}\\\\left(\\\\mathbf{x}\\\\_{b}\\\\right) $$\\r\\n\\r\\n$$ \\\\mathbf{s} = \\\\exp\\\\left(\\\\log{\\\\mathbf{s}}\\\\right) $$\\r\\n\\r\\n$$ \\\\mathbf{y}\\\\_{a} = \\\\mathbf{s} \\\\odot \\\\mathbf{x}\\\\_{a} + \\\\mathbf{t}  $$\\r\\n\\r\\n$$ \\\\mathbf{y}\\\\_{b} = \\\\mathbf{x}\\\\_{b} $$\\r\\n\\r\\n$$ \\\\mathbf{y} = \\\\text{concat}\\\\left(\\\\mathbf{y}\\\\_{a}, \\\\mathbf{y}\\\\_{b}\\\\right) $$\\r\\n\\r\\nImage: [GLOW](https://paperswithcode.com/method/glow)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1410.8516v6',\n",
       "    'source_title': 'NICE: Non-linear Independent Components Estimation',\n",
       "    'code_snippet_url': 'https://github.com/paultsw/nice_pytorch/blob/15cfc543fc3dc81ee70398b8dfc37b67269ede95/nice/layers.py#L109',\n",
       "    'main_collection': {'name': 'Bijective Transformation',\n",
       "     'description': '**Bijective Transformations** are transformations that are bijective, i.e. they can be reversed. They are used within the context of normalizing flow models. Below you can find a continuously updating list of bijective transformation methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Normalizing Flows',\n",
       "    'full_name': 'Normalizing Flows',\n",
       "    'description': \"**Normalizing Flows** are a method for constructing complex distributions by transforming a\\r\\nprobability density through a series of invertible mappings. By repeatedly applying the rule for change of variables, the initial density ‘flows’ through the sequence of invertible mappings. At the end of this sequence we obtain a valid probability distribution and hence this type of flow is referred to as a normalizing flow.\\r\\n\\r\\nIn the case of finite flows, the basic rule for the transformation of densities considers an invertible, smooth mapping $f : \\\\mathbb{R}^{d} \\\\rightarrow \\\\mathbb{R}^{d}$ with inverse $f^{-1} = g$, i.e. the composition $g \\\\cdot f\\\\left(z\\\\right) = z$. If we use this mapping to transform a random variable $z$ with distribution $q\\\\left(z\\\\right)$, the resulting random variable $z' = f\\\\left(z\\\\right)$ has a distribution:\\r\\n\\r\\n$$ q\\\\left(\\\\mathbf{z}'\\\\right) = q\\\\left(\\\\mathbf{z}\\\\right)\\\\bigl\\\\vert{\\\\text{det}}\\\\frac{\\\\delta{f}^{-1}}{\\\\delta{\\\\mathbf{z'}}}\\\\bigr\\\\vert = q\\\\left(\\\\mathbf{z}\\\\right)\\\\bigl\\\\vert{\\\\text{det}}\\\\frac{\\\\delta{f}}{\\\\delta{\\\\mathbf{z}}}\\\\bigr\\\\vert ^{-1} $$\\r\\n\\x0c\\r\\nwhere the last equality can be seen by applying the chain rule (inverse function theorem) and is a property of Jacobians of invertible functions. We can construct arbitrarily complex densities by composing several simple maps and successively applying the above equation. The density $q\\\\_{K}\\\\left(\\\\mathbf{z}\\\\right)$ obtained by successively transforming a random variable $z\\\\_{0}$ with distribution $q\\\\_{0}$ through a chain of $K$ transformations $f\\\\_{k}$ is:\\r\\n\\r\\n$$ z\\\\_{K} = f\\\\_{K} \\\\cdot \\\\dots \\\\cdot f\\\\_{2} \\\\cdot f\\\\_{1}\\\\left(z\\\\_{0}\\\\right) $$\\r\\n\\r\\n$$ \\\\ln{q}\\\\_{K}\\\\left(z\\\\_{K}\\\\right) = \\\\ln{q}\\\\_{0}\\\\left(z\\\\_{0}\\\\right) − \\\\sum^{K}\\\\_{k=1}\\\\ln\\\\vert\\\\det\\\\frac{\\\\delta{f\\\\_{k}}}{\\\\delta{\\\\mathbf{z\\\\_{k-1}}}}\\\\vert $$\\r\\n\\x0c\\r\\nThe path traversed by the random variables $z\\\\_{k} = f\\\\_{k}\\\\left(z\\\\_{k-1}\\\\right)$ with initial distribution $q\\\\_{0}\\\\left(z\\\\_{0}\\\\right)$ is called the flow and the path formed by the successive distributions $q\\\\_{k}$ is a normalizing flow.\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1505.05770v6',\n",
       "    'source_title': 'Variational Inference with Normalizing Flows',\n",
       "    'code_snippet_url': 'https://github.com/ex4sperans/variational-inference-with-normalizing-flows/blob/922b569f851e02fa74700cd0754fe2ef5c1f3180/flow.py#L9',\n",
       "    'main_collection': {'name': 'Distribution Approximation',\n",
       "     'description': '**Distribution Approximation** methods aim to approximate a complex distribution. Below you can find a continuously updating list of distribution approximation methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Adam',\n",
       "    'full_name': 'Adam',\n",
       "    'description': '**Adam** is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of [RMSProp](https://paperswithcode.com/method/rmsprop) and [SGD w/th Momentum](https://paperswithcode.com/method/sgd-with-momentum). The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. \\r\\n\\r\\nThe weight updates are performed as:\\r\\n\\r\\n$$ w_{t} = w_{t-1} - \\\\eta\\\\frac{\\\\hat{m}\\\\_{t}}{\\\\sqrt{\\\\hat{v}\\\\_{t}} + \\\\epsilon}  $$\\r\\n\\r\\nwith\\r\\n\\r\\n$$ \\\\hat{m}\\\\_{t} = \\\\frac{m_{t}}{1-\\\\beta^{t}_{1}} $$\\r\\n\\r\\n$$ \\\\hat{v}\\\\_{t} = \\\\frac{v_{t}}{1-\\\\beta^{t}_{2}} $$\\r\\n\\r\\n$$ m_{t} = \\\\beta_{1}m_{t-1} + (1-\\\\beta_{1})g_{t} $$\\r\\n\\r\\n$$ v_{t} = \\\\beta_{2}v_{t-1} + (1-\\\\beta_{2})g_{t}^{2}  $$\\r\\n\\r\\n\\r\\n$ \\\\eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \\\\epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \\\\beta_{1} $ and $ \\\\beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1412.6980v9',\n",
       "    'source_title': 'Adam: A Method for Stochastic Optimization',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}},\n",
       "   {'name': 'SGD',\n",
       "    'full_name': 'Stochastic Gradient Descent',\n",
       "    'description': '**Stochastic Gradient Descent** is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:\\r\\n\\r\\n$$ w\\\\_{t+1} = w\\\\_{t} - \\\\eta\\\\hat{\\\\nabla}\\\\_{w}{L(w\\\\_{t})} $$\\r\\n\\r\\nWhere $\\\\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.\\r\\n\\r\\n(Image Source: [here](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/))',\n",
       "    'introduced_year': 1951,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/4e0ac120e9a8b096069c2f892488d630a5c8f358/torch/optim/sgd.py#L97-L112',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['cifar-10',\n",
       "   'imagenet',\n",
       "   'cifar-100',\n",
       "   'penn-treebank'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'ImageNet', 'CIFAR-100', 'Penn Treebank'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-memory-network-approach-for-story-based',\n",
       "  'arxiv_id': '1805.02838',\n",
       "  'title': 'A Memory Network Approach for Story-based Temporal Summarization of 360° Videos',\n",
       "  'abstract': \"We address the problem of story-based temporal summarization of long\\n360{\\\\deg} videos. We propose a novel memory network model named Past-Future\\nMemory Network (PFMN), in which we first compute the scores of 81 normal field\\nof view (NFOV) region proposals cropped from the input 360{\\\\deg} video, and\\nthen recover a latent, collective summary using the network with two external\\nmemories that store the embeddings of previously selected subshots and future\\ncandidate subshots. Our major contributions are two-fold. First, our work is\\nthe first to address story-based temporal summarization of 360{\\\\deg} videos.\\nSecond, our model is the first attempt to leverage memory networks for video\\nsummarization tasks. For evaluation, we perform three sets of experiments.\\nFirst, we investigate the view selection capability of our model on the\\nPano2Vid dataset. Second, we evaluate the temporal summarization with a newly\\ncollected 360{\\\\deg} video dataset. Finally, we experiment our model's\\nperformance in another domain, with image-based storytelling VIST dataset. We\\nverify that our model achieves state-of-the-art performance on all the tasks.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.02838v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.02838v3.pdf',\n",
       "  'proceeding': 'CVPR 2018',\n",
       "  'authors': ['Sang-ho Lee', 'Jinyoung Sung', 'Youngjae Yu', 'Gunhee Kim'],\n",
       "  'tasks': ['Video Summarization'],\n",
       "  'date': '2018-05-08',\n",
       "  'methods': [{'name': 'Memory Network',\n",
       "    'full_name': 'Memory Network',\n",
       "    'description': 'A **Memory Network** provides a memory component that can be read from and written to with the inference capabilities of a neural network model. The motivation is that many neural networks lack a long-term memory component, and their existing memory component encoded by states and weights is too small and not compartmentalized enough to accurately remember facts from the past (RNNs for example, have difficult memorizing and doing tasks like copying). \\r\\n\\r\\nA memory network consists of a memory $\\\\textbf{m}$ (an array of objects indexed by $\\\\textbf{m}\\\\_{i}$ and four potentially learned components:\\r\\n\\r\\n- Input feature map $I$ - feature representation of the data input.\\r\\n- Generalization $G$ - updates old memories given the new input.\\r\\n- Output feature map $O$ - produces new feature map given $I$ and $G$.\\r\\n- Response $R$ - converts output into the desired response. \\r\\n\\r\\nGiven an input $x$ (e.g., an input character, word or sentence depending on the granularity chosen, an image or an audio signal) the flow of the model is as follows:\\r\\n\\r\\n1. Convert $x$ to an internal feature representation $I\\\\left(x\\\\right)$.\\r\\n2. Update memories $m\\\\_{i}$ given the new input: $m\\\\_{i} = G\\\\left(m\\\\_{i}, I\\\\left(x\\\\right), m\\\\right)$, $\\\\forall{i}$.\\r\\n3. Compute output features $o$ given the new input and the memory: $o = O\\\\left(I\\\\left(x\\\\right), m\\\\right)$.\\r\\n4. Finally, decode output features $o$ to give the final response: $r = R\\\\left(o\\\\right)$.\\r\\n\\r\\nThis process is applied at both train and test time, if there is a distinction between such phases, that\\r\\nis, memories are also stored at test time, but the model parameters of $I$, $G$, $O$ and $R$ are not updated. Memory networks cover a wide class of possible implementations. The components $I$, $G$, $O$ and $R$ can potentially use any existing ideas from the machine learning literature.\\r\\n\\r\\nImage Source: [Adrian Colyer](https://blog.acolyer.org/2016/03/10/memory-networks/)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1410.3916v11',\n",
       "    'source_title': 'Memory Networks',\n",
       "    'code_snippet_url': 'https://github.com/aykutaaykut/Memory-Networks',\n",
       "    'main_collection': {'name': 'Working Memory Models',\n",
       "     'description': '**Working Memory Models** aim to supplement neural networks with a memory module to increase their capability for memorization and allowing them to more easily perform tasks such as retrieving and copying information. Below you can find a continuously updating list of working memory models.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['imagenet', 'vist'],\n",
       "  'datasets_used_full': ['ImageNet', 'VIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/pots-protective-optimization-technologies',\n",
       "  'arxiv_id': '1806.02711',\n",
       "  'title': 'POTs: Protective Optimization Technologies',\n",
       "  'abstract': \"Algorithmic fairness aims to address the economic, moral, social, and political impact that digital systems have on populations through solutions that can be applied by service providers. Fairness frameworks do so, in part, by mapping these problems to a narrow definition and assuming the service providers can be trusted to deploy countermeasures. Not surprisingly, these decisions limit fairness frameworks' ability to capture a variety of harms caused by systems. We characterize fairness limitations using concepts from requirements engineering and from social sciences. We show that the focus on algorithms' inputs and outputs misses harms that arise from systems interacting with the world; that the focus on bias and discrimination omits broader harms on populations and their environments; and that relying on service providers excludes scenarios where they are not cooperative or intentionally adversarial. We propose Protective Optimization Technologies (POTs). POTs provide means for affected parties to address the negative impacts of systems in the environment, expanding avenues for political contestation. POTs intervene from outside the system, do not require service providers to cooperate, and can serve to correct, shift, or expose harms that systems impose on populations and their environments. We illustrate the potential and limitations of POTs in two case studies: countering road congestion caused by traffic-beating applications, and recalibrating credit scoring for loan applicants.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.02711v6',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.02711v6.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Bogdan Kulynych',\n",
       "   'Rebekah Overdorf',\n",
       "   'Carmela Troncoso',\n",
       "   'Seda Gürses'],\n",
       "  'tasks': ['Decision Making'],\n",
       "  'date': '2018-06-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/surface-networks',\n",
       "  'arxiv_id': '1705.10819',\n",
       "  'title': 'Surface Networks',\n",
       "  'abstract': 'We study data-driven representations for three-dimensional triangle meshes,\\nwhich are one of the prevalent objects used to represent 3D geometry. Recent\\nworks have developed models that exploit the intrinsic geometry of manifolds\\nand graphs, namely the Graph Neural Networks (GNNs) and its spectral variants,\\nwhich learn from the local metric tensor via the Laplacian operator. Despite\\noffering excellent sample complexity and built-in invariances, intrinsic\\ngeometry alone is invariant to isometric deformations, making it unsuitable for\\nmany applications. To overcome this limitation, we propose several upgrades to\\nGNNs to leverage extrinsic differential geometry properties of\\nthree-dimensional surfaces, increasing its modeling power.\\n  In particular, we propose to exploit the Dirac operator, whose spectrum\\ndetects principal curvature directions --- this is in stark contrast with the\\nclassical Laplace operator, which directly measures mean curvature. We coin the\\nresulting models \\\\emph{Surface Networks (SN)}. We prove that these models\\ndefine shape representations that are stable to deformation and to\\ndiscretization, and we demonstrate the efficiency and versatility of SNs on two\\nchallenging tasks: temporal prediction of mesh deformations under non-linear\\ndynamics and generative models using a variational autoencoder framework with\\nencoders/decoders given by SNs.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1705.10819v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1705.10819v2.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Ilya Kostrikov',\n",
       "   'Zhongshi Jiang',\n",
       "   'Daniele Panozzo',\n",
       "   'Denis Zorin',\n",
       "   'Joan Bruna'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-05-30',\n",
       "  'methods': [{'name': 'AutoEncoder',\n",
       "    'full_name': 'AutoEncoder',\n",
       "    'description': 'An **Autoencoder** is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then performs a reconstruction of the input with this latent code (the decoder).\\r\\n\\r\\nImage: [Michael Massi](https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://science.sciencemag.org/content/313/5786/504',\n",
       "    'source_title': 'Reducing the Dimensionality of Data with Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/L1aoXingyu/pytorch-beginner/blob/9c86be785c7c318a09cf29112dd1f1a58613239b/08-AutoEncoder/simple_autoencoder.py#L38',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/extracting-automata-from-recurrent-neural',\n",
       "  'arxiv_id': '1711.09576',\n",
       "  'title': 'Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples',\n",
       "  'abstract': \"We present a novel algorithm that uses exact learning and abstraction to extract a deterministic finite automaton describing the state dynamics of a given trained RNN. We do this using Angluin's L* algorithm as a learner and the trained RNN as an oracle. Our technique efficiently extracts accurate automata from trained RNNs, even when the state vectors are large and require fine differentiation.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1711.09576v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1711.09576v4.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Gail Weiss', 'Yoav Goldberg', 'Eran Yahav'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-11-27',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/unsupervised-word-segmentation-from-speech',\n",
       "  'arxiv_id': '1806.06734',\n",
       "  'title': 'Unsupervised Word Segmentation from Speech with Attention',\n",
       "  'abstract': 'We present a first attempt to perform attentional word segmentation directly\\nfrom the speech signal, with the final goal to automatically identify lexical\\nunits in a low-resource, unwritten language (UL). Our methodology assumes a\\npairing between recordings in the UL with translations in a well-resourced\\nlanguage. It uses Acoustic Unit Discovery (AUD) to convert speech into a\\nsequence of pseudo-phones that is segmented using neural soft-alignments\\nproduced by a neural machine translation model. Evaluation uses an actual Bantu\\nUL, Mboshi; comparisons to monolingual and bilingual baselines illustrate the\\npotential of attentional word segmentation for language documentation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06734v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06734v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Pierre Godard',\n",
       "   'Marcely Zanon-Boito',\n",
       "   'Lucas Ondel',\n",
       "   'Alexandre Berard',\n",
       "   'François Yvon',\n",
       "   'Aline Villavicencio',\n",
       "   'Laurent Besacier'],\n",
       "  'tasks': ['Acoustic Unit Discovery', 'Machine Translation', 'Translation'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/semantically-selective-augmentation-for-deep',\n",
       "  'arxiv_id': '1806.04074',\n",
       "  'title': 'Semantically Selective Augmentation for Deep Compact Person Re-Identification',\n",
       "  'abstract': 'We present a deep person re-identification approach that combines\\nsemantically selective, deep data augmentation with clustering-based network\\ncompression to generate high performance, light and fast inference networks. In\\nparticular, we propose to augment limited training data via sampling from a\\ndeep convolutional generative adversarial network (DCGAN), whose discriminator\\nis constrained by a semantic classifier to explicitly control the domain\\nspecificity of the generation process. Thereby, we encode information in the\\nclassifier network which can be utilized to steer adversarial synthesis, and\\nwhich fuels our CondenseNet ID-network training. We provide a quantitative and\\nqualitative analysis of the approach and its variants on a number of datasets,\\nobtaining results that outperform the state-of-the-art on the LIMA dataset for\\nlong-term monitoring in indoor living spaces.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04074v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04074v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Víctor Ponce-López',\n",
       "   'Tilo Burghardt',\n",
       "   'Sion Hannunna',\n",
       "   'Dima Damen',\n",
       "   'Alessandro Masullo',\n",
       "   'Majid Mirmehdi'],\n",
       "  'tasks': ['Data Augmentation', 'Person Re-Identification'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['market-1501', 'dukemtmc-reid'],\n",
       "  'datasets_used_full': ['Market-1501', 'DukeMTMC-reID'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/assessing-robustness-of-radiomic-features-by',\n",
       "  'arxiv_id': '1806.06719',\n",
       "  'title': 'Assessing robustness of radiomic features by image perturbation',\n",
       "  'abstract': 'Image features need to be robust against differences in positioning,\\nacquisition and segmentation to ensure reproducibility. Radiomic models that\\nonly include robust features can be used to analyse new images, whereas models\\nwith non-robust features may fail to predict the outcome of interest\\naccurately. Test-retest imaging is recommended to assess robustness, but may\\nnot be available for the phenotype of interest. We therefore investigated 18\\nmethods to determine feature robustness based on image perturbations.\\nTest-retest and perturbation robustness were compared for 4032 features that\\nwere computed from the gross tumour volume in two cohorts with computed\\ntomography imaging: I) 31 non-small-cell lung cancer (NSCLC) patients; II): 19\\nhead-and-neck squamous cell carcinoma (HNSCC) patients. Robustness was measured\\nusing the intraclass correlation coefficient (1,1) (ICC). Features with\\nICC$\\\\geq0.90$ were considered robust. The NSCLC cohort contained more robust\\nfeatures for test-retest imaging than the HNSCC cohort ($73.5\\\\%$ vs. $34.0\\\\%$).\\nA perturbation chain consisting of noise addition, affine translation, volume\\ngrowth/shrinkage and supervoxel-based contour randomisation identified the\\nfewest false positive robust features (NSCLC: $3.3\\\\%$; HNSCC: $10.0\\\\%$). Thus,\\nthis perturbation chain may be used to assess feature robustness.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06719v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06719v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alex Zwanenburg',\n",
       "   'Stefan Leger',\n",
       "   'Linda Agolli',\n",
       "   'Karoline Pilz',\n",
       "   'Esther G. C. Troost',\n",
       "   'Christian Richter',\n",
       "   'Steffen Löck'],\n",
       "  'tasks': ['Translation'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/reconvnet-video-object-segmentation-with',\n",
       "  'arxiv_id': '1806.05510',\n",
       "  'title': 'ReConvNet: Video Object Segmentation with Spatio-Temporal Features Modulation',\n",
       "  'abstract': 'We introduce ReConvNet, a recurrent convolutional architecture for\\nsemi-supervised video object segmentation that is able to fast adapt its\\nfeatures to focus on any specific object of interest at inference time.\\nGeneralization to new objects never observed during training is known to be a\\nhard task for supervised approaches that would need to be retrained. To tackle\\nthis problem, we propose a more efficient solution that learns spatio-temporal\\nfeatures self-adapting to the object of interest via conditional affine\\ntransformations. This approach is simple, can be trained end-to-end and does\\nnot necessarily require extra training steps at inference time. Our method\\nshows competitive results on DAVIS2016 with respect to state-of-the art\\napproaches that use online fine-tuning, and outperforms them on DAVIS2017.\\nReConvNet shows also promising results on the DAVIS-Challenge 2018 winning the\\n$10$-th position.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05510v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05510v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Francesco Lattari',\n",
       "   'Marco Ciccone',\n",
       "   'Matteo Matteucci',\n",
       "   'Jonathan Masci',\n",
       "   'Francesco Visin'],\n",
       "  'tasks': ['Semantic Segmentation',\n",
       "   'Semi-Supervised Video Object Segmentation',\n",
       "   'Video Object Segmentation',\n",
       "   'Video Semantic Segmentation'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['coco'],\n",
       "  'datasets_used_full': ['COCO'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/tree-edit-distance-learning-via-adaptive-1',\n",
       "  'arxiv_id': '1806.05009',\n",
       "  'title': 'Tree Edit Distance Learning via Adaptive Symbol Embeddings',\n",
       "  'abstract': 'Metric learning has the aim to improve classification accuracy by learning a\\ndistance measure which brings data points from the same class closer together\\nand pushes data points from different classes further apart. Recent research\\nhas demonstrated that metric learning approaches can also be applied to trees,\\nsuch as molecular structures, abstract syntax trees of computer programs, or\\nsyntax trees of natural language, by learning the cost function of an edit\\ndistance, i.e. the costs of replacing, deleting, or inserting nodes in a tree.\\nHowever, learning such costs directly may yield an edit distance which violates\\nmetric axioms, is challenging to interpret, and may not generalize well. In\\nthis contribution, we propose a novel metric learning approach for trees which\\nwe call embedding edit distance learning (BEDL) and which learns an edit\\ndistance indirectly by embedding the tree nodes as vectors, such that the\\nEuclidean distance between those vectors supports class discrimination. We\\nlearn such embeddings by reducing the distance to prototypical trees from the\\nsame class and increasing the distance to prototypical trees from different\\nclasses. In our experiments, we show that BEDL improves upon the\\nstate-of-the-art in metric learning for trees on six benchmark data sets,\\nranging from computer science over biomedical data to a natural-language\\nprocessing data set containing over 300,000 nodes.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05009v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05009v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Benjamin Paaßen',\n",
       "   'Claudio Gallicchio',\n",
       "   'Alessio Micheli',\n",
       "   'Barbara Hammer'],\n",
       "  'tasks': ['Metric Learning'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/towards-multi-instrument-drum-transcription',\n",
       "  'arxiv_id': '1806.06676',\n",
       "  'title': 'Towards multi-instrument drum transcription',\n",
       "  'abstract': 'Automatic drum transcription, a subtask of the more general automatic music\\ntranscription, deals with extracting drum instrument note onsets from an audio\\nsource. Recently, progress in transcription performance has been made using\\nnon-negative matrix factorization as well as deep learning methods. However,\\nthese works primarily focus on transcribing three drum instruments only: snare\\ndrum, bass drum, and hi-hat. Yet, for many applications, the ability to\\ntranscribe more drum instruments which make up standard drum kits used in\\nwestern popular music would be desirable. In this work, convolutional and\\nconvolutional recurrent neural networks are trained to transcribe a wider range\\nof drum instruments. First, the shortcomings of publicly available datasets in\\nthis context are discussed. To overcome these limitations, a larger synthetic\\ndataset is introduced. Then, methods to train models using the new dataset\\nfocusing on generalization to real world data are investigated. Finally, the\\ntrained models are evaluated on publicly available datasets and results are\\ndiscussed. The contributions of this work comprise: (i.) a large-scale\\nsynthetic dataset for drum transcription, (ii.) first steps towards an\\nautomatic drum transcription system that supports a larger range of instruments\\nby evaluating and discussing training setups and the impact of datasets in this\\ncontext, and (iii.) a publicly available set of trained models for drum\\ntranscription. Additional materials are available at\\nhttp://ifs.tuwien.ac.at/~vogl/dafx2018',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06676v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06676v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Richard Vogl', 'Gerhard Widmer', 'Peter Knees'],\n",
       "  'tasks': ['Drum Transcription', 'Music Transcription'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/subword-and-crossword-units-for-ctc-acoustic',\n",
       "  'arxiv_id': '1712.06855',\n",
       "  'title': 'Subword and Crossword Units for CTC Acoustic Models',\n",
       "  'abstract': 'This paper proposes a novel approach to create an unit set for CTC based\\nspeech recognition systems. By using Byte Pair Encoding we learn an unit set of\\nan arbitrary size on a given training text. In contrast to using characters or\\nwords as units this allows us to find a good trade-off between the size of our\\nunit set and the available training data. We evaluate both Crossword units,\\nthat may span multiple word, and Subword units. By combining this approach with\\ndecoding methods using a separate language model we are able to achieve state\\nof the art results for grapheme based CTC systems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.06855v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.06855v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Thomas Zenkel',\n",
       "   'Ramon Sanabria',\n",
       "   'Florian Metze',\n",
       "   'Alex Waibel'],\n",
       "  'tasks': ['Language Modelling', 'Speech Recognition'],\n",
       "  'date': '2017-12-19',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/cardinality-leap-for-open-ended-evolution',\n",
       "  'arxiv_id': '1806.06628',\n",
       "  'title': 'Cardinality Leap for Open-Ended Evolution: Theoretical Consideration and Demonstration by \"Hash Chemistry\"',\n",
       "  'abstract': 'Open-ended evolution requires unbounded possibilities that evolving entities\\ncan explore. The cardinality of a set of those possibilities thus has a\\nsignificant implication for the open-endedness of evolution. We propose that\\nfacilitating formation of higher-order entities is a generalizable, effective\\nway to cause a \"cardinality leap\" in the set of possibilities that promotes\\nopen-endedness. We demonstrate this idea with a simple, proof-of-concept toy\\nmodel called \"Hash Chemistry\" that uses a hash function as a fitness evaluator\\nof evolving entities of any size/order. Simulation results showed that the\\ncumulative number of unique replicating entities that appeared in evolution\\nincreased almost linearly along time without an apparent bound, demonstrating\\nthe effectiveness of the proposed cardinality leap. It was also observed that\\nthe number of individual entities involved in a single replication event\\ngradually increased over time, indicating evolutionary appearance of\\nhigher-order entities. Moreover, these behaviors were not observed in control\\nexperiments in which fitness evaluators were replaced by random number\\ngenerators. This strongly suggests that the dynamics observed in Hash Chemistry\\nwere indeed evolutionary behaviors driven by selection and adaptation taking\\nplace at multiple scales.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06628v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06628v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hiroki Sayama'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/warp-wavelets-with-adaptive-recursive',\n",
       "  'arxiv_id': '1711.00789',\n",
       "  'title': 'Learning Asymmetric and Local Features in Multi-Dimensional Data through Wavelets with Recursive Partitioning',\n",
       "  'abstract': 'Effective learning of asymmetric and local features in images and other data observed on multi-dimensional grids is a challenging objective critical for a wide range of image processing applications involving biomedical and natural images. It requires methods that are sensitive to local details while fast enough to handle massive numbers of images of ever increasing sizes. We introduce a probabilistic model-based framework that achieves these objectives by incorporating adaptivity into discrete wavelet transforms (DWT) through Bayesian hierarchical modeling, thereby allowing wavelet bases to adapt to the geometric structure of the data while maintaining the high computational scalability of wavelet methods---linear in the sample size (e.g., the resolution of an image). We derive a recursive representation of the Bayesian posterior model which leads to an exact message passing algorithm to complete learning and inference. While our framework is applicable to a range of problems including multi-dimensional signal processing, compression, and structural learning, we illustrate its work and evaluate its performance in the context of image reconstruction using real images from the ImageNet database, two widely used benchmark datasets, and a dataset from retinal optical coherence tomography and compare its performance to state-of-the-art methods based on basis transforms and deep learning.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1711.00789v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1711.00789v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Meng Li', 'Li Ma'],\n",
       "  'tasks': ['Bayesian Inference', 'Image Reconstruction'],\n",
       "  'date': '2017-11-02',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet', 'bsd'],\n",
       "  'datasets_used_full': ['ImageNet', 'BSD'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-enhancing-speech-emotion-recognition-using',\n",
       "  'arxiv_id': '1806.06626',\n",
       "  'title': 'On Enhancing Speech Emotion Recognition using Generative Adversarial Networks',\n",
       "  'abstract': \"Generative Adversarial Networks (GANs) have gained a lot of attention from\\nmachine learning community due to their ability to learn and mimic an input\\ndata distribution. GANs consist of a discriminator and a generator working in\\ntandem playing a min-max game to learn a target underlying data distribution;\\nwhen fed with data-points sampled from a simpler distribution (like uniform or\\nGaussian distribution). Once trained, they allow synthetic generation of\\nexamples sampled from the target distribution. We investigate the application\\nof GANs to generate synthetic feature vectors used for speech emotion\\nrecognition. Specifically, we investigate two set ups: (i) a vanilla GAN that\\nlearns the distribution of a lower dimensional representation of the actual\\nhigher dimensional feature vector and, (ii) a conditional GAN that learns the\\ndistribution of the higher dimensional feature vectors conditioned on the\\nlabels or the emotional class to which it belongs. As a potential practical\\napplication of these synthetically generated samples, we measure any\\nimprovement in a classifier's performance when the synthetic data is used along\\nwith real data for training. We perform cross-validation analyses followed by a\\ncross-corpus study.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06626v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06626v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Saurabh Sahu', 'Rahul Gupta', 'Carol Espy-Wilson'],\n",
       "  'tasks': ['Emotion Recognition', 'Speech Emotion Recognition'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['iemocap'],\n",
       "  'datasets_used_full': ['IEMOCAP'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/banach-wasserstein-gan',\n",
       "  'arxiv_id': '1806.06621',\n",
       "  'title': 'Banach Wasserstein GAN',\n",
       "  'abstract': 'Wasserstein Generative Adversarial Networks (WGANs) can be used to generate\\nrealistic samples from complicated image distributions. The Wasserstein metric\\nused in WGANs is based on a notion of distance between individual images, which\\ninduces a notion of distance between probability distributions of images. So\\nfar the community has considered $\\\\ell^2$ as the underlying distance. We\\ngeneralize the theory of WGAN with gradient penalty to Banach spaces, allowing\\npractitioners to select the features to emphasize in the generator. We further\\ndiscuss the effect of some particular choices of underlying norms, focusing on\\nSobolev norms. Finally, we demonstrate a boost in performance for an\\nappropriate choice of norm on CIFAR-10 and CelebA.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06621v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06621v2.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Jonas Adler', 'Sebastian Lunz'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'WGAN',\n",
       "    'full_name': 'Wasserstein GAN',\n",
       "    'description': \"**Wasserstein GAN**, or **WGAN**, is a type of generative adversarial network that minimizes an approximation of the Earth-Mover's distance (EM) rather than the Jensen-Shannon divergence as in the original [GAN](https://paperswithcode.com/method/gan) formulation. It leads to more stable training than original GANs with less evidence of mode collapse, as well as meaningful curves that can be used for debugging and searching hyperparameters.\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1701.07875v3',\n",
       "    'source_title': 'Wasserstein GAN',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan/wgan.py',\n",
       "    'main_collection': {'name': 'Generative Adversarial Networks',\n",
       "     'description': '**Generative Adversarial Networks (GANs)** are a type of generative model that use two networks, a generator to generate images and a discriminator to discriminate between real and fake, to train a model that approximates the distribution of the data. Below you can find a continuously updating list of GANs.',\n",
       "     'parent': 'Generative Models',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/comparison-based-random-forests',\n",
       "  'arxiv_id': '1806.06616',\n",
       "  'title': 'Comparison-Based Random Forests',\n",
       "  'abstract': 'Assume we are given a set of items from a general metric space, but we\\nneither have access to the representation of the data nor to the distances\\nbetween data points. Instead, suppose that we can actively choose a triplet of\\nitems (A,B,C) and ask an oracle whether item A is closer to item B or to item\\nC. In this paper, we propose a novel random forest algorithm for regression and\\nclassification that relies only on such triplet comparisons. In the theory part\\nof this paper, we establish sufficient conditions for the consistency of such a\\nforest. In a set of comprehensive experiments, we then demonstrate that the\\nproposed random forest is efficient both for classification and regression. In\\nparticular, it is even competitive with other methods that have direct access\\nto the metric representation of the data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06616v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06616v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Siavash Haghiri', 'Damien Garreau', 'Ulrike Von Luxburg'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-multi-resident-activity-recognition-in',\n",
       "  'arxiv_id': '1806.06611',\n",
       "  'title': 'On Multi-resident Activity Recognition in Ambient Smart-Homes',\n",
       "  'abstract': 'Increasing attention to the research on activity monitoring in smart homes\\nhas motivated the employment of ambient intelligence to reduce the deployment\\ncost and solve the privacy issue. Several approaches have been proposed for\\nmulti-resident activity recognition, however, there still lacks a comprehensive\\nbenchmark for future research and practical selection of models. In this paper\\nwe study different methods for multi-resident activity recognition and evaluate\\nthem on same sets of data. The experimental results show that recurrent neural\\nnetwork with gated recurrent units is better than other models and also\\nconsiderably efficient, and that using combined activities as single labels is\\nmore effective than represent them as separate labels.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06611v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06611v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Son N. Tran', 'Qing Zhang', 'Mohan Karunanithi'],\n",
       "  'tasks': ['Activity Recognition'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/evaluating-and-characterizing-incremental',\n",
       "  'arxiv_id': '1806.06610',\n",
       "  'title': 'Evaluating and Characterizing Incremental Learning from Non-Stationary Data',\n",
       "  'abstract': 'Incremental learning from non-stationary data poses special challenges to the\\nfield of machine learning. Although new algorithms have been developed for\\nthis, assessment of results and comparison of behaviors are still open\\nproblems, mainly because evaluation metrics, adapted from more traditional\\ntasks, can be ineffective in this context. Overall, there is a lack of common\\ntesting practices. This paper thus presents a testbed for incremental\\nnon-stationary learning algorithms, based on specially designed synthetic\\ndatasets. Also, test results are reported for some well-known algorithms to\\nshow that the proposed methodology is effective at characterizing their\\nstrengths and weaknesses. It is expected that this methodology will provide a\\ncommon basis for evaluating future contributions in the field.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06610v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06610v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alejandro Cervantes',\n",
       "   'Christian Gagné',\n",
       "   'Pedro Isasi',\n",
       "   'Marc Parizeau'],\n",
       "  'tasks': ['Incremental Learning'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/quantized-compressive-k-means',\n",
       "  'arxiv_id': '1804.10109',\n",
       "  'title': 'Quantized Compressive K-Means',\n",
       "  'abstract': 'The recent framework of compressive statistical learning aims at designing\\ntractable learning algorithms that use only a heavily compressed\\nrepresentation-or sketch-of massive datasets. Compressive K-Means (CKM) is such\\na method: it estimates the centroids of data clusters from pooled, non-linear,\\nrandom signatures of the learning examples. While this approach significantly\\nreduces computational time on very large datasets, its digital implementation\\nwastes acquisition resources because the learning examples are compressed only\\nafter the sensing stage. The present work generalizes the sketching procedure\\ninitially defined in Compressive K-Means to a large class of periodic\\nnonlinearities including hardware-friendly implementations that compressively\\nacquire entire datasets. This idea is exemplified in a Quantized Compressive\\nK-Means procedure, a variant of CKM that leverages 1-bit universal quantization\\n(i.e. retaining the least significant bit of a standard uniform quantizer) as\\nthe periodic sketch nonlinearity. Trading for this resource-efficient signature\\n(standard in most acquisition schemes) has almost no impact on the clustering\\nperformances, as illustrated by numerical experiments.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.10109v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.10109v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Vincent Schellekens', 'Laurent Jacques'],\n",
       "  'tasks': ['Quantization'],\n",
       "  'date': '2018-04-26',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/self-attentional-acoustic-models',\n",
       "  'arxiv_id': '1803.09519',\n",
       "  'title': 'Self-Attentional Acoustic Models',\n",
       "  'abstract': 'Self-attention is a method of encoding sequences of vectors by relating these\\nvectors to each-other based on pairwise similarities. These models have\\nrecently shown promising results for modeling discrete sequences, but they are\\nnon-trivial to apply to acoustic modeling due to computational and modeling\\nissues. In this paper, we apply self-attention to acoustic modeling, proposing\\nseveral improvements to mitigate these issues: First, self-attention memory\\ngrows quadratically in the sequence length, which we address through a\\ndownsampling technique. Second, we find that previous approaches to incorporate\\nposition information into the model are unsuitable and explore other\\nrepresentations and hybrid models to this end. Third, to stress the importance\\nof local context in the acoustic signal, we propose a Gaussian biasing approach\\nthat allows explicit control over the context range. Experiments find that our\\nmodel approaches a strong baseline based on LSTMs with network-in-network\\nconnections while being much faster to compute. Besides speed, we find that\\ninterpretability is a strength of self-attentional acoustic models, and\\ndemonstrate that self-attention heads learn a linguistically plausible division\\nof labor.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.09519v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.09519v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Matthias Sperber',\n",
       "   'Jan Niehues',\n",
       "   'Graham Neubig',\n",
       "   'Sebastian Stüker',\n",
       "   'Alex Waibel'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-03-26',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/snap-ml-a-hierarchical-framework-for-machine',\n",
       "  'arxiv_id': '1803.06333',\n",
       "  'title': 'Snap ML: A Hierarchical Framework for Machine Learning',\n",
       "  'abstract': 'We describe a new software framework for fast training of generalized linear\\nmodels. The framework, named Snap Machine Learning (Snap ML), combines recent\\nadvances in machine learning systems and algorithms in a nested manner to\\nreflect the hierarchical architecture of modern computing systems. We prove\\ntheoretically that such a hierarchical system can accelerate training in\\ndistributed environments where intra-node communication is cheaper than\\ninter-node communication. Additionally, we provide a review of the\\nimplementation of Snap ML in terms of GPU acceleration, pipelining,\\ncommunication patterns and software architecture, highlighting aspects that\\nwere critical for achieving high performance. We evaluate the performance of\\nSnap ML in both single-node and multi-node environments, quantifying the\\nbenefit of the hierarchical scheme and the data streaming functionality, and\\ncomparing with other widely-used machine learning software frameworks. Finally,\\nwe present a logistic regression benchmark on the Criteo Terabyte Click Logs\\ndataset and show that Snap ML achieves the same test loss an order of magnitude\\nfaster than any of the previously reported results, including those obtained\\nusing TensorFlow and scikit-learn.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.06333v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.06333v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Celestine Dünner',\n",
       "   'Thomas Parnell',\n",
       "   'Dimitrios Sarigiannis',\n",
       "   'Nikolas Ioannou',\n",
       "   'Andreea Anghel',\n",
       "   'Gummadi Ravi',\n",
       "   'Madhusudanan Kandasamy',\n",
       "   'Haralampos Pozidis'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-03-16',\n",
       "  'methods': [{'name': 'Logistic Regression',\n",
       "    'full_name': 'Logistic Regression',\n",
       "    'description': '**Logistic Regression**, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\\r\\n\\r\\nSource: [scikit-learn](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\\r\\n\\r\\nImage: [Michaelg2015](https://commons.wikimedia.org/wiki/User:Michaelg2015)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Generalized Linear Models',\n",
       "     'description': '**Generalized Linear Models (GLMs)** are a class of models that generalize upon linear regression by allowing many more distributions to be modeled for the response variable via a link function. Below you can find a continuously updating list of GLMs.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multilingual-bottleneck-features-for-subword',\n",
       "  'arxiv_id': '1803.08863',\n",
       "  'title': 'Multilingual bottleneck features for subword modeling in zero-resource languages',\n",
       "  'abstract': 'How can we effectively develop speech technology for languages where no\\ntranscribed data is available? Many existing approaches use no annotated\\nresources at all, yet it makes sense to leverage information from large\\nannotated corpora in other languages, for example in the form of multilingual\\nbottleneck features (BNFs) obtained from a supervised speech recognition\\nsystem. In this work, we evaluate the benefits of BNFs for subword modeling\\n(feature extraction) in six unseen languages on a word discrimination task.\\nFirst we establish a strong unsupervised baseline by combining two existing\\nmethods: vocal tract length normalisation (VTLN) and the correspondence\\nautoencoder (cAE). We then show that BNFs trained on a single language already\\nbeat this baseline; including up to 10 languages results in additional\\nimprovements which cannot be matched by just adding more data from a single\\nlanguage. Finally, we show that the cAE can improve further on the BNFs if\\nhigh-quality same-word pairs are available.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.08863v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.08863v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Enno Hermann', 'Sharon Goldwater'],\n",
       "  'tasks': ['Speech Recognition'],\n",
       "  'date': '2018-03-23',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-to-write-stylized-chinese-characters',\n",
       "  'arxiv_id': '1712.06424',\n",
       "  'title': 'Learning to Write Stylized Chinese Characters by Reading a Handful of Examples',\n",
       "  'abstract': 'Automatically writing stylized Chinese characters is an attractive yet\\nchallenging task due to its wide applicabilities. In this paper, we propose a\\nnovel framework named Style-Aware Variational Auto-Encoder (SA-VAE) to flexibly\\ngenerate Chinese characters. Specifically, we propose to capture the different\\ncharacteristics of a Chinese character by disentangling the latent features\\ninto content-related and style-related components. Considering of the complex\\nshapes and structures, we incorporate the structure information as prior\\nknowledge into our framework to guide the generation. Our framework shows a\\npowerful one-shot/low-shot generalization ability by inferring the style\\ncomponent given a character with unseen style. To the best of our knowledge,\\nthis is the first attempt to learn to write new-style Chinese characters by\\nobserving only one or a few examples. Extensive experiments demonstrate its\\neffectiveness in generating different stylized Chinese characters by fusing the\\nfeature vectors corresponding to different contents and styles, which is of\\nsignificant importance in real-world applications.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.06424v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.06424v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Danyang Sun',\n",
       "   'Tongzheng Ren',\n",
       "   'Chongxun Li',\n",
       "   'Hang Su',\n",
       "   'Jun Zhu'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-12-06',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/ipose-instance-aware-6d-pose-estimation-of',\n",
       "  'arxiv_id': '1712.01924',\n",
       "  'title': 'iPose: Instance-Aware 6D Pose Estimation of Partly Occluded Objects',\n",
       "  'abstract': 'We address the task of 6D pose estimation of known rigid objects from single\\ninput images in scenarios where the objects are partly occluded. Recent\\nRGB-D-based methods are robust to moderate degrees of occlusion. For RGB\\ninputs, no previous method works well for partly occluded objects. Our main\\ncontribution is to present the first deep learning-based system that estimates\\naccurate poses for partly occluded objects from RGB-D and RGB input. We achieve\\nthis with a new instance-aware pipeline that decomposes 6D object pose\\nestimation into a sequence of simpler steps, where each step removes specific\\naspects of the problem. The first step localizes all known objects in the image\\nusing an instance segmentation network, and hence eliminates surrounding\\nclutter and occluders. The second step densely maps pixels to 3D object surface\\npositions, so called object coordinates, using an encoder-decoder network, and\\nhence eliminates object appearance. The third, and final, step predicts the 6D\\npose using geometric optimization. We demonstrate that we significantly\\noutperform the state-of-the-art for pose estimation of partly occluded objects\\nfor both RGB and RGB-D input.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.01924v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.01924v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Omid Hosseini Jafari',\n",
       "   'Siva Karthik Mustikovela',\n",
       "   'Karl Pertsch',\n",
       "   'Eric Brachmann',\n",
       "   'Carsten Rother'],\n",
       "  'tasks': ['6D Pose Estimation',\n",
       "   '6D Pose Estimation using RGB',\n",
       "   'Instance Segmentation',\n",
       "   'Pose Estimation',\n",
       "   'Semantic Segmentation'],\n",
       "  'date': '2017-12-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['linemod-1'],\n",
       "  'datasets_used_full': ['LINEMOD'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/uncertainty-in-multitask-learning-joint',\n",
       "  'arxiv_id': '1806.06595',\n",
       "  'title': 'Uncertainty in multitask learning: joint representations for probabilistic MR-only radiotherapy planning',\n",
       "  'abstract': 'Multi-task neural network architectures provide a mechanism that jointly\\nintegrates information from distinct sources. It is ideal in the context of\\nMR-only radiotherapy planning as it can jointly regress a synthetic CT (synCT)\\nscan and segment organs-at-risk (OAR) from MRI. We propose a probabilistic\\nmulti-task network that estimates: 1) intrinsic uncertainty through a\\nheteroscedastic noise model for spatially-adaptive task loss weighting and 2)\\nparameter uncertainty through approximate Bayesian inference. This allows\\nsampling of multiple segmentations and synCTs that share their network\\nrepresentation. We test our model on prostate cancer scans and show that it\\nproduces more accurate and consistent synCTs with a better estimation in the\\nvariance of the errors, state of the art results in OAR segmentation and a\\nmethodology for quality assurance in radiotherapy treatment planning.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06595v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06595v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Felix J. S. Bragman',\n",
       "   'Ryutaro Tanno',\n",
       "   'Zach Eaton-Rosen',\n",
       "   'Wenqi Li',\n",
       "   'David J. Hawkes',\n",
       "   'Sebastien Ourselin',\n",
       "   'Daniel C. Alexander',\n",
       "   'Jamie R. McClelland',\n",
       "   'M. Jorge Cardoso'],\n",
       "  'tasks': ['Bayesian Inference'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-recurrent-neural-network-for-multi',\n",
       "  'arxiv_id': '1806.06594',\n",
       "  'title': 'Deep Recurrent Neural Network for Multi-target Filtering',\n",
       "  'abstract': 'This paper addresses the problem of fixed motion and measurement models for\\nmulti-target filtering using an adaptive learning framework. This is performed\\nby defining target tuples with random finite set terminology and utilisation of\\nrecurrent neural networks with a long short-term memory architecture. A novel\\ndata association algorithm compatible with the predicted tracklet tuples is\\nproposed, enabling the update of occluded targets, in addition to assigning\\nbirth, survival and death of targets. The algorithm is evaluated over a\\ncommonly used filtering simulation scenario, with highly promising results.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06594v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06594v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mehryar Emambakhsh', 'Alessandro Bay', 'Eduard Vazquez'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/low-resource-speech-to-text-translation',\n",
       "  'arxiv_id': '1803.09164',\n",
       "  'title': 'Low-Resource Speech-to-Text Translation',\n",
       "  'abstract': 'Speech-to-text translation has many potential applications for low-resource\\nlanguages, but the typical approach of cascading speech recognition with\\nmachine translation is often impossible, since the transcripts needed to train\\na speech recognizer are usually not available for low-resource languages.\\nRecent work has found that neural encoder-decoder models can learn to directly\\ntranslate foreign speech in high-resource scenarios, without the need for\\nintermediate transcription. We investigate whether this approach also works in\\nsettings where both data and computation are limited. To make the approach\\nefficient, we make several architectural changes, including a change from\\ncharacter-level to word-level decoding. We find that this choice yields crucial\\nspeed improvements that allow us to train with fewer computational resources,\\nyet still performs well on frequent words. We explore models trained on between\\n20 and 160 hours of data, and find that although models trained on less data\\nhave considerably lower BLEU scores, they can still predict words with\\nrelatively high precision and recall---around 50% for a model trained on 50\\nhours of data, versus around 60% for the full 160 hour model. Thus, they may\\nstill be useful for some low-resource scenarios.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.09164v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.09164v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sameer Bansal',\n",
       "   'Herman Kamper',\n",
       "   'Karen Livescu',\n",
       "   'Adam Lopez',\n",
       "   'Sharon Goldwater'],\n",
       "  'tasks': ['Machine Translation',\n",
       "   'Speech Recognition',\n",
       "   'Speech-to-Text Translation',\n",
       "   'Translation'],\n",
       "  'date': '2018-03-24',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/computational-theories-of-curiosity-driven',\n",
       "  'arxiv_id': '1802.10546',\n",
       "  'title': 'Computational Theories of Curiosity-Driven Learning',\n",
       "  'abstract': 'What are the functions of curiosity? What are the mechanisms of\\ncuriosity-driven learning? We approach these questions about the living using\\nconcepts and tools from machine learning and developmental robotics. We argue\\nthat curiosity-driven learning enables organisms to make discoveries to solve\\ncomplex problems with rare or deceptive rewards. By fostering exploration and\\ndiscovery of a diversity of behavioural skills, and ignoring these rewards,\\ncuriosity can be efficient to bootstrap learning when there is no information,\\nor deceptive information, about local improvement towards these problems. We\\nalso explain the key role of curiosity for efficient learning of world models.\\nWe review both normative and heuristic computational frameworks used to\\nunderstand the mechanisms of curiosity in humans, conceptualizing the child as\\na sense-making organism. These frameworks enable us to discuss the\\nbi-directional causal links between curiosity and learning, and to provide new\\nhypotheses about the fundamental role of curiosity in self-organizing\\ndevelopmental structures through curriculum learning. We present various\\ndevelopmental robotics experiments that study these mechanisms in action, both\\nsupporting these hypotheses to understand better curiosity in humans and\\nopening new research avenues in machine learning and artificial intelligence.\\nFinally, we discuss challenges for the design of experimental paradigms for\\nstudying curiosity in psychology and cognitive neuroscience.\\n  Keywords: Curiosity, intrinsic motivation, lifelong learning, predictions,\\nworld model, rewards, free-energy principle, learning progress, machine\\nlearning, AI, developmental robotics, development, curriculum learning,\\nself-organization.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.10546v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.10546v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Pierre-Yves Oudeyer'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-28',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/nonparametric-topic-modeling-with-neural',\n",
       "  'arxiv_id': '1806.06583',\n",
       "  'title': 'Nonparametric Topic Modeling with Neural Inference',\n",
       "  'abstract': 'This work focuses on combining nonparametric topic models with Auto-Encoding\\nVariational Bayes (AEVB). Specifically, we first propose iTM-VAE, where the\\ntopics are treated as trainable parameters and the document-specific topic\\nproportions are obtained by a stick-breaking construction. The inference of\\niTM-VAE is modeled by neural networks such that it can be computed in a simple\\nfeed-forward manner. We also describe how to introduce a hyper-prior into\\niTM-VAE so as to model the uncertainty of the prior parameter. Actually, the\\nhyper-prior technique is quite general and we show that it can be applied to\\nother AEVB based models to alleviate the {\\\\it collapse-to-prior} problem\\nelegantly. Moreover, we also propose HiTM-VAE, where the document-specific\\ntopic distributions are generated in a hierarchical manner. HiTM-VAE is even\\nmore flexible and can generate topic distributions with better variability.\\nExperimental results on 20News and Reuters RCV1-V2 datasets show that the\\nproposed models outperform the state-of-the-art baselines significantly. The\\nadvantages of the hyper-prior technique and the hierarchical model construction\\nare also confirmed by experiments.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06583v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06583v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xuefei Ning',\n",
       "   'Yin Zheng',\n",
       "   'Zhuxi Jiang',\n",
       "   'Yu Wang',\n",
       "   'Huazhong Yang',\n",
       "   'Junzhou Huang'],\n",
       "  'tasks': ['Topic Models'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/wsd-algorithm-based-on-a-new-method-of-vector',\n",
       "  'arxiv_id': '1805.09559',\n",
       "  'title': 'WSD algorithm based on a new method of vector-word contexts proximity calculation via epsilon-filtration',\n",
       "  'abstract': 'The problem of word sense disambiguation (WSD) is considered in the article.\\nGiven a set of synonyms (synsets) and sentences with these synonyms. It is\\nnecessary to select the meaning of the word in the sentence automatically. 1285\\nsentences were tagged by experts, namely, one of the dictionary meanings was\\nselected by experts for target words. To solve the WSD-problem, an algorithm\\nbased on a new method of vector-word contexts proximity calculation is\\nproposed. In order to achieve higher accuracy, a preliminary epsilon-filtering\\nof words is performed, both in the sentence and in the set of synonyms. An\\nextensive program of experiments was carried out. Four algorithms are\\nimplemented, including a new algorithm. Experiments have shown that in a number\\nof cases the new algorithm shows better results. The developed software and the\\ntagged corpus have an open license and are available online. Wiktionary and\\nWikisource are used. A brief description of this work can be viewed in slides\\n(https://goo.gl/9ak6Gt). Video lecture in Russian on this research is available\\nonline (https://youtu.be/-DLmRkepf58).',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.09559v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.09559v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alexander Kirillov',\n",
       "   'Natalia Krizhanovsky',\n",
       "   'Andrew Krizhanovsky'],\n",
       "  'tasks': ['Word Sense Disambiguation'],\n",
       "  'date': '2018-05-24',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-kanerva-machine-a-generative-distributed',\n",
       "  'arxiv_id': '1804.01756',\n",
       "  'title': 'The Kanerva Machine: A Generative Distributed Memory',\n",
       "  'abstract': \"We present an end-to-end trained memory system that quickly adapts to new\\ndata and generates samples like them. Inspired by Kanerva's sparse distributed\\nmemory, it has a robust distributed reading and writing mechanism. The memory\\nis analytically tractable, which enables optimal on-line compression via a\\nBayesian update-rule. We formulate it as a hierarchical conditional generative\\nmodel, where memory provides a rich data-dependent prior distribution.\\nConsequently, the top-down memory and bottom-up perception are combined to\\nproduce the code representing an observation. Empirically, we demonstrate that\\nthe adaptive memory significantly improves generative models trained on both\\nthe Omniglot and CIFAR datasets. Compared with the Differentiable Neural\\nComputer (DNC) and its variants, our memory model has greater capacity and is\\nsignificantly easier to train.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.01756v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.01756v3.pdf',\n",
       "  'proceeding': 'ICLR 2018 1',\n",
       "  'authors': ['Yan Wu', 'Greg Wayne', 'Alex Graves', 'Timothy Lillicrap'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-04-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['omniglot-1'],\n",
       "  'datasets_used_full': ['Omniglot'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/rendernet-a-deep-convolutional-network-for',\n",
       "  'arxiv_id': '1806.06575',\n",
       "  'title': 'RenderNet: A deep convolutional network for differentiable rendering from 3D shapes',\n",
       "  'abstract': 'Traditional computer graphics rendering pipeline is designed for procedurally\\ngenerating 2D quality images from 3D shapes with high performance. The\\nnon-differentiability due to discrete operations such as visibility computation\\nmakes it hard to explicitly correlate rendering parameters and the resulting\\nimage, posing a significant challenge for inverse rendering tasks. Recent work\\non differentiable rendering achieves differentiability either by designing\\nsurrogate gradients for non-differentiable operations or via an approximate but\\ndifferentiable renderer. These methods, however, are still limited when it\\ncomes to handling occlusion, and restricted to particular rendering effects. We\\npresent RenderNet, a differentiable rendering convolutional network with a\\nnovel projection unit that can render 2D images from 3D shapes. Spatial\\nocclusion and shading calculation are automatically encoded in the network. Our\\nexperiments show that RenderNet can successfully learn to implement different\\nshaders, and can be used in inverse rendering tasks to estimate shape, pose,\\nlighting and texture from a single image.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06575v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06575v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Thu Nguyen-Phuoc',\n",
       "   'Chuan Li',\n",
       "   'Stephen Balaban',\n",
       "   'Yong-Liang Yang'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['shapenet', '3d-chairs', 'chairs'],\n",
       "  'datasets_used_full': ['ShapeNet', '3D Chairs', 'Chairs'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/distributed-learning-with-compressed',\n",
       "  'arxiv_id': '1806.06573',\n",
       "  'title': 'Distributed learning with compressed gradients',\n",
       "  'abstract': 'Asynchronous computation and gradient compression have emerged as two key\\ntechniques for achieving scalability in distributed optimization for\\nlarge-scale machine learning. This paper presents a unified analysis framework\\nfor distributed gradient methods operating with staled and compressed\\ngradients. Non-asymptotic bounds on convergence rates and information exchange\\nare derived for several optimization algorithms. These bounds give explicit\\nexpressions for step-sizes and characterize how the amount of asynchrony and\\nthe compression accuracy affect iteration and communication complexity\\nguarantees. Numerical results highlight convergence properties of different\\ngradient compression algorithms and confirm that fast convergence under limited\\ninformation exchange is indeed possible.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06573v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06573v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sarit Khirirat',\n",
       "   'Hamid Reza Feyzmahdavian',\n",
       "   'Mikael Johansson'],\n",
       "  'tasks': ['Distributed Optimization'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/subgram-extending-skip-gram-word',\n",
       "  'arxiv_id': '1806.06571',\n",
       "  'title': 'SubGram: Extending Skip-gram Word Representation with Substrings',\n",
       "  'abstract': 'Skip-gram (word2vec) is a recent method for creating vector representations\\nof words (\"distributed word representations\") using a neural network. The\\nrepresentation gained popularity in various areas of natural language\\nprocessing, because it seems to capture syntactic and semantic information\\nabout words without any explicit supervision in this respect. We propose\\nSubGram, a refinement of the Skip-gram model to consider also the word\\nstructure during the training process, achieving large gains on the Skip-gram\\noriginal test set.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06571v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06571v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Tom Kocmi', 'Ondřej Bojar'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-from-outside-the-viability-kernel',\n",
       "  'arxiv_id': '1806.06569',\n",
       "  'title': 'Learning from Outside the Viability Kernel: Why we Should Build Robots that can Fall with Grace',\n",
       "  'abstract': 'Despite impressive results using reinforcement learning to solve complex\\nproblems from scratch, in robotics this has still been largely limited to\\nmodel-based learning with very informative reward functions. One of the major\\nchallenges is that the reward landscape often has large patches with no\\ngradient, making it difficult to sample gradients effectively. We show here\\nthat the robot state-initialization can have a more important effect on the\\nreward landscape than is generally expected. In particular, we show the\\ncounter-intuitive benefit of including initializations that are unviable, in\\nother words initializing in states that are doomed to fail.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06569v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06569v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Steve Heim', 'Alexander Spröwitz'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/ista-net-interpretable-optimization-inspired',\n",
       "  'arxiv_id': '1706.07929',\n",
       "  'title': 'ISTA-Net: Interpretable Optimization-Inspired Deep Network for Image Compressive Sensing',\n",
       "  'abstract': 'With the aim of developing a fast yet accurate algorithm for compressive\\nsensing (CS) reconstruction of natural images, we combine in this paper the\\nmerits of two existing categories of CS methods: the structure insights of\\ntraditional optimization-based methods and the speed of recent network-based\\nones. Specifically, we propose a novel structured deep network, dubbed\\nISTA-Net, which is inspired by the Iterative Shrinkage-Thresholding Algorithm\\n(ISTA) for optimizing a general $\\\\ell_1$ norm CS reconstruction model. To cast\\nISTA into deep network form, we develop an effective strategy to solve the\\nproximal mapping associated with the sparsity-inducing regularizer using\\nnonlinear transforms. All the parameters in ISTA-Net (\\\\eg nonlinear transforms,\\nshrinkage thresholds, step sizes, etc.) are learned end-to-end, rather than\\nbeing hand-crafted. Moreover, considering that the residuals of natural images\\nare more compressible, an enhanced version of ISTA-Net in the residual domain,\\ndubbed {ISTA-Net}$^+$, is derived to further improve CS reconstruction.\\nExtensive CS experiments demonstrate that the proposed ISTA-Nets outperform\\nexisting state-of-the-art optimization-based and network-based CS methods by\\nlarge margins, while maintaining fast computational speed. Our source codes are\\navailable: \\\\textsl{http://jianzhang.tech/projects/ISTA-Net}.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1706.07929v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1706.07929v2.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Jian Zhang', 'Bernard Ghanem'],\n",
       "  'tasks': ['Compressive Sensing'],\n",
       "  'date': '2017-06-24',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['bsd', 'set11'],\n",
       "  'datasets_used_full': ['BSD', 'Set11'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/state-gradients-for-rnn-memory-analysis',\n",
       "  'arxiv_id': '1805.04264',\n",
       "  'title': 'State Gradients for RNN Memory Analysis',\n",
       "  'abstract': 'We present a framework for analyzing what the state in RNNs remembers from\\nits input embeddings. Our approach is inspired by backpropagation, in the sense\\nthat we compute the gradients of the states with respect to the input\\nembeddings. The gradient matrix is decomposed with Singular Value Decomposition\\nto analyze which directions in the embedding space are best transferred to the\\nhidden state space, characterized by the largest singular values. We apply our\\napproach to LSTM language models and investigate to what extent and for how\\nlong certain classes of words are remembered on average for a certain corpus.\\nAdditionally, the extent to which a specific property or relationship is\\nremembered by the RNN can be tracked by comparing a vector characterizing that\\nproperty with the direction(s) in embedding space that are best preserved in\\nhidden state space.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.04264v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.04264v2.pdf',\n",
       "  'proceeding': 'WS 2018 11',\n",
       "  'authors': ['Lyan Verwimp',\n",
       "   'Hugo Van hamme',\n",
       "   'Vincent Renkens',\n",
       "   'Patrick Wambacq'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-11',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': ['penn-treebank'],\n",
       "  'datasets_used_full': ['Penn Treebank'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/convex-optimization-with-unbounded-nonconvex',\n",
       "  'arxiv_id': '1711.02621',\n",
       "  'title': 'Convex Optimization with Unbounded Nonconvex Oracles using Simulated Annealing',\n",
       "  'abstract': 'We consider the problem of minimizing a convex objective function $F$ when\\none can only evaluate its noisy approximation $\\\\hat{F}$. Unless one assumes\\nsome structure on the noise, $\\\\hat{F}$ may be an arbitrary nonconvex function,\\nmaking the task of minimizing $F$ intractable. To overcome this, prior work has\\noften focused on the case when $F(x)-\\\\hat{F}(x)$ is uniformly-bounded. In this\\npaper we study the more general case when the noise has magnitude $\\\\alpha F(x)\\n+ \\\\beta$ for some $\\\\alpha, \\\\beta > 0$, and present a polynomial time algorithm\\nthat finds an approximate minimizer of $F$ for this noise model. Previously,\\nMarkov chains, such as the stochastic gradient Langevin dynamics, have been\\nused to arrive at approximate solutions to these optimization problems.\\nHowever, for the noise model considered in this paper, no single temperature\\nallows such a Markov chain to both mix quickly and concentrate near the global\\nminimizer. We bypass this by combining \"simulated annealing\" with the\\nstochastic gradient Langevin dynamics, and gradually decreasing the temperature\\nof the chain in order to approach the global minimizer. As a corollary one can\\napproximately minimize a nonconvex function that is close to a convex function;\\nhowever, the closeness can deteriorate as one moves away from the optimum.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.02621v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.02621v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Oren Mangoubi', 'Nisheeth K. Vishnoi'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-11-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/incremental-sparse-bayesian-ordinal',\n",
       "  'arxiv_id': '1806.06553',\n",
       "  'title': 'Incremental Sparse Bayesian Ordinal Regression',\n",
       "  'abstract': 'Ordinal Regression (OR) aims to model the ordering information between\\ndifferent data categories, which is a crucial topic in multi-label learning. An\\nimportant class of approaches to OR models the problem as a linear combination\\nof basis functions that map features to a high dimensional non-linear space.\\nHowever, most of the basis function-based algorithms are time consuming. We\\npropose an incremental sparse Bayesian approach to OR tasks and introduce an\\nalgorithm to sequentially learn the relevant basis functions in the ordinal\\nscenario. Our method, called Incremental Sparse Bayesian Ordinal Regression\\n(ISBOR), automatically optimizes the hyper-parameters via the type-II maximum\\nlikelihood method. By exploiting fast marginal likelihood optimization, ISBOR\\ncan avoid big matrix inverses, which is the main bottleneck in applying basis\\nfunction-based algorithms to OR tasks on large-scale datasets. We show that\\nISBOR can make accurate predictions with parsimonious basis functions while\\noffering automatic estimates of the prediction uncertainty. Extensive\\nexperiments on synthetic and real word datasets demonstrate the efficiency and\\neffectiveness of ISBOR compared to other basis function-based OR approaches.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06553v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06553v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Chang Li', 'Maarten de Rijke'],\n",
       "  'tasks': ['Multi-Label Learning'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/sniper-efficient-multi-scale-training',\n",
       "  'arxiv_id': '1805.09300',\n",
       "  'title': 'SNIPER: Efficient Multi-Scale Training',\n",
       "  'abstract': 'We present SNIPER, an algorithm for performing efficient multi-scale training\\nin instance level visual recognition tasks. Instead of processing every pixel\\nin an image pyramid, SNIPER processes context regions around ground-truth\\ninstances (referred to as chips) at the appropriate scale. For background\\nsampling, these context-regions are generated using proposals extracted from a\\nregion proposal network trained with a short learning schedule. Hence, the\\nnumber of chips generated per image during training adaptively changes based on\\nthe scene complexity. SNIPER only processes 30% more pixels compared to the\\ncommonly used single scale training at 800x1333 pixels on the COCO dataset.\\nBut, it also observes samples from extreme resolutions of the image pyramid,\\nlike 1400x2000 pixels. As SNIPER operates on resampled low resolution chips\\n(512x512 pixels), it can have a batch size as large as 20 on a single GPU even\\nwith a ResNet-101 backbone. Therefore it can benefit from batch-normalization\\nduring training without the need for synchronizing batch-normalization\\nstatistics across GPUs. SNIPER brings training of instance level recognition\\ntasks like object detection closer to the protocol for image classification and\\nsuggests that the commonly accepted guideline that it is important to train on\\nhigh resolution images for instance level visual recognition tasks might not be\\ncorrect. Our implementation based on Faster-RCNN with a ResNet-101 backbone\\nobtains an mAP of 47.6% on the COCO dataset for bounding box detection and can\\nprocess 5 images per second during inference with a single GPU. Code is\\navailable at https://github.com/MahyarNajibi/SNIPER/.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.09300v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.09300v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Bharat Singh', 'Mahyar Najibi', 'Larry S. Davis'],\n",
       "  'tasks': ['Object Detection', 'Region Proposal'],\n",
       "  'date': '2018-05-23',\n",
       "  'methods': [{'name': 'Average Pooling',\n",
       "    'full_name': 'Average Pooling',\n",
       "    'description': '**Average Pooling** is a pooling operation that calculates the average value for patches of a feature map, and uses it to create a downsampled (pooled) feature map. It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs. It extracts features more smoothly than [Max Pooling](https://paperswithcode.com/method/max-pooling), whereas max pooling extracts more pronounced features like edges.\\r\\n\\r\\nImage Source: [here](https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'SNIPER',\n",
       "    'full_name': 'SNIPER',\n",
       "    'description': '**SNIPER** is a multi-scale training approach for instance-level recognition tasks like object detection and instance-level segmentation. Instead of processing all pixels in an image pyramid, SNIPER selectively processes context regions around the ground-truth objects (a.k.a chips). This can help to speed up multi-scale training as it operates on low-resolution chips. Due to its memory-efficient design, SNIPER can benefit from [Batch Normalization](https://paperswithcode.com/method/batch-normalization) during training and it makes larger batch-sizes possible for instance-level recognition tasks on a single GPU.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1805.09300v3',\n",
       "    'source_title': 'SNIPER: Efficient Multi-Scale Training',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Multi-Scale Training',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Weight Decay',\n",
       "    'full_name': 'Weight Decay',\n",
       "    'description': '**Weight Decay**, or **$L_{2}$ Regularization**, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L\\\\_{2}$ Norm of the weights:\\r\\n\\r\\n$$L\\\\_{new}\\\\left(w\\\\right) = L\\\\_{original}\\\\left(w\\\\right) + \\\\lambda{w^{T}w}$$\\r\\n\\r\\nwhere $\\\\lambda$ is a value determining the strength of the penalty (encouraging smaller weights). \\r\\n\\r\\nWeight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).\\r\\n\\r\\nImage Source: Deep Learning, Goodfellow et al',\n",
       "    'introduced_year': 1943,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'RoIPool',\n",
       "    'full_name': 'RoIPool',\n",
       "    'description': '**Region of Interest Pooling**, or **RoIPool**, is an operation for extracting a small feature map (e.g., $7×7$) from each RoI in detection and segmentation based tasks. Features are extracted from each candidate box, and thereafter in models like [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn), are then classified and bounding box regression performed.\\r\\n\\r\\nThe actual scaling to, e.g., $7×7$, occurs by dividing the region proposal into equally sized sections, finding the largest value in each section, and then copying these max values to the output buffer. In essence, **RoIPool** is [max pooling](https://paperswithcode.com/method/max-pooling) on a discrete grid based on a box.\\r\\n\\r\\nImage Source: [Joyce Xu](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1311.2524v5',\n",
       "    'source_title': 'Rich feature hierarchies for accurate object detection and semantic segmentation',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/5e9ebe8dadc0ea2841a46cfcd82a93b4ce0d4519/torchvision/ops/roi_pool.py#L10',\n",
       "    'main_collection': {'name': 'RoI Feature Extractors',\n",
       "     'description': '**RoI Feature Extractors** are used to extract regions of interest features for tasks such as object detection. Below you can find a continuously updating list of RoI Feature Extractors.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Faster R-CNN',\n",
       "    'full_name': 'Faster R-CNN',\n",
       "    'description': '**Faster R-CNN** is an object detection model that improves on [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn) by utilising a region proposal network ([RPN](https://paperswithcode.com/method/rpn)) with the CNN model. The RPN shares full-image convolutional features with the detection network, enabling nearly cost-free region proposals. It is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn) for detection. RPN and Fast [R-CNN](https://paperswithcode.com/method/r-cnn) are merged into a single network by sharing their convolutional features: the RPN component tells the unified network where to look.\\r\\n\\r\\nAs a whole, Faster R-CNN consists of two modules. The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector that uses the proposed regions.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1506.01497v3',\n",
       "    'source_title': 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks',\n",
       "    'code_snippet_url': 'https://github.com/chenyuntc/simple-faster-rcnn-pytorch/blob/367db367834efd8a2bc58ee0023b2b628a0e474d/model/faster_rcnn.py#L22',\n",
       "    'main_collection': {'name': 'Object Detection Models',\n",
       "     'description': '**Object Detection Models** are architectures used to perform the task of object detection. Below you can find a continuously updating list of object detection models.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'RPN',\n",
       "    'full_name': 'Region Proposal Network',\n",
       "    'description': 'A **Region Proposal Network**, or **RPN**, is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals. RPN and algorithms like [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn) can be merged into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with attention mechanisms, the RPN component tells the unified network where to look.\\r\\n\\r\\nRPNs are designed to efficiently predict region proposals with a wide range of scales and aspect ratios. RPNs use anchor boxes that serve as references at multiple scales and aspect ratios. The scheme can be thought of as a pyramid of regression references, which avoids enumerating images or filters of multiple scales or aspect ratios.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1506.01497v3',\n",
       "    'source_title': 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Region Proposal',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Residual Connection',\n",
       "    'full_name': 'Residual Connection',\n",
       "    'description': '**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': '1x1 Convolution',\n",
       "    'full_name': '1x1 Convolution',\n",
       "    'description': 'A **1 x 1 Convolution** is a [convolution](https://paperswithcode.com/method/convolution) with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an [MLP](https://paperswithcode.com/method/feedforward-network) looking at a particular pixel location.\\r\\n\\r\\nImage Credit: [http://deeplearning.ai](http://deeplearning.ai)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://www.healthnutra.org/es/maxup/',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Batch Normalization',\n",
       "    'full_name': 'Batch Normalization',\n",
       "    'description': '**Batch Normalization** aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for [Dropout](https://paperswithcode.com/method/dropout).\\r\\n\\r\\nWe apply a batch normalization layer as follows for a minibatch $\\\\mathcal{B}$:\\r\\n\\r\\n$$ \\\\mu\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}x\\\\_{i} $$\\r\\n\\r\\n$$ \\\\sigma^{2}\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}\\\\left(x\\\\_{i}-\\\\mu\\\\_{\\\\mathcal{B}}\\\\right)^{2} $$\\r\\n\\r\\n$$ \\\\hat{x}\\\\_{i} = \\\\frac{x\\\\_{i} - \\\\mu\\\\_{\\\\mathcal{B}}}{\\\\sqrt{\\\\sigma^{2}\\\\_{\\\\mathcal{B}}+\\\\epsilon}} $$\\r\\n\\r\\n$$ y\\\\_{i} = \\\\gamma\\\\hat{x}\\\\_{i} + \\\\beta = \\\\text{BN}\\\\_{\\\\gamma, \\\\beta}\\\\left(x\\\\_{i}\\\\right) $$\\r\\n\\r\\nWhere $\\\\gamma$ and $\\\\beta$ are learnable parameters.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.03167v3',\n",
       "    'source_title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/36f91261099b00194922bd93ed1286fe1c199724/jax/experimental/stax.py#L116',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Bottleneck Residual Block',\n",
       "    'full_name': 'Bottleneck Residual Block',\n",
       "    'description': 'A **Bottleneck Residual Block** is a variant of the [residual block](https://paperswithcode.com/method/residual-block) that utilises 1x1 convolutions to create a bottleneck. The use of a bottleneck reduces the number of parameters and matrix multiplications. The idea is to make residual blocks as thin as possible to increase depth and have less parameters. They were introduced as part of the [ResNet](https://paperswithcode.com/method/resnet) architecture, and are used as part of deeper ResNets such as ResNet-50 and ResNet-101.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L75',\n",
       "    'main_collection': {'name': 'Skip Connection Blocks',\n",
       "     'description': \"**Skip Connection Blocks** are building blocks for neural networks that feature skip connections. These skip connections 'skip' some layers allowing gradients to better flow through the network. Below you will find a continuously updating list of skip connection blocks:\",\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Global Average Pooling',\n",
       "    'full_name': 'Global Average Pooling',\n",
       "    'description': '**Global Average Pooling** is a pooling operation designed to replace fully connected layers in classical CNNs. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the [softmax](https://paperswithcode.com/method/softmax) layer. \\r\\n\\r\\nOne advantage of global [average pooling](https://paperswithcode.com/method/average-pooling) over the fully connected layers is that it is more native to the [convolution](https://paperswithcode.com/method/convolution) structure by enforcing correspondences between feature maps and categories. Thus the feature maps can be easily interpreted as categories confidence maps. Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. Furthermore, global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/baa592b215804927e28638f6a7f3318cbc411d49/torchvision/models/resnet.py#L157',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Residual Block',\n",
       "    'full_name': 'Residual Block',\n",
       "    'description': \"**Residual Blocks** are skip-connection blocks that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. They were introduced as part of the [ResNet](https://paperswithcode.com/method/resnet) architecture.\\r\\n \\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$. The $\\\\mathcal{F}({x})$ acts like a residual, hence the name 'residual block'.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. Having skip connections allows the network to more easily learn identity-like mappings.\\r\\n\\r\\nNote that in practice, [Bottleneck Residual Blocks](https://paperswithcode.com/method/bottleneck-residual-block) are used for deeper ResNets, such as ResNet-50 and ResNet-101, as these bottleneck blocks are less computationally intensive.\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L35',\n",
       "    'main_collection': {'name': 'Skip Connection Blocks',\n",
       "     'description': \"**Skip Connection Blocks** are building blocks for neural networks that feature skip connections. These skip connections 'skip' some layers allowing gradients to better flow through the network. Below you will find a continuously updating list of skip connection blocks:\",\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Kaiming Initialization',\n",
       "    'full_name': 'Kaiming Initialization',\n",
       "    'description': '**Kaiming Initialization**, or **He Initialization**, is an initialization method for neural networks that takes into account the non-linearity of activation functions, such as [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nA proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. Using a derivation they work out that the condition to stop this happening is:\\r\\n\\r\\n$$\\\\frac{1}{2}n\\\\_{l}\\\\text{Var}\\\\left[w\\\\_{l}\\\\right] = 1 $$\\r\\n\\r\\nThis implies an initialization scheme of:\\r\\n\\r\\n$$ w\\\\_{l} \\\\sim \\\\mathcal{N}\\\\left(0,  2/n\\\\_{l}\\\\right)$$\\r\\n\\r\\nThat is, a zero-centered Gaussian with standard deviation of $\\\\sqrt{2/{n}\\\\_{l}}$ (variance shown in equation above). Biases are initialized at $0$.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.01852v1',\n",
       "    'source_title': 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/0adb5843766092fba584791af76383125fd0d01c/torch/nn/init.py#L389',\n",
       "    'main_collection': {'name': 'Initialization',\n",
       "     'description': '**Initialization** methods are used to initialize the weights in a neural network. Below can you find a continuously updating list of initialization methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'ResNet',\n",
       "    'full_name': 'Residual Network',\n",
       "    'description': '**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack [residual blocks](https://paperswithcode.com/method/residual-block) ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}(x)$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}(x):=\\\\mathcal{H}(x)-x$. The original mapping is recast into $\\\\mathcal{F}(x)+x$.\\r\\n\\r\\nThere is empirical evidence that these types of network are easier to optimize, and can gain accuracy from considerably increased depth.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/resnet.py#L124',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['coco'],\n",
       "  'datasets_used_full': ['COCO'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/constraining-the-dynamics-of-deep',\n",
       "  'arxiv_id': '1802.05680',\n",
       "  'title': 'Constraining the Dynamics of Deep Probabilistic Models',\n",
       "  'abstract': 'We introduce a novel generative formulation of deep probabilistic models\\nimplementing \"soft\" constraints on their function dynamics. In particular, we\\ndevelop a flexible methodological framework where the modeled functions and\\nderivatives of a given order are subject to inequality or equality constraints.\\nWe then characterize the posterior distribution over model and constraint\\nparameters through stochastic variational inference. As a result, the proposed\\napproach allows for accurate and scalable uncertainty quantification on the\\npredictions and on all parameters. We demonstrate the application of equality\\nconstraints in the challenging problem of parameter inference in ordinary\\ndifferential equation models, while we showcase the application of inequality\\nconstraints on the problem of monotonic regression of count data. The proposed\\napproach is extensively tested in several experimental settings, leading to\\nhighly competitive results in challenging modeling applications, while offering\\nhigh expressiveness, flexibility and scalability.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.05680v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.05680v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Marco Lorenzi', 'Maurizio Filippone'],\n",
       "  'tasks': ['Variational Inference'],\n",
       "  'date': '2018-02-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-simple-reservoir-model-of-working-memory',\n",
       "  'arxiv_id': '1806.06545',\n",
       "  'title': 'A Simple Reservoir Model of Working Memory with Real Values',\n",
       "  'abstract': 'The prefrontal cortex is known to be involved in many high-level cognitive\\nfunctions, in particular, working memory. Here, we study to what extent a group\\nof randomly connected units (namely an Echo State Network, ESN) can store and\\nmaintain (as output) an arbitrary real value from a streamed input, i.e. can\\nact as a sustained working memory unit. Furthermore, we explore to what extent\\nsuch an architecture can take advantage of the stored value in order to produce\\nnon-linear computations. Comparison between different architectures (with and\\nwithout feedback, with and without a working memory unit) shows that an\\nexplicit memory improves the performances.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06545v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06545v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Anthony Strock', 'Nicolas Rougier', 'Xavier Hinaut'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/segmentation-of-photovoltaic-module-cells-in',\n",
       "  'arxiv_id': '1806.06530',\n",
       "  'title': 'Segmentation of Photovoltaic Module Cells in Uncalibrated Electroluminescence Images',\n",
       "  'abstract': 'High resolution electroluminescence (EL) images captured in the infrared spectrum allow to visually and non-destructively inspect the quality of photovoltaic (PV) modules. Currently, however, such a visual inspection requires trained experts to discern different kinds of defects, which is time-consuming and expensive. Automated segmentation of cells is therefore a key step in automating the visual inspection workflow. In this work, we propose a robust automated segmentation method for extraction of individual solar cells from EL images of PV modules. This enables controlled studies on large amounts of data to understanding the effects of module degradation over time-a process not yet fully understood. The proposed method infers in several steps a high-level solar module representation from low-level edge features. An important step in the algorithm is to formulate the segmentation problem in terms of lens calibration by exploiting the plumbline constraint. We evaluate our method on a dataset of various solar modules types containing a total of 408 solar cells with various defects. Our method robustly solves this task with a median weighted Jaccard index of 94.47% and an $F_1$ score of 97.62%, both indicating a very high similarity between automatically segmented and ground truth solar cell masks.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06530v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06530v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sergiu Deitsch',\n",
       "   'Claudia Buerhop-Lutz',\n",
       "   'Evgenii Sovetkin',\n",
       "   'Ansgar Steland',\n",
       "   'Andreas Maier',\n",
       "   'Florian Gallwitz',\n",
       "   'Christian Riess'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dual-recovery-network-with-online',\n",
       "  'arxiv_id': '1701.05652',\n",
       "  'title': 'Dual Recovery Network with Online Compensation for Image Super-Resolution',\n",
       "  'abstract': 'Image super-resolution (SR) methods essentially lead to a loss of some\\nhigh-frequency (HF) information when predicting high-resolution (HR) images\\nfrom low-resolution (LR) images without using external references. To address\\nthis issue, we additionally utilize online retrieved data to facilitate image\\nSR in a unified deep framework. A novel dual high-frequency recovery network\\n(DHN) is proposed to predict an HR image with three parts: an LR image, an\\ninternal inferred HF (IHF) map (HF missing part inferred solely from the LR\\nimage) and an external extracted HF (EHF) map. In particular, we infer the HF\\ninformation based on both the LR image and similar HR references which are\\nretrieved online. For the EHF map, we align the references with affine\\ntransformation and then in the aligned references, part of HF signals are\\nextracted by the proposed DHN to compensate for the HF loss. Extensive\\nexperimental results demonstrate that our DHN achieves notably better\\nperformance than state-of-the-art SR methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1701.05652v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1701.05652v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sifeng Xia', 'Wenhan Yang', 'Jiaying Liu', 'Zongming Guo'],\n",
       "  'tasks': ['Image Super-Resolution', 'Super-Resolution'],\n",
       "  'date': '2017-01-20',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/hitnet-a-neural-network-with-capsules',\n",
       "  'arxiv_id': '1806.06519',\n",
       "  'title': 'HitNet: a neural network with capsules embedded in a Hit-or-Miss layer, extended with hybrid data augmentation and ghost capsules',\n",
       "  'abstract': 'Neural networks designed for the task of classification have become a\\ncommodity in recent years. Many works target the development of better\\nnetworks, which results in a complexification of their architectures with more\\nlayers, multiple sub-networks, or even the combination of multiple classifiers.\\nIn this paper, we show how to redesign a simple network to reach excellent\\nperformances, which are better than the results reproduced with CapsNet on\\nseveral datasets, by replacing a layer with a Hit-or-Miss layer. This layer\\ncontains activated vectors, called capsules, that we train to hit or miss a\\ncentral capsule by tailoring a specific centripetal loss function. We also show\\nhow our network, named HitNet, is capable of synthesizing a representative\\nsample of the images of a given class by including a reconstruction network.\\nThis possibility allows to develop a data augmentation step combining\\ninformation from the data space and the feature space, resulting in a hybrid\\ndata augmentation process. In addition, we introduce the possibility for\\nHitNet, to adopt an alternative to the true target when needed by using the new\\nconcept of ghost capsules, which is used here to detect potentially mislabeled\\nimages in the training data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06519v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06519v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Adrien Deliège', 'Anthony Cioppa', 'Marc Van Droogenbroeck'],\n",
       "  'tasks': ['Data Augmentation'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'mnist', 'svhn'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'MNIST', 'SVHN'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-information-autoencoding-family-a',\n",
       "  'arxiv_id': '1806.06514',\n",
       "  'title': 'The Information Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Models',\n",
       "  'abstract': 'A large number of objectives have been proposed to train latent variable\\ngenerative models. We show that many of them are Lagrangian dual functions of\\nthe same primal optimization problem. The primal problem optimizes the mutual\\ninformation between latent and visible variables, subject to the constraints of\\naccurately modeling the data distribution and performing correct amortized\\ninference. By choosing to maximize or minimize mutual information, and choosing\\ndifferent Lagrange multipliers, we obtain different objectives including\\nInfoGAN, ALI/BiGAN, ALICE, CycleGAN, beta-VAE, adversarial autoencoders, AVB,\\nAS-VAE and InfoVAE. Based on this observation, we provide an exhaustive\\ncharacterization of the statistical and computational trade-offs made by all\\nthe training objectives in this class of Lagrangian duals. Next, we propose a\\ndual optimization method where we optimize model parameters as well as the\\nLagrange multipliers. This method achieves Pareto optimal solutions in terms of\\noptimizing information and satisfying the constraints.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06514v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06514v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shengjia Zhao', 'Jiaming Song', 'Stefano Ermon'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [{'name': 'Batch Normalization',\n",
       "    'full_name': 'Batch Normalization',\n",
       "    'description': '**Batch Normalization** aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for [Dropout](https://paperswithcode.com/method/dropout).\\r\\n\\r\\nWe apply a batch normalization layer as follows for a minibatch $\\\\mathcal{B}$:\\r\\n\\r\\n$$ \\\\mu\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}x\\\\_{i} $$\\r\\n\\r\\n$$ \\\\sigma^{2}\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}\\\\left(x\\\\_{i}-\\\\mu\\\\_{\\\\mathcal{B}}\\\\right)^{2} $$\\r\\n\\r\\n$$ \\\\hat{x}\\\\_{i} = \\\\frac{x\\\\_{i} - \\\\mu\\\\_{\\\\mathcal{B}}}{\\\\sqrt{\\\\sigma^{2}\\\\_{\\\\mathcal{B}}+\\\\epsilon}} $$\\r\\n\\r\\n$$ y\\\\_{i} = \\\\gamma\\\\hat{x}\\\\_{i} + \\\\beta = \\\\text{BN}\\\\_{\\\\gamma, \\\\beta}\\\\left(x\\\\_{i}\\\\right) $$\\r\\n\\r\\nWhere $\\\\gamma$ and $\\\\beta$ are learnable parameters.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.03167v3',\n",
       "    'source_title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/36f91261099b00194922bd93ed1286fe1c199724/jax/experimental/stax.py#L116',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Residual Connection',\n",
       "    'full_name': 'Residual Connection',\n",
       "    'description': '**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'PatchGAN',\n",
       "    'full_name': 'PatchGAN',\n",
       "    'description': '**PatchGAN** is a type of discriminator for generative adversarial networks which only penalizes structure at the scale of local image patches. The PatchGAN discriminator tries to classify if each $N \\\\times N$ patch in an image is real or fake. This discriminator is run convolutionally across the image, averaging all responses to provide the ultimate output of $D$. Such a discriminator effectively models the image as a Markov random field, assuming independence between pixels separated by more than a patch diameter. It can be understood as a type of texture/style loss.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1611.07004v3',\n",
       "    'source_title': 'Image-to-Image Translation with Conditional Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/znxlwm/pytorch-pix2pix/blob/3059f2af53324e77089bbcfc31279f01a38c40b8/network.py#L104',\n",
       "    'main_collection': {'name': 'Discriminators',\n",
       "     'description': '**Discriminators** are a type of module used in architectures such as generative adversarial networks to discriminate between real and generated samples. Below you can find a continuously updating list of discriminators.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Residual Block',\n",
       "    'full_name': 'Residual Block',\n",
       "    'description': \"**Residual Blocks** are skip-connection blocks that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. They were introduced as part of the [ResNet](https://paperswithcode.com/method/resnet) architecture.\\r\\n \\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$. The $\\\\mathcal{F}({x})$ acts like a residual, hence the name 'residual block'.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. Having skip connections allows the network to more easily learn identity-like mappings.\\r\\n\\r\\nNote that in practice, [Bottleneck Residual Blocks](https://paperswithcode.com/method/bottleneck-residual-block) are used for deeper ResNets, such as ResNet-50 and ResNet-101, as these bottleneck blocks are less computationally intensive.\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L35',\n",
       "    'main_collection': {'name': 'Skip Connection Blocks',\n",
       "     'description': \"**Skip Connection Blocks** are building blocks for neural networks that feature skip connections. These skip connections 'skip' some layers allowing gradients to better flow through the network. Below you will find a continuously updating list of skip connection blocks:\",\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Instance Normalization',\n",
       "    'full_name': 'Instance Normalization',\n",
       "    'description': '**Instance Normalization** (also known as contrast normalization) is a normalization layer where:\\r\\n\\r\\n$$\\r\\n    y_{tijk} =  \\\\frac{x_{tijk} - \\\\mu_{ti}}{\\\\sqrt{\\\\sigma_{ti}^2 + \\\\epsilon}},\\r\\n    \\\\quad\\r\\n    \\\\mu_{ti} = \\\\frac{1}{HW}\\\\sum_{l=1}^W \\\\sum_{m=1}^H x_{tilm},\\r\\n    \\\\quad\\r\\n    \\\\sigma_{ti}^2 = \\\\frac{1}{HW}\\\\sum_{l=1}^W \\\\sum_{m=1}^H (x_{tilm} - mu_{ti})^2.\\r\\n$$\\r\\n\\r\\nThis prevents instance-specific mean and covariance shift simplifying the learning process. Intuitively, the normalization process allows to remove instance-specific contrast information from the content image in a task like image stylization, which simplifies generation.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1607.08022v3',\n",
       "    'source_title': 'Instance Normalization: The Missing Ingredient for Fast Stylization',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/1c5c289b6218eb1026dcb5fd9738231401cfccea/torch/nn/modules/instancenorm.py#L141',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Leaky ReLU',\n",
       "    'full_name': 'Leaky ReLU',\n",
       "    'description': '**Leaky Rectified Linear Unit**, or **Leaky ReLU**, is a type of activation function based on a [ReLU](https://paperswithcode.com/method/relu), but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training. This type of activation function is popular in tasks where we we may suffer from sparse gradients, for example training generative adversarial networks.',\n",
       "    'introduced_year': 2014,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L649',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'GAN Least Squares Loss',\n",
       "    'full_name': 'GAN Least Squares Loss',\n",
       "    'description': '**GAN Least Squares Loss** is a least squares loss function for generative adversarial networks. Minimizing this objective function is equivalent to minimizing the Pearson $\\\\chi^{2}$ divergence. The objective function (here for [LSGAN](https://paperswithcode.com/method/lsgan)) can be defined as:\\r\\n\\r\\n$$ \\\\min\\\\_{D}V\\\\_{LS}\\\\left(D\\\\right) = \\\\frac{1}{2}\\\\mathbb{E}\\\\_{\\\\mathbf{x} \\\\sim p\\\\_{data}\\\\left(\\\\mathbf{x}\\\\right)}\\\\left[\\\\left(D\\\\left(\\\\mathbf{x}\\\\right) - b\\\\right)^{2}\\\\right] + \\\\frac{1}{2}\\\\mathbb{E}\\\\_{\\\\mathbf{z}\\\\sim p\\\\_{data}\\\\left(\\\\mathbf{z}\\\\right)}\\\\left[\\\\left(D\\\\left(G\\\\left(\\\\mathbf{z}\\\\right)\\\\right) - a\\\\right)^{2}\\\\right] $$\\r\\n\\r\\n$$ \\\\min\\\\_{G}V\\\\_{LS}\\\\left(G\\\\right) = \\\\frac{1}{2}\\\\mathbb{E}\\\\_{\\\\mathbf{z} \\\\sim p\\\\_{\\\\mathbf{z}}\\\\left(\\\\mathbf{z}\\\\right)}\\\\left[\\\\left(D\\\\left(G\\\\left(\\\\mathbf{z}\\\\right)\\\\right) - c\\\\right)^{2}\\\\right] $$\\r\\n\\r\\nwhere $a$ and $b$ are the labels for fake data and real data and $c$ denotes the value that $G$ wants $D$ to believe for fake data.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1611.04076v3',\n",
       "    'source_title': 'Least Squares Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/a163b82beff3d01688d8315a3fd39080400e7c01/implementations/lsgan/lsgan.py#L102',\n",
       "    'main_collection': {'name': 'Loss Functions',\n",
       "     'description': '**Loss Functions** are used to frame the problem to be optimized within deep learning. Below you will find a continuously updating list of (specialized) loss functions for neutral networks.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Cycle Consistency Loss',\n",
       "    'full_name': 'Cycle Consistency Loss',\n",
       "    'description': '**Cycle Consistency Loss** is a type of loss used for generative adversarial networks that performs unpaired image-to-image translation. It was introduced with the [CycleGAN](https://paperswithcode.com/method/cyclegan) architecture. For two domains $X$ and $Y$, we want to learn a mapping $G : X \\\\rightarrow Y$ and $F: Y \\\\rightarrow X$. We want to enforce the intuition that these mappings should be reverses of each other and that both mappings should be bijections. Cycle Consistency Loss encourages $F\\\\left(G\\\\left(x\\\\right)\\\\right) \\\\approx x$ and $G\\\\left(F\\\\left(y\\\\right)\\\\right) \\\\approx y$.  It reduces the space of possible mapping functions by enforcing forward and backwards consistency:\\r\\n\\r\\n$$ \\\\mathcal{L}\\\\_{cyc}\\\\left(G, F\\\\right) = \\\\mathbb{E}\\\\_{x \\\\sim p\\\\_{data}\\\\left(x\\\\right)}\\\\left[||F\\\\left(G\\\\left(x\\\\right)\\\\right) - x||\\\\_{1}\\\\right] + \\\\mathbb{E}\\\\_{y \\\\sim p\\\\_{data}\\\\left(y\\\\right)}\\\\left[||G\\\\left(F\\\\left(y\\\\right)\\\\right) - y||\\\\_{1}\\\\right] $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1703.10593v7',\n",
       "    'source_title': 'Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/f5834b3ed339ec268f40cf56928234eed8dfeb92/models/cycle_gan_model.py#L172',\n",
       "    'main_collection': {'name': 'Loss Functions',\n",
       "     'description': '**Loss Functions** are used to frame the problem to be optimized within deep learning. Below you will find a continuously updating list of (specialized) loss functions for neutral networks.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'CycleGAN',\n",
       "    'full_name': 'CycleGAN',\n",
       "    'description': '**CycleGAN**, or **Cycle-Consistent GAN**, is a type of generative adversarial network for unpaired image-to-image translation. For two domains $X$ and $Y$, CycleGAN learns a mapping $G : X \\\\rightarrow Y$ and $F: Y \\\\rightarrow X$. The novelty lies in trying to enforce the intuition that these mappings should be reverses of each other and that both mappings should be bijections. This is achieved through a [cycle consistency loss](https://paperswithcode.com/method/cycle-consistency-loss) that encourages $F\\\\left(G\\\\left(x\\\\right)\\\\right) \\\\approx x$ and $G\\\\left(Y\\\\left(y\\\\right)\\\\right) \\\\approx y$. Combining this loss with the adversarial losses on $X$ and $Y$ yields the full objective for unpaired image-to-image translation.\\r\\n\\r\\nFor the mapping $G : X \\\\rightarrow Y$ and its discriminator $D\\\\_{Y}$ we have the objective:\\r\\n\\r\\n$$ \\\\mathcal{L}\\\\_{GAN}\\\\left(G, D\\\\_{Y}, X, Y\\\\right) =\\\\mathbb{E}\\\\_{y \\\\sim p\\\\_{data}\\\\left(y\\\\right)}\\\\left[\\\\log D\\\\_{Y}\\\\left(y\\\\right)\\\\right] + \\\\mathbb{E}\\\\_{x \\\\sim p\\\\_{data}\\\\left(x\\\\right)}\\\\left[log(1 − D\\\\_{Y}\\\\left(G\\\\left(x\\\\right)\\\\right)\\\\right] $$\\r\\n\\r\\nwhere $G$ tries to generate images $G\\\\left(x\\\\right)$ that look similar to images from domain $Y$, while $D\\\\_{Y}$ tries to discriminate between translated samples $G\\\\left(x\\\\right)$ and real samples $y$. A similar loss is postulated for the mapping $F: Y \\\\rightarrow X$ and its discriminator $D\\\\_{X}$.\\r\\n\\r\\nThe Cycle Consistency Loss reduces the space of possible mapping functions by enforcing forward and backwards consistency:\\r\\n\\r\\n$$ \\\\mathcal{L}\\\\_{cyc}\\\\left(G, F\\\\right) = \\\\mathbb{E}\\\\_{x \\\\sim p\\\\_{data}\\\\left(x\\\\right)}\\\\left[||F\\\\left(G\\\\left(x\\\\right)\\\\right) - x||\\\\_{1}\\\\right] + \\\\mathbb{E}\\\\_{y \\\\sim p\\\\_{data}\\\\left(y\\\\right)}\\\\left[||G\\\\left(F\\\\left(y\\\\right)\\\\right) - y||\\\\_{1}\\\\right] $$\\r\\n\\r\\nThe full objective is:\\r\\n\\r\\n$$ \\\\mathcal{L}\\\\_{GAN}\\\\left(G, F, D\\\\_{X}, D\\\\_{Y}\\\\right) = \\\\mathcal{L}\\\\_{GAN}\\\\left(G, D\\\\_{Y}, X, Y\\\\right) + \\\\mathcal{L}\\\\_{GAN}\\\\left(F, D\\\\_{X}, X, Y\\\\right) + \\\\lambda\\\\mathcal{L}\\\\_{cyc}\\\\left(G, F\\\\right) $$\\r\\n\\r\\nWhere we aim to solve:\\r\\n\\r\\n$$ G^{\\\\*}, F^{\\\\*} = \\\\arg \\\\min\\\\_{G, F} \\\\min\\\\_{D\\\\_{X}, D\\\\_{Y}} \\\\mathcal{L}\\\\_{GAN}\\\\left(G, F, D\\\\_{X}, D\\\\_{Y}\\\\right) $$\\r\\n\\r\\nFor the original architecture the authors use:\\r\\n\\r\\n-  two stride-2 convolutions, several residual blocks, and two fractionally strided convolutions with stride $\\\\frac{1}{2}$.\\r\\n- [instance normalization](https://paperswithcode.com/method/instance-normalization)\\r\\n- PatchGANs for the discriminator\\r\\n- Least Square Loss for the [GAN](https://paperswithcode.com/method/gan) objectives.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1703.10593v7',\n",
       "    'source_title': 'Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/9e6fff7b7d5215a38be3cac074ca7087041bea0d/models/cycle_gan_model.py#L8',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/semi-tied-units-for-efficient-gating-in-lstm',\n",
       "  'arxiv_id': '1806.06513',\n",
       "  'title': 'Semi-tied Units for Efficient Gating in LSTM and Highway Networks',\n",
       "  'abstract': 'Gating is a key technique used for integrating information from multiple\\nsources by long short-term memory (LSTM) models and has recently also been\\napplied to other models such as the highway network. Although gating is\\npowerful, it is rather expensive in terms of both computation and storage as\\neach gating unit uses a separate full weight matrix. This issue can be severe\\nsince several gates can be used together in e.g. an LSTM cell. This paper\\nproposes a semi-tied unit (STU) approach to solve this efficiency issue, which\\nuses one shared weight matrix to replace those in all the units in the same\\nlayer. The approach is termed \"semi-tied\" since extra parameters are used to\\nseparately scale each of the shared output values. These extra scaling factors\\nare associated with the network activation functions and result in the use of\\nparameterised sigmoid, hyperbolic tangent, and rectified linear unit functions.\\nSpeech recognition experiments using British English multi-genre broadcast data\\nshowed that using STUs can reduce the calculation and storage cost by a factor\\nof three for highway networks and four for LSTMs, while giving similar word\\nerror rates to the original models.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06513v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06513v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Chao Zhang', 'Philip Woodland'],\n",
       "  'tasks': ['Speech Recognition'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [{'name': 'Highway networks',\n",
       "    'full_name': 'Highway networks',\n",
       "    'description': 'Please enter a description about the method here',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1505.00387v2',\n",
       "    'source_title': 'Highway Networks',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Attention Mechanisms',\n",
       "     'description': '**Attention Mechanisms** are a component used in neural networks to model long-range interaction, for example across a text in NLP. The key idea is to build shortcuts between a context vector and the input, to allow a model to attend to different parts. Below you can find a continuously updating list of attention mechanisms.',\n",
       "     'parent': 'Attention',\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/predicting-citation-counts-with-a-neural',\n",
       "  'arxiv_id': '1806.04641',\n",
       "  'title': 'Predicting Citation Counts with a Neural Network',\n",
       "  'abstract': \"We here describe and present results of a simple neural network that predicts\\nindividual researchers' future citation counts based on a variety of data from\\nthe researchers' past. For publications available on the open access-server\\narXiv.org we find a higher predictability than previous studies.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04641v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04641v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Tobias Mistele', 'Tom Price', 'Sabine Hossenfelder'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/an-ensemble-of-transfer-semi-supervised-and',\n",
       "  'arxiv_id': '1806.06506',\n",
       "  'title': 'An Ensemble of Transfer, Semi-supervised and Supervised Learning Methods for Pathological Heart Sound Classification',\n",
       "  'abstract': 'In this work, we propose an ensemble of classifiers to distinguish between\\nvarious degrees of abnormalities of the heart using Phonocardiogram (PCG)\\nsignals acquired using digital stethoscopes in a clinical setting, for the\\nINTERSPEECH 2018 Computational Paralinguistics (ComParE) Heart Beats\\nSubChallenge. Our primary classification framework constitutes a convolutional\\nneural network with 1D-CNN time-convolution (tConv) layers, which uses features\\ntransferred from a model trained on the 2016 Physionet Heart Sound Database. We\\nalso employ a Representation Learning (RL) approach to generate features in an\\nunsupervised manner using Deep Recurrent Autoencoders and use Support Vector\\nMachine (SVM) and Linear Discriminant Analysis (LDA) classifiers. Finally, we\\nutilize an SVM classifier on a high-dimensional segment-level feature extracted\\nusing various functionals on short-term acoustic features, i.e., Low-Level\\nDescriptors (LLD). An ensemble of the three different approaches provides a\\nrelative improvement of 11.13% compared to our best single sub-system in terms\\nof the Unweighted Average Recall (UAR) performance metric on the evaluation\\ndataset.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06506v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06506v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ahmed Imtiaz Humayun',\n",
       "   'Md. Tauhiduzzaman Khan',\n",
       "   'Shabnam Ghaffarzadegan',\n",
       "   'Zhe Feng',\n",
       "   'Taufiq Hasan'],\n",
       "  'tasks': ['General Classification', 'Representation Learning'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [{'name': 'SVM',\n",
       "    'full_name': 'Support Vector Machine',\n",
       "    'description': 'A **Support Vector Machine**, or **SVM**, is a non-parametric supervised learning model. For non-linear classification and regression, they utilise the kernel trick to map inputs to high-dimensional feature spaces. SVMs construct a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure to the right shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called “support vectors”. \\r\\n\\r\\nSource: [scikit-learn](https://scikit-learn.org/stable/modules/svm.html)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Non-Parametric Classification',\n",
       "     'description': '**Non-Parametric Classification** methods perform classification where we use non-parametric methods to approximate the functional form of the relationship. Below you can find a continuously updating list of non-parametric classification methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-unified-strategy-for-implementing-curiosity',\n",
       "  'arxiv_id': '1806.06505',\n",
       "  'title': 'A unified strategy for implementing curiosity and empowerment driven reinforcement learning',\n",
       "  'abstract': \"Although there are many approaches to implement intrinsically motivated\\nartificial agents, the combined usage of multiple intrinsic drives remains\\nstill a relatively unexplored research area. Specifically, we hypothesize that\\na mechanism capable of quantifying and controlling the evolution of the\\ninformation flow between the agent and the environment could be the fundamental\\ncomponent for implementing a higher degree of autonomy into artificial\\nintelligent agents. This paper propose a unified strategy for implementing two\\nsemantically orthogonal intrinsic motivations: curiosity and empowerment.\\nCuriosity reward informs the agent about the relevance of a recent agent\\naction, whereas empowerment is implemented as the opposite information flow\\nfrom the agent to the environment that quantifies the agent's potential of\\ncontrolling its own future. We show that an additional homeostatic drive is\\nderived from the curiosity reward, which generalizes and enhances the\\ninformation gain of a classical curious/heterostatic reinforcement learning\\nagent. We show how a shared internal model by curiosity and empowerment\\nfacilitates a more efficient training of the empowerment function. Finally, we\\ndiscuss future directions for further leveraging the interplay between these\\ntwo intrinsic rewards.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06505v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06505v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ildefons Magrans de Abril', 'Ryota Kanai'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multi-modal-data-augmentation-for-end-to-end',\n",
       "  'arxiv_id': '1803.10299',\n",
       "  'title': 'Multi-Modal Data Augmentation for End-to-End ASR',\n",
       "  'abstract': 'We present a new end-to-end architecture for automatic speech recognition\\n(ASR) that can be trained using \\\\emph{symbolic} input in addition to the\\ntraditional acoustic input. This architecture utilizes two separate encoders:\\none for acoustic input and another for symbolic input, both sharing the\\nattention and decoder parameters. We call this architecture a multi-modal data\\naugmentation network (MMDA), as it can support multi-modal (acoustic and\\nsymbolic) input and enables seamless mixing of large text datasets with\\nsignificantly smaller transcribed speech corpora during training. We study\\ndifferent ways of transforming large text corpora into a symbolic form suitable\\nfor training our MMDA network. Our best MMDA setup obtains small improvements\\non character error rate (CER), and as much as 7-10\\\\% relative word error rate\\n(WER) improvement over a baseline both with and without an external language\\nmodel.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.10299v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.10299v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Adithya Renduchintala',\n",
       "   'Shuoyang Ding',\n",
       "   'Matthew Wiesner',\n",
       "   'Shinji Watanabe'],\n",
       "  'tasks': ['Automatic Speech Recognition',\n",
       "   'Data Augmentation',\n",
       "   'Language Modelling',\n",
       "   'Speech Recognition'],\n",
       "  'date': '2018-03-27',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deforming-autoencoders-unsupervised',\n",
       "  'arxiv_id': '1806.06503',\n",
       "  'title': 'Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance',\n",
       "  'abstract': \"In this work we introduce Deforming Autoencoders, a generative model for\\nimages that disentangles shape from appearance in an unsupervised manner. As in\\nthe deformable template paradigm, shape is represented as a deformation between\\na canonical coordinate system (`template') and an observed image, while\\nappearance is modeled in `canonical', template, coordinates, thus discarding\\nvariability due to deformations. We introduce novel techniques that allow this\\napproach to be deployed in the setting of autoencoders and show that this\\nmethod can be used for unsupervised group-wise image alignment. We show\\nexperiments with expression morphing in humans, hands, and digits, face\\nmanipulation, such as shape and appearance interpolation, as well as\\nunsupervised landmark localization. A more powerful form of unsupervised\\ndisentangling becomes possible in template coordinates, allowing us to\\nsuccessfully decompose face images into shading and albedo, and further\\nmanipulate face images.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06503v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06503v1.pdf',\n",
       "  'proceeding': 'ECCV 2018 9',\n",
       "  'authors': ['Zhixin Shu',\n",
       "   'Mihir Sahasrabudhe',\n",
       "   'Alp Guler',\n",
       "   'Dimitris Samaras',\n",
       "   'Nikos Paragios',\n",
       "   'Iasonas Kokkinos'],\n",
       "  'tasks': ['Unsupervised Facial Landmark Detection'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['celeba', 'mafl'],\n",
       "  'datasets_used_full': ['CelebA', 'MAFL'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/conditional-affordance-learning-for-driving',\n",
       "  'arxiv_id': '1806.06498',\n",
       "  'title': 'Conditional Affordance Learning for Driving in Urban Environments',\n",
       "  'abstract': 'Most existing approaches to autonomous driving fall into one of two\\ncategories: modular pipelines, that build an extensive model of the\\nenvironment, and imitation learning approaches, that map images directly to\\ncontrol outputs. A recently proposed third paradigm, direct perception, aims to\\ncombine the advantages of both by using a neural network to learn appropriate\\nlow-dimensional intermediate representations. However, existing direct\\nperception approaches are restricted to simple highway situations, lacking the\\nability to navigate intersections, stop at traffic lights or respect speed\\nlimits. In this work, we propose a direct perception approach which maps video\\ninput to intermediate representations suitable for autonomous navigation in\\ncomplex urban environments given high-level directional inputs. Compared to\\nstate-of-the-art reinforcement and conditional imitation learning approaches,\\nwe achieve an improvement of up to 68 % in goal-directed navigation on the\\nchallenging CARLA simulation benchmark. In addition, our approach is the first\\nto handle traffic lights and speed signs by using image-level labels only, as\\nwell as smooth car-following, resulting in a significant reduction of traffic\\naccidents in simulation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06498v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06498v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Axel Sauer', 'Nikolay Savinov', 'Andreas Geiger'],\n",
       "  'tasks': ['Autonomous Driving',\n",
       "   'Autonomous Navigation',\n",
       "   'Imitation Learning'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [{'name': 'CARLA',\n",
       "    'full_name': 'CARLA: An Open Urban Driving Simulator',\n",
       "    'description': 'CARLA is an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. \\r\\n\\r\\nSource: [Dosovitskiy et al.](https://arxiv.org/pdf/1711.03938v1.pdf)\\r\\n\\r\\nImage source: [Dosovitskiy et al.](https://arxiv.org/pdf/1711.03938v1.pdf)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1711.03938v1',\n",
       "    'source_title': 'CARLA: An Open Urban Driving Simulator',\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Video Game Models',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Reinforcement Learning'}}],\n",
       "  'datasets_used_lower': ['carla'],\n",
       "  'datasets_used_full': ['CARLA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/detecting-zero-day-controller-hijacking',\n",
       "  'arxiv_id': '1806.06496',\n",
       "  'title': 'Power-Grid Controller Anomaly Detection with Enhanced Temporal Deep Learning',\n",
       "  'abstract': 'Controllers of security-critical cyber-physical systems, like the power grid, are a very important class of computer systems. Attacks against the control code of a power-grid system, especially zero-day attacks, can be catastrophic. Earlier detection of the anomalies can prevent further damage. However, detecting zero-day attacks is extremely challenging because they have no known code and have unknown behavior. Furthermore, if data collected from the controller is transferred to a server through networks for analysis and detection of anomalous behavior, this creates a very large attack surface and also delays detection. In order to address this problem, we propose Reconstruction Error Distribution (RED) of Hardware Performance Counters (HPCs), and a data-driven defense system based on it. Specifically, we first train a temporal deep learning model, using only normal HPC readings from legitimate processes that run daily in these power-grid systems, to model the normal behavior of the power-grid controller. Then, we run this model using real-time data from commonly available HPCs. We use the proposed RED to enhance the temporal deep learning detection of anomalous behavior, by estimating distribution deviations from the normal behavior with an effective statistical test. Experimental results on a real power-grid controller show that we can detect anomalous behavior with high accuracy (>99.9%), nearly zero false positives and short (<360ms) latency.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06496v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06496v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Zecheng He',\n",
       "   'Aswin Raghavan',\n",
       "   'Guangyuan Hu',\n",
       "   'Sek Chai',\n",
       "   'Ruby Lee'],\n",
       "  'tasks': ['Anomaly Detection'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/women-also-snowboard-overcoming-bias-in-1',\n",
       "  'arxiv_id': '1803.09797',\n",
       "  'title': 'Women also Snowboard: Overcoming Bias in Captioning Models',\n",
       "  'abstract': \"Most machine learning methods are known to capture and exploit biases of the\\ntraining data. While some biases are beneficial for learning, others are\\nharmful. Specifically, image captioning models tend to exaggerate biases\\npresent in training data (e.g., if a word is present in 60% of training\\nsentences, it might be predicted in 70% of sentences at test time). This can\\nlead to incorrect captions in domains where unbiased captions are desired, or\\nrequired, due to over-reliance on the learned prior and image context. In this\\nwork we investigate generation of gender-specific caption words (e.g. man,\\nwoman) based on the person's appearance or the image context. We introduce a\\nnew Equalizer model that ensures equal gender probability when gender evidence\\nis occluded in a scene and confident predictions when gender evidence is\\npresent. The resulting model is forced to look at a person rather than use\\ncontextual cues to make a gender-specific predictions. The losses that comprise\\nour model, the Appearance Confusion Loss and the Confident Loss, are general,\\nand can be added to any description model in order to mitigate impacts of\\nunwanted bias in a description dataset. Our proposed model has lower error than\\nprior work when describing images with people and mentioning their gender and\\nmore closely matches the ground truth ratio of sentences including women to\\nsentences including men. We also show that unlike other approaches, our model\\nis indeed more often looking at people when predicting their gender.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.09797v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.09797v4.pdf',\n",
       "  'proceeding': 'ECCV 2018 9',\n",
       "  'authors': ['Kaylee Burns',\n",
       "   'Lisa Anne Hendricks',\n",
       "   'Kate Saenko',\n",
       "   'Trevor Darrell',\n",
       "   'Anna Rohrbach'],\n",
       "  'tasks': ['Image Captioning'],\n",
       "  'date': '2018-03-26',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['coco'],\n",
       "  'datasets_used_full': ['COCO'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/boosted-density-estimation-remastered',\n",
       "  'arxiv_id': '1803.08178',\n",
       "  'title': 'Boosted Density Estimation Remastered',\n",
       "  'abstract': 'There has recently been a steady increase in the number iterative approaches\\nto density estimation. However, an accompanying burst of formal convergence\\nguarantees has not followed; all results pay the price of heavy assumptions\\nwhich are often unrealistic or hard to check. The Generative Adversarial\\nNetwork (GAN) literature --- seemingly orthogonal to the aforementioned pursuit\\n--- has had the side effect of a renewed interest in variational divergence\\nminimisation (notably $f$-GAN). We show that by introducing a weak learning\\nassumption (in the sense of the classical boosting framework) we are able to\\nimport some recent results from the GAN literature to develop an iterative\\nboosted density estimation algorithm, including formal convergence results with\\nrates, that does not suffer the shortcomings other approaches. We show that the\\ndensity fit is an exponential family, and as part of our analysis obtain an\\nimproved variational characterisation of $f$-GAN.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.08178v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.08178v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Zac Cranko', 'Richard Nock'],\n",
       "  'tasks': ['Density Estimation'],\n",
       "  'date': '2018-03-22',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/disturbance-grassmann-kernels-for-subspace',\n",
       "  'arxiv_id': '1802.03517',\n",
       "  'title': 'Disturbance Grassmann Kernels for Subspace-Based Learning',\n",
       "  'abstract': 'In this paper, we focus on subspace-based learning problems, where data\\nelements are linear subspaces instead of vectors. To handle this kind of data,\\nGrassmann kernels were proposed to measure the space structure and used with\\nclassifiers, e.g., Support Vector Machines (SVMs). However, the existing\\ndiscriminative algorithms mostly ignore the instability of subspaces, which\\nwould cause the classifiers misled by disturbed instances. Thus we propose\\nconsidering all potential disturbance of subspaces in learning processes to\\nobtain more robust classifiers. Firstly, we derive the dual optimization of\\nlinear classifiers with disturbance subject to a known distribution, resulting\\nin a new kernel, Disturbance Grassmann (DG) kernel. Secondly, we research into\\ntwo kinds of disturbance, relevant to the subspace matrix and singular values\\nof bases, with which we extend the Projection kernel on Grassmann manifolds to\\ntwo new kernels. Experiments on action data indicate that the proposed kernels\\nperform better compared to state-of-the-art subspace-based methods, even in a\\nworse environment.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.03517v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.03517v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Junyuan Hong', 'Huanhuan Chen', 'Feng Lin'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/entity-aware-language-model-as-an',\n",
       "  'arxiv_id': '1803.04291',\n",
       "  'title': 'Entity-Aware Language Model as an Unsupervised Reranker',\n",
       "  'abstract': 'In language modeling, it is difficult to incorporate entity relationships\\nfrom a knowledge-base. One solution is to use a reranker trained with global\\nfeatures, in which global features are derived from n-best lists. However,\\ntraining such a reranker requires manually annotated n-best lists, which is\\nexpensive to obtain. We propose a method based on the contrastive estimation\\nmethod that alleviates the need for such data. Experiments in the music domain\\ndemonstrate that global features, as well as features extracted from an\\nexternal knowledge-base, can be incorporated into our reranker. Our final\\nmodel, a simple ensemble of a language model and reranker, achieves a 0.44\\\\%\\nabsolute word error rate improvement over an LSTM language model on the blind\\ntest data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.04291v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.04291v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mohammad Sadegh Rasooli', 'Sarangarajan Parthasarathy'],\n",
       "  'tasks': ['Language Modelling'],\n",
       "  'date': '2018-03-12',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/co-training-embeddings-of-knowledge-graphs',\n",
       "  'arxiv_id': '1806.06478',\n",
       "  'title': 'Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-lingual Entity Alignment',\n",
       "  'abstract': 'Multilingual knowledge graph (KG) embeddings provide latent semantic\\nrepresentations of entities and structured knowledge with cross-lingual\\ninferences, which benefit various knowledge-driven cross-lingual NLP tasks.\\nHowever, precisely learning such cross-lingual inferences is usually hindered\\nby the low coverage of entity alignment in many KGs. Since many multilingual\\nKGs also provide literal descriptions of entities, in this paper, we introduce\\nan embedding-based approach which leverages a weakly aligned multilingual KG\\nfor semi-supervised cross-lingual learning using entity descriptions. Our\\napproach performs co-training of two embedding models, i.e. a multilingual KG\\nembedding model and a multilingual literal description embedding model. The\\nmodels are trained on a large Wikipedia-based trilingual dataset where most\\nentity alignment is unknown to training. Experimental results show that the\\nperformance of the proposed approach on the entity alignment task improves at\\neach iteration of co-training, and eventually reaches a stage at which it\\nsignificantly surpasses previous approaches. We also show that our approach has\\npromising abilities for zero-shot entity alignment, and cross-lingual KG\\ncompletion.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06478v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06478v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Muhao Chen',\n",
       "   'Yingtao Tian',\n",
       "   'Kai-Wei Chang',\n",
       "   'Steven Skiena',\n",
       "   'Carlo Zaniolo'],\n",
       "  'tasks': ['Entity Alignment', 'Knowledge Graphs'],\n",
       "  'date': '2018-06-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/video-salient-object-detection-using',\n",
       "  'arxiv_id': '1708.01447',\n",
       "  'title': 'Video Salient Object Detection Using Spatiotemporal Deep Features',\n",
       "  'abstract': \"This paper presents a method for detecting salient objects in videos where\\ntemporal information in addition to spatial information is fully taken into\\naccount. Following recent reports on the advantage of deep features over\\nconventional hand-crafted features, we propose a new set of SpatioTemporal Deep\\n(STD) features that utilize local and global contexts over frames. We also\\npropose new SpatioTemporal Conditional Random Field (STCRF) to compute saliency\\nfrom STD features. STCRF is our extension of CRF to the temporal domain and\\ndescribes the relationships among neighboring regions both in a frame and over\\nframes. STCRF leads to temporally consistent saliency maps over frames,\\ncontributing to the accurate detection of salient objects' boundaries and noise\\nreduction during detection. Our proposed method first segments an input video\\ninto multiple scales and then computes a saliency map at each scale level using\\nSTD features with STCRF. The final saliency map is computed by fusing saliency\\nmaps at different scale levels. Our experiments, using publicly available\\nbenchmark datasets, confirm that the proposed method significantly outperforms\\nstate-of-the-art methods. We also applied our saliency computation to the video\\nobject segmentation task, showing that our method outperforms existing video\\nobject segmentation methods.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1708.01447v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1708.01447v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Trung-Nghia Le', 'Akihiro Sugimoto'],\n",
       "  'tasks': ['Object Detection',\n",
       "   'RGB Salient Object Detection',\n",
       "   'Salient Object Detection',\n",
       "   'Semantic Segmentation',\n",
       "   'Video Object Segmentation',\n",
       "   'Video Salient Object Detection',\n",
       "   'Video Semantic Segmentation'],\n",
       "  'date': '2017-08-04',\n",
       "  'methods': [{'name': 'CRF',\n",
       "    'full_name': 'Conditional Random Field',\n",
       "    'description': '**Conditional Random Fields** or **CRFs** are a type of probabilistic graph model that take neighboring sample context into account for tasks like classification. Prediction is modeled as a graphical model, which implements dependencies between the predictions. Graph choice depends on the application, for example linear chain CRFs are popular in natural language processing, whereas in image-based tasks, the graph would connect to neighboring locations in an image to enforce that they have similar predictions.\\r\\n\\r\\nImage Credit: [Charles Sutton and Andrew McCallum, An Introduction to Conditional Random Fields](https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Structured Prediction',\n",
       "     'description': '**Structured Prediction** methods deal with structured outputs with multiple interdependent outputs. Below you can find a continuously updating list of structured prediction methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['davis'],\n",
       "  'datasets_used_full': ['DAVIS'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/reinforcement-learning-in-rich-observation',\n",
       "  'arxiv_id': '1611.03907',\n",
       "  'title': 'Reinforcement Learning in Rich-Observation MDPs using Spectral Methods',\n",
       "  'abstract': 'Reinforcement learning (RL) in Markov decision processes (MDPs) with large\\nstate spaces is a challenging problem. The performance of standard RL\\nalgorithms degrades drastically with the dimensionality of state space.\\nHowever, in practice, these large MDPs typically incorporate a latent or hidden\\nlow-dimensional structure. In this paper, we study the setting of\\nrich-observation Markov decision processes (ROMDP), where there are a small\\nnumber of hidden states which possess an injective mapping to the observation\\nstates. In other words, every observation state is generated through a single\\nhidden state, and this mapping is unknown a priori. We introduce a spectral\\ndecomposition method that consistently learns this mapping, and more\\nimportantly, achieves it with low regret. The estimated mapping is integrated\\ninto an optimistic RL algorithm (UCRL), which operates on the estimated hidden\\nspace. We derive finite-time regret bounds for our algorithm with a weak\\ndependence on the dimensionality of the observed space. In fact, our algorithm\\nasymptotically achieves the same average regret as the oracle UCRL algorithm,\\nwhich has the knowledge of the mapping from hidden to observed spaces. Thus, we\\nderive an efficient spectral RL algorithm for ROMDPs.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1611.03907v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1611.03907v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kamyar Azizzadenesheli',\n",
       "   'Alessandro Lazaric',\n",
       "   'Animashree Anandkumar'],\n",
       "  'tasks': [],\n",
       "  'date': '2016-11-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/breaking-transferability-of-adversarial',\n",
       "  'arxiv_id': '1805.04613',\n",
       "  'title': 'Breaking Transferability of Adversarial Samples with Randomness',\n",
       "  'abstract': \"We investigate the role of transferability of adversarial attacks in the\\nobserved vulnerabilities of Deep Neural Networks (DNNs). We demonstrate that\\nintroducing randomness to the DNN models is sufficient to defeat adversarial\\nattacks, given that the adversary does not have an unlimited attack budget.\\nInstead of making one specific DNN model robust to perfect knowledge attacks\\n(a.k.a, white box attacks), creating randomness within an army of DNNs\\ncompletely eliminates the possibility of perfect knowledge acquisition,\\nresulting in a significantly more robust DNN ensemble against the strongest\\nform of attacks. We also show that when the adversary has an unlimited budget\\nof data perturbation, all defensive techniques would eventually break down as\\nthe budget increases. Therefore, it is important to understand the game saddle\\npoint where the adversary would not further pursue this endeavor.\\n  Furthermore, we explore the relationship between attack severity and decision\\nboundary robustness in the version space. We empirically demonstrate that by\\nsimply adding a small Gaussian random noise to the learned weights, a DNN model\\ncan increase its resilience to adversarial attacks by as much as 74.2%. More\\nimportantly, we show that by randomly activating/revealing a model from a pool\\nof pre-trained DNNs at each query request, we can put a tremendous strain on\\nthe adversary's attack strategies. We compare our randomization techniques to\\nthe Ensemble Adversarial Training technique and show that our randomization\\ntechniques are superior under different attack budget constraints.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.04613v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.04613v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yan Zhou', 'Murat Kantarcioglu', 'Bowei Xi'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-rbo-dataset-of-articulated-objects-and',\n",
       "  'arxiv_id': '1806.06465',\n",
       "  'title': 'The RBO Dataset of Articulated Objects and Interactions',\n",
       "  'abstract': 'We present a dataset with models of 14 articulated objects commonly found in\\nhuman environments and with RGB-D video sequences and wrenches recorded of\\nhuman interactions with them. The 358 interaction sequences total 67 minutes of\\nhuman manipulation under varying experimental conditions (type of interaction,\\nlighting, perspective, and background). Each interaction with an object is\\nannotated with the ground truth poses of its rigid parts and the kinematic\\nstate obtained by a motion capture system. For a subset of 78 sequences (25\\nminutes), we also measured the interaction wrenches. The object models contain\\ntextured three-dimensional triangle meshes of each link and their motion\\nconstraints. We provide Python scripts to download and visualize the data. The\\ndata is available at https://tu-rbo.github.io/articulated-objects/ and hosted\\nat https://zenodo.org/record/1036660/.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06465v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06465v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Roberto Martín-Martín', 'Clemens Eppner', 'Oliver Brock'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': ['the-rbo-dataset-of-articulated-objects-and'],\n",
       "  'datasets_introduced_full': ['The RBO Dataset of Articulated Objects and Interactions']},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-policy-representations-in-multiagent',\n",
       "  'arxiv_id': '1806.06464',\n",
       "  'title': 'Learning Policy Representations in Multiagent Systems',\n",
       "  'abstract': 'Modeling agent behavior is central to understanding the emergence of complex\\nphenomena in multiagent systems. Prior work in agent modeling has largely been\\ntask-specific and driven by hand-engineering domain-specific prior knowledge.\\nWe propose a general learning framework for modeling agent behavior in any\\nmultiagent system using only a handful of interaction data. Our framework casts\\nagent modeling as a representation learning problem. Consequently, we construct\\na novel objective inspired by imitation learning and agent identification and\\ndesign an algorithm for unsupervised learning of representations of agent\\npolicies. We demonstrate empirically the utility of the proposed framework in\\n(i) a challenging high-dimensional competitive environment for continuous\\ncontrol and (ii) a cooperative environment for communication, on supervised\\npredictive tasks, unsupervised clustering, and policy optimization using deep\\nreinforcement learning.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06464v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06464v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Aditya Grover',\n",
       "   'Maruan Al-Shedivat',\n",
       "   'Jayesh K. Gupta',\n",
       "   'Yura Burda',\n",
       "   'Harrison Edwards'],\n",
       "  'tasks': ['Continuous Control',\n",
       "   'Imitation Learning',\n",
       "   'Representation Learning'],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/sub-gaussian-estimators-of-the-mean-of-a-1',\n",
       "  'arxiv_id': '1605.07129',\n",
       "  'title': 'Sub-Gaussian estimators of the mean of a random matrix with heavy-tailed entries',\n",
       "  'abstract': 'Estimation of the covariance matrix has attracted a lot of attention of the\\nstatistical research community over the years, partially due to important\\napplications such as Principal Component Analysis. However, frequently used\\nempirical covariance estimator (and its modifications) is very sensitive to\\noutliers in the data. As P. J. Huber wrote in 1964, \"...This raises a question\\nwhich could have been asked already by Gauss, but which was, as far as I know,\\nonly raised a few years ago (notably by Tukey): what happens if the true\\ndistribution deviates slightly from the assumed normal one? As is now well\\nknown, the sample mean then may have a catastrophically bad performance...\"\\nMotivated by this question, we develop a new estimator of the (element-wise)\\nmean of a random matrix, which includes covariance estimation problem as a\\nspecial case. Assuming that the entries of a matrix possess only finite second\\nmoment, this new estimator admits sub-Gaussian or sub-exponential concentration\\naround the unknown mean in the operator norm. We will explain the key ideas\\nbehind our construction, as well as applications to covariance estimation and\\nmatrix completion problems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1605.07129v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1605.07129v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Stanislav Minsker'],\n",
       "  'tasks': ['Matrix Completion'],\n",
       "  'date': '2016-05-23',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fast-convex-pruning-of-deep-neural-networks',\n",
       "  'arxiv_id': '1806.06457',\n",
       "  'title': 'Fast Convex Pruning of Deep Neural Networks',\n",
       "  'abstract': 'We develop a fast, tractable technique called Net-Trim for simplifying a\\ntrained neural network. The method is a convex post-processing module, which\\nprunes (sparsifies) a trained network layer by layer, while preserving the\\ninternal responses. We present a comprehensive analysis of Net-Trim from both\\nthe algorithmic and sample complexity standpoints, centered on a fast, scalable\\nconvex optimization program. Our analysis includes consistency results between\\nthe initial and retrained models before and after Net-Trim application and\\nguarantees on the number of training samples needed to discover a network that\\ncan be expressed using a certain number of nonzero terms. Specifically, if\\nthere is a set of weights that uses at most $s$ terms that can re-create the\\nlayer outputs from the layer inputs, we can find these weights from\\n$\\\\mathcal{O}(s\\\\log N/s)$ samples, where $N$ is the input size. These\\ntheoretical results are similar to those for sparse regression using the Lasso,\\nand our analysis uses some of the same recently-developed tools (namely recent\\nresults on the concentration of measure and convex analysis). Finally, we\\npropose an algorithmic framework based on the alternating direction method of\\nmultipliers (ADMM), which allows a fast and simple implementation of Net-Trim\\nfor network pruning and compression.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06457v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06457v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alireza Aghasi', 'Afshin Abdi', 'Justin Romberg'],\n",
       "  'tasks': ['Network Pruning'],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist'],\n",
       "  'datasets_used_full': ['MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/cross-modality-image-synthesis-from-unpaired',\n",
       "  'arxiv_id': '1803.06629',\n",
       "  'title': 'Cross-modality image synthesis from unpaired data using CycleGAN: Effects of gradient consistency loss and training data size',\n",
       "  'abstract': 'CT is commonly used in orthopedic procedures. MRI is used along with CT to\\nidentify muscle structures and diagnose osteonecrosis due to its superior soft\\ntissue contrast. However, MRI has poor contrast for bone structures. Clearly,\\nit would be helpful if a corresponding CT were available, as bone boundaries\\nare more clearly seen and CT has standardized (i.e., Hounsfield) units.\\nTherefore, we aim at MR-to-CT synthesis. The CycleGAN was successfully applied\\nto unpaired CT and MR images of the head, these images do not have as much\\nvariation of intensity pairs as do images in the pelvic region due to the\\npresence of joints and muscles. In this paper, we extended the CycleGAN\\napproach by adding the gradient consistency loss to improve the accuracy at the\\nboundaries. We conducted two experiments. To evaluate image synthesis, we\\ninvestigated dependency of image synthesis accuracy on 1) the number of\\ntraining data and 2) the gradient consistency loss. To demonstrate the\\napplicability of our method, we also investigated a segmentation accuracy on\\nsynthesized images.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.06629v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.06629v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yuta Hiasa',\n",
       "   'Yoshito Otake',\n",
       "   'Masaki Takao',\n",
       "   'Takumi Matsuoka',\n",
       "   'Kazuma Takashima',\n",
       "   'Jerry L. Prince',\n",
       "   'Nobuhiko Sugano',\n",
       "   'Yoshinobu Sato'],\n",
       "  'tasks': ['Image Generation'],\n",
       "  'date': '2018-03-18',\n",
       "  'methods': [{'name': 'Batch Normalization',\n",
       "    'full_name': 'Batch Normalization',\n",
       "    'description': '**Batch Normalization** aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for [Dropout](https://paperswithcode.com/method/dropout).\\r\\n\\r\\nWe apply a batch normalization layer as follows for a minibatch $\\\\mathcal{B}$:\\r\\n\\r\\n$$ \\\\mu\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}x\\\\_{i} $$\\r\\n\\r\\n$$ \\\\sigma^{2}\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}\\\\left(x\\\\_{i}-\\\\mu\\\\_{\\\\mathcal{B}}\\\\right)^{2} $$\\r\\n\\r\\n$$ \\\\hat{x}\\\\_{i} = \\\\frac{x\\\\_{i} - \\\\mu\\\\_{\\\\mathcal{B}}}{\\\\sqrt{\\\\sigma^{2}\\\\_{\\\\mathcal{B}}+\\\\epsilon}} $$\\r\\n\\r\\n$$ y\\\\_{i} = \\\\gamma\\\\hat{x}\\\\_{i} + \\\\beta = \\\\text{BN}\\\\_{\\\\gamma, \\\\beta}\\\\left(x\\\\_{i}\\\\right) $$\\r\\n\\r\\nWhere $\\\\gamma$ and $\\\\beta$ are learnable parameters.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.03167v3',\n",
       "    'source_title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/36f91261099b00194922bd93ed1286fe1c199724/jax/experimental/stax.py#L116',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Residual Connection',\n",
       "    'full_name': 'Residual Connection',\n",
       "    'description': '**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'PatchGAN',\n",
       "    'full_name': 'PatchGAN',\n",
       "    'description': '**PatchGAN** is a type of discriminator for generative adversarial networks which only penalizes structure at the scale of local image patches. The PatchGAN discriminator tries to classify if each $N \\\\times N$ patch in an image is real or fake. This discriminator is run convolutionally across the image, averaging all responses to provide the ultimate output of $D$. Such a discriminator effectively models the image as a Markov random field, assuming independence between pixels separated by more than a patch diameter. It can be understood as a type of texture/style loss.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1611.07004v3',\n",
       "    'source_title': 'Image-to-Image Translation with Conditional Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/znxlwm/pytorch-pix2pix/blob/3059f2af53324e77089bbcfc31279f01a38c40b8/network.py#L104',\n",
       "    'main_collection': {'name': 'Discriminators',\n",
       "     'description': '**Discriminators** are a type of module used in architectures such as generative adversarial networks to discriminate between real and generated samples. Below you can find a continuously updating list of discriminators.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Residual Block',\n",
       "    'full_name': 'Residual Block',\n",
       "    'description': \"**Residual Blocks** are skip-connection blocks that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. They were introduced as part of the [ResNet](https://paperswithcode.com/method/resnet) architecture.\\r\\n \\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$. The $\\\\mathcal{F}({x})$ acts like a residual, hence the name 'residual block'.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. Having skip connections allows the network to more easily learn identity-like mappings.\\r\\n\\r\\nNote that in practice, [Bottleneck Residual Blocks](https://paperswithcode.com/method/bottleneck-residual-block) are used for deeper ResNets, such as ResNet-50 and ResNet-101, as these bottleneck blocks are less computationally intensive.\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L35',\n",
       "    'main_collection': {'name': 'Skip Connection Blocks',\n",
       "     'description': \"**Skip Connection Blocks** are building blocks for neural networks that feature skip connections. These skip connections 'skip' some layers allowing gradients to better flow through the network. Below you will find a continuously updating list of skip connection blocks:\",\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Instance Normalization',\n",
       "    'full_name': 'Instance Normalization',\n",
       "    'description': '**Instance Normalization** (also known as contrast normalization) is a normalization layer where:\\r\\n\\r\\n$$\\r\\n    y_{tijk} =  \\\\frac{x_{tijk} - \\\\mu_{ti}}{\\\\sqrt{\\\\sigma_{ti}^2 + \\\\epsilon}},\\r\\n    \\\\quad\\r\\n    \\\\mu_{ti} = \\\\frac{1}{HW}\\\\sum_{l=1}^W \\\\sum_{m=1}^H x_{tilm},\\r\\n    \\\\quad\\r\\n    \\\\sigma_{ti}^2 = \\\\frac{1}{HW}\\\\sum_{l=1}^W \\\\sum_{m=1}^H (x_{tilm} - mu_{ti})^2.\\r\\n$$\\r\\n\\r\\nThis prevents instance-specific mean and covariance shift simplifying the learning process. Intuitively, the normalization process allows to remove instance-specific contrast information from the content image in a task like image stylization, which simplifies generation.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1607.08022v3',\n",
       "    'source_title': 'Instance Normalization: The Missing Ingredient for Fast Stylization',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/1c5c289b6218eb1026dcb5fd9738231401cfccea/torch/nn/modules/instancenorm.py#L141',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Leaky ReLU',\n",
       "    'full_name': 'Leaky ReLU',\n",
       "    'description': '**Leaky Rectified Linear Unit**, or **Leaky ReLU**, is a type of activation function based on a [ReLU](https://paperswithcode.com/method/relu), but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training. This type of activation function is popular in tasks where we we may suffer from sparse gradients, for example training generative adversarial networks.',\n",
       "    'introduced_year': 2014,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L649',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'GAN Least Squares Loss',\n",
       "    'full_name': 'GAN Least Squares Loss',\n",
       "    'description': '**GAN Least Squares Loss** is a least squares loss function for generative adversarial networks. Minimizing this objective function is equivalent to minimizing the Pearson $\\\\chi^{2}$ divergence. The objective function (here for [LSGAN](https://paperswithcode.com/method/lsgan)) can be defined as:\\r\\n\\r\\n$$ \\\\min\\\\_{D}V\\\\_{LS}\\\\left(D\\\\right) = \\\\frac{1}{2}\\\\mathbb{E}\\\\_{\\\\mathbf{x} \\\\sim p\\\\_{data}\\\\left(\\\\mathbf{x}\\\\right)}\\\\left[\\\\left(D\\\\left(\\\\mathbf{x}\\\\right) - b\\\\right)^{2}\\\\right] + \\\\frac{1}{2}\\\\mathbb{E}\\\\_{\\\\mathbf{z}\\\\sim p\\\\_{data}\\\\left(\\\\mathbf{z}\\\\right)}\\\\left[\\\\left(D\\\\left(G\\\\left(\\\\mathbf{z}\\\\right)\\\\right) - a\\\\right)^{2}\\\\right] $$\\r\\n\\r\\n$$ \\\\min\\\\_{G}V\\\\_{LS}\\\\left(G\\\\right) = \\\\frac{1}{2}\\\\mathbb{E}\\\\_{\\\\mathbf{z} \\\\sim p\\\\_{\\\\mathbf{z}}\\\\left(\\\\mathbf{z}\\\\right)}\\\\left[\\\\left(D\\\\left(G\\\\left(\\\\mathbf{z}\\\\right)\\\\right) - c\\\\right)^{2}\\\\right] $$\\r\\n\\r\\nwhere $a$ and $b$ are the labels for fake data and real data and $c$ denotes the value that $G$ wants $D$ to believe for fake data.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1611.04076v3',\n",
       "    'source_title': 'Least Squares Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/a163b82beff3d01688d8315a3fd39080400e7c01/implementations/lsgan/lsgan.py#L102',\n",
       "    'main_collection': {'name': 'Loss Functions',\n",
       "     'description': '**Loss Functions** are used to frame the problem to be optimized within deep learning. Below you will find a continuously updating list of (specialized) loss functions for neutral networks.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Cycle Consistency Loss',\n",
       "    'full_name': 'Cycle Consistency Loss',\n",
       "    'description': '**Cycle Consistency Loss** is a type of loss used for generative adversarial networks that performs unpaired image-to-image translation. It was introduced with the [CycleGAN](https://paperswithcode.com/method/cyclegan) architecture. For two domains $X$ and $Y$, we want to learn a mapping $G : X \\\\rightarrow Y$ and $F: Y \\\\rightarrow X$. We want to enforce the intuition that these mappings should be reverses of each other and that both mappings should be bijections. Cycle Consistency Loss encourages $F\\\\left(G\\\\left(x\\\\right)\\\\right) \\\\approx x$ and $G\\\\left(F\\\\left(y\\\\right)\\\\right) \\\\approx y$.  It reduces the space of possible mapping functions by enforcing forward and backwards consistency:\\r\\n\\r\\n$$ \\\\mathcal{L}\\\\_{cyc}\\\\left(G, F\\\\right) = \\\\mathbb{E}\\\\_{x \\\\sim p\\\\_{data}\\\\left(x\\\\right)}\\\\left[||F\\\\left(G\\\\left(x\\\\right)\\\\right) - x||\\\\_{1}\\\\right] + \\\\mathbb{E}\\\\_{y \\\\sim p\\\\_{data}\\\\left(y\\\\right)}\\\\left[||G\\\\left(F\\\\left(y\\\\right)\\\\right) - y||\\\\_{1}\\\\right] $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1703.10593v7',\n",
       "    'source_title': 'Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/f5834b3ed339ec268f40cf56928234eed8dfeb92/models/cycle_gan_model.py#L172',\n",
       "    'main_collection': {'name': 'Loss Functions',\n",
       "     'description': '**Loss Functions** are used to frame the problem to be optimized within deep learning. Below you will find a continuously updating list of (specialized) loss functions for neutral networks.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'CycleGAN',\n",
       "    'full_name': 'CycleGAN',\n",
       "    'description': '**CycleGAN**, or **Cycle-Consistent GAN**, is a type of generative adversarial network for unpaired image-to-image translation. For two domains $X$ and $Y$, CycleGAN learns a mapping $G : X \\\\rightarrow Y$ and $F: Y \\\\rightarrow X$. The novelty lies in trying to enforce the intuition that these mappings should be reverses of each other and that both mappings should be bijections. This is achieved through a [cycle consistency loss](https://paperswithcode.com/method/cycle-consistency-loss) that encourages $F\\\\left(G\\\\left(x\\\\right)\\\\right) \\\\approx x$ and $G\\\\left(Y\\\\left(y\\\\right)\\\\right) \\\\approx y$. Combining this loss with the adversarial losses on $X$ and $Y$ yields the full objective for unpaired image-to-image translation.\\r\\n\\r\\nFor the mapping $G : X \\\\rightarrow Y$ and its discriminator $D\\\\_{Y}$ we have the objective:\\r\\n\\r\\n$$ \\\\mathcal{L}\\\\_{GAN}\\\\left(G, D\\\\_{Y}, X, Y\\\\right) =\\\\mathbb{E}\\\\_{y \\\\sim p\\\\_{data}\\\\left(y\\\\right)}\\\\left[\\\\log D\\\\_{Y}\\\\left(y\\\\right)\\\\right] + \\\\mathbb{E}\\\\_{x \\\\sim p\\\\_{data}\\\\left(x\\\\right)}\\\\left[log(1 − D\\\\_{Y}\\\\left(G\\\\left(x\\\\right)\\\\right)\\\\right] $$\\r\\n\\r\\nwhere $G$ tries to generate images $G\\\\left(x\\\\right)$ that look similar to images from domain $Y$, while $D\\\\_{Y}$ tries to discriminate between translated samples $G\\\\left(x\\\\right)$ and real samples $y$. A similar loss is postulated for the mapping $F: Y \\\\rightarrow X$ and its discriminator $D\\\\_{X}$.\\r\\n\\r\\nThe Cycle Consistency Loss reduces the space of possible mapping functions by enforcing forward and backwards consistency:\\r\\n\\r\\n$$ \\\\mathcal{L}\\\\_{cyc}\\\\left(G, F\\\\right) = \\\\mathbb{E}\\\\_{x \\\\sim p\\\\_{data}\\\\left(x\\\\right)}\\\\left[||F\\\\left(G\\\\left(x\\\\right)\\\\right) - x||\\\\_{1}\\\\right] + \\\\mathbb{E}\\\\_{y \\\\sim p\\\\_{data}\\\\left(y\\\\right)}\\\\left[||G\\\\left(F\\\\left(y\\\\right)\\\\right) - y||\\\\_{1}\\\\right] $$\\r\\n\\r\\nThe full objective is:\\r\\n\\r\\n$$ \\\\mathcal{L}\\\\_{GAN}\\\\left(G, F, D\\\\_{X}, D\\\\_{Y}\\\\right) = \\\\mathcal{L}\\\\_{GAN}\\\\left(G, D\\\\_{Y}, X, Y\\\\right) + \\\\mathcal{L}\\\\_{GAN}\\\\left(F, D\\\\_{X}, X, Y\\\\right) + \\\\lambda\\\\mathcal{L}\\\\_{cyc}\\\\left(G, F\\\\right) $$\\r\\n\\r\\nWhere we aim to solve:\\r\\n\\r\\n$$ G^{\\\\*}, F^{\\\\*} = \\\\arg \\\\min\\\\_{G, F} \\\\min\\\\_{D\\\\_{X}, D\\\\_{Y}} \\\\mathcal{L}\\\\_{GAN}\\\\left(G, F, D\\\\_{X}, D\\\\_{Y}\\\\right) $$\\r\\n\\r\\nFor the original architecture the authors use:\\r\\n\\r\\n-  two stride-2 convolutions, several residual blocks, and two fractionally strided convolutions with stride $\\\\frac{1}{2}$.\\r\\n- [instance normalization](https://paperswithcode.com/method/instance-normalization)\\r\\n- PatchGANs for the discriminator\\r\\n- Least Square Loss for the [GAN](https://paperswithcode.com/method/gan) objectives.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1703.10593v7',\n",
       "    'source_title': 'Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/9e6fff7b7d5215a38be3cac074ca7087041bea0d/models/cycle_gan_model.py#L8',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/self-attentive-neural-collaborative-filtering',\n",
       "  'arxiv_id': '1806.06446',\n",
       "  'title': 'Self-Attentive Neural Collaborative Filtering',\n",
       "  'abstract': 'This paper has been withdrawn as we discovered a bug in our tensorflow\\nimplementation that involved accidental mixing of vectors across batches. This\\nlead to different inference results given different batch sizes which is\\ncompletely strange. The performance scores still remain the same but we\\nconcluded that it was not the self-attention that contributed to the\\nperformance. We are withdrawing the paper because this renders the main claim\\nof the paper false. Thanks to Guan Xinyu from NUS for discovering this issue in\\nour previously open source code.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06446v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06446v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yi Tay', 'Shuai Zhang', 'Luu Anh Tuan', 'Siu Cheung Hui'],\n",
       "  'tasks': ['Collaborative Filtering'],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/ncrf-an-open-source-neural-sequence-labeling',\n",
       "  'arxiv_id': '1806.05626',\n",
       "  'title': 'NCRF++: An Open-source Neural Sequence Labeling Toolkit',\n",
       "  'abstract': 'This paper describes NCRF++, a toolkit for neural sequence labeling. NCRF++\\nis designed for quick implementation of different neural sequence labeling\\nmodels with a CRF inference layer. It provides users with an inference for\\nbuilding the custom model structure through configuration file with flexible\\nneural feature design and utilization. Built on PyTorch, the core operations\\nare calculated in batch, making the toolkit efficient with the acceleration of\\nGPU. It also includes the implementations of most state-of-the-art neural\\nsequence labeling models such as LSTM-CRF, facilitating reproducing and\\nrefinement on those methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05626v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05626v2.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Jie Yang', 'Yue Zhang'],\n",
       "  'tasks': ['Chunking', 'Named Entity Recognition', 'Part-Of-Speech Tagging'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'CRF',\n",
       "    'full_name': 'Conditional Random Field',\n",
       "    'description': '**Conditional Random Fields** or **CRFs** are a type of probabilistic graph model that take neighboring sample context into account for tasks like classification. Prediction is modeled as a graphical model, which implements dependencies between the predictions. Graph choice depends on the application, for example linear chain CRFs are popular in natural language processing, whereas in image-based tasks, the graph would connect to neighboring locations in an image to enforce that they have similar predictions.\\r\\n\\r\\nImage Credit: [Charles Sutton and Andrew McCallum, An Introduction to Conditional Random Fields](https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Structured Prediction',\n",
       "     'description': '**Structured Prediction** methods deal with structured outputs with multiple interdependent outputs. Below you can find a continuously updating list of structured prediction methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['penn-treebank', 'conll-2003'],\n",
       "  'datasets_used_full': ['Penn Treebank', 'CoNLL-2003'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/predicting-switching-graph-labelings-with',\n",
       "  'arxiv_id': '1806.06439',\n",
       "  'title': 'Online Prediction of Switching Graph Labelings with Cluster Specialists',\n",
       "  'abstract': 'We address the problem of predicting the labeling of a graph in an online setting when the labeling is changing over time. We present an algorithm based on a specialist approach; we develop the machinery of cluster specialists which probabilistically exploits the cluster structure in the graph. Our algorithm has two variants, one of which surprisingly only requires $\\\\mathcal{O}(\\\\log n)$ time on any trial $t$ on an $n$-vertex graph, an exponential speed up over existing methods. We prove switching mistake-bound guarantees for both variants of our algorithm. Furthermore these mistake bounds smoothly vary with the magnitude of the change between successive labelings. We perform experiments on Chicago Divvy Bicycle Sharing data and show that our algorithms significantly outperform an existing algorithm (a kernelized Perceptron) as well as several natural benchmarks.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06439v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06439v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2019 12',\n",
       "  'authors': ['Mark Herbster', 'James Robinson'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/compressed-sensing-with-deep-image-prior-and',\n",
       "  'arxiv_id': '1806.06438',\n",
       "  'title': 'Compressed Sensing with Deep Image Prior and Learned Regularization',\n",
       "  'abstract': 'We propose a novel method for compressed sensing recovery using untrained deep generative models. Our method is based on the recently proposed Deep Image Prior (DIP), wherein the convolutional weights of the network are optimized to match the observed measurements. We show that this approach can be applied to solve any differentiable linear inverse problem, outperforming previous unlearned methods. Unlike various learned approaches based on generative models, our method does not require pre-training over large datasets. We further introduce a novel learned regularization technique, which incorporates prior information on the network weights. This reduces reconstruction error, especially for noisy measurements. Finally, we prove that, using the DIP optimization approach, moderately overparameterized single-layer networks can perfectly fit any signal despite the non-convex nature of the fitting problem. This theoretical result provides justification for early stopping.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06438v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06438v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Dave Van Veen',\n",
       "   'Ajil Jalal',\n",
       "   'Mahdi Soltanolkotabi',\n",
       "   'Eric Price',\n",
       "   'Sriram Vishwanath',\n",
       "   'Alexandros G. Dimakis'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist', 'celeba'],\n",
       "  'datasets_used_full': ['MNIST', 'CelebA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/subspace-embedding-and-linear-regression-with',\n",
       "  'arxiv_id': '1806.06430',\n",
       "  'title': 'Subspace Embedding and Linear Regression with Orlicz Norm',\n",
       "  'abstract': 'We consider a generalization of the classic linear regression problem to the\\ncase when the loss is an Orlicz norm. An Orlicz norm is parameterized by a\\nnon-negative convex function $G:\\\\mathbb{R}_+\\\\rightarrow\\\\mathbb{R}_+$ with\\n$G(0)=0$: the Orlicz norm of a vector $x\\\\in\\\\mathbb{R}^n$ is defined as $\\n\\\\|x\\\\|_G=\\\\inf\\\\left\\\\{\\\\alpha>0\\\\large\\\\mid\\\\sum_{i=1}^n G(|x_i|/\\\\alpha)\\\\leq\\n1\\\\right\\\\}. $ We consider the cases where the function $G(\\\\cdot)$ grows\\nsubquadratically. Our main result is based on a new oblivious embedding which\\nembeds the column space of a given matrix $A\\\\in\\\\mathbb{R}^{n\\\\times d}$ with\\nOrlicz norm into a lower dimensional space with $\\\\ell_2$ norm. Specifically, we\\nshow how to efficiently find an embedding matrix $S\\\\in\\\\mathbb{R}^{m\\\\times\\nn},m<n$ such that $\\\\forall x\\\\in\\\\mathbb{R}^{d},\\\\Omega(1/(d\\\\log n)) \\\\cdot\\n\\\\|Ax\\\\|_G\\\\leq \\\\|SAx\\\\|_2\\\\leq O(d^2\\\\log n) \\\\cdot \\\\|Ax\\\\|_G.$ By applying this\\nsubspace embedding technique, we show an approximation algorithm for the\\nregression problem $\\\\min_{x\\\\in\\\\mathbb{R}^d} \\\\|Ax-b\\\\|_G$, up to a $O(d\\\\log^2 n)$\\nfactor. As a further application of our techniques, we show how to also use\\nthem to improve on the algorithm for the $\\\\ell_p$ low rank matrix approximation\\nproblem for $1\\\\leq p<2$.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06430v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06430v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Alexandr Andoni',\n",
       "   'Chengyu Lin',\n",
       "   'Ying Sheng',\n",
       "   'Peilin Zhong',\n",
       "   'Ruiqi Zhong'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [{'name': 'Linear Regression',\n",
       "    'full_name': 'Linear Regression',\n",
       "    'description': '**Linear Regression** is a method for modelling a relationship between a dependent variable and independent variables. These models can be fit with numerous approaches. The most common is *least squares*, where we minimize the mean square error between the predicted values $\\\\hat{y} = \\\\textbf{X}\\\\hat{\\\\beta}$ and actual values $y$: $\\\\left(y-\\\\textbf{X}\\\\beta\\\\right)^{2}$.\\r\\n\\r\\nWe can also define the problem in probabilistic terms as a generalized linear model (GLM) where the pdf is a Gaussian distribution, and then perform maximum likelihood estimation to estimate $\\\\hat{\\\\beta}$.\\r\\n\\r\\nImage Source: [Wikipedia](https://en.wikipedia.org/wiki/Linear_regression)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Generalized Linear Models',\n",
       "     'description': '**Generalized Linear Models (GLMs)** are a class of models that generalize upon linear regression by allowing many more distributions to be modeled for the response variable via a link function. Below you can find a continuously updating list of GLMs.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/scalable-methods-for-8-bit-training-of-neural',\n",
       "  'arxiv_id': '1805.11046',\n",
       "  'title': 'Scalable Methods for 8-bit Training of Neural Networks',\n",
       "  'abstract': \"Quantized Neural Networks (QNNs) are often used to improve network efficiency\\nduring the inference phase, i.e. after the network has been trained. Extensive\\nresearch in the field suggests many different quantization schemes. Still, the\\nnumber of bits required, as well as the best quantization scheme, are yet\\nunknown. Our theoretical analysis suggests that most of the training process is\\nrobust to substantial precision reduction, and points to only a few specific\\noperations that require higher precision. Armed with this knowledge, we\\nquantize the model parameters, activations and layer gradients to 8-bit,\\nleaving at a higher precision only the final step in the computation of the\\nweight gradients. Additionally, as QNNs require batch-normalization to be\\ntrained at high precision, we introduce Range Batch-Normalization (BN) which\\nhas significantly higher tolerance to quantization noise and improved\\ncomputational complexity. Our simulations show that Range BN is equivalent to\\nthe traditional batch norm if a precise scale adjustment, which can be\\napproximated analytically, is applied. To the best of the authors' knowledge,\\nthis work is the first to quantize the weights, activations, as well as a\\nsubstantial volume of the gradients stream, in all layers (including batch\\nnormalization) to 8-bit while showing state-of-the-art results over the\\nImageNet-1K dataset.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.11046v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.11046v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Ron Banner', 'Itay Hubara', 'Elad Hoffer', 'Daniel Soudry'],\n",
       "  'tasks': ['Quantization'],\n",
       "  'date': '2018-05-25',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'imagenet'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'ImageNet'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-novel-hybrid-machine-learning-model-for',\n",
       "  'arxiv_id': '1806.06423',\n",
       "  'title': 'A Novel Hybrid Machine Learning Model for Auto-Classification of Retinal Diseases',\n",
       "  'abstract': 'Automatic clinical diagnosis of retinal diseases has emerged as a promising\\napproach to facilitate discovery in areas with limited access to specialists.\\nWe propose a novel visual-assisted diagnosis hybrid model based on the support\\nvector machine (SVM) and deep neural networks (DNNs). The model incorporates\\ncomplementary strengths of DNNs and SVM. Furthermore, we present a new clinical\\nretina label collection for ophthalmology incorporating 32 retina diseases\\nclasses. Using EyeNet, our model achieves 89.73% diagnosis accuracy and the\\nmodel performance is comparable to the professional ophthalmologists.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06423v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06423v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['C. -H. Huck Yang',\n",
       "   'Jia-Hong Huang',\n",
       "   'Fangyu Liu',\n",
       "   'Fang-Yi Chiu',\n",
       "   'Mengya Gao',\n",
       "   'Weifeng Lyu',\n",
       "   'I-Hung Lin M. D.',\n",
       "   'Jesper Tegner'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [{'name': 'SVM',\n",
       "    'full_name': 'Support Vector Machine',\n",
       "    'description': 'A **Support Vector Machine**, or **SVM**, is a non-parametric supervised learning model. For non-linear classification and regression, they utilise the kernel trick to map inputs to high-dimensional feature spaces. SVMs construct a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure to the right shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called “support vectors”. \\r\\n\\r\\nSource: [scikit-learn](https://scikit-learn.org/stable/modules/svm.html)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Non-Parametric Classification',\n",
       "     'description': '**Non-Parametric Classification** methods perform classification where we use non-parametric methods to approximate the functional form of the relationship. Below you can find a continuously updating list of non-parametric classification methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-to-evaluate-image-captioning',\n",
       "  'arxiv_id': '1806.06422',\n",
       "  'title': 'Learning to Evaluate Image Captioning',\n",
       "  'abstract': 'Evaluation metrics for image captioning face two challenges. Firstly,\\ncommonly used metrics such as CIDEr, METEOR, ROUGE and BLEU often do not\\ncorrelate well with human judgments. Secondly, each metric has well known blind\\nspots to pathological caption constructions, and rule-based metrics lack\\nprovisions to repair such blind spots once identified. For example, the newly\\nproposed SPICE correlates well with human judgments, but fails to capture the\\nsyntactic structure of a sentence. To address these two challenges, we propose\\na novel learning based discriminative evaluation metric that is directly\\ntrained to distinguish between human and machine-generated captions. In\\naddition, we further propose a data augmentation scheme to explicitly\\nincorporate pathological transformations as negative examples during training.\\nThe proposed metric is evaluated with three kinds of robustness tests and its\\ncorrelation with human judgments. Extensive experiments show that the proposed\\ndata augmentation scheme not only makes our metric more robust toward several\\npathological transformations, but also improves its correlation with human\\njudgments. Our metric outperforms other metrics on both caption level human\\ncorrelation in Flickr 8k and system level human correlation in COCO. The\\nproposed approach could be served as a learning based evaluation metric that is\\ncomplementary to existing rule-based metrics.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06422v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06422v1.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Yin Cui',\n",
       "   'Guandao Yang',\n",
       "   'Andreas Veit',\n",
       "   'Xun Huang',\n",
       "   'Serge Belongie'],\n",
       "  'tasks': ['Data Augmentation', 'Image Captioning'],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet', 'coco'],\n",
       "  'datasets_used_full': ['ImageNet', 'COCO'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/high-speed-tracking-with-multi-kernel',\n",
       "  'arxiv_id': '1806.06418',\n",
       "  'title': 'High-speed Tracking with Multi-kernel Correlation Filters',\n",
       "  'abstract': 'Correlation filter (CF) based trackers are currently ranked top in terms of\\ntheir performances. Nevertheless, only some of them, such as\\nKCF~\\\\cite{henriques15} and MKCF~\\\\cite{tangm15}, are able to exploit the\\npowerful discriminability of non-linear kernels. Although MKCF achieves more\\npowerful discriminability than KCF through introducing multi-kernel learning\\n(MKL) into KCF, its improvement over KCF is quite limited and its computational\\nburden increases significantly in comparison with KCF. In this paper, we will\\nintroduce the MKL into KCF in a different way than MKCF. We reformulate the MKL\\nversion of CF objective function with its upper bound, alleviating the negative\\nmutual interference of different kernels significantly. Our novel MKCF tracker,\\nMKCFup, outperforms KCF and MKCF with large margins and can still work at very\\nhigh fps. Extensive experiments on public datasets show that our method is\\nsuperior to state-of-the-art algorithms for target objects of small move at\\nvery high speed.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06418v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06418v1.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Ming Tang', 'Bin Yu', 'Fan Zhang', 'Jinqiao Wang'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/feature-learning-and-classification-in',\n",
       "  'arxiv_id': '1806.06415',\n",
       "  'title': 'Feature Learning and Classification in Neuroimaging: Predicting Cognitive Impairment from Magnetic Resonance Imaging',\n",
       "  'abstract': \"Due to the rapid innovation of technology and the desire to find and employ\\nbiomarkers for neurodegenerative disease, high-dimensional data classification\\nproblems are routinely encountered in neuroimaging studies. To avoid\\nover-fitting and to explore relationships between disease and potential\\nbiomarkers, feature learning and selection plays an important role in\\nclassifier construction and is an important area in machine learning. In this\\narticle, we review several important feature learning and selection techniques\\nincluding lasso-based methods, PCA, the two-sample t-test, and stacked\\nauto-encoders. We compare these approaches using a numerical study involving\\nthe prediction of Alzheimer's disease from Magnetic Resonance Imaging.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06415v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06415v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shan Shi', 'Farouk Nathoo'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [{'name': 'PCA',\n",
       "    'full_name': 'Principal Components Analysis',\n",
       "    'description': '**Principle Components Analysis (PCA)** is an unsupervised method primary used for dimensionality reduction within machine learning.  PCA is calculated via a singular value decomposition (SVD) of the design matrix, or alternatively, by calculating the covariance matrix of the data and performing eigenvalue decomposition on the covariance matrix. The results of PCA provide a low-dimensional picture of the structure of the data and the leading (uncorrelated) latent factors determining variation in the data.\\r\\n\\r\\nImage Source: [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#/media/File:GaussianScatterPCA.svg)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Dimensionality Reduction',\n",
       "     'description': '**Dimensionality Reduction** methods transform data from a high-dimensional space into a low-dimensional space so that the low-dimensional space retains the most important properties of the original data. Below you can find a continuously updating list of dimensionality reduction methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/one-to-one-mapping-between-stimulus-and',\n",
       "  'arxiv_id': '1805.09001',\n",
       "  'title': 'One-to-one Mapping between Stimulus and Neural State: Memory and Classification',\n",
       "  'abstract': 'Synaptic strength can be seen as probability to propagate impulse, and\\naccording to synaptic plasticity, function could exist from propagation\\nactivity to synaptic strength. If the function satisfies constraints such as\\ncontinuity and monotonicity, neural network under external stimulus will always\\ngo to fixed point, and there could be one-to-one mapping between external\\nstimulus and synaptic strength at fixed point. In other words, neural network\\n\"memorizes\" external stimulus in its synapses. A biological classifier is\\nproposed to utilize this mapping.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.09001v6',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.09001v6.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sizhong Lan'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-05-23',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/negative-learning-rates-and-p-learning',\n",
       "  'arxiv_id': '1603.08253',\n",
       "  'title': 'Negative Learning Rates and P-Learning',\n",
       "  'abstract': 'We present a method of training a differentiable function approximator for a\\nregression task using negative examples. We effect this training using negative\\nlearning rates. We also show how this method can be used to perform direct\\npolicy learning in a reinforcement learning setting.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1603.08253v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1603.08253v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Devon Merrill'],\n",
       "  'tasks': [],\n",
       "  'date': '2016-03-27',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/measuring-semantic-coherence-of-a',\n",
       "  'arxiv_id': '1806.06411',\n",
       "  'title': 'Measuring Semantic Coherence of a Conversation',\n",
       "  'abstract': 'Conversational systems have become increasingly popular as a way for humans\\nto interact with computers. To be able to provide intelligent responses,\\nconversational systems must correctly model the structure and semantics of a\\nconversation. We introduce the task of measuring semantic (in)coherence in a\\nconversation with respect to background knowledge, which relies on the\\nidentification of semantic relations between concepts introduced during a\\nconversation. We propose and evaluate graph-based and machine learning-based\\napproaches for measuring semantic coherence using knowledge graphs, their\\nvector space embeddings and word embedding models, as sources of background\\nknowledge. We demonstrate how these approaches are able to uncover different\\ncoherence patterns in conversations on the Ubuntu Dialogue Corpus.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06411v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06411v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Svitlana Vakulenko',\n",
       "   'Maarten de Rijke',\n",
       "   'Michael Cochez',\n",
       "   'Vadim Savenkov',\n",
       "   'Axel Polleres'],\n",
       "  'tasks': ['Knowledge Graphs'],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-a-prior-over-intent-via-meta-inverse',\n",
       "  'arxiv_id': '1805.12573',\n",
       "  'title': 'Learning a Prior over Intent via Meta-Inverse Reinforcement Learning',\n",
       "  'abstract': 'A significant challenge for the practical application of reinforcement learning in the real world is the need to specify an oracle reward function that correctly defines a task. Inverse reinforcement learning (IRL) seeks to avoid this challenge by instead inferring a reward function from expert behavior. While appealing, it can be impractically expensive to collect datasets of demonstrations that cover the variation common in the real world (e.g. opening any type of door). Thus in practice, IRL must commonly be performed with only a limited set of demonstrations where it can be exceedingly difficult to unambiguously recover a reward function. In this work, we exploit the insight that demonstrations from other tasks can be used to constrain the set of possible reward functions by learning a \"prior\" that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. We demonstrate that our method can efficiently recover rewards from images for novel tasks and provide intuition as to how our approach is analogous to learning a prior.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1805.12573v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1805.12573v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kelvin Xu',\n",
       "   'Ellis Ratner',\n",
       "   'Anca Dragan',\n",
       "   'Sergey Levine',\n",
       "   'Chelsea Finn'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-31',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['suncg'],\n",
       "  'datasets_used_full': ['SUNCG'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/gated-path-planning-networks',\n",
       "  'arxiv_id': '1806.06408',\n",
       "  'title': 'Gated Path Planning Networks',\n",
       "  'abstract': 'Value Iteration Networks (VINs) are effective differentiable path planning\\nmodules that can be used by agents to perform navigation while still\\nmaintaining end-to-end differentiability of the entire architecture. Despite\\ntheir effectiveness, they suffer from several disadvantages including training\\ninstability, random seed sensitivity, and other optimization problems. In this\\nwork, we reframe VINs as recurrent-convolutional networks which demonstrates\\nthat VINs couple recurrent convolutions with an unconventional max-pooling\\nactivation. From this perspective, we argue that standard gated recurrent\\nupdate equations could potentially alleviate the optimization issues plaguing\\nVIN. The resulting architecture, which we call the Gated Path Planning Network,\\nis shown to empirically outperform VIN on a variety of metrics such as learning\\nspeed, hyperparameter sensitivity, iteration count, and even generalization.\\nFurthermore, we show that this performance gap is consistent across different\\nmaze transition types, maze sizes and even show success on a challenging 3D\\nenvironment, where the planner is only provided with first-person RGB images.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06408v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06408v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Lisa Lee',\n",
       "   'Emilio Parisotto',\n",
       "   'Devendra Singh Chaplot',\n",
       "   'Eric Xing',\n",
       "   'Ruslan Salakhutdinov'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['vizdoom'],\n",
       "  'datasets_used_full': ['VizDoom'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/an-improved-text-sentiment-classification',\n",
       "  'arxiv_id': '1806.06407',\n",
       "  'title': 'An Improved Text Sentiment Classification Model Using TF-IDF and Next Word Negation',\n",
       "  'abstract': 'With the rapid growth of Text sentiment analysis, the demand for automatic\\nclassification of electronic documents has increased by leaps and bound. The\\nparadigm of text classification or text mining has been the subject of many\\nresearch works in recent time. In this paper we propose a technique for text\\nsentiment classification using term frequency- inverse document frequency\\n(TF-IDF) along with Next Word Negation (NWN). We have also compared the\\nperformances of binary bag of words model, TF-IDF model and TF-IDF with next\\nword negation (TF-IDF-NWN) model for text classification. Our proposed model is\\nthen applied on three different text mining algorithms and we found the Linear\\nSupport vector machine (LSVM) is the most appropriate to work with our proposed\\nmodel. The achieved results show significant increase in accuracy compared to\\nearlier methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06407v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06407v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Bijoyan Das', 'Sarit Chakraborty'],\n",
       "  'tasks': ['Classification',\n",
       "   'General Classification',\n",
       "   'Sentiment Analysis',\n",
       "   'Text Classification'],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/high-speed-kernelized-correlation-filters',\n",
       "  'arxiv_id': '1806.06406',\n",
       "  'title': 'Fast Kernelized Correlation Filters without Boundary Effect',\n",
       "  'abstract': \"In recent years, correlation filter based trackers (CF trackers) have attracted much attention from the vision community because of their top performance in both localization accuracy and efficiency. The society of visual tracking, however, still needs to deal with the following difficulty on CF trackers: avoiding or eliminating the boundary effect completely, in the meantime, exploiting non-linear kernels and running efficiently. In this paper, we propose a fast kernelized correlation filter without boundary effect (nBEKCF) to solve this problem. To avoid the boundary effect thoroughly, a set of \\\\emph{real} and \\\\emph{dense} patches is sampled through the traditional sliding window and used as the training samples to train nBEKCF to fit a Gaussian response map. Non-linear kernels can be applied naturally in nBEKCF due to its different theoretical foundation from the existing CF trackers'. To achieve the fast training and detection, a set of cyclic bases is introduced to construct the filter. Two algorithms, ACSII and CCIM, are developed to significantly accelerate the calculation of kernel correlation matrices. ACSII and CCIM fully exploit the density of training samples and cyclic structure of bases, and totally run in space domain. The efficiency of CCIM exceeds that of the FFT counterpart remarkably in our task. Extensive experiments on six public datasets, OTB-2013, OTB-2015, NfS, VOT2018, GOT10k, and TrackingNet, show that compared to the CF trackers designed to relax the boundary effect, BACF and SRDCF, our nBEKCF achieves higher localization accuracy without tricks, in the meanwhile, runs at higher FPS.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06406v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06406v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ming Tang', 'Linyu Zheng', 'Bin Yu', 'Jinqiao Wang'],\n",
       "  'tasks': ['Visual Tracking'],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['otb',\n",
       "   'otb-2015',\n",
       "   'trackingnet',\n",
       "   'vot2018',\n",
       "   'otb-2013'],\n",
       "  'datasets_used_full': ['OTB',\n",
       "   'OTB-2015',\n",
       "   'TrackingNet',\n",
       "   'VOT2018',\n",
       "   'OTB-2013'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/medgan-medical-image-translation-using-gans',\n",
       "  'arxiv_id': '1806.06397',\n",
       "  'title': 'MedGAN: Medical Image Translation using GANs',\n",
       "  'abstract': 'Image-to-image translation is considered a new frontier in the field of\\nmedical image analysis, with numerous potential applications. However, a large\\nportion of recent approaches offers individualized solutions based on\\nspecialized task-specific architectures or require refinement through\\nnon-end-to-end training. In this paper, we propose a new framework, named\\nMedGAN, for medical image-to-image translation which operates on the image\\nlevel in an end-to-end manner. MedGAN builds upon recent advances in the field\\nof generative adversarial networks (GANs) by merging the adversarial framework\\nwith a new combination of non-adversarial losses. We utilize a discriminator\\nnetwork as a trainable feature extractor which penalizes the discrepancy\\nbetween the translated medical images and the desired modalities. Moreover,\\nstyle-transfer losses are utilized to match the textures and fine-structures of\\nthe desired target images to the translated images. Additionally, we present a\\nnew generator architecture, titled CasNet, which enhances the sharpness of the\\ntranslated medical outputs through progressive refinement via encoder-decoder\\npairs. Without any application-specific modifications, we apply MedGAN on three\\ndifferent tasks: PET-CT translation, correction of MR motion artefacts and PET\\nimage denoising. Perceptual analysis by radiologists and quantitative\\nevaluations illustrate that the MedGAN outperforms other existing translation\\napproaches.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06397v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06397v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Karim Armanious',\n",
       "   'Chenming Jiang',\n",
       "   'Marc Fischer',\n",
       "   'Thomas Küstner',\n",
       "   'Konstantin Nikolaou',\n",
       "   'Sergios Gatidis',\n",
       "   'Bin Yang'],\n",
       "  'tasks': ['Denoising',\n",
       "   'Image Denoising',\n",
       "   'Image-to-Image Translation',\n",
       "   'Style Transfer',\n",
       "   'Translation'],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/task-relevant-object-discovery-and',\n",
       "  'arxiv_id': '1806.06392',\n",
       "  'title': 'Task-Relevant Object Discovery and Categorization for Playing First-person Shooter Games',\n",
       "  'abstract': \"We consider the problem of learning to play first-person shooter (FPS) video\\ngames using raw screen images as observations and keyboard inputs as actions.\\nThe high-dimensionality of the observations in this type of applications leads\\nto prohibitive needs of training data for model-free methods, such as the deep\\nQ-network (DQN), and its recurrent variant DRQN. Thus, recent works focused on\\nlearning low-dimensional representations that may reduce the need for data.\\nThis paper presents a new and efficient method for learning such\\nrepresentations. Salient segments of consecutive frames are detected from their\\noptical flow, and clustered based on their feature descriptors. The clusters\\ntypically correspond to different discovered categories of objects. Segments\\ndetected in new frames are then classified based on their nearest clusters.\\nBecause only a few categories are relevant to a given task, the importance of a\\ncategory is defined as the correlation between its occurrence and the agent's\\nperformance. The result is encoded as a vector indicating objects that are in\\nthe frame and their locations, and used as a side input to DRQN. Experiments on\\nthe game Doom provide a good evidence for the benefit of this approach.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06392v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06392v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Junchi Liang', 'Abdeslam Boularias'],\n",
       "  'tasks': ['Object Discovery', 'Optical Flow Estimation'],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['3d-chairs', 'vizdoom'],\n",
       "  'datasets_used_full': ['3D Chairs', 'VizDoom'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/scraping-and-preprocessing-commercial-auction',\n",
       "  'arxiv_id': '1806.00656',\n",
       "  'title': 'Scraping and Preprocessing Commercial Auction Data for Fraud Classification',\n",
       "  'abstract': 'In the last three decades, we have seen a significant increase in trading\\ngoods and services through online auctions. However, this business created an\\nattractive environment for malicious moneymakers who can commit different types\\nof fraud activities, such as Shill Bidding (SB). The latter is predominant\\nacross many auctions but this type of fraud is difficult to detect due to its\\nsimilarity to normal bidding behaviour. The unavailability of SB datasets makes\\nthe development of SB detection and classification models burdensome.\\nFurthermore, to implement efficient SB detection models, we should produce SB\\ndata from actual auctions of commercial sites. In this study, we first scraped\\na large number of eBay auctions of a popular product. After preprocessing the\\nraw auction data, we build a high-quality SB dataset based on the most reliable\\nSB strategies. The aim of our research is to share the preprocessed auction\\ndataset as well as the SB training (unlabelled) dataset, thereby researchers\\ncan apply various machine learning techniques by using authentic data of\\nauctions and fraud.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.00656v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.00656v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ahmad Alzahrani', 'Samira Sadaoui'],\n",
       "  'tasks': ['Classification', 'General Classification'],\n",
       "  'date': '2018-06-02',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multi-variable-lstm-neural-network-for',\n",
       "  'arxiv_id': '1806.06384',\n",
       "  'title': 'Multi-variable LSTM neural network for autoregressive exogenous model',\n",
       "  'abstract': 'In this paper, we propose multi-variable LSTM capable of accurate forecasting\\nand variable importance interpretation for time series with exogenous\\nvariables. Current attention mechanism in recurrent neural networks mostly\\nfocuses on the temporal aspect of data and falls short of characterizing\\nvariable importance. To this end, the multi-variable LSTM equipped with\\ntensorized hidden states is developed to learn hidden states for individual\\nvariables, which give rise to our mixture temporal and variable attention.\\nBased on such attention mechanism, we infer and quantify variable importance.\\nExtensive experiments using real datasets with Granger-causality test and the\\nsynthetic dataset with ground truth demonstrate the prediction performance and\\ninterpretability of multi-variable LSTM in comparison to a variety of\\nbaselines. It exhibits the prospect of multi-variable LSTM as an end-to-end\\nframework for both forecasting and knowledge discovery.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06384v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06384v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Tian Guo', 'Tao Lin'],\n",
       "  'tasks': ['Time Series'],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/geodesic-convex-optimization-differentiation',\n",
       "  'arxiv_id': '1806.06373',\n",
       "  'title': 'Geodesic Convex Optimization: Differentiation on Manifolds, Geodesics, and Convexity',\n",
       "  'abstract': 'Convex optimization is a vibrant and successful area due to the existence of\\na variety of efficient algorithms that leverage the rich structure provided by\\nconvexity. Convexity of a smooth set or a function in a Euclidean space is\\ndefined by how it interacts with the standard differential structure in this\\nspace -- the Hessian of a convex function has to be positive semi-definite\\neverywhere. However, in recent years, there is a growing demand to understand\\nnon-convexity and develop computational methods to optimize non-convex\\nfunctions. Intriguingly, there is a type of non-convexity that disappears once\\none introduces a suitable differentiable structure and redefines convexity with\\nrespect to the straight lines, or {\\\\em geodesics}, with respect to this\\nstructure. Such convexity is referred to as {\\\\em geodesic convexity}. Interest\\nin studying it arises due to recent reformulations of some non-convex problems\\nas geodesically convex optimization problems over geodesically convex sets.\\nGeodesics on manifolds have been extensively studied in various branches of\\nMathematics and Physics. However, unlike convex optimization, understanding\\ngeodesics and geodesic convexity from a computational point of view largely\\nremains a mystery. The goal of this exposition is to introduce the first part\\nof geodesic convex optimization -- geodesic convexity -- in a self-contained\\nmanner. We first present a variety of notions from differential and Riemannian\\ngeometry such as differentiation on manifolds, geodesics, and then introduce\\ngeodesic convexity. We conclude by showing that certain non-convex optimization\\nproblems such as computing the Brascamp-Lieb constant and the operator scaling\\nproblem have geodesically convex formulations.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06373v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06373v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Nisheeth K. Vishnoi'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-the-mathematics-of-beauty-beautiful-images',\n",
       "  'arxiv_id': '1705.08244',\n",
       "  'title': 'On the mathematics of beauty: beautiful images',\n",
       "  'abstract': 'In this paper, we will study the simplest kind of beauty which can be found in simple visual patterns. The proposed approach shows that aesthetically appealing patterns deliver higher amount of information over multiple levels in comparison with less aesthetically appealing patterns when the same amount of energy is used. The proposed approach is used to classify aesthetically appealing patterns.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1705.08244v8',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1705.08244v8.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['A. M. Khalili'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-05-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multimodal-grounding-for-language-processing',\n",
       "  'arxiv_id': '1806.06371',\n",
       "  'title': 'Multimodal Grounding for Language Processing',\n",
       "  'abstract': 'This survey discusses how recent developments in multimodal processing facilitate conceptual grounding of language. We categorize the information flow in multimodal processing with respect to cognitive models of human information processing and analyze different methods for combining multimodal representations. Based on this methodological inventory, we discuss the benefit of multimodal grounding for a variety of language processing tasks and the challenges that arise. We particularly focus on multimodal grounding of verbs which play a crucial role for the compositional power of language.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06371v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06371v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Lisa Beinborn', 'Teresa Botschen', 'Iryna Gurevych'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/how-could-polyhedral-theory-harness-deep',\n",
       "  'arxiv_id': '1806.06365',\n",
       "  'title': 'How Could Polyhedral Theory Harness Deep Learning?',\n",
       "  'abstract': 'The holy grail of deep learning is to come up with an automatic method to\\ndesign optimal architectures for different applications. In other words, how\\ncan we effectively dimension and organize neurons along the network layers\\nbased on the computational resources, input size, and amount of training data?\\nWe outline promising research directions based on polyhedral theory and\\nmixed-integer representability that may offer an analytical approach to this\\nquestion, in contrast to the empirical techniques often employed.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06365v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06365v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Thiago Serra',\n",
       "   'Christian Tjandraatmadja',\n",
       "   'Srikumar Ramalingam'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/exact-information-propagation-through-fully',\n",
       "  'arxiv_id': '1806.06362',\n",
       "  'title': 'Initialization of ReLUs for Dynamical Isometry',\n",
       "  'abstract': 'Deep learning relies on good initialization schemes and hyperparameter choices prior to training a neural network. Random weight initializations induce random network ensembles, which give rise to the trainability, training speed, and sometimes also generalization ability of an instance. In addition, such ensembles provide theoretical insights into the space of candidate models of which one is selected during training. The results obtained so far rely on mean field approximations that assume infinite layer width and that study average squared signals. We derive the joint signal output distribution exactly, without mean field assumptions, for fully-connected networks with Gaussian weights and biases, and analyze deviations from the mean field results. For rectified linear units, we further discuss limitations of the standard initialization scheme, such as its lack of dynamical isometry, and propose a simple alternative that overcomes these by initial parameter sharing.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06362v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06362v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2019 12',\n",
       "  'authors': ['Rebekka Burkholz', 'Alina Dubatovka'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist'],\n",
       "  'datasets_used_full': ['MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/incorporating-chinese-characters-of-words-for',\n",
       "  'arxiv_id': '1806.06349',\n",
       "  'title': 'Incorporating Chinese Characters of Words for Lexical Sememe Prediction',\n",
       "  'abstract': 'Sememes are minimum semantic units of concepts in human languages, such that\\neach word sense is composed of one or multiple sememes. Words are usually\\nmanually annotated with their sememes by linguists, and form linguistic\\ncommon-sense knowledge bases widely used in various NLP tasks. Recently, the\\nlexical sememe prediction task has been introduced. It consists of\\nautomatically recommending sememes for words, which is expected to improve\\nannotation efficiency and consistency. However, existing methods of lexical\\nsememe prediction typically rely on the external context of words to represent\\nthe meaning, which usually fails to deal with low-frequency and\\nout-of-vocabulary words. To address this issue for Chinese, we propose a novel\\nframework to take advantage of both internal character information and external\\ncontext information of words. We experiment on HowNet, a Chinese sememe\\nknowledge base, and demonstrate that our framework outperforms state-of-the-art\\nbaselines by a large margin, and maintains a robust performance even for\\nlow-frequency words.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06349v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06349v1.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Huiming Jin',\n",
       "   'Hao Zhu',\n",
       "   'Zhiyuan Liu',\n",
       "   'Ruobing Xie',\n",
       "   'Maosong Sun',\n",
       "   'Fen Lin',\n",
       "   'Leyu Lin'],\n",
       "  'tasks': ['Common Sense Reasoning'],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/neural-style-transfer-a-review',\n",
       "  'arxiv_id': '1705.04058',\n",
       "  'title': 'Neural Style Transfer: A Review',\n",
       "  'abstract': 'The seminal work of Gatys et al. demonstrated the power of Convolutional\\nNeural Networks (CNNs) in creating artistic imagery by separating and\\nrecombining image content and style. This process of using CNNs to render a\\ncontent image in different styles is referred to as Neural Style Transfer\\n(NST). Since then, NST has become a trending topic both in academic literature\\nand industrial applications. It is receiving increasing attention and a variety\\nof approaches are proposed to either improve or extend the original NST\\nalgorithm. In this paper, we aim to provide a comprehensive overview of the\\ncurrent progress towards NST. We first propose a taxonomy of current algorithms\\nin the field of NST. Then, we present several evaluation methods and compare\\ndifferent NST algorithms both qualitatively and quantitatively. The review\\nconcludes with a discussion of various applications of NST and open problems\\nfor future research. A list of papers discussed in this review, corresponding\\ncodes, pre-trained models and more comparison results are publicly available at\\nhttps://github.com/ycjing/Neural-Style-Transfer-Papers.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1705.04058v7',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1705.04058v7.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yongcheng Jing',\n",
       "   'Yezhou Yang',\n",
       "   'Zunlei Feng',\n",
       "   'Jingwen Ye',\n",
       "   'Yizhou Yu',\n",
       "   'Mingli Song'],\n",
       "  'tasks': ['Style Transfer'],\n",
       "  'date': '2017-05-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/investigating-generative-adversarial-networks',\n",
       "  'arxiv_id': '1803.10132',\n",
       "  'title': 'Investigating Generative Adversarial Networks based Speech Dereverberation for Robust Speech Recognition',\n",
       "  'abstract': 'We investigate the use of generative adversarial networks (GANs) in speech\\ndereverberation for robust speech recognition. GANs have been recently studied\\nfor speech enhancement to remove additive noises, but there still lacks of a\\nwork to examine their ability in speech dereverberation and the advantages of\\nusing GANs have not been fully established. In this paper, we provide deep\\ninvestigations in the use of GAN-based dereverberation front-end in ASR. First,\\nwe study the effectiveness of different dereverberation networks (the generator\\nin GAN) and find that LSTM leads a significant improvement as compared with\\nfeed-forward DNN and CNN in our dataset. Second, further adding residual\\nconnections in the deep LSTMs can boost the performance as well. Finally, we\\nfind that, for the success of GAN, it is important to update the generator and\\nthe discriminator using the same mini-batch data during training. Moreover,\\nusing reverberant spectrogram as a condition to discriminator, as suggested in\\nprevious studies, may degrade the performance. In summary, our GAN-based\\ndereverberation front-end achieves 14%-19% relative CER reduction as compared\\nto the baseline DNN dereverberation network when tested on a strong\\nmulti-condition training acoustic model.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.10132v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.10132v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ke Wang',\n",
       "   'Junbo Zhang',\n",
       "   'Sining Sun',\n",
       "   'Yujun Wang',\n",
       "   'Fei Xiang',\n",
       "   'Lei Xie'],\n",
       "  'tasks': ['Robust Speech Recognition',\n",
       "   'Speech Dereverberation',\n",
       "   'Speech Enhancement',\n",
       "   'Speech Recognition'],\n",
       "  'date': '2018-03-27',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/empirical-evaluation-of-speaker-adaptation-on',\n",
       "  'arxiv_id': '1803.10146',\n",
       "  'title': 'Empirical Evaluation of Speaker Adaptation on DNN based Acoustic Model',\n",
       "  'abstract': \"Speaker adaptation aims to estimate a speaker specific acoustic model from a\\nspeaker independent one to minimize the mismatch between the training and\\ntesting conditions arisen from speaker variabilities. A variety of neural\\nnetwork adaptation methods have been proposed since deep learning models have\\nbecome the main stream. But there still lacks an experimental comparison\\nbetween different methods, especially when DNN-based acoustic models have been\\nadvanced greatly. In this paper, we aim to close this gap by providing an\\nempirical evaluation of three typical speaker adaptation methods: LIN, LHUC and\\nKLD. Adaptation experiments, with different size of adaptation data, are\\nconducted on a strong TDNN-LSTM acoustic model. More challengingly, here, the\\nsource and target we are concerned with are standard Mandarin speaker model and\\naccented Mandarin speaker model. We compare the performances of different\\nmethods and their combinations. Speaker adaptation performance is also examined\\nby speaker's accent degree.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.10146v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.10146v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ke Wang', 'Junbo Zhang', 'Yujun Wang', 'Lei Xie'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-03-27',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-acoustic-word-embeddings-with-1',\n",
       "  'arxiv_id': '1806.03621',\n",
       "  'title': 'Learning Acoustic Word Embeddings with Temporal Context for Query-by-Example Speech Search',\n",
       "  'abstract': 'We propose to learn acoustic word embeddings with temporal context for\\nquery-by-example (QbE) speech search. The temporal context includes the leading\\nand trailing word sequences of a word. We assume that there exist spoken word\\npairs in the training database. We pad the word pairs with their original\\ntemporal context to form fixed-length speech segment pairs. We obtain the\\nacoustic word embeddings through a deep convolutional neural network (CNN)\\nwhich is trained on the speech segment pairs with a triplet loss. Shifting a\\nfixed-length analysis window through the search content, we obtain a running\\nsequence of embeddings. In this way, searching for the spoken query is\\nequivalent to the matching of acoustic word embeddings. The experiments show\\nthat our proposed acoustic word embeddings learned with temporal context are\\neffective in QbE speech search. They outperform the state-of-the-art\\nframe-level feature representations and reduce run-time computation since no\\ndynamic time warping is required in QbE speech search. We also find that it is\\nimportant to have sufficient speech segment pairs to train the deep CNN for\\neffective acoustic word embeddings.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03621v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03621v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yougen Yuan',\n",
       "   'Cheung-Chi Leung',\n",
       "   'Lei Xie',\n",
       "   'Hongjie Chen',\n",
       "   'Bin Ma',\n",
       "   'Haizhou Li'],\n",
       "  'tasks': ['Dynamic Time Warping', 'Word Embeddings'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/neural-feature-learning-from-relational',\n",
       "  'arxiv_id': '1801.05372',\n",
       "  'title': 'Neural Feature Learning From Relational Database',\n",
       "  'abstract': 'Feature engineering is one of the most important but most tedious tasks in data science. This work studies automation of feature learning from relational database. We first prove theoretically that finding the optimal features from relational data for predictive tasks is NP-hard. We propose an efficient rule-based approach based on heuristics and a deep neural network to automatically learn appropriate features from relational data. We benchmark our approaches in ensembles in past Kaggle competitions. Our new approach wins late medals and beats the state-of-the-art solutions with significant margins. To the best of our knowledge, this is the first time an automated data science system could win medals in Kaggle competitions with complex relational database.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1801.05372v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1801.05372v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hoang Thanh Lam',\n",
       "   'Tran Ngoc Minh',\n",
       "   'Mathieu Sinn',\n",
       "   'Beat Buesser',\n",
       "   'Martin Wistuba'],\n",
       "  'tasks': ['Feature Engineering'],\n",
       "  'date': '2018-01-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/extending-recurrent-neural-aligner-for',\n",
       "  'arxiv_id': '1806.06342',\n",
       "  'title': 'Extending Recurrent Neural Aligner for Streaming End-to-End Speech Recognition in Mandarin',\n",
       "  'abstract': 'End-to-end models have been showing superiority in Automatic Speech\\nRecognition (ASR). At the same time, the capacity of streaming recognition has\\nbecome a growing requirement for end-to-end models. Following these trends, an\\nencoder-decoder recurrent neural network called Recurrent Neural Aligner (RNA)\\nhas been freshly proposed and shown its competitiveness on two English ASR\\ntasks. However, it is not clear if RNA can be further improved and applied to\\nother spoken language. In this work, we explore the applicability of RNA in\\nMandarin Chinese and present four effective extensions: In the encoder, we\\nredesign the temporal down-sampling and introduce a powerful convolutional\\nstructure. In the decoder, we utilize a regularizer to smooth the output\\ndistribution and conduct joint training with a language model. On two Mandarin\\nChinese conversational telephone speech recognition (MTS) datasets, our\\nExtended-RNA obtains promising performance. Particularly, it achieves 27.7%\\ncharacter error rate (CER), which is superior to current state-of-the-art\\nresult on the popular HKUST task.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06342v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06342v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Linhao Dong', 'Shiyu Zhou', 'Wei Chen', 'Bo Xu'],\n",
       "  'tasks': ['Automatic Speech Recognition',\n",
       "   'Language Modelling',\n",
       "   'Speech Recognition'],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/inverse-reinforcement-learning-from-summary',\n",
       "  'arxiv_id': '1703.09700',\n",
       "  'title': 'Inverse Reinforcement Learning from Summary Data',\n",
       "  'abstract': \"Inverse reinforcement learning (IRL) aims to explain observed strategic\\nbehavior by fitting reinforcement learning models to behavioral data. However,\\ntraditional IRL methods are only applicable when the observations are in the\\nform of state-action paths. This assumption may not hold in many real-world\\nmodeling settings, where only partial or summarized observations are available.\\nIn general, we may assume that there is a summarizing function $\\\\sigma$, which\\nacts as a filter between us and the true state-action paths that constitute the\\ndemonstration. Some initial approaches to extending IRL to such situations have\\nbeen presented, but with very specific assumptions about the structure of\\n$\\\\sigma$, such as that only certain state observations are missing. This paper\\ninstead focuses on the most general case of the problem, where no assumptions\\nare made about the summarizing function, except that it can be evaluated. We\\ndemonstrate that inference is still possible. The paper presents exact and\\napproximate inference algorithms that allow full posterior inference, which is\\nparticularly important for assessing parameter uncertainty in this challenging\\ninference situation. Empirical scalability is demonstrated to reasonably sized\\nproblems, and practical applicability is demonstrated by estimating the\\nposterior for a cognitive science RL model based on an observed user's task\\ncompletion time only.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1703.09700v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1703.09700v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Antti Kangasrääsiö', 'Samuel Kaski'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-03-28',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/glomo-unsupervisedly-learned-relational',\n",
       "  'arxiv_id': '1806.05662',\n",
       "  'title': 'GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations',\n",
       "  'abstract': 'Modern deep transfer learning approaches have mainly focused on learning\\ngeneric feature vectors from one task that are transferable to other tasks,\\nsuch as word embeddings in language and pretrained convolutional features in\\nvision. However, these approaches usually transfer unary features and largely\\nignore more structured graphical representations. This work explores the\\npossibility of learning generic latent relational graphs that capture\\ndependencies between pairs of data units (e.g., words or pixels) from\\nlarge-scale unlabeled data and transferring the graphs to downstream tasks. Our\\nproposed transfer learning framework improves performance on various tasks\\nincluding question answering, natural language inference, sentiment analysis,\\nand image classification. We also show that the learned graphs are generic\\nenough to be transferred to different embeddings on which the graphs have not\\nbeen trained (including GloVe embeddings, ELMo embeddings, and task-specific\\nRNN hidden unit), or embedding-free units such as image pixels.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05662v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05662v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Zhilin Yang',\n",
       "   'Jake Zhao',\n",
       "   'Bhuwan Dhingra',\n",
       "   'Kaiming He',\n",
       "   'William W. Cohen',\n",
       "   'Ruslan Salakhutdinov',\n",
       "   'Yann Lecun'],\n",
       "  'tasks': ['Image Classification',\n",
       "   'Natural Language Inference',\n",
       "   'Question Answering',\n",
       "   'Sentiment Analysis',\n",
       "   'Transfer Learning',\n",
       "   'Word Embeddings'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'GloVe',\n",
       "    'full_name': 'GloVe Embeddings',\n",
       "    'description': '**GloVe Embeddings** are a type of word embedding that encode the co-occurrence probability ratio between two words as vector differences. GloVe uses a weighted least squares objective $J$ that minimizes the difference between the dot product of the vectors of two words and the logarithm of their number of co-occurrences:\\r\\n\\r\\n$$ J=\\\\sum\\\\_{i, j=1}^{V}f\\\\left(𝑋\\\\_{i j}\\\\right)(w^{T}\\\\_{i}\\\\tilde{w}_{j} + b\\\\_{i} + \\\\tilde{b}\\\\_{j} - \\\\log{𝑋}\\\\_{ij})^{2} $$\\r\\n\\r\\nwhere $w\\\\_{i}$ and $b\\\\_{i}$ are the word vector and bias respectively of word $i$, $\\\\tilde{w}_{j}$ and $b\\\\_{j}$ are the context word vector and bias respectively of word $j$, $X\\\\_{ij}$ is the number of times word $i$ occurs in the context of word $j$, and $f$ is a weighting function that assigns lower weights to rare and frequent co-occurrences.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://aclanthology.org/D14-1162',\n",
       "    'source_title': 'GloVe: Global Vectors for Word Representation',\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Word Embeddings',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Natural Language Processing'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}},\n",
       "   {'name': 'BiLSTM',\n",
       "    'full_name': 'Bidirectional LSTM',\n",
       "    'description': 'A **Bidirectional LSTM**, or **biLSTM**, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction. BiLSTMs effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow *and* precede a word in a sentence).\\r\\n\\r\\nImage Source: Modelling Radiological Language with Bidirectional Long Short-Term Memory Networks, Cornegruta et al',\n",
       "    'introduced_year': 2001,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Bidirectional Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': 'Recurrent Neural Networks',\n",
       "     'area': 'Sequential'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ELMo',\n",
       "    'full_name': 'ELMo',\n",
       "    'description': '**Embeddings from Language Models**, or **ELMo**, is a type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus.\\r\\n\\r\\nA biLM combines both a forward and backward LM.  ELMo jointly maximizes the log likelihood of the forward and backward directions. To add ELMo to a supervised model, we freeze the weights of the biLM and then concatenate the ELMo vector $\\\\textbf{ELMO}^{task}_k$ with $\\\\textbf{x}_k$ and pass the ELMO enhanced representation $[\\\\textbf{x}_k; \\\\textbf{ELMO}^{task}_k]$ into the task RNN. Here $\\\\textbf{x}_k$ is a context-independent token representation for each token position. \\r\\n\\r\\nImage Source: [here](https://medium.com/@duyanhnguyen_38925/create-a-strong-text-classification-with-the-help-from-elmo-e90809ba29da)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1802.05365v2',\n",
       "    'source_title': 'Deep contextualized word representations',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Word Embeddings',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Natural Language Processing'}}],\n",
       "  'datasets_used_lower': ['imagenet', 'squad', 'imdb-movie-reviews', 'snli'],\n",
       "  'datasets_used_full': ['ImageNet', 'SQuAD', 'IMDb Movie Reviews', 'SNLI'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/compnet-complementary-segmentation-network',\n",
       "  'arxiv_id': '1804.00521',\n",
       "  'title': 'CompNet: Complementary Segmentation Network for Brain MRI Extraction',\n",
       "  'abstract': 'Brain extraction is a fundamental step for most brain imaging studies. In\\nthis paper, we investigate the problem of skull stripping and propose\\ncomplementary segmentation networks (CompNets) to accurately extract the brain\\nfrom T1-weighted MRI scans, for both normal and pathological brain images. The\\nproposed networks are designed in the framework of encoder-decoder networks and\\nhave two pathways to learn features from both the brain tissue and its\\ncomplementary part located outside of the brain. The complementary pathway\\nextracts the features in the non-brain region and leads to a robust solution to\\nbrain extraction from MRIs with pathologies, which do not exist in our training\\ndataset. We demonstrate the effectiveness of our networks by evaluating them on\\nthe OASIS dataset, resulting in the state of the art performance under the\\ntwo-fold cross-validation setting. Moreover, the robustness of our networks is\\nverified by testing on images with introduced pathologies and by showing its\\ninvariance to unseen brain pathologies. In addition, our complementary network\\ndesign is general and can be extended to address other image segmentation\\nproblems with better generalization.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.00521v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.00521v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Raunak Dey', 'Yi Hong'],\n",
       "  'tasks': ['Semantic Segmentation', 'Skull Stripping'],\n",
       "  'date': '2018-03-27',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/training-augmentation-with-adversarial',\n",
       "  'arxiv_id': '1806.02782',\n",
       "  'title': 'Training Augmentation with Adversarial Examples for Robust Speech Recognition',\n",
       "  'abstract': 'This paper explores the use of adversarial examples in training speech\\nrecognition systems to increase robustness of deep neural network acoustic\\nmodels. During training, the fast gradient sign method is used to generate\\nadversarial examples augmenting the original training data. Different from\\nconventional data augmentation based on data transformations, the examples are\\ndynamically generated based on current acoustic model parameters. We assess the\\nimpact of adversarial data augmentation in experiments on the Aurora-4 and\\nCHiME-4 single-channel tasks, showing improved robustness against noise and\\nchannel variation. Further improvement is obtained when combining adversarial\\nexamples with teacher/student training, leading to a 23% relative word error\\nrate reduction on Aurora-4.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.02782v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.02782v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sining Sun',\n",
       "   'Ching-Feng Yeh',\n",
       "   'Mari Ostendorf',\n",
       "   'Mei-Yuh Hwang',\n",
       "   'Lei Xie'],\n",
       "  'tasks': ['Data Augmentation',\n",
       "   'Robust Speech Recognition',\n",
       "   'Speech Recognition'],\n",
       "  'date': '2018-06-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/approximate-submodular-functions-and',\n",
       "  'arxiv_id': '1806.06323',\n",
       "  'title': 'Approximate Submodular Functions and Performance Guarantees',\n",
       "  'abstract': 'We consider the problem of maximizing non-negative non-decreasing set\\nfunctions. Although most of the recent work focus on exploiting submodularity,\\nit turns out that several objectives we encounter in practice are not\\nsubmodular. Nonetheless, often we leverage the greedy algorithms used in\\nsubmodular functions to determine a solution to the non-submodular functions.\\nHereafter, we propose to address the original problem by \\\\emph{approximating}\\nthe non-submodular function and analyze the incurred error, as well as the\\nperformance trade-offs. To quantify the approximation error, we introduce a\\nnovel concept of $\\\\delta$-approximation of a function, which we used to define\\nthe space of submodular functions that lie within an approximation error. We\\nprovide necessary conditions on the existence of such $\\\\delta$-approximation\\nfunctions, which might not be unique. Consequently, we characterize this\\nsubspace which we refer to as \\\\emph{region of submodularity}. Furthermore,\\nsubmodular functions are known to lead to different sub-optimality guarantees,\\nso we generalize those dependencies upon a $\\\\delta$-approximation into the\\nnotion of \\\\emph{greedy curvature}. Finally, we used this latter notion to\\nsimplify some of the existing results and efficiently (i.e., linear complexity)\\ndetermine tightened bounds on the sub-optimality guarantees using objective\\nfunctions commonly used in practical setups and validate them using real data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06323v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06323v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Gaurav Gupta', 'Sergio Pequito', 'Paul Bogdan'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/comparative-survey-of-visual-object',\n",
       "  'arxiv_id': '1806.06321',\n",
       "  'title': 'Comparative survey of visual object classifiers',\n",
       "  'abstract': 'Classification of Visual Object Classes represents one of the most elaborated\\nareas of interest in Computer Vision. It is always challenging to get one\\nspecific detector, descriptor or classifier that provides the expected object\\nclassification result. Consequently, it critical to compare the different\\ndetection, descriptor and classifier methods available and chose a single or\\ncombination of two or three to get an optimal result. In this paper, we have\\npresented a comparative survey of different feature descriptors and\\nclassifiers. From feature descriptors, SIFT (Sparse & Dense) and HeuSIFT\\ncombination colour descriptors; From classification techniques, Support Vector\\nClassifier, K-Nearest Neighbor, ADABOOST, and fisher are covered in comparative\\npractical implementation survey.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06321v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06321v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hiliwi Leake Kidane'],\n",
       "  'tasks': ['Classification', 'General Classification'],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-neural-nets-with-interpolating-function',\n",
       "  'arxiv_id': '1802.00168',\n",
       "  'title': 'Deep Neural Nets with Interpolating Function as Output Activation',\n",
       "  'abstract': 'We replace the output layer of deep neural nets, typically the softmax\\nfunction, by a novel interpolating function. And we propose end-to-end training\\nand testing algorithms for this new architecture. Compared to classical neural\\nnets with softmax function as output activation, the surrogate with\\ninterpolating function as output activation combines advantages of both deep\\nand manifold learning. The new framework demonstrates the following major\\nadvantages: First, it is better applicable to the case with insufficient\\ntraining data. Second, it significantly improves the generalization accuracy on\\na wide variety of networks. The algorithm is implemented in PyTorch, and code\\nwill be made publicly available.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.00168v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.00168v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Bao Wang',\n",
       "   'Xiyang Luo',\n",
       "   'Zhen Li',\n",
       "   'Wei Zhu',\n",
       "   'Zuoqiang Shi',\n",
       "   'Stanley J. Osher'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-01',\n",
       "  'methods': [{'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['cifar-10', 'cifar-100', 'svhn'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'CIFAR-100', 'SVHN'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/laplacian-smoothing-gradient-descent',\n",
       "  'arxiv_id': '1806.06317',\n",
       "  'title': 'Laplacian Smoothing Gradient Descent',\n",
       "  'abstract': 'We propose a class of very simple modifications of gradient descent and\\nstochastic gradient descent. We show that when applied to a large variety of\\nmachine learning problems, ranging from logistic regression to deep neural\\nnets, the proposed surrogates can dramatically reduce the variance, allow to\\ntake a larger step size, and improve the generalization accuracy. The methods\\nonly involve multiplying the usual (stochastic) gradient by the inverse of a\\npositive definitive matrix (which can be computed efficiently by FFT) with a\\nlow condition number coming from a one-dimensional discrete Laplacian or its\\nhigh order generalizations. It also preserves the mean and increases the\\nsmallest component and decreases the largest component. The theory of\\nHamilton-Jacobi partial differential equations demonstrates that the implicit\\nversion of the new algorithm is almost the same as doing gradient descent on a\\nnew function which (i) has the same global minima as the original function and\\n(ii) is ``more convex\". Moreover, we show that optimization algorithms with\\nthese surrogates converge uniformly in the discrete Sobolev $H_\\\\sigma^p$ sense\\nand reduce the optimality gap for convex optimization problems. The code is\\navailable at:\\n\\\\url{https://github.com/BaoWangMath/LaplacianSmoothing-GradientDescent}',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06317v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06317v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Stanley Osher',\n",
       "   'Bao Wang',\n",
       "   'Penghang Yin',\n",
       "   'Xiyang Luo',\n",
       "   'Farzin Barekat',\n",
       "   'Minh Pham',\n",
       "   'Alex Lin'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-17',\n",
       "  'methods': [{'name': 'Logistic Regression',\n",
       "    'full_name': 'Logistic Regression',\n",
       "    'description': '**Logistic Regression**, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\\r\\n\\r\\nSource: [scikit-learn](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\\r\\n\\r\\nImage: [Michaelg2015](https://commons.wikimedia.org/wiki/User:Michaelg2015)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Generalized Linear Models',\n",
       "     'description': '**Generalized Linear Models (GLMs)** are a class of models that generalize upon linear regression by allowing many more distributions to be modeled for the response variable via a link function. Below you can find a continuously updating list of GLMs.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['cifar-10', 'mnist'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/minimal-i-map-mcmc-for-scalable-structure',\n",
       "  'arxiv_id': '1803.05554',\n",
       "  'title': 'Minimal I-MAP MCMC for Scalable Structure Discovery in Causal DAG Models',\n",
       "  'abstract': 'Learning a Bayesian network (BN) from data can be useful for decision-making\\nor discovering causal relationships. However, traditional methods often fail in\\nmodern applications, which exhibit a larger number of observed variables than\\ndata points. The resulting uncertainty about the underlying network as well as\\nthe desire to incorporate prior information recommend a Bayesian approach to\\nlearning the BN, but the highly combinatorial structure of BNs poses a striking\\nchallenge for inference. The current state-of-the-art methods such as order\\nMCMC are faster than previous methods but prevent the use of many natural\\nstructural priors and still have running time exponential in the maximum\\nindegree of the true directed acyclic graph (DAG) of the BN. We here propose an\\nalternative posterior approximation based on the observation that, if we\\nincorporate empirical conditional independence tests, we can focus on a\\nhigh-probability DAG associated with each order of the vertices. We show that\\nour method allows the desired flexibility in prior specification, removes\\ntiming dependence on the maximum indegree and yields provably good posterior\\napproximations; in addition, we show that it achieves superior accuracy,\\nscalability, and sampler mixing on several datasets.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.05554v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.05554v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Raj Agrawal', 'Tamara Broderick', 'Caroline Uhler'],\n",
       "  'tasks': ['Decision Making'],\n",
       "  'date': '2018-03-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/personalized-saliency-and-its-prediction',\n",
       "  'arxiv_id': '1710.03011',\n",
       "  'title': 'Personalized Saliency and its Prediction',\n",
       "  'abstract': 'Nearly all existing visual saliency models by far have focused on predicting\\na universal saliency map across all observers. Yet psychology studies suggest\\nthat visual attention of different observers can vary significantly under\\nspecific circumstances, especially a scene is composed of multiple salient\\nobjects. To study such heterogenous visual attention pattern across observers,\\nwe first construct a personalized saliency dataset and explore correlations\\nbetween visual attention, personal preferences, and image contents.\\nSpecifically, we propose to decompose a personalized saliency map (referred to\\nas PSM) into a universal saliency map (referred to as USM) predictable by\\nexisting saliency detection models and a new discrepancy map across users that\\ncharacterizes personalized saliency. We then present two solutions towards\\npredicting such discrepancy maps, i.e., a multi-task convolutional neural\\nnetwork (CNN) framework and an extended CNN with Person-specific Information\\nEncoded Filters (CNN-PIEF). Extensive experimental results demonstrate the\\neffectiveness of our models for PSM prediction as well their generalization\\ncapability for unseen observers.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1710.03011v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1710.03011v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yanyu Xu',\n",
       "   'Shenghua Gao',\n",
       "   'Junru Wu',\n",
       "   'Nianyi Li',\n",
       "   'Jingyi Yu'],\n",
       "  'tasks': ['Saliency Detection'],\n",
       "  'date': '2017-10-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['pascal-s', 'salicon', 'isun'],\n",
       "  'datasets_used_full': ['PASCAL-S', 'SALICON', 'iSUN'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/causal-generative-domain-adaptation-networks',\n",
       "  'arxiv_id': '1804.04333',\n",
       "  'title': 'Causal Generative Domain Adaptation Networks',\n",
       "  'abstract': 'An essential problem in domain adaptation is to understand and make use of\\ndistribution changes across domains. For this purpose, we first propose a\\nflexible Generative Domain Adaptation Network (G-DAN) with specific latent\\nvariables to capture changes in the generating process of features across\\ndomains. By explicitly modeling the changes, one can even generate data in new\\ndomains using the generating process with new values for the latent variables\\nin G-DAN. In practice, the process to generate all features together may\\ninvolve high-dimensional latent variables, requiring dealing with distributions\\nin high dimensions and making it difficult to learn domain changes from few\\nsource domains. Interestingly, by further making use of the causal\\nrepresentation of joint distributions, we then decompose the joint distribution\\ninto separate modules, each of which involves different low-dimensional latent\\nvariables and can be learned separately, leading to a Causal G-DAN (CG-DAN).\\nThis improves both statistical and computational efficiency of the learning\\nprocedure. Finally, by matching the feature distribution in the target domain,\\nwe can recover the target-domain joint distribution and derive the learning\\nmachine for the target domain. We demonstrate the efficacy of both G-DAN and\\nCG-DAN in domain generation and cross-domain prediction on both synthetic and\\nreal data experiments.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.04333v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.04333v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mingming Gong',\n",
       "   'Kun Zhang',\n",
       "   'Biwei Huang',\n",
       "   'Clark Glymour',\n",
       "   'DaCheng Tao',\n",
       "   'Kayhan Batmanghelich'],\n",
       "  'tasks': ['Domain Adaptation'],\n",
       "  'date': '2018-04-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/biased-embeddings-from-wild-data-measuring',\n",
       "  'arxiv_id': '1806.06301',\n",
       "  'title': 'Biased Embeddings from Wild Data: Measuring, Understanding and Removing',\n",
       "  'abstract': 'Many modern Artificial Intelligence (AI) systems make use of data embeddings,\\nparticularly in the domain of Natural Language Processing (NLP). These\\nembeddings are learnt from data that has been gathered \"from the wild\" and have\\nbeen found to contain unwanted biases. In this paper we make three\\ncontributions towards measuring, understanding and removing this problem. We\\npresent a rigorous way to measure some of these biases, based on the use of\\nword lists created for social psychology applications; we observe how gender\\nbias in occupations reflects actual gender bias in the same occupations in the\\nreal world; and finally we demonstrate how a simple projection can\\nsignificantly reduce the effects of embedding bias. All this is part of an\\nongoing effort to understand how trust can be built into AI systems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06301v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06301v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Adam Sutton', 'Thomas Lansdall-Welfare', 'Nello Cristianini'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deformable-generator-network-unsupervised',\n",
       "  'arxiv_id': '1806.06298',\n",
       "  'title': 'Deformable Generator Network: Unsupervised Disentanglement of Appearance and Geometry',\n",
       "  'abstract': 'We present a deformable generator model to disentangle the appearance and geometric information for both image and video data in a purely unsupervised manner. The appearance generator network models the information related to appearance, including color, illumination, identity or category, while the geometric generator performs geometric warping, such as rotation and stretching, through generating deformation field which is used to warp the generated appearance to obtain the final image or video sequences. Two generators take independent latent vectors as input to disentangle the appearance and geometric information from image or video sequences. For video data, a nonlinear transition model is introduced to both the appearance and geometric generators to capture the dynamics over time. The proposed scheme is general and can be easily integrated into different generative models. An extensive set of qualitative and quantitative experiments shows that the appearance and geometric information can be well disentangled, and the learned geometric generator can be conveniently transferred to other image datasets to facilitate knowledge transfer tasks.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06298v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06298v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xianglei Xing',\n",
       "   'Ruiqi Gao',\n",
       "   'Tian Han',\n",
       "   'Song-Chun Zhu',\n",
       "   'Ying Nian Wu'],\n",
       "  'tasks': ['Disentanglement', 'Transfer Learning'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'celeba', 'ck', 'mafl'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'CelebA', 'CK+', 'MAFL'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/right-for-the-right-reason-training-agnostic',\n",
       "  'arxiv_id': '1806.06296',\n",
       "  'title': 'Right for the Right Reason: Training Agnostic Networks',\n",
       "  'abstract': 'We consider the problem of a neural network being requested to classify\\nimages (or other inputs) without making implicit use of a \"protected concept\",\\nthat is a concept that should not play any role in the decision of the network.\\nTypically these concepts include information such as gender or race, or other\\ncontextual information such as image backgrounds that might be implicitly\\nreflected in unknown correlations with other variables, making it insufficient\\nto simply remove them from the input features. In other words, making accurate\\npredictions is not good enough if those predictions rely on information that\\nshould not be used: predictive performance is not the only important metric for\\nlearning systems. We apply a method developed in the context of domain\\nadaptation to address this problem of \"being right for the right reason\", where\\nwe request a classifier to make a decision in a way that is entirely \\'agnostic\\'\\nto a given protected concept (e.g. gender, race, background etc.), even if this\\ncould be implicitly reflected in other attributes via unknown correlations.\\nAfter defining the concept of an \\'agnostic model\\', we demonstrate how the\\nDomain-Adversarial Neural Network can remove unwanted information from a model\\nusing a gradient reversal layer.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06296v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06296v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sen Jia', 'Thomas Lansdall-Welfare', 'Nello Cristianini'],\n",
       "  'tasks': ['Domain Adaptation'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet', 'places'],\n",
       "  'datasets_used_full': ['ImageNet', 'Places'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deepmimic-example-guided-deep-reinforcement',\n",
       "  'arxiv_id': '1804.02717',\n",
       "  'title': 'DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills',\n",
       "  'abstract': 'A longstanding goal in character animation is to combine data-driven\\nspecification of behavior with a system that can execute a similar behavior in\\na physical simulation, thus enabling realistic responses to perturbations and\\nenvironmental variation. We show that well-known reinforcement learning (RL)\\nmethods can be adapted to learn robust control policies capable of imitating a\\nbroad range of example motion clips, while also learning complex recoveries,\\nadapting to changes in morphology, and accomplishing user-specified goals. Our\\nmethod handles keyframed motions, highly-dynamic actions such as\\nmotion-captured flips and spins, and retargeted motions. By combining a\\nmotion-imitation objective with a task objective, we can train characters that\\nreact intelligently in interactive settings, e.g., by walking in a desired\\ndirection or throwing a ball at a user-specified target. This approach thus\\ncombines the convenience and motion quality of using motion clips to define the\\ndesired style and appearance, with the flexibility and generality afforded by\\nRL methods and physics-based animation. We further explore a number of methods\\nfor integrating multiple clips into the learning process to develop\\nmulti-skilled agents capable of performing a rich repertoire of diverse skills.\\nWe demonstrate results using multiple characters (human, Atlas robot, bipedal\\ndinosaur, dragon) and a large variety of skills, including locomotion,\\nacrobatics, and martial arts.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.02717v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.02717v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xue Bin Peng',\n",
       "   'Pieter Abbeel',\n",
       "   'Sergey Levine',\n",
       "   'Michiel Van de Panne'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-04-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/latent-convolutional-models',\n",
       "  'arxiv_id': '1806.06284',\n",
       "  'title': 'Latent Convolutional Models',\n",
       "  'abstract': 'We present a new latent model of natural images that can be learned on\\nlarge-scale datasets. The learning process provides a latent embedding for\\nevery image in the training dataset, as well as a deep convolutional network\\nthat maps the latent space to the image space. After training, the new model\\nprovides a strong and universal image prior for a variety of image restoration\\ntasks such as large-hole inpainting, superresolution, and colorization. To\\nmodel high-resolution natural images, our approach uses latent spaces of very\\nhigh dimensionality (one to two orders of magnitude higher than previous latent\\nimage models). To tackle this high dimensionality, we use latent spaces with a\\nspecial manifold structure (convolutional manifolds) parameterized by a ConvNet\\nof a certain architecture. In the experiments, we compare the learned latent\\nmodels with latent models learned by autoencoders, advanced variants of\\ngenerative adversarial networks, and a strong baseline system using simpler\\nparameterization of the latent space. Our model outperforms the competing\\napproaches over a range of restoration tasks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06284v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06284v2.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['ShahRukh Athar', 'Evgeny Burnaev', 'Victor Lempitsky'],\n",
       "  'tasks': ['Colorization', 'Image Restoration'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['celeba', 'celeba-hq'],\n",
       "  'datasets_used_full': ['CelebA', 'CelebA-HQ'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/first-and-second-order-methods-for-online',\n",
       "  'arxiv_id': '1709.00106',\n",
       "  'title': 'First and Second Order Methods for Online Convolutional Dictionary Learning',\n",
       "  'abstract': 'Convolutional sparse representations are a form of sparse representation with\\na structured, translation invariant dictionary. Most convolutional dictionary\\nlearning algorithms to date operate in batch mode, requiring simultaneous\\naccess to all training images during the learning process, which results in\\nvery high memory usage and severely limits the training data that can be used.\\nVery recently, however, a number of authors have considered the design of\\nonline convolutional dictionary learning algorithms that offer far better\\nscaling of memory and computational cost with training set size than batch\\nmethods. This paper extends our prior work, improving a number of aspects of\\nour previous algorithm; proposing an entirely new one, with better performance,\\nand that supports the inclusion of a spatial mask for learning from incomplete\\ndata; and providing a rigorous theoretical analysis of these methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1709.00106v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1709.00106v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jialin Liu',\n",
       "   'Cristina Garcia-Cardona',\n",
       "   'Brendt Wohlberg',\n",
       "   'Wotao Yin'],\n",
       "  'tasks': ['Dictionary Learning', 'Translation'],\n",
       "  'date': '2017-08-31',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/stable-prediction-across-unknown-environments',\n",
       "  'arxiv_id': '1806.06270',\n",
       "  'title': 'Stable Prediction across Unknown Environments',\n",
       "  'abstract': 'In many important machine learning applications, the training distribution\\nused to learn a probabilistic classifier differs from the testing distribution\\non which the classifier will be used to make predictions. Traditional methods\\ncorrect the distribution shift by reweighting the training data with the ratio\\nof the density between test and training data. In many applications training\\ntakes place without prior knowledge of the testing distribution on which the\\nalgorithm will be applied in the future. Recently, methods have been proposed\\nto address the shift by learning causal structure, but those methods rely on\\nthe diversity of multiple training data to a good performance, and have\\ncomplexity limitations in high dimensions. In this paper, we propose a novel\\nDeep Global Balancing Regression (DGBR) algorithm to jointly optimize a deep\\nauto-encoder model for feature selection and a global balancing model for\\nstable prediction across unknown environments. The global balancing model\\nconstructs balancing weights that facilitate estimating of partial effects of\\nfeatures (holding fixed all other features), a problem that is challenging in\\nhigh dimensions, and thus helps to identify stable, causal relationships\\nbetween features and outcomes. The deep auto-encoder model is designed to\\nreduce the dimensionality of the feature space, thus making global balancing\\neasier. We show, both theoretically and with empirical experiments, that our\\nalgorithm can make stable predictions across unknown environments. Our\\nexperiments on both synthetic and real world datasets demonstrate that our DGBR\\nalgorithm outperforms the state-of-the-art methods for stable prediction across\\nunknown environments.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06270v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06270v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kun Kuang',\n",
       "   'Ruoxuan Xiong',\n",
       "   'Peng Cui',\n",
       "   'Susan Athey',\n",
       "   'Bo Li'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/extraction-of-technical-information-from',\n",
       "  'arxiv_id': '1806.02242',\n",
       "  'title': 'Extraction Of Technical Information From Normative Documents Using Automated Methods Based On Ontologies : Application To The Iso 15531 Mandate Standard - Methodology And First Results',\n",
       "  'abstract': 'Problems faced by international standardization bodies become more and more\\ncrucial as the number and the size of the standards they produce increase.\\nSometimes, also, the lack of coordination among the committees in charge of the\\ndevelopment of standards may lead to overlaps, mistakes or incompatibilities in\\nthe documents. The aim of this study is to present a methodology enabling an\\nautomatic extraction of the technical concepts (terms) found in normative\\ndocuments, through the use of semantic tools coming from the field of language\\nprocessing. The first part of the paper provides a description of the\\nstandardization world, its structure, its way of working and the problems\\nfaced; we then introduce the concepts of semantic annotation, information\\nextraction and the software tools available in this domain. The next section\\nexplains the concept of ontology and its potential use in the field of\\nstandardization. We propose here a methodology enabling the extraction of\\ntechnical information from a given normative corpus, based on a semantic\\nannotation process done according to reference ontologies. The application to\\nthe ISO 15531 MANDATE corpus provides a first use case of the methodology\\ndescribed in this paper. The paper ends with the description of the first\\nexperimental results produced by this approach, and with some issues and\\nperspectives, notably its application to other standards and, or Technical\\nCommittees and the possibility offered to create pre-defined technical\\ndictionaries of terms.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.02242v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.02242v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['A. F. Cutting-Decelle',\n",
       "   'A. Digeon',\n",
       "   'R. I. Young',\n",
       "   'J. L. Barraud',\n",
       "   'P. Lamboley'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-06',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-strategyproof-conference-peer-review',\n",
       "  'arxiv_id': '1806.06266',\n",
       "  'title': 'On Strategyproof Conference Peer Review',\n",
       "  'abstract': \"We consider peer review in a conference setting where there is typically an overlap between the set of reviewers and the set of authors. This overlap can incentivize strategic reviews to influence the final ranking of one's own papers. In this work, we address this problem through the lens of social choice, and present a theoretical framework for strategyproof and efficient peer review. We first present and analyze an algorithm for reviewer-assignment and aggregation that guarantees strategyproofness and a natural efficiency property called unanimity, when the authorship graph satisfies a simple property. Our algorithm is based on the so-called partitioning method, and can be thought as a generalization of this method to conference peer review settings. We then empirically show that the requisite property on the authorship graph is indeed satisfied in the submission data from the ICLR conference, and further demonstrate a simple trick to make the partitioning method more practically appealing for conference peer review. Finally, we complement our positive results with negative theoretical results where we prove that under various ways of strengthening the requirements, it is impossible for any algorithm to be strategyproof and efficient.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06266v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06266v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yichong Xu',\n",
       "   'Han Zhao',\n",
       "   'Xiaofei Shi',\n",
       "   'Jeremy Zhang',\n",
       "   'Nihar B. Shah'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/evaluation-of-sentence-embeddings-in',\n",
       "  'arxiv_id': '1806.06259',\n",
       "  'title': 'Evaluation of sentence embeddings in downstream and linguistic probing tasks',\n",
       "  'abstract': 'Despite the fast developmental pace of new sentence embedding methods, it is\\nstill challenging to find comprehensive evaluations of these different\\ntechniques. In the past years, we saw significant improvements in the field of\\nsentence embeddings and especially towards the development of universal\\nsentence encoders that could provide inductive transfer to a wide variety of\\ndownstream tasks. In this work, we perform a comprehensive evaluation of recent\\nmethods using a wide variety of downstream and linguistic feature probing\\ntasks. We show that a simple approach using bag-of-words with a recently\\nintroduced language model for deep context-dependent word embeddings proved to\\nyield better results in many tasks when compared to sentence encoders trained\\non entailment datasets. We also show, however, that we are still far away from\\na universal encoder that can perform consistently across several downstream\\ntasks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06259v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06259v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Christian S. Perone', 'Roberto Silveira', 'Thomas S. Paula'],\n",
       "  'tasks': ['Language Modelling',\n",
       "   'Sentence Embedding',\n",
       "   'Sentence-Embedding',\n",
       "   'Sentence Embeddings',\n",
       "   'Word Embeddings'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['coco',\n",
       "   'sst',\n",
       "   'snli',\n",
       "   'mpqa-opinion-corpus',\n",
       "   'senteval'],\n",
       "  'datasets_used_full': ['COCO',\n",
       "   'SST',\n",
       "   'SNLI',\n",
       "   'MPQA Opinion Corpus',\n",
       "   'SentEval'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dynmat-a-network-that-can-learn-after',\n",
       "  'arxiv_id': '1806.06253',\n",
       "  'title': 'DynMat, a network that can learn after learning',\n",
       "  'abstract': \"To survive in the dynamically-evolving world, we accumulate knowledge and\\nimprove our skills based on experience. In the process, gaining new knowledge\\ndoes not disrupt our vigilance to external stimuli. In other words, our\\nlearning process is 'accumulative' and 'online' without interruption. However,\\ndespite the recent success, artificial neural networks (ANNs) must be trained\\noffline, and they suffer catastrophic interference between old and new\\nlearning, indicating that ANNs' conventional learning algorithms may not be\\nsuitable for building intelligent agents comparable to our brain. In this\\nstudy, we propose a novel neural network architecture (DynMat) consisting of\\ndual learning systems, inspired by the complementary learning system (CLS)\\ntheory suggesting that the brain relies on short- and long-term learning\\nsystems to learn continuously. Our experiments show that 1) DynMat can learn a\\nnew class without catastrophic interference and 2) it does not strictly require\\noffline training.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06253v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06253v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jung H. Lee'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'mnist', 'cifar-100', 'fashion-mnist'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'MNIST', 'CIFAR-100', 'Fashion-MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/neyman-pearson-classification-parametrics-and',\n",
       "  'arxiv_id': '1802.02557',\n",
       "  'title': 'Neyman-Pearson classification: parametrics and sample size requirement',\n",
       "  'abstract': 'The Neyman-Pearson (NP) paradigm in binary classification seeks classifiers that achieve a minimal type II error while enforcing the prioritized type I error controlled under some user-specified level $\\\\alpha$. This paradigm serves naturally in applications such as severe disease diagnosis and spam detection, where people have clear priorities among the two error types. Recently, Tong, Feng and Li (2018) proposed a nonparametric umbrella algorithm that adapts all scoring-type classification methods (e.g., logistic regression, support vector machines, random forest) to respect the given type I error upper bound $\\\\alpha$ with high probability, without specific distributional assumptions on the features and the responses. Universal the umbrella algorithm is, it demands an explicit minimum sample size requirement on class $0$, which is often the more scarce class, such as in rare disease diagnosis applications. In this work, we employ the parametric linear discriminant analysis (LDA) model and propose a new parametric thresholding algorithm, which does not need the minimum sample size requirements on class $0$ observations and thus is suitable for small sample applications such as rare disease diagnosis. Leveraging both the existing nonparametric and the newly proposed parametric thresholding rules, we propose four LDA-based NP classifiers, for both low- and high-dimensional settings. On the theoretical front, we prove NP oracle inequalities for one proposed classifier, where the rate for excess type II error benefits from the explicit parametric model assumption. Furthermore, as NP classifiers involve a sample splitting step of class $0$ observations, we construct a new adaptive sample splitting scheme that can be applied universally to NP classifiers, and this adaptive strategy reduces the type II error of these classifiers.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1802.02557v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1802.02557v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xin Tong', 'Lucy Xia', 'Jiacheng Wang', 'Yang Feng'],\n",
       "  'tasks': ['Classification', 'General Classification'],\n",
       "  'date': '2018-02-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/real-time-prediction-of-segmentation-quality',\n",
       "  'arxiv_id': '1806.06244',\n",
       "  'title': 'Real-time Prediction of Segmentation Quality',\n",
       "  'abstract': 'Recent advances in deep learning based image segmentation methods have\\nenabled real-time performance with human-level accuracy. However, occasionally\\neven the best method fails due to low image quality, artifacts or unexpected\\nbehaviour of black box algorithms. Being able to predict segmentation quality\\nin the absence of ground truth is of paramount importance in clinical practice,\\nbut also in large-scale studies to avoid the inclusion of invalid data in\\nsubsequent analysis.\\n  In this work, we propose two approaches of real-time automated quality\\ncontrol for cardiovascular MR segmentations using deep learning. First, we\\ntrain a neural network on 12,880 samples to predict Dice Similarity\\nCoefficients (DSC) on a per-case basis. We report a mean average error (MAE) of\\n0.03 on 1,610 test samples and 97% binary classification accuracy for\\nseparating low and high quality segmentations. Secondly, in the scenario where\\nno manually annotated data is available, we train a network to predict DSC\\nscores from estimated quality obtained via a reverse testing strategy. We\\nreport an MAE=0.14 and 91% binary classification accuracy for this case.\\nPredictions are obtained in real-time which, when combined with real-time\\nsegmentation methods, enables instant feedback on whether an acquired scan is\\nanalysable while the patient is still in the scanner. This further enables new\\napplications of optimising image acquisition towards best possible analysis\\nresults.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06244v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06244v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Robert Robinson',\n",
       "   'Ozan Oktay',\n",
       "   'Wenjia Bai',\n",
       "   'Vanya Valindria',\n",
       "   'Mihir Sanghvi',\n",
       "   'Nay Aung',\n",
       "   'José Paiva',\n",
       "   'Filip Zemrak',\n",
       "   'Kenneth Fung',\n",
       "   'Elena Lukaschuk',\n",
       "   'Aaron Lee',\n",
       "   'Valentina Carapella',\n",
       "   'Young Jin Kim',\n",
       "   'Bernhard Kainz',\n",
       "   'Stefan Piechnik',\n",
       "   'Stefan Neubauer',\n",
       "   'Steffen Petersen',\n",
       "   'Chris Page',\n",
       "   'Daniel Rueckert',\n",
       "   'Ben Glocker'],\n",
       "  'tasks': ['General Classification', 'Semantic Segmentation'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/retrofitting-distributional-embeddings-to',\n",
       "  'arxiv_id': '1708.00112',\n",
       "  'title': 'Retrofitting Distributional Embeddings to Knowledge Graphs with Functional Relations',\n",
       "  'abstract': 'Knowledge graphs are a versatile framework to encode richly structured data\\nrelationships, but it can be challenging to combine these graphs with\\nunstructured data. Methods for retrofitting pre-trained entity representations\\nto the structure of a knowledge graph typically assume that entities are\\nembedded in a connected space and that relations imply similarity. However,\\nuseful knowledge graphs often contain diverse entities and relations (with\\npotentially disjoint underlying corpora) which do not accord with these\\nassumptions. To overcome these limitations, we present Functional Retrofitting,\\na framework that generalizes current retrofitting methods by explicitly\\nmodeling pairwise relations. Our framework can directly incorporate a variety\\nof pairwise penalty functions previously developed for knowledge graph\\ncompletion. Further, it allows users to encode, learn, and extract information\\nabout relation semantics. We present both linear and neural instantiations of\\nthe framework. Functional Retrofitting significantly outperforms existing\\nretrofitting methods on complex knowledge graphs and loses no accuracy on\\nsimpler graphs (in which relations do imply similarity). Finally, we\\ndemonstrate the utility of the framework by predicting new drug--disease\\ntreatment pairs in a large, complex health knowledge graph.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1708.00112v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1708.00112v3.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Benjamin J. Lengerich', 'Andrew L. Maas', 'Christopher Potts'],\n",
       "  'tasks': ['Knowledge Graph Completion', 'Knowledge Graphs'],\n",
       "  'date': '2017-08-01',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['framenet'],\n",
       "  'datasets_used_full': ['FrameNet'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/machine-translation-in-indian-languages',\n",
       "  'arxiv_id': '1708.07950',\n",
       "  'title': 'Machine Translation in Indian Languages: Challenges and Resolution',\n",
       "  'abstract': 'English to Indian language machine translation poses the challenge of\\nstructural and morphological divergence. This paper describes English to Indian\\nlanguage statistical machine translation using pre-ordering and suffix\\nseparation. The pre-ordering uses rules to transfer the structure of the source\\nsentences prior to training and translation. This syntactic restructuring helps\\nstatistical machine translation to tackle the structural divergence and hence\\nbetter translation quality. The suffix separation is used to tackle the\\nmorphological divergence between English and highly agglutinative Indian\\nlanguages. We demonstrate that the use of pre-ordering and suffix separation\\nhelps in improving the quality of English to Indian Language machine\\ntranslation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1708.07950v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1708.07950v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Raj Nath Patel', 'Prakash B. Pimpale', 'M Sasikumar'],\n",
       "  'tasks': ['Machine Translation', 'Translation'],\n",
       "  'date': '2017-08-26',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/peerreview4all-fair-and-accurate-reviewer',\n",
       "  'arxiv_id': '1806.06237',\n",
       "  'title': 'PeerReview4All: Fair and Accurate Reviewer Assignment in Peer Review',\n",
       "  'abstract': 'We consider the problem of automated assignment of papers to reviewers in conference peer review, with a focus on fairness and statistical accuracy. Our fairness objective is to maximize the review quality of the most disadvantaged paper, in contrast to the commonly used objective of maximizing the total quality over all papers. We design an assignment algorithm based on an incremental max-flow procedure that we prove is near-optimally fair. Our statistical accuracy objective is to ensure correct recovery of the papers that should be accepted. We provide a sharp minimax analysis of the accuracy of the peer-review process for a popular objective-score model as well as for a novel subjective-score model that we propose in the paper. Our analysis proves that our proposed assignment algorithm also leads to a near-optimal statistical accuracy. Finally, we design a novel experiment that allows for an objective comparison of various assignment algorithms, and overcomes the inherent difficulty posed by the absence of a ground truth in experiments on peer-review. The results of this experiment as well as of other experiments on synthetic and real data corroborate the theoretical guarantees of our algorithm.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06237v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06237v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ivan Stelmakh', 'Nihar B. Shah', 'Aarti Singh'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/binary-classification-in-unstructured-space',\n",
       "  'arxiv_id': '1806.06232',\n",
       "  'title': 'Binary Classification in Unstructured Space With Hypergraph Case-Based Reasoning',\n",
       "  'abstract': 'Binary classification is one of the most common problem in machine learning.\\nIt consists in predicting whether a given element belongs to a particular\\nclass. In this paper, a new algorithm for binary classification is proposed\\nusing a hypergraph representation. The method is agnostic to data\\nrepresentation, can work with multiple data sources or in non-metric spaces,\\nand accommodates with missing values. As a result, it drastically reduces the\\nneed for data preprocessing or feature engineering. Each element to be\\nclassified is partitioned according to its interactions with the training set.\\nFor each class, a seminorm over the training set partition is learnt to\\nrepresent the distribution of evidence supporting this class.\\n  Empirical validation demonstrates its high potential on a wide range of\\nwell-known datasets and the results are compared to the state-of-the-art. The\\ntime complexity is given and empirically validated. Its robustness with regard\\nto hyperparameter sensitivity is studied and compared to standard\\nclassification methods. Finally, the limitation of the model space is\\ndiscussed, and some potential solutions proposed.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06232v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06232v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alexandre Quemy'],\n",
       "  'tasks': ['Classification', 'Feature Engineering', 'General Classification'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multimodal-sentiment-analysis-using',\n",
       "  'arxiv_id': '1806.06228',\n",
       "  'title': 'Multimodal Sentiment Analysis using Hierarchical Fusion with Context Modeling',\n",
       "  'abstract': 'Multimodal sentiment analysis is a very actively growing field of research. A\\npromising area of opportunity in this field is to improve the multimodal fusion\\nmechanism. We present a novel feature fusion strategy that proceeds in a\\nhierarchical fashion, first fusing the modalities two in two and only then\\nfusing all three modalities. On multimodal sentiment analysis of individual\\nutterances, our strategy outperforms conventional concatenation of features by\\n1%, which amounts to 5% reduction in error rate. On utterance-level multimodal\\nsentiment analysis of multi-utterance video clips, for which current\\nstate-of-the-art techniques incorporate contextual information from other\\nutterances of the same clip, our hierarchical fusion gives up to 2.4% (almost\\n10% error rate reduction) over currently used concatenation. The implementation\\nof our method is publicly available in the form of open-source code.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06228v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06228v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['N. Majumder',\n",
       "   'D. Hazarika',\n",
       "   'A. Gelbukh',\n",
       "   'E. Cambria',\n",
       "   'S. Poria'],\n",
       "  'tasks': ['Multimodal Emotion Recognition',\n",
       "   'Multimodal Sentiment Analysis',\n",
       "   'Sentiment Analysis'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['iemocap',\n",
       "   'multimodal-opinionlevel-sentiment-intensity'],\n",
       "  'datasets_used_full': ['IEMOCAP',\n",
       "   'Multimodal Opinionlevel Sentiment Intensity'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/gile-a-generalized-input-label-embedding-for',\n",
       "  'arxiv_id': '1806.06219',\n",
       "  'title': 'GILE: A Generalized Input-Label Embedding for Text Classification',\n",
       "  'abstract': 'Neural text classification models typically treat output labels as\\ncategorical variables which lack description and semantics. This forces their\\nparametrization to be dependent on the label set size, and, hence, they are\\nunable to scale to large label sets and generalize to unseen ones. Existing\\njoint input-label text models overcome these issues by exploiting label\\ndescriptions, but they are unable to capture complex label relationships, have\\nrigid parametrization, and their gains on unseen labels happen often at the\\nexpense of weak performance on the labels seen during training. In this paper,\\nwe propose a new input-label model which generalizes over previous such models,\\naddresses their limitations and does not compromise performance on seen labels.\\nThe model consists of a joint non-linear input-label embedding with\\ncontrollable capacity and a joint-space-dependent classification unit which is\\ntrained with cross-entropy loss to optimize classification performance. We\\nevaluate models on full-resource and low- or zero-resource text classification\\nof multilingual news and biomedical text with a large label set. Our model\\noutperforms monolingual and multilingual models which do not leverage label\\nsemantics and previous joint input-label space models in both scenarios.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06219v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06219v3.pdf',\n",
       "  'proceeding': 'TACL 2019 3',\n",
       "  'authors': ['Nikolaos Pappas', 'James Henderson'],\n",
       "  'tasks': ['Classification',\n",
       "   'General Classification',\n",
       "   'Multi-Task Learning',\n",
       "   'Text Classification',\n",
       "   'Zero-Shot Learning'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/natasha-faster-non-convex-stochastic',\n",
       "  'arxiv_id': '1702.00763',\n",
       "  'title': 'Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter',\n",
       "  'abstract': 'Given a nonconvex function that is an average of $n$ smooth functions, we\\ndesign stochastic first-order methods to find its approximate stationary\\npoints. The convergence of our new methods depends on the smallest (negative)\\neigenvalue $-\\\\sigma$ of the Hessian, a parameter that describes how nonconvex\\nthe function is.\\n  Our methods outperform known results for a range of parameter $\\\\sigma$, and\\ncan be used to find approximate local minima. Our result implies an interesting\\ndichotomy: there exists a threshold $\\\\sigma_0$ so that the currently fastest\\nmethods for $\\\\sigma>\\\\sigma_0$ and for $\\\\sigma<\\\\sigma_0$ have different\\nbehaviors: the former scales with $n^{2/3}$ and the latter scales with\\n$n^{3/4}$.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1702.00763v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1702.00763v5.pdf',\n",
       "  'proceeding': 'ICML 2017 8',\n",
       "  'authors': ['Zeyuan Allen-Zhu'],\n",
       "  'tasks': ['Stochastic Optimization'],\n",
       "  'date': '2017-02-02',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-reduced-pc-algorithm-improved-causal',\n",
       "  'arxiv_id': '1806.06209',\n",
       "  'title': 'The Reduced PC-Algorithm: Improved Causal Structure Learning in Large Random Networks',\n",
       "  'abstract': 'We consider the task of estimating a high-dimensional directed acyclic graph, given observations from a linear structural equation model with arbitrary noise distribution. By exploiting properties of common random graphs, we develop a new algorithm that requires conditioning only on small sets of variables. The proposed algorithm, which is essentially a modified version of the PC-Algorithm, offers significant gains in both computational complexity and estimation accuracy. In particular, it results in more efficient and accurate estimation in large networks containing hub nodes, which are common in biological systems. We prove the consistency of the proposed algorithm, and show that it also requires a less stringent faithfulness assumption than the PC-Algorithm. Simulations in low and high-dimensional settings are used to illustrate these findings. An application to gene expression data suggests that the proposed algorithm can identify a greater number of clinically relevant genes than current methods.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06209v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06209v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Arjun Sondhi', 'Ali Shojaie'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['biogrid'],\n",
       "  'datasets_used_full': ['BioGRID'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/offline-extraction-of-indic-regional-language',\n",
       "  'arxiv_id': '1806.06208',\n",
       "  'title': 'Offline Extraction of Indic Regional Language from Natural Scene Image using Text Segmentation and Deep Convolutional Sequence',\n",
       "  'abstract': 'Regional language extraction from a natural scene image is always a\\nchallenging proposition due to its dependence on the text information extracted\\nfrom Image. Text Extraction on the other hand varies on different lighting\\ncondition, arbitrary orientation, inadequate text information, heavy background\\ninfluence over text and change of text appearance. This paper presents a novel\\nunified method for tackling the above challenges. The proposed work uses an\\nimage correction and segmentation technique on the existing Text Detection\\nPipeline an Efficient and Accurate Scene Text Detector (EAST). EAST uses\\nstandard PVAnet architecture to select features and non maximal suppression to\\ndetect text from image. Text recognition is done using combined architecture of\\nMaxOut convolution neural network (CNN) and Bidirectional long short term\\nmemory (LSTM) network. After recognizing text using the Deep Learning based\\napproach, the native Languages are translated to English and tokenized using\\nstandard Text Tokenizers. The tokens that very likely represent a location is\\nused to find the Global Positioning System (GPS) coordinates of the location\\nand subsequently the regional languages spoken in that location is extracted.\\nThe proposed method is tested on a self generated dataset collected from\\nGovernment of India dataset and experimented on Standard Dataset to evaluate\\nthe performance of the proposed technique. Comparative study with a few\\nstate-of-the-art methods on text detection, recognition and extraction of\\nregional language from images shows that the proposed method outperforms the\\nexisting methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06208v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06208v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sauradip Nag',\n",
       "   'Pallab Kumar Ganguly',\n",
       "   'Sumit Roy',\n",
       "   'Sourab Jha',\n",
       "   'Krishna Bose',\n",
       "   'Abhishek Jha',\n",
       "   'Kousik Dasgupta'],\n",
       "  'tasks': ['Text Segmentation'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/meta-learning-searching-in-the-model-space',\n",
       "  'arxiv_id': '1806.06207',\n",
       "  'title': 'Meta-learning: searching in the model space',\n",
       "  'abstract': 'There is no free lunch, no single learning algorithm that will outperform\\nother algorithms on all data. In practice different approaches are tried and\\nthe best algorithm selected. An alternative solution is to build new algorithms\\non demand by creating a framework that accommodates many algorithms. The best\\ncombination of parameters and procedures is searched here in the space of all\\npossible models belonging to the framework of Similarity-Based Methods (SBMs).\\nSuch meta-learning approach gives a chance to find the best method in all\\ncases. Issues related to the meta-learning and first tests of this approach are\\npresented.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06207v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06207v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Włodzisław Duch', 'Karol Grudzińsk'],\n",
       "  'tasks': ['Meta-Learning'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/earl-joint-entity-and-relation-linking-for',\n",
       "  'arxiv_id': '1801.03825',\n",
       "  'title': 'EARL: Joint Entity and Relation Linking for Question Answering over Knowledge Graphs',\n",
       "  'abstract': 'Many question answering systems over knowledge graphs rely on entity and\\nrelation linking components in order to connect the natural language input to\\nthe underlying knowledge graph. Traditionally, entity linking and relation\\nlinking have been performed either as dependent sequential tasks or as\\nindependent parallel tasks. In this paper, we propose a framework called EARL,\\nwhich performs entity linking and relation linking as a joint task. EARL\\nimplements two different solution strategies for which we provide a comparative\\nanalysis in this paper: The first strategy is a formalisation of the joint\\nentity and relation linking tasks as an instance of the Generalised Travelling\\nSalesman Problem (GTSP). In order to be computationally feasible, we employ\\napproximate GTSP solvers. The second strategy uses machine learning in order to\\nexploit the connection density between nodes in the knowledge graph. It relies\\non three base features and re-ranking steps in order to predict entities and\\nrelations. We compare the strategies and evaluate them on a dataset with 5000\\nquestions. Both strategies significantly outperform the current\\nstate-of-the-art approaches for entity and relation linking.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1801.03825v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1801.03825v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mohnish Dubey',\n",
       "   'Debayan Banerjee',\n",
       "   'Debanjan Chaudhuri',\n",
       "   'Jens Lehmann'],\n",
       "  'tasks': ['Entity Linking',\n",
       "   'Knowledge Graphs',\n",
       "   'Question Answering',\n",
       "   'Relation Linking',\n",
       "   'Re-Ranking'],\n",
       "  'date': '2018-01-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/trquery-an-embedding-based-framework-for',\n",
       "  'arxiv_id': '1806.06205',\n",
       "  'title': 'TrQuery: An Embedding-based Framework for Recommanding SPARQL Queries',\n",
       "  'abstract': 'In this paper, we present an embedding-based framework (TrQuery) for\\nrecommending solutions of a SPARQL query, including approximate solutions when\\nexact querying solutions are not available due to incompleteness or\\ninconsistencies of real-world RDF data. Within this framework, embedding is\\napplied to score solutions together with edit distance so that we could obtain\\nmore fine-grained recommendations than those recommendations via edit distance.\\nFor instance, graphs of two querying solutions with a similar structure can be\\ndistinguished in our proposed framework while the edit distance depending on\\nstructural difference becomes unable. To this end, we propose a novel score\\nmodel built on vector space generated in embedding system to compute the\\nsimilarity between an approximate subgraph matching and a whole graph matching.\\nFinally, we evaluate our approach on large RDF datasets DBpedia and YAGO, and\\nexperimental results show that TrQuery exhibits an excellent behavior in terms\\nof both effectiveness and efficiency.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06205v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06205v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Lijing Zhang', 'Xiaowang Zhang', 'Zhiyong Feng'],\n",
       "  'tasks': ['Graph Matching'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-towards-minimum-hyperspherical',\n",
       "  'arxiv_id': '1805.09298',\n",
       "  'title': 'Learning towards Minimum Hyperspherical Energy',\n",
       "  'abstract': 'Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics -- Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as evenly as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply neural networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our intuition, by showing the superior performance with MHE regularization.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1805.09298v9',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1805.09298v9.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Weiyang Liu',\n",
       "   'Rongmei Lin',\n",
       "   'Zhen Liu',\n",
       "   'Lixin Liu',\n",
       "   'Zhiding Yu',\n",
       "   'Bo Dai',\n",
       "   'Le Song'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-23',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['lfw', 'megaface'],\n",
       "  'datasets_used_full': ['LFW', 'MegaFace'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/study-of-semi-supervised-approaches-to',\n",
       "  'arxiv_id': '1806.06200',\n",
       "  'title': 'Study of Semi-supervised Approaches to Improving English-Mandarin Code-Switching Speech Recognition',\n",
       "  'abstract': 'In this paper, we present our overall efforts to improve the performance of a\\ncode-switching speech recognition system using semi-supervised training methods\\nfrom lexicon learning to acoustic modeling, on the South East Asian\\nMandarin-English (SEAME) data. We first investigate semi-supervised lexicon\\nlearning approach to adapt the canonical lexicon, which is meant to alleviate\\nthe heavily accented pronunciation issue within the code-switching conversation\\nof the local area. As a result, the learned lexicon yields improved\\nperformance. Furthermore, we attempt to use semi-supervised training to deal\\nwith those transcriptions that are highly mismatched between human transcribers\\nand ASR system. Specifically, we conduct semi-supervised training assuming\\nthose poorly transcribed data as unsupervised data. We found the\\nsemi-supervised acoustic modeling can lead to improved results. Finally, to\\nmake up for the limitation of the conventional n-gram language models due to\\ndata sparsity issue, we perform lattice rescoring using neural network language\\nmodels, and significant WER reduction is obtained.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06200v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06200v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Pengcheng Guo', 'Hai-Hua Xu', 'Lei Xie', 'Eng Siong Chng'],\n",
       "  'tasks': ['Speech Recognition'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/part-aware-fine-grained-object-categorization',\n",
       "  'arxiv_id': '1806.06198',\n",
       "  'title': 'Part-Aware Fine-grained Object Categorization using Weakly Supervised Part Detection Network',\n",
       "  'abstract': 'Fine-grained object categorization aims for distinguishing objects of subordinate categories that belong to the same entry-level object category. The task is challenging due to the facts that (1) training images with ground-truth labels are difficult to obtain, and (2) variations among different subordinate categories are subtle. It is well established that characterizing features of different subordinate categories are located on local parts of object instances. In fact, careful part annotations are available in many fine-grained categorization datasets. However, manually annotating object parts requires expertise, which is also difficult to generalize to new fine-grained categorization tasks. In this work, we propose a Weakly Supervised Part Detection Network (PartNet) that is able to detect discriminative local parts for use of fine-grained categorization. A vanilla PartNet builds on top of a base subnetwork two parallel streams of upper network layers, which respectively compute scores of classification probabilities (over subordinate categories) and detection probabilities (over a specified number of discriminative part detectors) for local regions of interest (RoIs). The image-level prediction is obtained by aggregating element-wise products of these region-level probabilities. To generate a diverse set of RoIs as inputs of PartNet, we propose a simple Discretized Part Proposals module (DPP) that directly targets for proposing candidates of discriminative local parts, with no bridging via object-level proposals. Experiments on the benchmark CUB-200-2011 and Oxford Flower 102 datasets show the efficacy of our proposed method for both discriminative part detection and fine-grained categorization. In particular, we achieve the new state-of-the-art performance on CUB-200-2011 dataset when ground-truth part annotations are not available.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06198v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06198v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yabin Zhang', 'Kui Jia', 'Zhixin Wang'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet', 'cub-200-2011', 'oxford-102-flower'],\n",
       "  'datasets_used_full': ['ImageNet', 'CUB-200-2011', 'Oxford 102 Flower'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/show-attend-and-translate-unsupervised-image',\n",
       "  'arxiv_id': '1806.06195',\n",
       "  'title': 'Show, Attend and Translate: Unsupervised Image Translation with Self-Regularization and Attention',\n",
       "  'abstract': 'Image translation between two domains is a class of problems aiming to learn mapping from an input image in the source domain to an output image in the target domain. It has been applied to numerous domains, such as data augmentation, domain adaptation, and unsupervised training. When paired training data is not accessible, image translation becomes an ill-posed problem. We constrain the problem with the assumption that the translated image needs to be perceptually similar to the original image and also appears to be drawn from the new domain, and propose a simple yet effective image translation model consisting of a single generator trained with a self-regularization term and an adversarial term. We further notice that existing image translation techniques are agnostic to the subjects of interest and often introduce unwanted changes or artifacts to the input. Thus we propose to add an attention module to predict an attention map to guide the image translation process. The module learns to attend to key parts of the image while keeping everything else unaltered, essentially avoiding undesired artifacts or changes. The predicted attention map also opens door to applications such as unsupervised segmentation and saliency detection. Extensive experiments and evaluations show that our model while being simpler, achieves significantly better performance than existing image translation methods.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06195v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06195v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Chao Yang',\n",
       "   'Taehwan Kim',\n",
       "   'Ruizhe Wang',\n",
       "   'Hao Peng',\n",
       "   'C. -C. Jay Kuo'],\n",
       "  'tasks': ['Data Augmentation',\n",
       "   'Domain Adaptation',\n",
       "   'Saliency Detection',\n",
       "   'Translation'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['synthia', 'mnist-m'],\n",
       "  'datasets_used_full': ['SYNTHIA', 'MNIST-M'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/large-scale-fine-grained-categorization-and',\n",
       "  'arxiv_id': '1806.06193',\n",
       "  'title': 'Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning',\n",
       "  'abstract': \"Transferring the knowledge learned from large scale datasets (e.g., ImageNet)\\nvia fine-tuning offers an effective solution for domain-specific fine-grained\\nvisual categorization (FGVC) tasks (e.g., recognizing bird species or car make\\nand model). In such scenarios, data annotation often calls for specialized\\ndomain knowledge and thus is difficult to scale. In this work, we first tackle\\na problem in large scale FGVC. Our method won first place in iNaturalist 2017\\nlarge scale species classification challenge. Central to the success of our\\napproach is a training scheme that uses higher image resolution and deals with\\nthe long-tailed distribution of training data. Next, we study transfer learning\\nvia fine-tuning from large scale datasets to small scale, domain-specific FGVC\\ndatasets. We propose a measure to estimate domain similarity via Earth Mover's\\nDistance and demonstrate that transfer learning benefits from pre-training on a\\nsource domain that is similar to the target domain by this measure. Our\\nproposed transfer learning outperforms ImageNet pre-training and obtains\\nstate-of-the-art results on multiple commonly used FGVC datasets.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06193v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06193v1.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Yin Cui',\n",
       "   'Yang song',\n",
       "   'Chen Sun',\n",
       "   'Andrew Howard',\n",
       "   'Serge Belongie'],\n",
       "  'tasks': ['Fine-Grained Image Classification',\n",
       "   'Fine-Grained Visual Categorization',\n",
       "   'Transfer Learning'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet',\n",
       "   'cub-200-2011',\n",
       "   'oxford-102-flower',\n",
       "   'stanford-cars',\n",
       "   'food-101',\n",
       "   'inaturalist',\n",
       "   'nabirds'],\n",
       "  'datasets_used_full': ['ImageNet',\n",
       "   'CUB-200-2011',\n",
       "   'Oxford 102 Flower',\n",
       "   'Stanford Cars',\n",
       "   'Food-101',\n",
       "   'iNaturalist',\n",
       "   'NABirds'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/statistics-of-deep-generated-images',\n",
       "  'arxiv_id': '1708.02688',\n",
       "  'title': 'Statistics of Deep Generated Images',\n",
       "  'abstract': 'Here, we explore the low-level statistics of images generated by state-of-the-art deep generative models. First, Variational auto-encoder (VAE~\\\\cite{kingma2013auto}), Wasserstein generative adversarial network (WGAN~\\\\cite{arjovsky2017wasserstein}) and deep convolutional generative adversarial network (DCGAN~\\\\cite{radford2015unsupervised}) are trained on the ImageNet dataset and a large set of cartoon frames from animations. Then, for images generated by these models as well as natural scenes and cartoons, statistics including mean power spectrum, the number of connected components in a given image area, distribution of random filter responses, and contrast distribution are computed. Our analyses on training images support current findings on scale invariance, non-Gaussianity, and Weibull contrast distribution of natural scenes. We find that although similar results hold over cartoon images, there is still a significant difference between statistics of natural scenes and images generated by VAE, DCGAN and WGAN models. In particular, generated images do not have scale invariant mean power spectrum magnitude, which indicates existence of extra structures in these images. Inspecting how well the statistics of deep generated images match the known statistical properties of natural images, such as scale invariance, non-Gaussianity, and Weibull contrast distribution, can a) reveal the degree to which deep learning models capture the essence of the natural scenes, b) provide a new dimension to evaluate models, and c) allow possible improvement of image generative models (e.g., via defining new loss functions).',\n",
       "  'url_abs': 'https://arxiv.org/abs/1708.02688v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1708.02688v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yu Zeng', 'Huchuan Lu', 'Ali Borji'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-08-09',\n",
       "  'methods': [{'name': 'Leaky ReLU',\n",
       "    'full_name': 'Leaky ReLU',\n",
       "    'description': '**Leaky Rectified Linear Unit**, or **Leaky ReLU**, is a type of activation function based on a [ReLU](https://paperswithcode.com/method/relu), but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training. This type of activation function is popular in tasks where we we may suffer from sparse gradients, for example training generative adversarial networks.',\n",
       "    'introduced_year': 2014,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L649',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Batch Normalization',\n",
       "    'full_name': 'Batch Normalization',\n",
       "    'description': '**Batch Normalization** aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for [Dropout](https://paperswithcode.com/method/dropout).\\r\\n\\r\\nWe apply a batch normalization layer as follows for a minibatch $\\\\mathcal{B}$:\\r\\n\\r\\n$$ \\\\mu\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}x\\\\_{i} $$\\r\\n\\r\\n$$ \\\\sigma^{2}\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}\\\\left(x\\\\_{i}-\\\\mu\\\\_{\\\\mathcal{B}}\\\\right)^{2} $$\\r\\n\\r\\n$$ \\\\hat{x}\\\\_{i} = \\\\frac{x\\\\_{i} - \\\\mu\\\\_{\\\\mathcal{B}}}{\\\\sqrt{\\\\sigma^{2}\\\\_{\\\\mathcal{B}}+\\\\epsilon}} $$\\r\\n\\r\\n$$ y\\\\_{i} = \\\\gamma\\\\hat{x}\\\\_{i} + \\\\beta = \\\\text{BN}\\\\_{\\\\gamma, \\\\beta}\\\\left(x\\\\_{i}\\\\right) $$\\r\\n\\r\\nWhere $\\\\gamma$ and $\\\\beta$ are learnable parameters.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.03167v3',\n",
       "    'source_title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/36f91261099b00194922bd93ed1286fe1c199724/jax/experimental/stax.py#L116',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'DCGAN',\n",
       "    'full_name': 'Deep Convolutional GAN',\n",
       "    'description': '**DCGAN**, or **Deep Convolutional GAN**, is a generative adversarial network architecture. It uses a couple of guidelines, in particular:\\r\\n\\r\\n- Replacing any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).\\r\\n- Using batchnorm in both the generator and the discriminator.\\r\\n- Removing fully connected hidden layers for deeper architectures.\\r\\n- Using [ReLU](https://paperswithcode.com/method/relu) activation in generator for all layers except for the output, which uses tanh.\\r\\n- Using LeakyReLU activation in the discriminator for all layer.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1511.06434v2',\n",
       "    'source_title': 'Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/dcgan/dcgan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'VAE',\n",
       "    'full_name': 'Variational Autoencoder',\n",
       "    'description': 'A **Variational Autoencoder** is a type of likelihood-based generative model. It consists of an encoder, that takes in data $x$ as input and transforms this into a latent representation $z$,  and a decoder, that takes a latent representation $z$ and returns a reconstruction $\\\\hat{x}$. Inference is performed via variational inference to approximate the posterior of the model.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.6114v10',\n",
       "    'source_title': 'Auto-Encoding Variational Bayes',\n",
       "    'code_snippet_url': 'https://github.com/AntixK/PyTorch-VAE/blob/8700d245a9735640dda458db4cf40708caf2e77f/models/vanilla_vae.py#L8',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'WGAN',\n",
       "    'full_name': 'Wasserstein GAN',\n",
       "    'description': \"**Wasserstein GAN**, or **WGAN**, is a type of generative adversarial network that minimizes an approximation of the Earth-Mover's distance (EM) rather than the Jensen-Shannon divergence as in the original [GAN](https://paperswithcode.com/method/gan) formulation. It leads to more stable training than original GANs with less evidence of mode collapse, as well as meaningful curves that can be used for debugging and searching hyperparameters.\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1701.07875v3',\n",
       "    'source_title': 'Wasserstein GAN',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan/wgan.py',\n",
       "    'main_collection': {'name': 'Generative Adversarial Networks',\n",
       "     'description': '**Generative Adversarial Networks (GANs)** are a type of generative model that use two networks, a generator to generate images and a discriminator to discriminate between real and fake, to train a model that approximates the distribution of the data. Below you can find a continuously updating list of GANs.',\n",
       "     'parent': 'Generative Models',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['imagenet'],\n",
       "  'datasets_used_full': ['ImageNet'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/handling-cold-start-collaborative-filtering',\n",
       "  'arxiv_id': '1806.06192',\n",
       "  'title': 'Handling Cold-Start Collaborative Filtering with Reinforcement Learning',\n",
       "  'abstract': 'A major challenge in recommender systems is handling new users, whom are also\\ncalled $\\\\textit{cold-start}$ users. In this paper, we propose a novel approach\\nfor learning an optimal series of questions with which to interview cold-start\\nusers for movie recommender systems. We propose learning interview questions\\nusing Deep Q Networks to create user profiles to make better recommendations to\\ncold-start users. While our proposed system is trained using a movie\\nrecommender system, our Deep Q Network model should generalize across various\\ntypes of recommender systems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06192v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06192v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hima Varsha Dureddy', 'Zachary Kaden'],\n",
       "  'tasks': ['Collaborative Filtering', 'Recommendation Systems'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/accurate-spectral-super-resolution-from',\n",
       "  'arxiv_id': '1806.03575',\n",
       "  'title': 'Accurate Spectral Super-resolution from Single RGB Image Using Multi-scale CNN',\n",
       "  'abstract': 'Different from traditional hyperspectral super-resolution approaches that\\nfocus on improving the spatial resolution, spectral super-resolution aims at\\nproducing a high-resolution hyperspectral image from the RGB observation with\\nsuper-resolution in spectral domain. However, it is challenging to accurately\\nreconstruct a high-dimensional continuous spectrum from three discrete\\nintensity values at each pixel, since too much information is lost during the\\nprocedure where the latent hyperspectral image is downsampled (e.g., with x10\\nscaling factor) in spectral domain to produce an RGB observation. To address\\nthis problem, we present a multi-scale deep convolutional neural network (CNN)\\nto explicitly map the input RGB image into a hyperspectral image. Through\\nsymmetrically downsampling and upsampling the intermediate feature maps in a\\ncascading paradigm, the local and non-local image information can be jointly\\nencoded for spectral representation, ultimately improving the spectral\\nreconstruction accuracy. Extensive experiments on a large hyperspectral dataset\\ndemonstrate the effectiveness of the proposed method.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03575v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03575v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yiqi Yan', 'Lei Zhang', 'Jun Li', 'Wei Wei', 'Yanning Zhang'],\n",
       "  'tasks': ['Super-Resolution'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/scheduled-policy-optimization-for-natural',\n",
       "  'arxiv_id': '1806.06187',\n",
       "  'title': 'Scheduled Policy Optimization for Natural Language Communication with Intelligent Agents',\n",
       "  'abstract': 'We investigate the task of learning to follow natural language instructions\\nby jointly reasoning with visual observations and language inputs. In contrast\\nto existing methods which start with learning from demonstrations (LfD) and\\nthen use reinforcement learning (RL) to fine-tune the model parameters, we\\npropose a novel policy optimization algorithm which dynamically schedules\\ndemonstration learning and RL. The proposed training paradigm provides\\nefficient exploration and better generalization beyond existing methods.\\nComparing to existing ensemble models, the best single model based on our\\nproposed method tremendously decreases the execution error by over 50% on a\\nblock-world environment. To further illustrate the exploration strategy of our\\nRL algorithm, We also include systematic studies on the evolution of policy\\nentropy during training.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06187v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06187v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Wenhan Xiong',\n",
       "   'Xiaoxiao Guo',\n",
       "   'Mo Yu',\n",
       "   'Shiyu Chang',\n",
       "   'Bo-Wen Zhou',\n",
       "   'William Yang Wang'],\n",
       "  'tasks': ['Efficient Exploration'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/orthogonal-machine-learning-power-and',\n",
       "  'arxiv_id': '1711.00342',\n",
       "  'title': 'Orthogonal Machine Learning: Power and Limitations',\n",
       "  'abstract': \"Double machine learning provides $\\\\sqrt{n}$-consistent estimates of\\nparameters of interest even when high-dimensional or nonparametric nuisance\\nparameters are estimated at an $n^{-1/4}$ rate. The key is to employ\\nNeyman-orthogonal moment equations which are first-order insensitive to\\nperturbations in the nuisance parameters. We show that the $n^{-1/4}$\\nrequirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order\\nnotion of orthogonality that grants robustness to more complex or\\nhigher-dimensional nuisance parameters. In the partially linear regression\\nsetting popular in causal inference, we show that we can construct second-order\\northogonal moments if and only if the treatment residual is not normally\\ndistributed. Our proof relies on Stein's lemma and may be of independent\\ninterest. We conclude by demonstrating the robustness benefits of an explicit\\ndoubly-orthogonal estimation procedure for treatment effect.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.00342v6',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.00342v6.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Lester Mackey', 'Vasilis Syrgkanis', 'Ilias Zadik'],\n",
       "  'tasks': ['Causal Inference'],\n",
       "  'date': '2017-11-01',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/formal-security-analysis-of-neural-networks',\n",
       "  'arxiv_id': '1804.10829',\n",
       "  'title': 'Formal Security Analysis of Neural Networks using Symbolic Intervals',\n",
       "  'abstract': 'Due to the increasing deployment of Deep Neural Networks (DNNs) in real-world\\nsecurity-critical domains including autonomous vehicles and collision avoidance\\nsystems, formally checking security properties of DNNs, especially under\\ndifferent attacker capabilities, is becoming crucial. Most existing security\\ntesting techniques for DNNs try to find adversarial examples without providing\\nany formal security guarantees about the non-existence of such adversarial\\nexamples. Recently, several projects have used different types of\\nSatisfiability Modulo Theory (SMT) solvers to formally check security\\nproperties of DNNs. However, all of these approaches are limited by the high\\noverhead caused by the solver.\\n  In this paper, we present a new direction for formally checking security\\nproperties of DNNs without using SMT solvers. Instead, we leverage interval\\narithmetic to compute rigorous bounds on the DNN outputs. Our approach, unlike\\nexisting solver-based approaches, is easily parallelizable. We further present\\nsymbolic interval analysis along with several other optimizations to minimize\\noverestimations of output bounds.\\n  We design, implement, and evaluate our approach as part of ReluVal, a system\\nfor formally checking security properties of Relu-based DNNs. Our extensive\\nempirical results show that ReluVal outperforms Reluplex, a state-of-the-art\\nsolver-based system, by 200 times on average. On a single 8-core machine\\nwithout GPUs, within 4 hours, ReluVal is able to verify a security property\\nthat Reluplex deemed inconclusive due to timeout after running for more than 5\\ndays. Our experiments demonstrate that symbolic interval analysis is a\\npromising new direction towards rigorously analyzing different security\\nproperties of DNNs.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.10829v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.10829v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shiqi Wang',\n",
       "   'Kexin Pei',\n",
       "   'Justin Whitehouse',\n",
       "   'Junfeng Yang',\n",
       "   'Suman Jana'],\n",
       "  'tasks': ['Autonomous Vehicles'],\n",
       "  'date': '2018-04-28',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-neural-painter-multi-turn-image',\n",
       "  'arxiv_id': '1806.06183',\n",
       "  'title': 'The Neural Painter: Multi-Turn Image Generation',\n",
       "  'abstract': 'In this work we combine two research threads from Vision/ Graphics and\\nNatural Language Processing to formulate an image generation task conditioned\\non attributes in a multi-turn setting. By multiturn, we mean the image is\\ngenerated in a series of steps of user-specified conditioning information. Our\\nproposed approach is practically useful and offers insights into neural\\ninterpretability. We introduce a framework that includes a novel training\\nalgorithm as well as model improvements built for the multi-turn setting. We\\ndemonstrate that this framework generates a sequence of images that match the\\ngiven conditioning information and that this task is useful for more detailed\\nbenchmarking and analysis of conditional image generation methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06183v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06183v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ryan Y. Benmalek',\n",
       "   'Claire Cardie',\n",
       "   'Serge Belongie',\n",
       "   'Xiadong He',\n",
       "   'Jianfeng Gao'],\n",
       "  'tasks': ['Conditional Image Generation', 'Image Generation'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/component-spd-matrices-a-lower-dimensional',\n",
       "  'arxiv_id': '1806.06178',\n",
       "  'title': 'Component SPD Matrices: A lower-dimensional discriminative data descriptor for image set classification',\n",
       "  'abstract': \"In the domain of pattern recognition, using the SPD (Symmetric Positive\\nDefinite) matrices to represent data and taking the metrics of resulting\\nRiemannian manifold into account have been widely used for the task of image\\nset classification. In this paper, we propose a new data representation\\nframework for image sets named CSPD (Component Symmetric Positive Definite).\\nFirstly, we obtain sub-image sets by dividing the image set into square blocks\\nwith the same size, and use traditional SPD model to describe them. Then, we\\nuse the results of the Riemannian kernel on SPD matrices as similarities of\\ncorresponding sub-image sets. Finally, the CSPD matrix appears in the form of\\nthe kernel matrix for all the sub-image sets, and CSPDi,j denotes the\\nsimilarity between i-th sub-image set and j-th sub-image set. Here, the\\nRiemannian kernel is shown to satisfy the Mercer's theorem, so our proposed\\nCSPD matrix is symmetric and positive definite and also lies on a Riemannian\\nmanifold. On three benchmark datasets, experimental results show that CSPD is a\\nlower-dimensional and more discriminative data descriptor for the task of image\\nset classification.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06178v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06178v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kai-Xuan Chen', 'Xiao-Jun Wu'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/riemannian-kernel-based-nystrom-method-for',\n",
       "  'arxiv_id': '1806.06177',\n",
       "  'title': 'Riemannian kernel based Nyström method for approximate infinite-dimensional covariance descriptors with application to image set classification',\n",
       "  'abstract': 'In the domain of pattern recognition, using the CovDs (Covariance Descriptors) to represent data and taking the metrics of the resulting Riemannian manifold into account have been widely adopted for the task of image set classification. Recently, it has been proven that infinite-dimensional CovDs are more discriminative than their low-dimensional counterparts. However, the form of infinite-dimensional CovDs is implicit and the computational load is high. We propose a novel framework for representing image sets by approximating infinite-dimensional CovDs in the paradigm of the Nystr\\\\\"om method based on a Riemannian kernel. We start by modeling the images via CovDs, which lie on the Riemannian manifold spanned by SPD (Symmetric Positive Definite) matrices. We then extend the Nystr\\\\\"om method to the SPD manifold and obtain the approximations of CovDs in RKHS (Reproducing Kernel Hilbert Space). Finally, we approximate infinite-dimensional CovDs via these approximations. Empirically, we apply our framework to the task of image set classification. The experimental results obtained on three benchmark datasets show that our proposed approximate infinite-dimensional CovDs outperform the original CovDs.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06177v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06177v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kai-Xuan Chen', 'Xiao-Jun Wu', 'Rui Wang', 'Josef Kittler'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-factorized-multimodal',\n",
       "  'arxiv_id': '1806.06176',\n",
       "  'title': 'Learning Factorized Multimodal Representations',\n",
       "  'abstract': 'Learning multimodal representations is a fundamentally complex research problem due to the presence of multiple heterogeneous sources of information. Although the presence of multiple modalities provides additional valuable information, there are two key challenges to address when learning from multimodal data: 1) models must learn the complex intra-modal and cross-modal interactions for prediction and 2) models must be robust to unexpected missing or noisy modalities during testing. In this paper, we propose to optimize for a joint generative-discriminative objective across multimodal data and labels. We introduce a model that factorizes representations into two sets of independent factors: multimodal discriminative and modality-specific generative factors. Multimodal discriminative factors are shared across all modalities and contain joint multimodal features required for discriminative tasks such as sentiment prediction. Modality-specific generative factors are unique for each modality and contain the information required for generating data. Experimental results show that our model is able to learn meaningful multimodal representations that achieve state-of-the-art or competitive performance on six multimodal datasets. Our model demonstrates flexible generative capabilities by conditioning on independent factors and can reconstruct missing modalities without significantly impacting performance. Lastly, we interpret our factorized representations to understand the interactions that influence multimodal learning.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06176v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06176v3.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Yao-Hung Hubert Tsai',\n",
       "   'Paul Pu Liang',\n",
       "   'Amir Zadeh',\n",
       "   'Louis-Philippe Morency',\n",
       "   'Ruslan Salakhutdinov'],\n",
       "  'tasks': ['Representation Learning'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist', 'svhn', 'iemocap'],\n",
       "  'datasets_used_full': ['MNIST', 'SVHN', 'IEMOCAP'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/ensemble-pruning-based-on-objection',\n",
       "  'arxiv_id': '1806.04899',\n",
       "  'title': 'Ensemble Pruning based on Objection Maximization with a General Distributed Framework',\n",
       "  'abstract': 'Ensemble pruning, selecting a subset of individual learners from an original ensemble, alleviates the deficiencies of ensemble learning on the cost of time and space. Accuracy and diversity serve as two crucial factors while they usually conflict with each other. To balance both of them, we formalize the ensemble pruning problem as an objection maximization problem based on information entropy. Then we propose an ensemble pruning method including a centralized version and a distributed version, in which the latter is to speed up the former. At last, we extract a general distributed framework for ensemble pruning, which can be widely suitable for most of the existing ensemble pruning methods and achieve less time consuming without much accuracy degradation. Experimental results validate the efficiency of our framework and methods, particularly concerning a remarkable improvement of the execution speed, accompanied by gratifying accuracy performance.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04899v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04899v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yijun Bian', 'Yijun Wang', 'Yaqiang Yao', 'Huanhuan Chen'],\n",
       "  'tasks': ['Ensemble Learning', 'Ensemble Pruning'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/revisiting-deep-intrinsic-image',\n",
       "  'arxiv_id': '1701.02965',\n",
       "  'title': 'Revisiting Deep Intrinsic Image Decompositions',\n",
       "  'abstract': 'While invaluable for many computer vision applications, decomposing a natural\\nimage into intrinsic reflectance and shading layers represents a challenging,\\nunderdetermined inverse problem. As opposed to strict reliance on conventional\\noptimization or filtering solutions with strong prior assumptions, deep\\nlearning based approaches have also been proposed to compute intrinsic image\\ndecompositions when granted access to sufficient labeled training data. The\\ndownside is that current data sources are quite limited, and broadly speaking\\nfall into one of two categories: either dense fully-labeled images in\\nsynthetic/narrow settings, or weakly-labeled data from relatively diverse\\nnatural scenes. In contrast to many previous learning-based approaches, which\\nare often tailored to the structure of a particular dataset (and may not work\\nwell on others), we adopt core network structures that universally reflect\\nloose prior knowledge regarding the intrinsic image formation process and can\\nbe largely shared across datasets. We then apply flexibly supervised loss\\nlayers that are customized for each source of ground truth labels. The\\nresulting deep architecture achieves state-of-the-art results on all of the\\nmajor intrinsic image benchmarks, and runs considerably faster than most at\\ntest time.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1701.02965v8',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1701.02965v8.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Qingnan Fan',\n",
       "   'Jiaolong Yang',\n",
       "   'Gang Hua',\n",
       "   'Baoquan Chen',\n",
       "   'David Wipf'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-01-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-the-complexity-of-detecting-convexity-over',\n",
       "  'arxiv_id': '1806.06173',\n",
       "  'title': 'On the Complexity of Detecting Convexity over a Box',\n",
       "  'abstract': 'It has recently been shown that the problem of testing global convexity of\\npolynomials of degree four is {strongly} NP-hard, answering an open question of\\nN.Z. Shor. This result is minimal in the degree of the polynomial when global\\nconvexity is of concern. In a number of applications however, one is interested\\nin testing convexity only over a compact region, most commonly a box (i.e.,\\nhyper-rectangle). In this paper, we show that this problem is also strongly\\nNP-hard, in fact for polynomials of degree as low as three. This result is\\nminimal in the degree of the polynomial and in some sense justifies why\\nconvexity detection in nonlinear optimization solvers is limited to quadratic\\nfunctions or functions with special structure. As a byproduct, our proof shows\\nthat the problem of testing whether all matrices in an interval family are\\npositive semidefinite is strongly NP-hard. This problem, which was previously\\nshown to be (weakly) NP-hard by Nemirovski, is of independent interest in the\\ntheory of robust control.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06173v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06173v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Amir Ali Ahmadi', 'Georgina Hall'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/random-forest-for-label-ranking',\n",
       "  'arxiv_id': '1608.07710',\n",
       "  'title': 'Random Forest for Label Ranking',\n",
       "  'abstract': 'Label ranking aims to learn a mapping from instances to rankings over a\\nfinite number of predefined labels. Random forest is a powerful and one of the\\nmost successful general-purpose machine learning algorithms of modern times. In\\nthis paper, we present a powerful random forest label ranking method which uses\\nrandom decision trees to retrieve nearest neighbors. We have developed a novel\\ntwo-step rank aggregation strategy to effectively aggregate neighboring\\nrankings discovered by the random forest into a final predicted ranking.\\nCompared with existing methods, the new random forest method has many\\nadvantages including its intrinsically scalable tree data structure, highly\\nparallel-able computational architecture and much superior performance. We\\npresent extensive experimental results to demonstrate that our new method\\nachieves the highly competitive performance compared with state-of-the-art\\nmethods for datasets with complete ranking and datasets with only partial\\nranking information.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1608.07710v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1608.07710v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yangming Zhou', 'Guoping Qiu'],\n",
       "  'tasks': [],\n",
       "  'date': '2016-08-27',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/discrete-sequential-prediction-of-continuous',\n",
       "  'arxiv_id': '1705.05035',\n",
       "  'title': 'Discrete Sequential Prediction of Continuous Actions for Deep RL',\n",
       "  'abstract': 'It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1705.05035v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1705.05035v3.pdf',\n",
       "  'proceeding': 'ICLR 2018 1',\n",
       "  'authors': ['Luke Metz', 'Julian Ibarz', 'Navdeep Jaitly', 'James Davidson'],\n",
       "  'tasks': ['Continuous Control', 'Q-Learning', 'Structured Prediction'],\n",
       "  'date': '2017-05-14',\n",
       "  'methods': [{'name': 'Weight Decay',\n",
       "    'full_name': 'Weight Decay',\n",
       "    'description': '**Weight Decay**, or **$L_{2}$ Regularization**, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L\\\\_{2}$ Norm of the weights:\\r\\n\\r\\n$$L\\\\_{new}\\\\left(w\\\\right) = L\\\\_{original}\\\\left(w\\\\right) + \\\\lambda{w^{T}w}$$\\r\\n\\r\\nwhere $\\\\lambda$ is a value determining the strength of the penalty (encouraging smaller weights). \\r\\n\\r\\nWeight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).\\r\\n\\r\\nImage Source: Deep Learning, Goodfellow et al',\n",
       "    'introduced_year': 1943,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Adam',\n",
       "    'full_name': 'Adam',\n",
       "    'description': '**Adam** is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of [RMSProp](https://paperswithcode.com/method/rmsprop) and [SGD w/th Momentum](https://paperswithcode.com/method/sgd-with-momentum). The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. \\r\\n\\r\\nThe weight updates are performed as:\\r\\n\\r\\n$$ w_{t} = w_{t-1} - \\\\eta\\\\frac{\\\\hat{m}\\\\_{t}}{\\\\sqrt{\\\\hat{v}\\\\_{t}} + \\\\epsilon}  $$\\r\\n\\r\\nwith\\r\\n\\r\\n$$ \\\\hat{m}\\\\_{t} = \\\\frac{m_{t}}{1-\\\\beta^{t}_{1}} $$\\r\\n\\r\\n$$ \\\\hat{v}\\\\_{t} = \\\\frac{v_{t}}{1-\\\\beta^{t}_{2}} $$\\r\\n\\r\\n$$ m_{t} = \\\\beta_{1}m_{t-1} + (1-\\\\beta_{1})g_{t} $$\\r\\n\\r\\n$$ v_{t} = \\\\beta_{2}v_{t-1} + (1-\\\\beta_{2})g_{t}^{2}  $$\\r\\n\\r\\n\\r\\n$ \\\\eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \\\\epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \\\\beta_{1} $ and $ \\\\beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1412.6980v9',\n",
       "    'source_title': 'Adam: A Method for Stochastic Optimization',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Dense Connections',\n",
       "    'full_name': 'Dense Connections',\n",
       "    'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\\\_{\\\\text{inputs}}*n\\\\_{\\\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\\r\\n\\r\\n$$h\\\\_{l} = g\\\\left(\\\\textbf{W}^{T}h\\\\_{l-1}\\\\right)$$\\r\\n\\r\\nwhere $g$ is an activation function.\\r\\n\\r\\nImage Source: Deep Learning by Goodfellow, Bengio and Courville',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Batch Normalization',\n",
       "    'full_name': 'Batch Normalization',\n",
       "    'description': '**Batch Normalization** aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for [Dropout](https://paperswithcode.com/method/dropout).\\r\\n\\r\\nWe apply a batch normalization layer as follows for a minibatch $\\\\mathcal{B}$:\\r\\n\\r\\n$$ \\\\mu\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}x\\\\_{i} $$\\r\\n\\r\\n$$ \\\\sigma^{2}\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}\\\\left(x\\\\_{i}-\\\\mu\\\\_{\\\\mathcal{B}}\\\\right)^{2} $$\\r\\n\\r\\n$$ \\\\hat{x}\\\\_{i} = \\\\frac{x\\\\_{i} - \\\\mu\\\\_{\\\\mathcal{B}}}{\\\\sqrt{\\\\sigma^{2}\\\\_{\\\\mathcal{B}}+\\\\epsilon}} $$\\r\\n\\r\\n$$ y\\\\_{i} = \\\\gamma\\\\hat{x}\\\\_{i} + \\\\beta = \\\\text{BN}\\\\_{\\\\gamma, \\\\beta}\\\\left(x\\\\_{i}\\\\right) $$\\r\\n\\r\\nWhere $\\\\gamma$ and $\\\\beta$ are learnable parameters.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.03167v3',\n",
       "    'source_title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/36f91261099b00194922bd93ed1286fe1c199724/jax/experimental/stax.py#L116',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Experience Replay',\n",
       "    'full_name': 'Experience Replay',\n",
       "    'description': '**Experience Replay** is a replay memory technique used in reinforcement learning where we store the agent’s experiences at each time-step, $e\\\\_{t} = \\\\left(s\\\\_{t}, a\\\\_{t}, r\\\\_{t}, s\\\\_{t+1}\\\\right)$ in a data-set $D = e\\\\_{1}, \\\\cdots, e\\\\_{N}$ , pooled over many episodes into a replay memory. We then usually sample the memory randomly for a minibatch of experience, and use this to learn off-policy, as with Deep Q-Networks. This tackles the problem of autocorrelation leading to unstable training, by making the problem more like a supervised learning problem.\\r\\n\\r\\nImage Credit: [Hands-On Reinforcement Learning with Python, Sudharsan Ravichandiran](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781788836524)',\n",
       "    'introduced_year': 1993,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Replay Memory',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Reinforcement Learning'}},\n",
       "   {'name': 'DDPG',\n",
       "    'full_name': 'Deep Deterministic Policy Gradient',\n",
       "    'description': '**DDPG**, or **Deep Deterministic Policy Gradient**, is an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. It combines the actor-critic approach with insights from [DQNs](https://paperswithcode.com/method/dqn): in particular, the insights that 1) the network is trained off-policy with samples from a replay buffer to minimize correlations between samples, and 2) the network is trained with a target Q network to give consistent targets during temporal difference backups. DDPG makes use of the same ideas along with [batch normalization](https://paperswithcode.com/method/batch-normalization).',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1509.02971v6',\n",
       "    'source_title': 'Continuous control with deep reinforcement learning',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Policy Gradient Methods',\n",
       "     'description': '**Policy Gradient Methods** try to optimize the policy function directly in reinforcement learning. This contrasts with, for example Q-Learning, where the policy manifests itself as maximizing a value function. Below you can find a continuously updating catalogue of policy gradient methods.',\n",
       "     'parent': None,\n",
       "     'area': 'Reinforcement Learning'}}],\n",
       "  'datasets_used_lower': ['mujoco'],\n",
       "  'datasets_used_full': ['MuJoCo'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/semantic-video-segmentation-a-review-on',\n",
       "  'arxiv_id': '1806.06172',\n",
       "  'title': 'Semantic Video Segmentation: A Review on Recent Approaches',\n",
       "  'abstract': \"This paper gives an overview on semantic segmentation consists of an\\nexplanation of this field, it's status and relation with other vision\\nfundamental tasks, different datasets and common evaluation parameters that\\nhave been used by researchers. This survey also includes an overall review on a\\nvariety of recent approaches (RDF, MRF, CRF, etc.) and their advantages and\\nchallenges and shows the superiority of CNN-based semantic segmentation systems\\non CamVid and NYUDv2 datasets. In addition, some areas that is ideal for future\\nwork have mentioned.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06172v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06172v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mohammad Hajizadeh Saffar',\n",
       "   'Mohsen Fayyaz',\n",
       "   'Mohammad Sabokrou',\n",
       "   'Mahmood Fathy'],\n",
       "  'tasks': ['Semantic Segmentation',\n",
       "   'Video Segmentation',\n",
       "   'Video Semantic Segmentation'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [{'name': 'CRF',\n",
       "    'full_name': 'Conditional Random Field',\n",
       "    'description': '**Conditional Random Fields** or **CRFs** are a type of probabilistic graph model that take neighboring sample context into account for tasks like classification. Prediction is modeled as a graphical model, which implements dependencies between the predictions. Graph choice depends on the application, for example linear chain CRFs are popular in natural language processing, whereas in image-based tasks, the graph would connect to neighboring locations in an image to enforce that they have similar predictions.\\r\\n\\r\\nImage Credit: [Charles Sutton and Andrew McCallum, An Introduction to Conditional Random Fields](https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Structured Prediction',\n",
       "     'description': '**Structured Prediction** methods deal with structured outputs with multiple interdependent outputs. Below you can find a continuously updating list of structured prediction methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['cityscapes', 'nyuv2', 'camvid'],\n",
       "  'datasets_used_full': ['Cityscapes', 'NYUv2', 'CamVid'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/barc-backward-reachability-curriculum-for',\n",
       "  'arxiv_id': '1806.06161',\n",
       "  'title': 'BaRC: Backward Reachability Curriculum for Robotic Reinforcement Learning',\n",
       "  'abstract': 'Model-free Reinforcement Learning (RL) offers an attractive approach to learn\\ncontrol policies for high-dimensional systems, but its relatively poor sample\\ncomplexity often forces training in simulated environments. Even in simulation,\\ngoal-directed tasks whose natural reward function is sparse remain intractable\\nfor state-of-the-art model-free algorithms for continuous control. The\\nbottleneck in these tasks is the prohibitive amount of exploration required to\\nobtain a learning signal from the initial state of the system. In this work, we\\nleverage physical priors in the form of an approximate system dynamics model to\\ndesign a curriculum scheme for a model-free policy optimization algorithm. Our\\nBackward Reachability Curriculum (BaRC) begins policy training from states that\\nrequire a small number of actions to accomplish the task, and expands the\\ninitial state distribution backwards in a dynamically-consistent manner once\\nthe policy optimization algorithm demonstrates sufficient performance. BaRC is\\ngeneral, in that it can accelerate training of any model-free RL algorithm on a\\nbroad class of goal-directed continuous control MDPs. Its curriculum strategy\\nis physically intuitive, easy-to-tune, and allows incorporating physical priors\\nto accelerate training without hindering the performance, flexibility, and\\napplicability of the model-free RL algorithm. We evaluate our approach on two\\nrepresentative dynamic robotic learning problems and find substantial\\nperformance improvement relative to previous curriculum generation techniques\\nand naive exploration strategies.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06161v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06161v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Boris Ivanovic',\n",
       "   'James Harrison',\n",
       "   'Apoorva Sharma',\n",
       "   'Mo Chen',\n",
       "   'Marco Pavone'],\n",
       "  'tasks': ['Continuous Control'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/object-level-visual-reasoning-in-videos',\n",
       "  'arxiv_id': '1806.06157',\n",
       "  'title': 'Object Level Visual Reasoning in Videos',\n",
       "  'abstract': 'Human activity recognition is typically addressed by detecting key concepts\\nlike global and local motion, features related to object classes present in the\\nscene, as well as features related to the global context. The next open\\nchallenges in activity recognition require a level of understanding that pushes\\nbeyond this and call for models with capabilities for fine distinction and\\ndetailed comprehension of interactions between actors and objects in a scene.\\nWe propose a model capable of learning to reason about semantically meaningful\\nspatiotemporal interactions in videos. The key to our approach is a choice of\\nperforming this reasoning at the object level through the integration of state\\nof the art object detection networks. This allows the model to learn detailed\\nspatial interactions that exist at a semantic, object-interaction relevant\\nlevel. We evaluate our method on three standard datasets (Twenty-BN\\nSomething-Something, VLOG and EPIC Kitchens) and achieve state of the art\\nresults on all of them. Finally, we show visualizations of the interactions\\nlearned by the model, which illustrate object classes and their interactions\\ncorresponding to different activity classes.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06157v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06157v3.pdf',\n",
       "  'proceeding': 'ECCV 2018 9',\n",
       "  'authors': ['Fabien Baradel',\n",
       "   'Natalia Neverova',\n",
       "   'Christian Wolf',\n",
       "   'Julien Mille',\n",
       "   'Greg Mori'],\n",
       "  'tasks': ['Activity Recognition', 'Object Detection', 'Visual Reasoning'],\n",
       "  'date': '2018-06-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['vlog-dataset'],\n",
       "  'datasets_used_full': ['VLOG Dataset'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/knowledge-enriched-two-layered-attention',\n",
       "  'arxiv_id': '1805.07819',\n",
       "  'title': 'Knowledge-enriched Two-layered Attention Network for Sentiment Analysis',\n",
       "  'abstract': 'We propose a novel two-layered attention network based on Bidirectional Long\\nShort-Term Memory for sentiment analysis. The novel two-layered attention\\nnetwork takes advantage of the external knowledge bases to improve the\\nsentiment prediction. It uses the Knowledge Graph Embedding generated using the\\nWordNet. We build our model by combining the two-layered attention network with\\nthe supervised model based on Support Vector Regression using a Multilayer\\nPerceptron network for sentiment analysis. We evaluate our model on the\\nbenchmark dataset of SemEval 2017 Task 5. Experimental results show that the\\nproposed model surpasses the top system of SemEval 2017 Task 5. The model\\nperforms significantly better by improving the state-of-the-art system at\\nSemEval 2017 Task 5 by 1.7 and 3.7 points for sub-tracks 1 and 2 respectively.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.07819v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.07819v4.pdf',\n",
       "  'proceeding': 'NAACL 2018 6',\n",
       "  'authors': ['Abhishek Kumar', 'Daisuke Kawahara', 'Sadao Kurohashi'],\n",
       "  'tasks': ['Graph Embedding',\n",
       "   'Knowledge Graph Embedding',\n",
       "   'Sentiment Analysis'],\n",
       "  'date': '2018-05-20',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/new-techniques-for-preserving-global',\n",
       "  'arxiv_id': '1805.03383',\n",
       "  'title': 'New Techniques for Preserving Global Structure and Denoising with Low Information Loss in Single-Image Super-Resolution',\n",
       "  'abstract': 'This work identifies and addresses two important technical challenges in\\nsingle-image super-resolution: (1) how to upsample an image without magnifying\\nnoise and (2) how to preserve large scale structure when upsampling. We\\nsummarize the techniques we developed for our second place entry in Track 1\\n(Bicubic Downsampling), seventh place entry in Track 2 (Realistic Adverse\\nConditions), and seventh place entry in Track 3 (Realistic difficult) in the\\n2018 NTIRE Super-Resolution Challenge. Furthermore, we present new neural\\nnetwork architectures that specifically address the two challenges listed\\nabove: denoising and preservation of large-scale structure.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.03383v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.03383v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yijie Bei',\n",
       "   'Alex Damian',\n",
       "   'Shijia Hu',\n",
       "   'Sachit Menon',\n",
       "   'Nikhil Ravi',\n",
       "   'Cynthia Rudin'],\n",
       "  'tasks': ['Denoising', 'Image Super-Resolution', 'Super-Resolution'],\n",
       "  'date': '2018-05-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['div2k'],\n",
       "  'datasets_used_full': ['DIV2K'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/anticipation-in-human-robot-cooperation-a',\n",
       "  'arxiv_id': '1802.10503',\n",
       "  'title': 'Anticipation in Human-Robot Cooperation: A Recurrent Neural Network Approach for Multiple Action Sequences Prediction',\n",
       "  'abstract': \"Close human-robot cooperation is a key enabler for new developments in\\nadvanced manufacturing and assistive applications. Close cooperation require\\nrobots that can predict human actions and intent, and understand human\\nnon-verbal cues. Recent approaches based on neural networks have led to\\nencouraging results in the human action prediction problem both in continuous\\nand discrete spaces. Our approach extends the research in this direction. Our\\ncontributions are three-fold. First, we validate the use of gaze and body pose\\ncues as a means of predicting human action through a feature selection method.\\nNext, we address two shortcomings of existing literature: predicting multiple\\nand variable-length action sequences. This is achieved by introducing an\\nencoder-decoder recurrent neural network topology in the discrete action\\nprediction problem. In addition, we theoretically demonstrate the importance of\\npredicting multiple action sequences as a means of estimating the stochastic\\nreward in a human robot cooperation scenario. Finally, we show the ability to\\neffectively train the prediction model on a action prediction dataset,\\ninvolving human motion data, and explore the influence of the model's\\nparameters on its performance. Source code repository:\\nhttps://github.com/pschydlo/ActionAnticipation\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.10503v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.10503v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Paul Schydlo',\n",
       "   'Mirko Rakovic',\n",
       "   'Lorenzo Jamone',\n",
       "   'José Santos-Victor'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-28',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cad-120'],\n",
       "  'datasets_used_full': ['CAD-120'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/image-transformer',\n",
       "  'arxiv_id': '1802.05751',\n",
       "  'title': 'Image Transformer',\n",
       "  'abstract': 'Image generation has been successfully cast as an autoregressive sequence\\ngeneration or transformation problem. Recent work has shown that self-attention\\nis an effective way of modeling textual sequences. In this work, we generalize\\na recently proposed model architecture based on self-attention, the\\nTransformer, to a sequence modeling formulation of image generation with a\\ntractable likelihood. By restricting the self-attention mechanism to attend to\\nlocal neighborhoods we significantly increase the size of images the model can\\nprocess in practice, despite maintaining significantly larger receptive fields\\nper layer than typical convolutional neural networks. While conceptually\\nsimple, our generative models significantly outperform the current state of the\\nart in image generation on ImageNet, improving the best published negative\\nlog-likelihood on ImageNet from 3.83 to 3.77. We also present results on image\\nsuper-resolution with a large magnification ratio, applying an encoder-decoder\\nconfiguration of our architecture. In a human evaluation study, we find that\\nimages generated by our super-resolution model fool human observers three times\\nmore often than the previous state of the art.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.05751v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.05751v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Niki Parmar',\n",
       "   'Ashish Vaswani',\n",
       "   'Jakob Uszkoreit',\n",
       "   'Łukasz Kaiser',\n",
       "   'Noam Shazeer',\n",
       "   'Alexander Ku',\n",
       "   'Dustin Tran'],\n",
       "  'tasks': ['Image Generation', 'Image Super-Resolution', 'Super-Resolution'],\n",
       "  'date': '2018-02-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'imagenet'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'ImageNet'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/characterizing-departures-from-linearity-in',\n",
       "  'arxiv_id': '1806.04508',\n",
       "  'title': 'Characterizing Departures from Linearity in Word Translation',\n",
       "  'abstract': 'We investigate the behavior of maps learned by machine translation methods.\\nThe maps translate words by projecting between word embedding spaces of\\ndifferent languages. We locally approximate these maps using linear maps, and\\nfind that they vary across the word embedding space. This demonstrates that the\\nunderlying maps are non-linear. Importantly, we show that the locally linear\\nmaps vary by an amount that is tightly correlated with the distance between the\\nneighborhoods on which they are trained. Our results can be used to test\\nnon-linear methods, and to drive the design of more accurate maps for word\\ntranslation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04508v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04508v2.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Ndapa Nakashole', 'Raphael Flauger'],\n",
       "  'tasks': ['Machine Translation', 'Translation', 'Word Translation'],\n",
       "  'date': '2018-06-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-what-information-to-give-in',\n",
       "  'arxiv_id': '1805.08263',\n",
       "  'title': 'Learning What Information to Give in Partially Observed Domains',\n",
       "  'abstract': \"In many robotic applications, an autonomous agent must act within and explore\\na partially observed environment that is unobserved by its human teammate. We\\nconsider such a setting in which the agent can, while acting, transmit\\ndeclarative information to the human that helps them understand aspects of this\\nunseen environment. In this work, we address the algorithmic question of how\\nthe agent should plan out what actions to take and what information to\\ntransmit. Naturally, one would expect the human to have preferences, which we\\nmodel information-theoretically by scoring transmitted information based on the\\nchange it induces in weighted entropy of the human's belief state. We formulate\\nthis setting as a belief MDP and give a tractable algorithm for solving it\\napproximately. Then, we give an algorithm that allows the agent to learn the\\nhuman's preferences online, through exploration. We validate our approach\\nexperimentally in simulated discrete and continuous partially observed\\nsearch-and-recover domains. Visit http://tinyurl.com/chitnis-corl-18 for a\\nsupplementary video.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.08263v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.08263v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Rohan Chitnis', 'Leslie Pack Kaelbling', 'Tomás Lozano-Pérez'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-21',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/kernel-machines-that-adapt-to-gpus-for',\n",
       "  'arxiv_id': '1806.06144',\n",
       "  'title': 'Kernel machines that adapt to GPUs for effective large batch training',\n",
       "  'abstract': 'Modern machine learning models are typically trained using Stochastic\\nGradient Descent (SGD) on massively parallel computing resources such as GPUs.\\nIncreasing mini-batch size is a simple and direct way to utilize the parallel\\ncomputing capacity. For small batch an increase in batch size results in the\\nproportional reduction in the training time, a phenomenon known as linear\\nscaling. However, increasing batch size beyond a certain value leads to no\\nfurther improvement in training time. In this paper we develop the first\\nanalytical framework that extends linear scaling to match the parallel\\ncomputing capacity of a resource. The framework is designed for a class of\\nclassical kernel machines. It automatically modifies a standard kernel machine\\nto output a mathematically equivalent prediction function, yet allowing for\\nextended linear scaling, i.e., higher effective parallelization and faster\\ntraining time on given hardware.\\n  The resulting algorithms are accurate, principled and very fast. For example,\\nusing a single Titan Xp GPU, training on ImageNet with $1.3\\\\times 10^6$ data\\npoints and $1000$ labels takes under an hour, while smaller datasets, such as\\nMNIST, take seconds. As the parameters are chosen analytically, based on the\\ntheoretical bounds, little tuning beyond selecting the kernel and the kernel\\nparameter is needed, further facilitating the practical use of these methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06144v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06144v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Siyuan Ma', 'Mikhail Belkin'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'imagenet', 'svhn'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'ImageNet', 'SVHN'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/sparse-pseudo-input-local-kriging-for-large',\n",
       "  'arxiv_id': '1508.01248',\n",
       "  'title': 'Sparse Pseudo-input Local Kriging for Large Spatial Datasets with Exogenous Variables',\n",
       "  'abstract': 'We study large-scale spatial systems that contain exogenous variables, e.g. environmental factors that are significant predictors in spatial processes. Building predictive models for such processes is challenging because the large numbers of observations present makes it inefficient to apply full Kriging. In order to reduce computational complexity, this paper proposes Sparse Pseudo-input Local Kriging (SPLK), which utilizes hyperplanes to partition a domain into smaller subdomains and then applies a sparse approximation of the full Kriging to each subdomain. We also develop an optimization procedure to find the desired hyperplanes. To alleviate the problem of discontinuity in the global predictor, we impose continuity constraints on the boundaries of the neighboring subdomains. Furthermore, partitioning the domain into smaller subdomains makes it possible to use different parameter values for the covariance function in each region and, therefore, the heterogeneity in the data structure can be effectively captured. Numerical experiments demonstrate that SPLK outperforms, or is comparable to, the algorithms commonly applied to spatial datasets.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1508.01248v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1508.01248v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Babak Farmanesh', 'Arash Pourhabib'],\n",
       "  'tasks': [],\n",
       "  'date': '2015-08-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/morse-theory-and-an-impossibility-theorem-for',\n",
       "  'arxiv_id': '1806.06142',\n",
       "  'title': 'Possibility results for graph clustering: A novel consistency axiom',\n",
       "  'abstract': \"Kleinberg introduced three natural clustering properties, or axioms, and showed they cannot be simultaneously satisfied by any clustering algorithm. We present a new clustering property, Monotonic Consistency, which avoids the well-known problematic behaviour of Kleinberg's Consistency axiom, and the impossibility result. Namely, we describe a clustering algorithm, Morse Clustering, inspired by Morse Theory in Differential Topology, which satisfies Kleinberg's original axioms with Consistency replaced by Monotonic Consistency. Morse clustering uncovers the underlying flow structure on a set or graph and returns a partition into trees representing basins of attraction of critical vertices. We also generalise Kleinberg's axiomatic approach to sparse graphs, showing an impossibility result for Consistency, and a possibility result for Monotonic Consistency and Morse clustering.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06142v6',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06142v6.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Fabio Strazzeri', 'Rubén J. Sánchez-García'],\n",
       "  'tasks': ['Graph Clustering'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/decomposition-of-uncertainty-in-bayesian-deep',\n",
       "  'arxiv_id': '1710.07283',\n",
       "  'title': 'Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning',\n",
       "  'abstract': 'Bayesian neural networks with latent variables are scalable and flexible\\nprobabilistic models: They account for uncertainty in the estimation of the\\nnetwork weights and, by making use of latent variables, can capture complex\\nnoise patterns in the data. We show how to extract and decompose uncertainty\\ninto epistemic and aleatoric components for decision-making purposes. This\\nallows us to successfully identify informative points for active learning of\\nfunctions with heteroscedastic and bimodal noise. Using the decomposition we\\nfurther define a novel risk-sensitive criterion for reinforcement learning to\\nidentify policies that balance expected cost, model-bias and noise aversion.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1710.07283v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1710.07283v4.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Stefan Depeweg',\n",
       "   'José Miguel Hernández-Lobato',\n",
       "   'Finale Doshi-Velez',\n",
       "   'Steffen Udluft'],\n",
       "  'tasks': ['Active Learning', 'Decision Making'],\n",
       "  'date': '2017-10-19',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['industrial-benchmark'],\n",
       "  'datasets_used_full': ['Industrial Benchmark'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/high-dimensional-data-enrichment',\n",
       "  'arxiv_id': '1806.04047',\n",
       "  'title': 'High Dimensional Data Enrichment: Interpretable, Fast, and Data-Efficient',\n",
       "  'abstract': 'High dimensional structured data enriched model describes groups of\\nobservations by shared and per-group individual parameters, each with its own\\nstructure such as sparsity or group sparsity. In this paper, we consider the\\ngeneral form of data enrichment where data comes in a fixed but arbitrary\\nnumber of groups G. Any convex function, e.g., norms, can characterize the\\nstructure of both shared and individual parameters. We propose an estimator for\\nhigh dimensional data enriched model and provide conditions under which it\\nconsistently estimates both shared and individual parameters. We also delineate\\nsample complexity of the estimator and present high probability non-asymptotic\\nbound on estimation error of all parameters. Interestingly the sample\\ncomplexity of our estimator translates to conditions on both per-group sample\\nsizes and the total number of samples. We propose an iterative estimation\\nalgorithm with linear convergence rate and supplement our theoretical analysis\\nwith synthetic and real experimental results. Particularly, we show the\\npredictive power of data-enriched model along with its interpretable results in\\nanticancer drug sensitivity analysis.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04047v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04047v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Amir Asiaee',\n",
       "   'Samet Oymak',\n",
       "   'Kevin R. Coombes',\n",
       "   'Arindam Banerjee'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/near-lossless-deep-feature-compression-for',\n",
       "  'arxiv_id': '1804.09963',\n",
       "  'title': 'Near-Lossless Deep Feature Compression for Collaborative Intelligence',\n",
       "  'abstract': 'Collaborative intelligence is a new paradigm for efficient deployment of deep\\nneural networks across the mobile-cloud infrastructure. By dividing the network\\nbetween the mobile and the cloud, it is possible to distribute the\\ncomputational workload such that the overall energy and/or latency of the\\nsystem is minimized. However, this necessitates sending deep feature data from\\nthe mobile to the cloud in order to perform inference. In this work, we examine\\nthe differences between the deep feature data and natural image data, and\\npropose a simple and effective near-lossless deep feature compressor. The\\nproposed method achieves up to 5% bit rate reduction compared to HEVC-Intra and\\neven more against other popular image codecs. Finally, we suggest an approach\\nfor reconstructing the input image from compressed deep features in the cloud,\\nthat could serve to supplement the inference performed by the deep model.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.09963v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.09963v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hyomin Choi', 'Ivan V. Bajic'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-04-26',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/teaching-machines-to-code-neural-markup',\n",
       "  'arxiv_id': '1802.05415',\n",
       "  'title': 'Teaching Machines to Code: Neural Markup Generation with Visual Attention',\n",
       "  'abstract': 'We present a neural transducer model with visual attention that learns to\\ngenerate LaTeX markup of a real-world math formula given its image. Applying\\nsequence modeling and transduction techniques that have been very successful\\nacross modalities such as natural language, image, handwriting, speech and\\naudio; we construct an image-to-markup model that learns to produce\\nsyntactically and semantically correct LaTeX markup code over 150 words long\\nand achieves a BLEU score of 89%; improving upon the previous state-of-art for\\nthe Im2Latex problem. We also demonstrate with heat-map visualization how\\nattention helps in interpreting the model and can pinpoint (detect and\\nlocalize) symbols on the image accurately despite having been trained without\\nany bounding box data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.05415v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.05415v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sumeet S. Singh'],\n",
       "  'tasks': ['Optical Character Recognition'],\n",
       "  'date': '2018-02-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['im2latex-100k'],\n",
       "  'datasets_used_full': ['im2latex-100k'],\n",
       "  'datasets_introduced_lower': ['im2latex-90k', 'i2l-140k'],\n",
       "  'datasets_introduced_full': ['Im2latex-90k', 'I2L-140K']},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/supervised-fuzzy-partitioning',\n",
       "  'arxiv_id': '1806.06124',\n",
       "  'title': 'Supervised Fuzzy Partitioning',\n",
       "  'abstract': 'Centroid-based methods including k-means and fuzzy c-means are known as effective and easy-to-implement approaches to clustering purposes in many applications. However, these algorithms cannot be directly applied to supervised tasks. This paper thus presents a generative model extending the centroid-based clustering approach to be applicable to classification and regression tasks. Given an arbitrary loss function, the proposed approach, termed Supervised Fuzzy Partitioning (SFP), incorporates labels information into its objective function through a surrogate term penalizing the empirical risk. Entropy-based regularization is also employed to fuzzify the partition and to weight features, enabling the method to capture more complex patterns, identify significant features, and yield better performance facing high-dimensional data. An iterative algorithm based on block coordinate descent scheme is formulated to efficiently find a local optimum. Extensive classification experiments on synthetic, real-world, and high-dimensional datasets demonstrate that the predictive performance of SFP is competitive with state-of-the-art algorithms such as SVM and random forest. SFP has a major advantage over such methods, in that it not only leads to a flexible, nonlinear model but also can exploit any convex loss function in the training phase without compromising computational efficiency.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06124v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06124v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Pooya Ashtari', 'Fateme Nateghi Haredasht', 'Hamid Beigy'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [{'name': 'SVM',\n",
       "    'full_name': 'Support Vector Machine',\n",
       "    'description': 'A **Support Vector Machine**, or **SVM**, is a non-parametric supervised learning model. For non-linear classification and regression, they utilise the kernel trick to map inputs to high-dimensional feature spaces. SVMs construct a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure to the right shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called “support vectors”. \\r\\n\\r\\nSource: [scikit-learn](https://scikit-learn.org/stable/modules/svm.html)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Non-Parametric Classification',\n",
       "     'description': '**Non-Parametric Classification** methods perform classification where we use non-parametric methods to approximate the functional form of the relationship. Below you can find a continuously updating list of non-parametric classification methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-the-relationship-between-data-efficiency',\n",
       "  'arxiv_id': '1806.06123',\n",
       "  'title': 'On the Relationship between Data Efficiency and Error for Uncertainty Sampling',\n",
       "  'abstract': 'While active learning offers potential cost savings, the actual data\\nefficiency---the reduction in amount of labeled data needed to obtain the same\\nerror rate---observed in practice is mixed. This paper poses a basic question:\\nwhen is active learning actually helpful? We provide an answer for logistic\\nregression with the popular active learning algorithm, uncertainty sampling.\\nEmpirically, on 21 datasets from OpenML, we find a strong inverse correlation\\nbetween data efficiency and the error rate of the final classifier.\\nTheoretically, we show that for a variant of uncertainty sampling, the\\nasymptotic data efficiency is within a constant factor of the inverse error\\nrate of the limiting classifier.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06123v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06123v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Stephen Mussmann', 'Percy Liang'],\n",
       "  'tasks': ['Active Learning'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fairness-under-composition',\n",
       "  'arxiv_id': '1806.06122',\n",
       "  'title': 'Fairness Under Composition',\n",
       "  'abstract': 'Algorithmic fairness, and in particular the fairness of scoring and\\nclassification algorithms, has become a topic of increasing social concern and\\nhas recently witnessed an explosion of research in theoretical computer\\nscience, machine learning, statistics, the social sciences, and law. Much of\\nthe literature considers the case of a single classifier (or scoring function)\\nused once, in isolation. In this work, we initiate the study of the fairness\\nproperties of systems composed of algorithms that are fair in isolation; that\\nis, we study fairness under composition. We identify pitfalls of naive\\ncomposition and give general constructions for fair composition, demonstrating\\nboth that classifiers that are fair in isolation do not necessarily compose\\ninto fair systems and also that seemingly unfair components may be carefully\\ncombined to construct fair systems. We focus primarily on the individual\\nfairness setting proposed in [Dwork, Hardt, Pitassi, Reingold, Zemel, 2011],\\nbut also extend our results to a large class of group fairness definitions\\npopular in the recent literature, exhibiting several cases in which group\\nfairness definitions give misleading signals under composition.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06122v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06122v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Cynthia Dwork', 'Christina Ilvento'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/stochastic-wavenet-a-generative-latent',\n",
       "  'arxiv_id': '1806.06116',\n",
       "  'title': 'Stochastic WaveNet: A Generative Latent Variable Model for Sequential Data',\n",
       "  'abstract': 'How to model distribution of sequential data, including but not limited to\\nspeech and human motions, is an important ongoing research problem. It has been\\ndemonstrated that model capacity can be significantly enhanced by introducing\\nstochastic latent variables in the hidden states of recurrent neural networks.\\nSimultaneously, WaveNet, equipped with dilated convolutions, achieves\\nastonishing empirical performance in natural speech generation task. In this\\npaper, we combine the ideas from both stochastic latent variables and dilated\\nconvolutions, and propose a new architecture to model sequential data, termed\\nas Stochastic WaveNet, where stochastic latent variables are injected into the\\nWaveNet structure. We argue that Stochastic WaveNet enjoys powerful\\ndistribution modeling capacity and the advantage of parallel training from\\ndilated convolutions. In order to efficiently infer the posterior distribution\\nof the latent variables, a novel inference network structure is designed based\\non the characteristics of WaveNet architecture. State-of-the-art performances\\non benchmark datasets are obtained by Stochastic WaveNet on natural speech\\nmodeling and high quality human handwriting samples can be generated as well.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06116v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06116v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Guokun Lai', 'Bohan Li', 'Guoqing Zheng', 'Yiming Yang'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [{'name': 'Mixture of Logistic Distributions',\n",
       "    'full_name': 'Mixture of Logistic Distributions',\n",
       "    'description': '**Mixture of Logistic Distributions (MoL)** is a type of output function, and an alternative to a [softmax](https://paperswithcode.com/method/softmax) layer. Discretized logistic mixture likelihood is used in [PixelCNN](https://paperswithcode.com/method/pixelcnn)++ and [WaveNet](https://paperswithcode.com/method/wavenet) to predict discrete values.\\r\\n\\r\\nImage Credit: [Hao Gao](https://medium.com/@smallfishbigsea/an-explanation-of-discretized-logistic-mixture-likelihood-bdfe531751f0)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Dilated Causal Convolution',\n",
       "    'full_name': 'Dilated Causal Convolution',\n",
       "    'description': 'A **Dilated Causal Convolution** is a [causal convolution](https://paperswithcode.com/method/causal-convolution) where the filter is applied over an area larger than its length by skipping input values with a certain step. A dilated causal [convolution](https://paperswithcode.com/method/convolution) effectively allows the network to have very large receptive fields with just a few layers.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1609.03499v2',\n",
       "    'source_title': 'WaveNet: A Generative Model for Raw Audio',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Temporal Convolutions',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}},\n",
       "   {'name': 'WaveNet',\n",
       "    'full_name': 'WaveNet',\n",
       "    'description': '**WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation, architectures are developed based on dilated causal convolutions, which exhibit very large receptive fields.\\r\\n\\r\\nThe joint probability of a waveform $\\\\vec{x} = \\\\{ x_1, \\\\dots, x_T \\\\}$ is factorised as a product of conditional probabilities as follows:\\r\\n\\r\\n$$p\\\\left(\\\\vec{x}\\\\right) = \\\\prod_{t=1}^{T} p\\\\left(x_t \\\\mid x_1, \\\\dots ,x_{t-1}\\\\right)$$\\r\\n\\r\\nEach audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1609.03499v2',\n",
       "    'source_title': 'WaveNet: A Generative Model for Raw Audio',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Generative Audio Models',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Audio'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/mv-yolo-motion-vector-aided-tracking-by',\n",
       "  'arxiv_id': '1805.00107',\n",
       "  'title': 'MV-YOLO: Motion Vector-aided Tracking by Semantic Object Detection',\n",
       "  'abstract': 'Object tracking is the cornerstone of many visual analytics systems. While\\nconsiderable progress has been made in this area in recent years, robust,\\nefficient, and accurate tracking in real-world video remains a challenge. In\\nthis paper, we present a hybrid tracker that leverages motion information from\\nthe compressed video stream and a general-purpose semantic object detector\\nacting on decoded frames to construct a fast and efficient tracking engine. The\\nproposed approach is compared with several well-known recent trackers on the\\nOTB tracking dataset. The results indicate advantages of the proposed method in\\nterms of speed and/or accuracy.Other desirable features of the proposed method\\nare its simplicity and deployment efficiency, which stems from the fact that it\\nreuses the resources and information that may already exist in the system for\\nother reasons.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.00107v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.00107v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Saeed Ranjbar Alvar', 'Ivan V. Bajić'],\n",
       "  'tasks': ['Object Detection', 'Object Tracking'],\n",
       "  'date': '2018-04-30',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['otb'],\n",
       "  'datasets_used_full': ['OTB'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/non-negative-networks-against-adversarial',\n",
       "  'arxiv_id': '1806.06108',\n",
       "  'title': 'Non-Negative Networks Against Adversarial Attacks',\n",
       "  'abstract': 'Adversarial attacks against neural networks are a problem of considerable\\nimportance, for which effective defenses are not yet readily available. We make\\nprogress toward this problem by showing that non-negative weight constraints\\ncan be used to improve resistance in specific scenarios. In particular, we show\\nthat they can provide an effective defense for binary classification problems\\nwith asymmetric cost, such as malware or spam detection. We also show the\\npotential for non-negativity to be helpful to non-binary problems by applying\\nit to image classification.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06108v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06108v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['William Fleshman',\n",
       "   'Edward Raff',\n",
       "   'Jared Sylvester',\n",
       "   'Steven Forsyth',\n",
       "   'Mark McLean'],\n",
       "  'tasks': ['Classification',\n",
       "   'General Classification',\n",
       "   'Image Classification'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-limits-of-post-selection-generalization',\n",
       "  'arxiv_id': '1806.06100',\n",
       "  'title': 'The Limits of Post-Selection Generalization',\n",
       "  'abstract': 'While statistics and machine learning offers numerous methods for ensuring\\ngeneralization, these methods often fail in the presence of adaptivity---the\\ncommon practice in which the choice of analysis depends on previous\\ninteractions with the same dataset. A recent line of work has introduced\\npowerful, general purpose algorithms that ensure post hoc generalization (also\\ncalled robust or post-selection generalization), which says that, given the\\noutput of the algorithm, it is hard to find any statistic for which the data\\ndiffers significantly from the population it came from.\\n  In this work we show several limitations on the power of algorithms\\nsatisfying post hoc generalization. First, we show a tight lower bound on the\\nerror of any algorithm that satisfies post hoc generalization and answers\\nadaptively chosen statistical queries, showing a strong barrier to progress in\\npost selection data analysis. Second, we show that post hoc generalization is\\nnot closed under composition, despite many examples of such algorithms\\nexhibiting strong composition properties.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06100v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06100v1.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Kobbi Nissim',\n",
       "   'Adam Smith',\n",
       "   'Thomas Steinke',\n",
       "   'Uri Stemmer',\n",
       "   'Jonathan Ullman'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/unsupervised-training-for-3d-morphable-model',\n",
       "  'arxiv_id': '1806.06098',\n",
       "  'title': 'Unsupervised Training for 3D Morphable Model Regression',\n",
       "  'abstract': 'We present a method for training a regression network from image pixels to 3D\\nmorphable model coordinates using only unlabeled photographs. The training loss\\nis based on features from a facial recognition network, computed on-the-fly by\\nrendering the predicted faces with a differentiable renderer. To make training\\nfrom features feasible and avoid network fooling effects, we introduce three\\nobjectives: a batch distribution loss that encourages the output distribution\\nto match the distribution of the morphable model, a loopback loss that ensures\\nthe network can correctly reinterpret its own output, and a multi-view identity\\nloss that compares the features of the predicted 3D face and the input\\nphotograph from multiple viewing angles. We train a regression network using\\nthese objectives, a set of unlabeled photographs, and the morphable model\\nitself, and demonstrate state-of-the-art results.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06098v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06098v1.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Kyle Genova',\n",
       "   'Forrester Cole',\n",
       "   'Aaron Maschinot',\n",
       "   'Aaron Sarna',\n",
       "   'Daniel Vlasic',\n",
       "   'William T. Freeman'],\n",
       "  'tasks': ['3D Face Reconstruction'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['lfw', 'florence'],\n",
       "  'datasets_used_full': ['LFW', 'Florence'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/integrating-human-provided-information-into',\n",
       "  'arxiv_id': '1803.00119',\n",
       "  'title': 'Integrating Human-Provided Information Into Belief State Representation Using Dynamic Factorization',\n",
       "  'abstract': \"In partially observed environments, it can be useful for a human to provide\\nthe robot with declarative information that represents probabilistic relational\\nconstraints on properties of objects in the world, augmenting the robot's\\nsensory observations. For instance, a robot tasked with a search-and-rescue\\nmission may be informed by the human that two victims are probably in the same\\nroom. An important question arises: how should we represent the robot's\\ninternal knowledge so that this information is correctly processed and combined\\nwith raw sensory information? In this paper, we provide an efficient belief\\nstate representation that dynamically selects an appropriate factoring,\\ncombining aspects of the belief when they are correlated through information\\nand separating them when they are not. This strategy works in open domains, in\\nwhich the set of possible objects is not known in advance, and provides\\nsignificant improvements in inference time over a static factoring, leading to\\nmore efficient planning for complex partially observed tasks. We validate our\\napproach experimentally in two open-domain planning problems: a 2D discrete\\ngridworld task and a 3D continuous cooking task. A supplementary video can be\\nfound at http://tinyurl.com/chitnis-iros-18.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.00119v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.00119v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Rohan Chitnis', 'Leslie Pack Kaelbling', 'Tomás Lozano-Pérez'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-28',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/crime-event-embedding-with-unsupervised',\n",
       "  'arxiv_id': '1806.06095',\n",
       "  'title': 'Crime Event Embedding with Unsupervised Feature Selection',\n",
       "  'abstract': 'We present a novel event embedding algorithm for crime data that can jointly\\ncapture time, location, and the complex free-text component of each event. The\\nembedding is achieved by regularized Restricted Boltzmann Machines (RBMs), and\\nwe introduce a new way to regularize by imposing a $\\\\ell_1$ penalty on the\\nconditional distributions of the observed variables of RBMs. This choice of\\nregularization performs feature selection and it also leads to efficient\\ncomputation since the gradient can be computed in a closed form. The feature\\nselection forces embedding to be based on the most important keywords, which\\ncaptures the common modus operandi (M. O.) in crime series. Using numerical\\nexperiments on a large-scale crime dataset, we show that our regularized RBMs\\ncan achieve better event embedding and the selected features are highly\\ninterpretable from human understanding.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06095v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06095v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shixiang Zhu', 'Yao Xie'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/training-vaes-under-structured-residuals',\n",
       "  'arxiv_id': '1804.01050',\n",
       "  'title': 'Training VAEs Under Structured Residuals',\n",
       "  'abstract': 'Variational auto-encoders (VAEs) are a popular and powerful deep generative\\nmodel. Previous works on VAEs have assumed a factorized likelihood model,\\nwhereby the output uncertainty of each pixel is assumed to be independent. This\\napproximation is clearly limited as demonstrated by observing a residual image\\nfrom a VAE reconstruction, which often possess a high level of structure. This\\npaper demonstrates a novel scheme to incorporate a structured Gaussian\\nlikelihood prediction network within the VAE that allows the residual\\ncorrelations to be modeled. Our novel architecture, with minimal increase in\\ncomplexity, incorporates the covariance matrix prediction within the VAE. We\\nalso propose a new mechanism for allowing structured uncertainty on color\\nimages. Furthermore, we provide a scheme for effectively training this model,\\nand include some suggestions for improving performance in terms of efficiency\\nor modeling longer range correlations.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.01050v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.01050v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Garoe Dorta',\n",
       "   'Sara Vicente',\n",
       "   'Lourdes Agapito',\n",
       "   'Neill D. F. Campbell',\n",
       "   'Ivor Simpson'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-04-03',\n",
       "  'methods': [{'name': 'VAE',\n",
       "    'full_name': 'Variational Autoencoder',\n",
       "    'description': 'A **Variational Autoencoder** is a type of likelihood-based generative model. It consists of an encoder, that takes in data $x$ as input and transforms this into a latent representation $z$,  and a decoder, that takes a latent representation $z$ and returns a reconstruction $\\\\hat{x}$. Inference is performed via variational inference to approximate the posterior of the model.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.6114v10',\n",
       "    'source_title': 'Auto-Encoding Variational Bayes',\n",
       "    'code_snippet_url': 'https://github.com/AntixK/PyTorch-VAE/blob/8700d245a9735640dda458db4cf40708caf2e77f/models/vanilla_vae.py#L8',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['celeba', 'lsun'],\n",
       "  'datasets_used_full': ['CelebA', 'LSUN'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/minibatch-gibbs-sampling-on-large-graphical',\n",
       "  'arxiv_id': '1806.06086',\n",
       "  'title': 'Minibatch Gibbs Sampling on Large Graphical Models',\n",
       "  'abstract': 'Gibbs sampling is the de facto Markov chain Monte Carlo method used for\\ninference and learning on large scale graphical models. For complicated factor\\ngraphs with lots of factors, the performance of Gibbs sampling can be limited\\nby the computational cost of executing a single update step of the Markov\\nchain. This cost is proportional to the degree of the graph, the number of\\nfactors adjacent to each variable. In this paper, we show how this cost can be\\nreduced by using minibatching: subsampling the factors to form an estimate of\\ntheir sum. We introduce several minibatched variants of Gibbs, show that they\\ncan be made unbiased, prove bounds on their convergence rates, and show that\\nunder some conditions they can result in asymptotic single-update-run-time\\nspeedups over plain Gibbs sampling.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06086v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06086v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Christopher De Sa', 'Vincent Chen', 'Wing Wong'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/detecting-dead-weights-and-units-in-neural',\n",
       "  'arxiv_id': '1806.06068',\n",
       "  'title': 'Detecting Dead Weights and Units in Neural Networks',\n",
       "  'abstract': \"Deep Neural Networks are highly over-parameterized and the size of the neural\\nnetworks can be reduced significantly after training without any decrease in\\nperformance. One can clearly see this phenomenon in a wide range of\\narchitectures trained for various problems. Weight/channel pruning,\\ndistillation, quantization, matrix factorization are some of the main methods\\none can use to remove the redundancy to come up with smaller and faster models.\\n  This work starts with a short informative chapter, where we motivate the\\npruning idea and provide the necessary notation. In the second chapter, we\\ncompare various saliency scores in the context of parameter pruning. Using the\\ninsights obtained from this comparison and stating the problems it brings we\\nmotivate why pruning units instead of the individual parameters might be a\\nbetter idea. We propose some set of definitions to quantify and analyze units\\nthat don't learn and create any useful information. We propose an efficient way\\nfor detecting dead units and use it to select which units to prune. We get 5x\\nmodel size reduction through unit-wise pruning on MNIST.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06068v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06068v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Utku Evci'],\n",
       "  'tasks': ['Quantization'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/classification-with-fairness-constraints-a',\n",
       "  'arxiv_id': '1806.06055',\n",
       "  'title': 'Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees',\n",
       "  'abstract': 'Developing classification algorithms that are fair with respect to sensitive attributes of the data has become an important problem due to the growing deployment of classification algorithms in various social contexts. Several recent works have focused on fairness with respect to a specific metric, modeled the corresponding fair classification problem as a constrained optimization problem, and developed tailored algorithms to solve them. Despite this, there still remain important metrics for which we do not have fair classifiers and many of the aforementioned algorithms do not come with theoretical guarantees; perhaps because the resulting optimization problem is non-convex. The main contribution of this paper is a new meta-algorithm for classification that takes as input a large class of fairness constraints, with respect to multiple non-disjoint sensitive attributes, and which comes with provable guarantees. This is achieved by first developing a meta-algorithm for a large family of classification problems with convex constraints, and then showing that classification problems with general types of fairness constraints can be reduced to those in this family. We present empirical results that show that our algorithm can achieve near-perfect fairness with respect to various fairness metrics, and that the loss in accuracy due to the imposed fairness constraints is often small. Overall, this work unifies several prior works on fair classification, presents a practical algorithm with theoretical guarantees, and can handle fairness metrics that were previously not possible.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06055v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06055v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['L. Elisa Celis',\n",
       "   'Lingxiao Huang',\n",
       "   'Vijay Keswani',\n",
       "   'Nisheeth K. Vishnoi'],\n",
       "  'tasks': ['Classification', 'General Classification'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/an-online-prediction-algorithm-for',\n",
       "  'arxiv_id': '1806.06720',\n",
       "  'title': 'An Online Prediction Algorithm for Reinforcement Learning with Linear Function Approximation using Cross Entropy Method',\n",
       "  'abstract': 'In this paper, we provide two new stable online algorithms for the problem of\\nprediction in reinforcement learning, \\\\emph{i.e.}, estimating the value\\nfunction of a model-free Markov reward process using the linear function\\napproximation architecture and with memory and computation costs scaling\\nquadratically in the size of the feature set. The algorithms employ the\\nmulti-timescale stochastic approximation variant of the very popular cross\\nentropy (CE) optimization method which is a model based search method to find\\nthe global optimum of a real-valued function. A proof of convergence of the\\nalgorithms using the ODE method is provided. We supplement our theoretical\\nresults with experimental comparisons. The algorithms achieve good performance\\nfairly consistently on many RL benchmark problems with regards to computational\\nefficiency, accuracy and stability.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06720v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06720v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ajin George Joseph', 'Shalabh Bhatnagar'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-lip-reading-a-comparison-of-models-and',\n",
       "  'arxiv_id': '1806.06053',\n",
       "  'title': 'Deep Lip Reading: a comparison of models and an online application',\n",
       "  'abstract': 'The goal of this paper is to develop state-of-the-art models for lip reading\\n-- visual speech recognition. We develop three architectures and compare their\\naccuracy and training times: (i) a recurrent model using LSTMs; (ii) a fully\\nconvolutional model; and (iii) the recently proposed transformer model. The\\nrecurrent and fully convolutional models are trained with a Connectionist\\nTemporal Classification loss and use an explicit language model for decoding,\\nthe transformer is a sequence-to-sequence model. Our best performing model\\nimproves the state-of-the-art word error rate on the challenging BBC-Oxford Lip\\nReading Sentences 2 (LRS2) benchmark dataset by over 20 percent.\\n  As a further contribution we investigate the fully convolutional model when\\nused for online (real time) lip reading of continuous speech, and show that it\\nachieves high performance with low latency.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06053v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06053v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Triantafyllos Afouras', 'Joon Son Chung', 'Andrew Zisserman'],\n",
       "  'tasks': ['Language Modelling',\n",
       "   'Lip Reading',\n",
       "   'Speech Recognition',\n",
       "   'Visual Speech Recognition'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [{'name': 'Absolute Position Encodings',\n",
       "    'full_name': 'Absolute Position Encodings',\n",
       "    'description': '**Absolute Position Encodings** are a type of position embeddings for [[Transformer](https://paperswithcode.com/method/transformer)-based models] where positional encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension $d\\\\_{model}$ as the embeddings, so that the two can be summed. In the original implementation, sine and cosine functions of different frequencies are used:\\r\\n\\r\\n$$ \\\\text{PE}\\\\left(pos, 2i\\\\right) = \\\\sin\\\\left(pos/10000^{2i/d\\\\_{model}}\\\\right) $$\\r\\n\\r\\n$$ \\\\text{PE}\\\\left(pos, 2i+1\\\\right) = \\\\cos\\\\left(pos/10000^{2i/d\\\\_{model}}\\\\right) $$\\r\\n\\r\\nwhere $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\\\pi$ to $10000 \\\\dot 2\\\\pi$. This function was chosen because the authors hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$,  $\\\\text{PE}\\\\_{pos+k}$ can be represented as a linear function of $\\\\text{PE}\\\\_{pos}$.\\r\\n\\r\\nImage Source: [D2L.ai](https://d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Position Embeddings',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Position-Wise Feed-Forward Layer',\n",
       "    'full_name': 'Position-Wise Feed-Forward Layer',\n",
       "    'description': '**Position-Wise Feed-Forward Layer** is a type of [feedforward layer](https://www.paperswithcode.com/method/category/feedforwad-networks) consisting of two [dense layers](https://www.paperswithcode.com/method/dense-connections) that applies to the last dimension, which means the same dense layers are used for each position item in the sequence, so called position-wise.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Residual Connection',\n",
       "    'full_name': 'Residual Connection',\n",
       "    'description': '**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'BPE',\n",
       "    'full_name': 'Byte Pair Encoding',\n",
       "    'description': '**Byte Pair Encoding**, or **BPE**, is a subword segmentation algorithm that encodes rare and unknown words as sequences of subword units. The intuition is that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).\\r\\n\\r\\n[Lei Mao](https://leimao.github.io/blog/Byte-Pair-Encoding/) has a detailed blog post that explains how this works.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1508.07909v5',\n",
       "    'source_title': 'Neural Machine Translation of Rare Words with Subword Units',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Subword Segmentation',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Natural Language Processing'}},\n",
       "   {'name': 'Dense Connections',\n",
       "    'full_name': 'Dense Connections',\n",
       "    'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\\\_{\\\\text{inputs}}*n\\\\_{\\\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\\r\\n\\r\\n$$h\\\\_{l} = g\\\\left(\\\\textbf{W}^{T}h\\\\_{l-1}\\\\right)$$\\r\\n\\r\\nwhere $g$ is an activation function.\\r\\n\\r\\nImage Source: Deep Learning by Goodfellow, Bengio and Courville',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Label Smoothing',\n",
       "    'full_name': 'Label Smoothing',\n",
       "    'description': '**Label Smoothing** is a regularization technique that introduces noise for the labels. This accounts for the fact that datasets may have mistakes in them, so maximizing the likelihood of $\\\\log{p}\\\\left(y\\\\mid{x}\\\\right)$ directly can be harmful. Assume for a small constant $\\\\epsilon$, the training set label $y$ is correct with probability $1-\\\\epsilon$ and incorrect otherwise. Label Smoothing regularizes a model based on a [softmax](https://paperswithcode.com/method/softmax) with $k$ output values by replacing the hard $0$ and $1$ classification targets with targets of $\\\\frac{\\\\epsilon}{k-1}$ and $1-\\\\epsilon$ respectively.\\r\\n\\r\\nSource: Deep Learning, Goodfellow et al\\r\\n\\r\\nImage Source: [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629)',\n",
       "    'introduced_year': 1985,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Adam',\n",
       "    'full_name': 'Adam',\n",
       "    'description': '**Adam** is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of [RMSProp](https://paperswithcode.com/method/rmsprop) and [SGD w/th Momentum](https://paperswithcode.com/method/sgd-with-momentum). The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. \\r\\n\\r\\nThe weight updates are performed as:\\r\\n\\r\\n$$ w_{t} = w_{t-1} - \\\\eta\\\\frac{\\\\hat{m}\\\\_{t}}{\\\\sqrt{\\\\hat{v}\\\\_{t}} + \\\\epsilon}  $$\\r\\n\\r\\nwith\\r\\n\\r\\n$$ \\\\hat{m}\\\\_{t} = \\\\frac{m_{t}}{1-\\\\beta^{t}_{1}} $$\\r\\n\\r\\n$$ \\\\hat{v}\\\\_{t} = \\\\frac{v_{t}}{1-\\\\beta^{t}_{2}} $$\\r\\n\\r\\n$$ m_{t} = \\\\beta_{1}m_{t-1} + (1-\\\\beta_{1})g_{t} $$\\r\\n\\r\\n$$ v_{t} = \\\\beta_{2}v_{t-1} + (1-\\\\beta_{2})g_{t}^{2}  $$\\r\\n\\r\\n\\r\\n$ \\\\eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \\\\epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \\\\beta_{1} $ and $ \\\\beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1412.6980v9',\n",
       "    'source_title': 'Adam: A Method for Stochastic Optimization',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Dropout',\n",
       "    'full_name': 'Dropout',\n",
       "    'description': '**Dropout** is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p=0.5$). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).\\r\\n\\r\\nThe idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://jmlr.org/papers/v15/srivastava14a.html',\n",
       "    'source_title': 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/7f3078b70d0ed9bea6228efa420879c56f72ef69/jax/experimental/stax.py#L271-L275',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Multi-Head Attention',\n",
       "    'full_name': 'Multi-Head Attention',\n",
       "    'description': '**Multi-head Attention** is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies). \\r\\n\\r\\n$$ \\\\text{MultiHead}\\\\left(\\\\textbf{Q}, \\\\textbf{K}, \\\\textbf{V}\\\\right) = \\\\left[\\\\text{head}\\\\_{1},\\\\dots,\\\\text{head}\\\\_{h}\\\\right]\\\\textbf{W}_{0}$$\\r\\n\\r\\n$$\\\\text{where} \\\\text{ head}\\\\_{i} = \\\\text{Attention} \\\\left(\\\\textbf{Q}\\\\textbf{W}\\\\_{i}^{Q}, \\\\textbf{K}\\\\textbf{W}\\\\_{i}^{K}, \\\\textbf{V}\\\\textbf{W}\\\\_{i}^{V} \\\\right) $$\\r\\n\\r\\nAbove $\\\\textbf{W}$ are all learnable parameter matrices.\\r\\n\\r\\nNote that [scaled dot-product attention](https://paperswithcode.com/method/scaled) is most commonly used in this module, although in principle it can be swapped out for other types of attention mechanism.\\r\\n\\r\\nSource: [Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': 'https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/fec78a687210851f055f792d45300d27cc60ae41/transformer/SubLayers.py#L9',\n",
       "    'main_collection': {'name': 'Attention Modules',\n",
       "     'description': '**Attention Modules** refer to modules that incorporate attention mechanisms. For example, multi-head attention is a module that incorporates multiple attention heads. Below you can find a continuously updating list of attention modules.',\n",
       "     'parent': 'Attention',\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Layer Normalization',\n",
       "    'full_name': 'Layer Normalization',\n",
       "    'description': 'Unlike [batch normalization](https://paperswithcode.com/method/batch-normalization), **Layer Normalization** directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. It works well for [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) and improves both the training time and the generalization performance of several existing RNN models. More recently, it has been used with [Transformer](https://paperswithcode.com/methods/category/transformers) models.\\r\\n\\r\\nWe compute the layer normalization statistics over all the hidden units in the same layer as follows:\\r\\n\\r\\n$$ \\\\mu^{l} = \\\\frac{1}{H}\\\\sum^{H}\\\\_{i=1}a\\\\_{i}^{l} $$\\r\\n\\r\\n$$ \\\\sigma^{l} = \\\\sqrt{\\\\frac{1}{H}\\\\sum^{H}\\\\_{i=1}\\\\left(a\\\\_{i}^{l}-\\\\mu^{l}\\\\right)^{2}}  $$\\r\\n\\r\\nwhere $H$ denotes the number of hidden units in a layer. Under layer normalization, all the hidden units in a layer share the same normalization terms $\\\\mu$ and $\\\\sigma$, but different training cases have different normalization terms. Unlike batch normalization, layer normalization does not impose any constraint on the size of the mini-batch and it can be used in the pure online regime with batch size 1.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1607.06450v1',\n",
       "    'source_title': 'Layer Normalization',\n",
       "    'code_snippet_url': 'https://github.com/CyberZHG/torch-layer-normalization/blob/89f405b60f53f85da6f03fe685c190ef394ce50c/torch_layer_normalization/layer_normalization.py#L8',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Scaled Dot-Product Attention',\n",
       "    'full_name': 'Scaled Dot-Product Attention',\n",
       "    'description': '**Scaled dot-product attention** is an attention mechanism where the dot products are scaled down by $\\\\sqrt{d_k}$. Formally we have a query $Q$, a key $K$ and a value $V$ and calculate the attention as:\\r\\n\\r\\n$$ {\\\\text{Attention}}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^{T}}{\\\\sqrt{d_k}}\\\\right)V $$\\r\\n\\r\\nIf we assume that $q$ and $k$ are $d_k$-dimensional vectors whose components are independent random variables with mean $0$ and variance $1$, then their dot product, $q \\\\cdot k = \\\\sum_{i=1}^{d_k} u_iv_i$, has mean $0$ and variance $d_k$.  Since we would prefer these values to have variance $1$, we divide by $\\\\sqrt{d_k}$.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': 'https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/5c0264915ab43485adc576f88971fc3d42b10445/transformer/Modules.py#L7',\n",
       "    'main_collection': {'name': 'Attention Mechanisms',\n",
       "     'description': '**Attention Mechanisms** are a component used in neural networks to model long-range interaction, for example across a text in NLP. The key idea is to build shortcuts between a context vector and the input, to allow a model to attend to different parts. Below you can find a continuously updating list of attention mechanisms.',\n",
       "     'parent': 'Attention',\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Transformer',\n",
       "    'full_name': 'Transformer',\n",
       "    'description': 'A **Transformer** is a model architecture that eschews recurrence and instead relies entirely on an [attention mechanism](https://paperswithcode.com/methods/category/attention-mechanisms-1) to draw global dependencies between input and output. Before Transformers, the dominant sequence transduction models were based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The Transformer also employs an encoder and decoder, but removing recurrence in favor of [attention mechanisms](https://paperswithcode.com/methods/category/attention-mechanisms-1) allows for significantly more parallelization than methods like [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) and [CNNs](https://paperswithcode.com/methods/category/convolutional-neural-networks).',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': 'https://github.com/tunz/transformer-pytorch/blob/e7266679f0b32fd99135ea617213f986ceede056/model/transformer.py#L201',\n",
       "    'main_collection': {'name': 'Transformers',\n",
       "     'description': '**Transformers** are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embedidngs.',\n",
       "     'parent': 'Language Models',\n",
       "     'area': 'Natural Language Processing'}}],\n",
       "  'datasets_used_lower': ['lrw', 'lrs2'],\n",
       "  'datasets_used_full': ['LRW', 'LRS2'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/versatile-auxiliary-classifier-with-1',\n",
       "  'arxiv_id': '1805.00316',\n",
       "  'title': 'Versatile Auxiliary Classifier with Generative Adversarial Network (VAC+GAN)',\n",
       "  'abstract': 'One of the most interesting challenges in Artificial Intelligence is to train\\nconditional generators which are able to provide labeled adversarial samples\\ndrawn from a specific distribution. In this work, a new framework is presented\\nto train a deep conditional generator by placing a classifier in parallel with\\nthe discriminator and back propagate the classification error through the\\ngenerator network. The method is versatile and is applicable to any variations\\nof Generative Adversarial Network (GAN) implementation, and also gives superior\\nresults compared to similar methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.00316v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.00316v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shabab Bazrafkan', 'Hossein Javidnia', 'Peter Corcoran'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-05-01',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['celeba'],\n",
       "  'datasets_used_full': ['CelebA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/computationally-efficient-estimation-of-the',\n",
       "  'arxiv_id': '1806.06047',\n",
       "  'title': 'Computationally Efficient Estimation of the Spectral Gap of a Markov Chain',\n",
       "  'abstract': 'We consider the problem of estimating from sample paths the absolute spectral\\ngap $\\\\gamma_*$ of a reversible, irreducible and aperiodic Markov chain\\n$(X_t)_{t \\\\in \\\\mathbb{N}}$ over a finite state space $\\\\Omega$. We propose the\\n${\\\\tt UCPI}$ (Upper Confidence Power Iteration) algorithm for this problem, a\\nlow-complexity algorithm which estimates the spectral gap in time ${\\\\cal O}(n)$\\nand memory space ${\\\\cal O}((\\\\ln n)^2)$ given $n$ samples. This is in stark\\ncontrast with most known methods which require at least memory space ${\\\\cal\\nO}(|\\\\Omega|)$, so that they cannot be applied to large state spaces.\\nFurthermore, ${\\\\tt UCPI}$ is amenable to parallel implementation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06047v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06047v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Richard Combes', 'Mikael Touati'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/high-quality-prediction-intervals-for-deep',\n",
       "  'arxiv_id': '1802.07167',\n",
       "  'title': 'High-Quality Prediction Intervals for Deep Learning: A Distribution-Free, Ensembled Approach',\n",
       "  'abstract': 'This paper considers the generation of prediction intervals (PIs) by neural\\nnetworks for quantifying uncertainty in regression tasks. It is axiomatic that\\nhigh-quality PIs should be as narrow as possible, whilst capturing a specified\\nportion of data. We derive a loss function directly from this axiom that\\nrequires no distributional assumption. We show how its form derives from a\\nlikelihood principle, that it can be used with gradient descent, and that model\\nuncertainty is accounted for in ensembled form. Benchmark experiments show the\\nmethod outperforms current state-of-the-art uncertainty quantification methods,\\nreducing average PI width by over 10%.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.07167v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.07167v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Tim Pearce',\n",
       "   'Mohamed Zaki',\n",
       "   'Alexandra Brintrup',\n",
       "   'Andy Neely'],\n",
       "  'tasks': ['Prediction Intervals'],\n",
       "  'date': '2018-02-20',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/bayesian-inference-with-anchored-ensembles-of',\n",
       "  'arxiv_id': '1805.11324',\n",
       "  'title': 'Bayesian Inference with Anchored Ensembles of Neural Networks, and Application to Exploration in Reinforcement Learning',\n",
       "  'abstract': 'The use of ensembles of neural networks (NNs) for the quantification of\\npredictive uncertainty is widespread. However, the current justification is\\nintuitive rather than analytical. This work proposes one minor modification to\\nthe normal ensembling methodology, which we prove allows the ensemble to\\nperform Bayesian inference, hence converging to the corresponding Gaussian\\nProcess as both the total number of NNs, and the size of each, tend to\\ninfinity. This working paper provides early-stage results in a reinforcement\\nlearning setting, analysing the practicality of the technique for an ensemble\\nof small, finite number. Using the uncertainty estimates produced by anchored\\nensembles to govern the exploration-exploitation process results in steadier,\\nmore stable learning.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.11324v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.11324v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Tim Pearce',\n",
       "   'Nicolas Anastassacos',\n",
       "   'Mohamed Zaki',\n",
       "   'Andy Neely'],\n",
       "  'tasks': ['Bayesian Inference'],\n",
       "  'date': '2018-05-29',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/three-factors-influencing-minima-in-sgd',\n",
       "  'arxiv_id': '1711.04623',\n",
       "  'title': 'Three Factors Influencing Minima in SGD',\n",
       "  'abstract': 'We investigate the dynamical and convergent properties of stochastic gradient\\ndescent (SGD) applied to Deep Neural Networks (DNNs). Characterizing the\\nrelation between learning rate, batch size and the properties of the final\\nminima, such as width or generalization, remains an open question. In order to\\ntackle this problem we investigate the previously proposed approximation of SGD\\nby a stochastic differential equation (SDE). We theoretically argue that three\\nfactors - learning rate, batch size and gradient covariance - influence the\\nminima found by SGD. In particular we find that the ratio of learning rate to\\nbatch size is a key determinant of SGD dynamics and of the width of the final\\nminima, and that higher values of the ratio lead to wider minima and often\\nbetter generalization. We confirm these findings experimentally. Further, we\\ninclude experiments which show that learning rate schedules can be replaced\\nwith batch size schedules and that the ratio of learning rate to batch size is\\nan important factor influencing the memorization process.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.04623v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.04623v3.pdf',\n",
       "  'proceeding': 'ICLR 2018 1',\n",
       "  'authors': ['Stanisław Jastrzębski',\n",
       "   'Zachary Kenton',\n",
       "   'Devansh Arpit',\n",
       "   'Nicolas Ballas',\n",
       "   'Asja Fischer',\n",
       "   'Yoshua Bengio',\n",
       "   'Amos Storkey'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-11-13',\n",
       "  'methods': [{'name': 'SGD',\n",
       "    'full_name': 'Stochastic Gradient Descent',\n",
       "    'description': '**Stochastic Gradient Descent** is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:\\r\\n\\r\\n$$ w\\\\_{t+1} = w\\\\_{t} - \\\\eta\\\\hat{\\\\nabla}\\\\_{w}{L(w\\\\_{t})} $$\\r\\n\\r\\nWhere $\\\\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.\\r\\n\\r\\n(Image Source: [here](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/))',\n",
       "    'introduced_year': 1951,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/4e0ac120e9a8b096069c2f892488d630a5c8f358/torch/optim/sgd.py#L97-L112',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['fashion-mnist'],\n",
       "  'datasets_used_full': ['Fashion-MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-toybox-dataset-of-egocentric-visual',\n",
       "  'arxiv_id': '1806.06034',\n",
       "  'title': 'The Toybox Dataset of Egocentric Visual Object Transformations',\n",
       "  'abstract': 'In object recognition research, many commonly used datasets (e.g., ImageNet\\nand similar) contain relatively sparse distributions of object instances and\\nviews, e.g., one might see a thousand different pictures of a thousand\\ndifferent giraffes, mostly taken from a few conventionally photographed angles.\\nThese distributional properties constrain the types of computational\\nexperiments that are able to be conducted with such datasets, and also do not\\nreflect naturalistic patterns of embodied visual experience. As a contribution\\nto the small (but growing) number of multi-view object datasets that have been\\ncreated to bridge this gap, we introduce a new video dataset called Toybox that\\ncontains egocentric (i.e., first-person perspective) videos of common household\\nobjects and toys being manually manipulated to undergo structured\\ntransformations, such as rotation, translation, and zooming. To illustrate\\npotential uses of Toybox, we also present initial neural network experiments\\nthat examine 1) how training on different distributions of object instances and\\nviews affects recognition performance, and 2) how viewpoint-dependent object\\nconcepts are represented within the hidden layers of a trained network.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06034v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06034v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xiaohan Wang',\n",
       "   'Tengyu Ma',\n",
       "   'James Ainooson',\n",
       "   'Seunghwan Cha',\n",
       "   'Xiaotian Wang',\n",
       "   'Azhar Molla',\n",
       "   'Maithilee Kunda'],\n",
       "  'tasks': ['Object Recognition', 'Translation'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet'],\n",
       "  'datasets_used_full': ['ImageNet'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/latent-space-physics-towards-learning-the',\n",
       "  'arxiv_id': '1802.10123',\n",
       "  'title': 'Latent-space Physics: Towards Learning the Temporal Evolution of Fluid Flow',\n",
       "  'abstract': 'We propose a method for the data-driven inference of temporal evolutions of\\nphysical functions with deep learning. More specifically, we target fluid\\nflows, i.e. Navier-Stokes problems, and we propose a novel LSTM-based approach\\nto predict the changes of pressure fields over time. The central challenge in\\nthis context is the high dimensionality of Eulerian space-time data sets. We\\ndemonstrate for the first time that dense 3D+time functions of physics system\\ncan be predicted within the latent spaces of neural networks, and we arrive at\\na neural-network based simulation algorithm with significant practical\\nspeed-ups. We highlight the capabilities of our method with a series of complex\\nliquid simulations, and with a set of single-phase buoyancy simulations. With a\\nset of trained networks, our method is more than two orders of magnitudes\\nfaster than a traditional pressure solver. Additionally, we present and discuss\\na series of detailed evaluations for the different components of our algorithm.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.10123v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.10123v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Steffen Wiewel', 'Moritz Becher', 'Nils Thuerey'],\n",
       "  'tasks': ['Dimensionality Reduction'],\n",
       "  'date': '2018-02-27',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/one-shot-unsupervised-cross-domain',\n",
       "  'arxiv_id': '1806.06029',\n",
       "  'title': 'One-Shot Unsupervised Cross Domain Translation',\n",
       "  'abstract': 'Given a single image x from domain A and a set of images from domain B, our\\ntask is to generate the analogous of x in B. We argue that this task could be a\\nkey AI capability that underlines the ability of cognitive agents to act in the\\nworld and present empirical evidence that the existing unsupervised domain\\ntranslation methods fail on this task. Our method follows a two step process.\\nFirst, a variational autoencoder for domain B is trained. Then, given the new\\nsample x, we create a variational autoencoder for domain A by adapting the\\nlayers that are close to the image in order to directly fit x, and only\\nindirectly adapt the other layers. Our experiments indicate that the new method\\ndoes as well, when trained on one sample x, as the existing domain transfer\\nmethods, when these enjoy a multitude of training samples from domain A. Our\\ncode is made publicly available at\\nhttps://github.com/sagiebenaim/OneShotTranslation',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06029v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06029v2.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Sagie Benaim', 'Lior Wolf'],\n",
       "  'tasks': ['Translation',\n",
       "   'Unsupervised Image-To-Image Translation',\n",
       "   'Zero-Shot Learning'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [{'name': 'AutoEncoder',\n",
       "    'full_name': 'AutoEncoder',\n",
       "    'description': 'An **Autoencoder** is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then performs a reconstruction of the input with this latent code (the decoder).\\r\\n\\r\\nImage: [Michael Massi](https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://science.sciencemag.org/content/313/5786/504',\n",
       "    'source_title': 'Reducing the Dimensionality of Data with Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/L1aoXingyu/pytorch-beginner/blob/9c86be785c7c318a09cf29112dd1f1a58613239b/08-AutoEncoder/simple_autoencoder.py#L38',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['cityscapes', 'svhn'],\n",
       "  'datasets_used_full': ['Cityscapes', 'SVHN'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/variational-attention-for-sequence-to',\n",
       "  'arxiv_id': '1712.08207',\n",
       "  'title': 'Variational Attention for Sequence-to-Sequence Models',\n",
       "  'abstract': 'The variational encoder-decoder (VED) encodes source information as a set of\\nrandom variables using a neural network, which in turn is decoded into target\\ndata using another neural network. In natural language processing,\\nsequence-to-sequence (Seq2Seq) models typically serve as encoder-decoder\\nnetworks. When combined with a traditional (deterministic) attention mechanism,\\nthe variational latent space may be bypassed by the attention model, and thus\\nbecomes ineffective. In this paper, we propose a variational attention\\nmechanism for VED, where the attention vector is also modeled as Gaussian\\ndistributed random variables. Results on two experiments show that, without\\nloss of quality, our proposed method alleviates the bypassing phenomenon as it\\nincreases the diversity of generated sentences.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.08207v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.08207v3.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Hareesh Bahuleyan',\n",
       "   'Lili Mou',\n",
       "   'Olga Vechtomova',\n",
       "   'Pascal Poupart'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-12-21',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/low-shot-learning-with-large-scale-diffusion',\n",
       "  'arxiv_id': '1706.02332',\n",
       "  'title': 'Low-shot learning with large-scale diffusion',\n",
       "  'abstract': 'This paper considers the problem of inferring image labels from images when\\nonly a few annotated examples are available at training time. This setup is\\noften referred to as low-shot learning, where a standard approach is to\\nre-train the last few layers of a convolutional neural network learned on\\nseparate classes for which training examples are abundant. We consider a\\nsemi-supervised setting based on a large collection of images to support label\\npropagation. This is possible by leveraging the recent advances on large-scale\\nsimilarity graph construction.\\n  We show that despite its conceptual simplicity, scaling label propagation up\\nto hundred millions of images leads to state of the art accuracy in the\\nlow-shot learning regime.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1706.02332v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1706.02332v3.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Matthijs Douze',\n",
       "   'Arthur Szlam',\n",
       "   'Bharath Hariharan',\n",
       "   'Hervé Jégou'],\n",
       "  'tasks': ['graph construction'],\n",
       "  'date': '2017-06-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet', 'yfcc100m'],\n",
       "  'datasets_used_full': ['ImageNet', 'YFCC100M'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/homonym-detection-in-curated-bibliographies',\n",
       "  'arxiv_id': '1806.06017',\n",
       "  'title': \"Homonym Detection in Curated Bibliographies: Learning from dblp's Experience (full version)\",\n",
       "  'abstract': 'Identifying (and fixing) homonymous and synonymous author profiles is one of\\nthe major tasks of curating personalized bibliographic metadata repositories\\nlike the dblp computer science bibliography. In this paper, we present and\\nevaluate a machine learning approach to identify homonymous author\\nbibliographies using a simple multilayer perceptron setup. We train our model\\non a novel gold-standard data set derived from the past years of active, manual\\ncuration at the dblp computer science bibliography.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06017v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06017v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Marcel R. Ackermann', 'Florian Reitz'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/detecting-abnormal-events-in-video-using',\n",
       "  'arxiv_id': '1801.05030',\n",
       "  'title': 'Detecting abnormal events in video using Narrowed Normality Clusters',\n",
       "  'abstract': 'We formulate the abnormal event detection problem as an outlier detection\\ntask and we propose a two-stage algorithm based on k-means clustering and\\none-class Support Vector Machines (SVM) to eliminate outliers. In the feature\\nextraction stage, we propose to augment spatio-temporal cubes with deep\\nappearance features extracted from the last convolutional layer of a\\npre-trained neural network. After extracting motion and appearance features\\nfrom the training video containing only normal events, we apply k-means\\nclustering to find clusters representing different types of normal motion and\\nappearance features. In the first stage, we consider that clusters with fewer\\nsamples (with respect to a given threshold) contain mostly outliers, and we\\neliminate these clusters altogether. In the second stage, we shrink the borders\\nof the remaining clusters by training a one-class SVM model on each cluster. To\\ndetected abnormal events in the test video, we analyze each test sample and\\nconsider its maximum normality score provided by the trained one-class SVM\\nmodels, based on the intuition that a test sample can belong to only one\\ncluster of normality. If the test sample does not fit well in any narrowed\\nnormality cluster, then it is labeled as abnormal. We compare our method with\\nseveral state-of-the-art methods on three benchmark data sets. The empirical\\nresults indicate that our abnormal event detection framework can achieve better\\nresults in most cases, while processing the test video in real-time at 24\\nframes per second on a single CPU.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1801.05030v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1801.05030v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Radu Tudor Ionescu',\n",
       "   'Sorina Smeureanu',\n",
       "   'Marius Popescu',\n",
       "   'Bogdan Alexe'],\n",
       "  'tasks': ['Event Detection', 'Outlier Detection'],\n",
       "  'date': '2018-01-12',\n",
       "  'methods': [{'name': 'SVM',\n",
       "    'full_name': 'Support Vector Machine',\n",
       "    'description': 'A **Support Vector Machine**, or **SVM**, is a non-parametric supervised learning model. For non-linear classification and regression, they utilise the kernel trick to map inputs to high-dimensional feature spaces. SVMs construct a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure to the right shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called “support vectors”. \\r\\n\\r\\nSource: [scikit-learn](https://scikit-learn.org/stable/modules/svm.html)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Non-Parametric Classification',\n",
       "     'description': '**Non-Parametric Classification** methods perform classification where we use non-parametric methods to approximate the functional form of the relationship. Below you can find a continuously updating list of non-parametric classification methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'k-Means Clustering',\n",
       "    'full_name': 'k-Means Clustering',\n",
       "    'description': '**k-Means Clustering** is a clustering algorithm that divides a training set into $k$ different clusters of examples that are near each other. It works by initializing $k$ different centroids {$\\\\mu\\\\left(1\\\\right),\\\\ldots,\\\\mu\\\\left(k\\\\right)$} to different values, then alternating between two steps until convergence:\\r\\n\\r\\n(i) each training example is assigned to cluster $i$ where $i$ is the index of the nearest centroid $\\\\mu^{(i)}$\\r\\n\\r\\n(ii) each centroid $\\\\mu^{(i)}$ is updated to the mean of all training examples $x^{(j)}$ assigned to cluster $i$.\\r\\n\\r\\nText Source: Deep Learning, Goodfellow et al\\r\\n\\r\\nImage Source: [scikit-learn](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://cryptoabout.info',\n",
       "    'main_collection': {'name': 'Clustering',\n",
       "     'description': '**Clustering** methods cluster a dataset so that similar datapoints are located in the same group. Below you can find a continuously updating list of clustering methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/real-time-deep-learning-method-for-abandoned',\n",
       "  'arxiv_id': '1803.01160',\n",
       "  'title': 'Real-Time Deep Learning Method for Abandoned Luggage Detection in Video',\n",
       "  'abstract': \"Recent terrorist attacks in major cities around the world have brought many\\ncasualties among innocent citizens. One potential threat is represented by\\nabandoned luggage items (that could contain bombs or biological warfare) in\\npublic areas. In this paper, we describe an approach for real-time automatic\\ndetection of abandoned luggage in video captured by surveillance cameras. The\\napproach is comprised of two stages: (i) static object detection based on\\nbackground subtraction and motion estimation and (ii) abandoned luggage\\nrecognition based on a cascade of convolutional neural networks (CNN). To train\\nour neural networks we provide two types of examples: images collected from the\\nInternet and realistic examples generated by imposing various suitcases and\\nbags over the scene's background. We present empirical results demonstrating\\nthat our approach yields better performance than a strong CNN baseline method.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.01160v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.01160v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sorina Smeureanu', 'Radu Tudor Ionescu'],\n",
       "  'tasks': ['Motion Estimation', 'Object Detection'],\n",
       "  'date': '2018-03-03',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/optimizing-the-trade-off-between-single-stage',\n",
       "  'arxiv_id': '1803.08707',\n",
       "  'title': 'Optimizing the Trade-off between Single-Stage and Two-Stage Object Detectors using Image Difficulty Prediction',\n",
       "  'abstract': 'There are mainly two types of state-of-the-art object detectors. On one hand,\\nwe have two-stage detectors, such as Faster R-CNN (Region-based Convolutional\\nNeural Networks) or Mask R-CNN, that (i) use a Region Proposal Network to\\ngenerate regions of interests in the first stage and (ii) send the region\\nproposals down the pipeline for object classification and bounding-box\\nregression. Such models reach the highest accuracy rates, but are typically\\nslower. On the other hand, we have single-stage detectors, such as YOLO (You\\nOnly Look Once) and SSD (Singe Shot MultiBox Detector), that treat object\\ndetection as a simple regression problem by taking an input image and learning\\nthe class probabilities and bounding box coordinates. Such models reach lower\\naccuracy rates, but are much faster than two-stage object detectors. In this\\npaper, we propose to use an image difficulty predictor to achieve an optimal\\ntrade-off between accuracy and speed in object detection. The image difficulty\\npredictor is applied on the test images to split them into easy versus hard\\nimages. Once separated, the easy images are sent to the faster single-stage\\ndetector, while the hard images are sent to the more accurate two-stage\\ndetector. Our experiments on PASCAL VOC 2007 show that using image difficulty\\ncompares favorably to a random split of the images. Our method is flexible, in\\nthat it allows to choose a desired threshold for splitting the images into easy\\nversus hard.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.08707v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.08707v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Petru Soviany', 'Radu Tudor Ionescu'],\n",
       "  'tasks': ['Object Detection', 'Region Proposal'],\n",
       "  'date': '2018-03-23',\n",
       "  'methods': [{'name': 'RoIAlign',\n",
       "    'full_name': 'RoIAlign',\n",
       "    'description': '**Region of Interest Align**, or **RoIAlign**, is an operation for extracting a small feature map from each RoI in detection and segmentation based tasks. It removes the harsh quantization of [RoI Pool](https://paperswithcode.com/method/roi-pooling), properly *aligning* the extracted features with the input. To avoid any quantization of the RoI boundaries or bins (using $x/16$ instead of $[x/16]$), RoIAlign uses bilinear interpolation to compute the exact values of the input features at four regularly sampled locations in each RoI bin, and the result is then aggregated (using max or average).',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1703.06870v3',\n",
       "    'source_title': 'Mask R-CNN',\n",
       "    'code_snippet_url': 'https://github.com/facebookresearch/detectron2/blob/bb9f5d8e613358519c9865609ab3fe7b6571f2ba/detectron2/layers/roi_align.py#L51',\n",
       "    'main_collection': {'name': 'RoI Feature Extractors',\n",
       "     'description': '**RoI Feature Extractors** are used to extract regions of interest features for tasks such as object detection. Below you can find a continuously updating list of RoI Feature Extractors.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Non Maximum Suppression',\n",
       "    'full_name': 'Non Maximum Suppression',\n",
       "    'description': '**Non Maximum Suppression** is a computer vision method that selects a single entity out of many overlapping entities (for example bounding boxes in object detection). The criteria is usually discarding entities that are below a given probability bound. With remaining entities we repeatedly pick the entity with the highest probability, output that as the prediction, and discard any remaining box where a $\\\\text{IoU} \\\\geq 0.5$ with the box output in the previous step.\\r\\n\\r\\nImage Credit: [Martin Kersner](https://github.com/martinkersner/non-maximum-suppression-cpp)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Proposal Filtering',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Mask R-CNN',\n",
       "    'full_name': 'Mask R-CNN',\n",
       "    'description': \"**Mask R-CNN** extends [Faster R-CNN](http://paperswithcode.com/method/faster-r-cnn) to solve instance segmentation tasks. It achieves this by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. In principle, Mask R-CNN is an intuitive extension of Faster [R-CNN](https://paperswithcode.com/method/r-cnn), but constructing the mask branch properly is critical for good results. \\r\\n\\r\\nMost importantly, Faster R-CNN was not designed for pixel-to-pixel alignment between network inputs and outputs. This is evident in how [RoIPool](http://paperswithcode.com/method/roi-pooling), the *de facto* core operation for attending to instances, performs coarse spatial quantization for feature extraction. To fix the misalignment, Mask R-CNN utilises a simple, quantization-free layer, called [RoIAlign](http://paperswithcode.com/method/roi-align), that faithfully preserves exact spatial locations. \\r\\n\\r\\nSecondly, Mask R-CNN *decouples* mask and class prediction: it predicts a binary mask for each class independently, without competition among classes, and relies on the network's RoI classification branch to predict the category. In contrast, an [FCN](http://paperswithcode.com/method/fcn) usually perform per-pixel multi-class categorization, which couples segmentation and classification.\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1703.06870v3',\n",
       "    'source_title': 'Mask R-CNN',\n",
       "    'code_snippet_url': 'https://github.com/facebookresearch/detectron2/blob/601d7666faaf7eb0ba64c9f9ce5811b13861fe12/detectron2/modeling/roi_heads/mask_head.py#L154',\n",
       "    'main_collection': {'name': 'Instance Segmentation Models',\n",
       "     'description': '**Instance Segmentation** models are models that perform the task of [Instance Segmentation](https://paperswithcode.com/task/instance-segmentation).',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': '1x1 Convolution',\n",
       "    'full_name': '1x1 Convolution',\n",
       "    'description': 'A **1 x 1 Convolution** is a [convolution](https://paperswithcode.com/method/convolution) with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an [MLP](https://paperswithcode.com/method/feedforward-network) looking at a particular pixel location.\\r\\n\\r\\nImage Credit: [http://deeplearning.ai](http://deeplearning.ai)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://www.healthnutra.org/es/maxup/',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'SSD',\n",
       "    'full_name': 'SSD',\n",
       "    'description': '**SSD** is a single-stage object detection method that discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. \\r\\n\\r\\nThe fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage. Improvements over competing single-stage methods include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.02325v5',\n",
       "    'source_title': 'SSD: Single Shot MultiBox Detector',\n",
       "    'code_snippet_url': 'https://github.com/amdegroot/ssd.pytorch/blob/5b0b77faa955c1917b0c710d770739ba8fbff9b7/ssd.py#L10',\n",
       "    'main_collection': {'name': 'Object Detection Models',\n",
       "     'description': '**Object Detection Models** are architectures used to perform the task of object detection. Below you can find a continuously updating list of object detection models.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'RPN',\n",
       "    'full_name': 'Region Proposal Network',\n",
       "    'description': 'A **Region Proposal Network**, or **RPN**, is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals. RPN and algorithms like [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn) can be merged into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with attention mechanisms, the RPN component tells the unified network where to look.\\r\\n\\r\\nRPNs are designed to efficiently predict region proposals with a wide range of scales and aspect ratios. RPNs use anchor boxes that serve as references at multiple scales and aspect ratios. The scheme can be thought of as a pyramid of regression references, which avoids enumerating images or filters of multiple scales or aspect ratios.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1506.01497v3',\n",
       "    'source_title': 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Region Proposal',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'RoIPool',\n",
       "    'full_name': 'RoIPool',\n",
       "    'description': '**Region of Interest Pooling**, or **RoIPool**, is an operation for extracting a small feature map (e.g., $7×7$) from each RoI in detection and segmentation based tasks. Features are extracted from each candidate box, and thereafter in models like [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn), are then classified and bounding box regression performed.\\r\\n\\r\\nThe actual scaling to, e.g., $7×7$, occurs by dividing the region proposal into equally sized sections, finding the largest value in each section, and then copying these max values to the output buffer. In essence, **RoIPool** is [max pooling](https://paperswithcode.com/method/max-pooling) on a discrete grid based on a box.\\r\\n\\r\\nImage Source: [Joyce Xu](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1311.2524v5',\n",
       "    'source_title': 'Rich feature hierarchies for accurate object detection and semantic segmentation',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/5e9ebe8dadc0ea2841a46cfcd82a93b4ce0d4519/torchvision/ops/roi_pool.py#L10',\n",
       "    'main_collection': {'name': 'RoI Feature Extractors',\n",
       "     'description': '**RoI Feature Extractors** are used to extract regions of interest features for tasks such as object detection. Below you can find a continuously updating list of RoI Feature Extractors.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Faster R-CNN',\n",
       "    'full_name': 'Faster R-CNN',\n",
       "    'description': '**Faster R-CNN** is an object detection model that improves on [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn) by utilising a region proposal network ([RPN](https://paperswithcode.com/method/rpn)) with the CNN model. The RPN shares full-image convolutional features with the detection network, enabling nearly cost-free region proposals. It is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn) for detection. RPN and Fast [R-CNN](https://paperswithcode.com/method/r-cnn) are merged into a single network by sharing their convolutional features: the RPN component tells the unified network where to look.\\r\\n\\r\\nAs a whole, Faster R-CNN consists of two modules. The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector that uses the proposed regions.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1506.01497v3',\n",
       "    'source_title': 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks',\n",
       "    'code_snippet_url': 'https://github.com/chenyuntc/simple-faster-rcnn-pytorch/blob/367db367834efd8a2bc58ee0023b2b628a0e474d/model/faster_rcnn.py#L22',\n",
       "    'main_collection': {'name': 'Object Detection Models',\n",
       "     'description': '**Object Detection Models** are architectures used to perform the task of object detection. Below you can find a continuously updating list of object detection models.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/partially-supervised-image-captioning',\n",
       "  'arxiv_id': '1806.06004',\n",
       "  'title': 'Partially-Supervised Image Captioning',\n",
       "  'abstract': 'Image captioning models are becoming increasingly successful at describing\\nthe content of images in restricted domains. However, if these models are to\\nfunction in the wild - for example, as assistants for people with impaired\\nvision - a much larger number and variety of visual concepts must be\\nunderstood. To address this problem, we teach image captioning models new\\nvisual concepts from labeled images and object detection datasets. Since image\\nlabels and object classes can be interpreted as partial captions, we formulate\\nthis problem as learning from partially-specified sequence data. We then\\npropose a novel algorithm for training sequence models, such as recurrent\\nneural networks, on partially-specified sequences which we represent using\\nfinite state automata. In the context of image captioning, our method lifts the\\nrestriction that previously required image captioning models to be trained on\\npaired image-sentence corpora only, or otherwise required specialized model\\narchitectures to take advantage of alternative data modalities. Applying our\\napproach to an existing neural captioning model, we achieve state of the art\\nresults on the novel object captioning task using the COCO dataset. We further\\nshow that we can train a captioning model to describe new visual concepts from\\nthe Open Images dataset while maintaining competitive COCO evaluation scores.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06004v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06004v2.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Peter Anderson', 'Stephen Gould', 'Mark Johnson'],\n",
       "  'tasks': ['Image Captioning', 'Object Detection'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['coco', 'coco-captions'],\n",
       "  'datasets_used_full': ['COCO', 'COCO Captions'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-machine-learning-and-structure-for-mobile',\n",
       "  'arxiv_id': '1806.06003',\n",
       "  'title': 'On Machine Learning and Structure for Mobile Robots',\n",
       "  'abstract': 'Due to recent advances - compute, data, models - the role of learning in\\nautonomous systems has expanded significantly, rendering new applications\\npossible for the first time. While some of the most significant benefits are\\nobtained in the perception modules of the software stack, other aspects\\ncontinue to rely on known manual procedures based on prior knowledge on\\ngeometry, dynamics, kinematics etc. Nonetheless, learning gains relevance in\\nthese modules when data collection and curation become easier than manual rule\\ndesign. Building on this coarse and broad survey of current research, the final\\nsections aim to provide insights into future potentials and challenges as well\\nas the necessity of structure in current practical applications.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06003v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06003v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Markus Wulfmeier'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/adapting-neural-text-classification-for',\n",
       "  'arxiv_id': '1806.01742',\n",
       "  'title': 'Adapting Neural Text Classification for Improved Software Categorization',\n",
       "  'abstract': 'Software Categorization is the task of organizing software into groups that\\nbroadly describe the behavior of the software, such as \"editors\" or \"science.\"\\nCategorization plays an important role in several maintenance tasks, such as\\nrepository navigation and feature elicitation. Current approaches attempt to\\ncast the problem as text classification, to make use of the rich body of\\nliterature from the NLP domain. However, as we will show in this paper, text\\nclassification algorithms are generally not applicable off-the-shelf to source\\ncode; we found that they work well when high-level project descriptions are\\navailable, but suffer very large performance penalties when classifying source\\ncode and comments only. We propose a set of adaptations to a state-of-the-art\\nneural classification algorithm and perform two evaluations: one with reference\\ndata from Debian end-user programs, and one with a set of C/C++ libraries that\\nwe hired professional programmers to annotate. We show that our proposed\\napproach achieves performance exceeding that of previous software\\nclassification techniques as well as a state-of-the-art neural text\\nclassification technique.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.01742v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.01742v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alexander LeClair', 'Zachary Eberhart', 'Collin McMillan'],\n",
       "  'tasks': ['Classification', 'General Classification', 'Text Classification'],\n",
       "  'date': '2018-06-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-dataset-for-building-code-mixed-goal',\n",
       "  'arxiv_id': '1806.05997',\n",
       "  'title': 'A Dataset for Building Code-Mixed Goal Oriented Conversation Systems',\n",
       "  'abstract': 'There is an increasing demand for goal-oriented conversation systems which\\ncan assist users in various day-to-day activities such as booking tickets,\\nrestaurant reservations, shopping, etc. Most of the existing datasets for\\nbuilding such conversation systems focus on monolingual conversations and there\\nis hardly any work on multilingual and/or code-mixed conversations. Such\\ndatasets and systems thus do not cater to the multilingual regions of the\\nworld, such as India, where it is very common for people to speak more than one\\nlanguage and seamlessly switch between them resulting in code-mixed\\nconversations. For example, a Hindi speaking user looking to book a restaurant\\nwould typically ask, \"Kya tum is restaurant mein ek table book karne mein meri\\nhelp karoge?\" (\"Can you help me in booking a table at this restaurant?\"). To\\nfacilitate the development of such code-mixed conversation models, we build a\\ngoal-oriented dialog dataset containing code-mixed conversations. Specifically,\\nwe take the text from the DSTC2 restaurant reservation dataset and create\\ncode-mixed versions of it in Hindi-English, Bengali-English, Gujarati-English\\nand Tamil-English. We also establish initial baselines on this dataset using\\nexisting state of the art models. This dataset along with our baseline\\nimplementations is made publicly available for research purposes.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05997v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05997v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Suman Banerjee',\n",
       "   'Nikita Moghe',\n",
       "   'Siddhartha Arora',\n",
       "   'Mitesh M. Khapra'],\n",
       "  'tasks': ['Goal-Oriented Dialog'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/ego-lane-analysis-system-elas-dataset-and',\n",
       "  'arxiv_id': '1806.05984',\n",
       "  'title': 'Ego-Lane Analysis System (ELAS): Dataset and Algorithms',\n",
       "  'abstract': 'Decreasing costs of vision sensors and advances in embedded hardware boosted\\nlane related research detection, estimation, and tracking in the past two\\ndecades. The interest in this topic has increased even more with the demand for\\nadvanced driver assistance systems (ADAS) and self-driving cars. Although\\nextensively studied independently, there is still need for studies that propose\\na combined solution for the multiple problems related to the ego-lane, such as\\nlane departure warning (LDW), lane change detection, lane marking type (LMT)\\nclassification, road markings detection and classification, and detection of\\nadjacent lanes (i.e., immediate left and right lanes) presence. In this paper,\\nwe propose a real-time Ego-Lane Analysis System (ELAS) capable of estimating\\nego-lane position, classifying LMTs and road markings, performing LDW and\\ndetecting lane change events. The proposed vision-based system works on a\\ntemporal sequence of images. Lane marking features are extracted in perspective\\nand Inverse Perspective Mapping (IPM) images that are combined to increase\\nrobustness. The final estimated lane is modeled as a spline using a combination\\nof methods (Hough lines with Kalman filter and spline with particle filter).\\nBased on the estimated lane, all other events are detected. To validate ELAS\\nand cover the lack of lane datasets in the literature, a new dataset with more\\nthan 20 different scenes (in more than 15,000 frames) and considering a variety\\nof scenarios (urban road, highways, traffic, shadows, etc.) was created. The\\ndataset was manually annotated and made publicly available to enable evaluation\\nof several events that are of interest for the research community (i.e., lane\\nestimation, change, and centering; road markings; intersections; LMTs;\\ncrosswalks and adjacent lanes). ELAS achieved high detection rates in all\\nreal-world events and proved to be ready for real-time applications.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05984v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05984v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Rodrigo F. Berriel',\n",
       "   'Edilson de Aguiar',\n",
       "   'Alberto F. de Souza',\n",
       "   'Thiago Oliveira-Santos'],\n",
       "  'tasks': ['Change Detection', 'General Classification', 'Self-Driving Cars'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': ['elas'],\n",
       "  'datasets_introduced_full': ['ELAS']},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/bayesian-convolutional-neural-networks-with-1',\n",
       "  'arxiv_id': '1806.05978',\n",
       "  'title': 'Uncertainty Estimations by Softplus normalization in Bayesian Convolutional Neural Networks with Variational Inference',\n",
       "  'abstract': 'We introduce a novel uncertainty estimation for classification tasks for Bayesian convolutional neural networks with variational inference. By normalizing the output of a Softplus function in the final layer, we estimate aleatoric and epistemic uncertainty in a coherent manner. The intractable posterior probability distributions over weights are inferred by Bayes by Backprop. Firstly, we demonstrate how this reliable variational inference method can serve as a fundamental construct for various network architectures. On multiple datasets in supervised learning settings (MNIST, CIFAR-10, CIFAR-100), this variational inference method achieves performances equivalent to frequentist inference in identical architectures, while the two desiderata, a measure for uncertainty and regularization are incorporated naturally. Secondly, we examine how our proposed measure for aleatoric and epistemic uncertainties is derived and validate it on the aforementioned datasets.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05978v6',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05978v6.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kumar Shridhar', 'Felix Laumann', 'Marcus Liwicki'],\n",
       "  'tasks': ['Bayesian Inference',\n",
       "   'General Classification',\n",
       "   'Variational Inference'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [{'name': 'Softplus',\n",
       "    'full_name': 'Softplus',\n",
       "    'description': '**Softplus** is an activation function $f\\\\left(x\\\\right) = \\\\log\\\\left(1+\\\\exp\\\\left(x\\\\right)\\\\right)$. It can be viewed as a smooth version of [ReLU](https://paperswithcode.com/method/relu).',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L723',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['cifar-10', 'mnist', 'cifar-100'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'MNIST', 'CIFAR-100'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/supervised-learning-with-generalized-tensor',\n",
       "  'arxiv_id': '1806.05964',\n",
       "  'title': 'From probabilistic graphical models to generalized tensor networks for supervised learning',\n",
       "  'abstract': 'Tensor networks have found a wide use in a variety of applications in physics and computer science, recently leading to both theoretical insights as well as practical algorithms in machine learning. In this work we explore the connection between tensor networks and probabilistic graphical models, and show that it motivates the definition of generalized tensor networks where information from a tensor can be copied and reused in other parts of the network. We discuss the relationship between generalized tensor network architectures used in quantum physics, such as string-bond states, and architectures commonly used in machine learning. We provide an algorithm to train these networks in a supervised-learning context and show that they overcome the limitations of regular tensor networks in higher dimensions, while keeping the computation efficient. A method to combine neural networks and tensor networks as part of a common deep learning architecture is also introduced. We benchmark our algorithm for several generalized tensor network architectures on the task of classifying images and sounds, and show that they outperform previously introduced tensor-network algorithms. The models we consider also have a natural implementation on a quantum computer and may guide the development of near-term quantum machine learning architectures.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05964v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05964v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ivan Glasser', 'Nicola Pancotti', 'J. Ignacio Cirac'],\n",
       "  'tasks': ['Tensor Networks'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/techniques-for-visualizing-lstms-applied-to',\n",
       "  'arxiv_id': '1705.08153',\n",
       "  'title': 'Techniques for visualizing LSTMs applied to electrocardiograms',\n",
       "  'abstract': 'This paper explores four different visualization techniques for long\\nshort-term memory (LSTM) networks applied to continuous-valued time series. On\\nthe datasets analysed, we find that the best visualization technique is to\\nlearn an input deletion mask that optimally reduces the true class score. With\\na specific focus on single-lead electrocardiograms from the MIT-BIH arrhythmia\\ndataset, we show that salient input features for the LSTM classifier align well\\nwith medical theory.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1705.08153v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1705.08153v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jos van der Westhuizen', 'Joan Lasenby'],\n",
       "  'tasks': ['Time Series'],\n",
       "  'date': '2017-05-23',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-temporal-lstm-for-daily-living-action',\n",
       "  'arxiv_id': '1802.00421',\n",
       "  'title': 'Deep-Temporal LSTM for Daily Living Action Recognition',\n",
       "  'abstract': 'In this paper, we propose to improve the traditional use of RNNs by employing\\na many to many model for video classification. We analyze the importance of\\nmodeling spatial layout and temporal encoding for daily living action\\nrecognition. Many RGB methods focus only on short term temporal information\\nobtained from optical flow. Skeleton based methods on the other hand show that\\nmodeling long term skeleton evolution improves action recognition accuracy. In\\nthis work, we propose a deep-temporal LSTM architecture which extends standard\\nLSTM and allows better encoding of temporal information. In addition, we\\npropose to fuse 3D skeleton geometry with deep static appearance. We validate\\nour approach on public available CAD60, MSRDailyActivity3D and NTU-RGB+D,\\nachieving competitive performance as compared to the state-of-the art.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.00421v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.00421v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Srijan Das',\n",
       "   'Michal Koperski',\n",
       "   'Francois Bremond',\n",
       "   'Gianpiero Francesca'],\n",
       "  'tasks': ['Action Recognition',\n",
       "   'General Classification',\n",
       "   'Optical Flow Estimation',\n",
       "   'Temporal Action Localization',\n",
       "   'Video Classification'],\n",
       "  'date': '2018-02-01',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': ['msrdailyactivity3d'],\n",
       "  'datasets_used_full': ['MSRDailyActivity3D'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/controllable-semantic-image-inpainting',\n",
       "  'arxiv_id': '1806.05953',\n",
       "  'title': 'Controllable Semantic Image Inpainting',\n",
       "  'abstract': 'We develop a method for user-controllable semantic image inpainting: Given an\\narbitrary set of observed pixels, the unobserved pixels can be imputed in a\\nuser-controllable range of possibilities, each of which is semantically\\ncoherent and locally consistent with the observed pixels. We achieve this using\\na deep generative model bringing together: an encoder which can encode an\\narbitrary set of observed pixels, latent variables which are trained to\\nrepresent disentangled factors of variations, and a bidirectional PixelCNN\\nmodel. We experimentally demonstrate that our method can generate plausible\\ninpainting results matching the user-specified semantics, but is still coherent\\nwith observed pixels. We justify our choices of architecture and training\\nregime through more experiments.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05953v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05953v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jin Xu', 'Yee Whye Teh'],\n",
       "  'tasks': ['Image Inpainting'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/amortized-context-vector-inference-for',\n",
       "  'arxiv_id': '1805.09039',\n",
       "  'title': 'Amortized Context Vector Inference for Sequence-to-Sequence Networks',\n",
       "  'abstract': 'Neural attention (NA) has become a key component of sequence-to-sequence\\nmodels that yield state-of-the-art performance in as hard tasks as abstractive\\ndocument summarization (ADS) and video captioning (VC). NA mechanisms perform\\ninference of context vectors; these constitute weighted sums of deterministic\\ninput sequence encodings, adaptively sourced over long temporal horizons.\\nInspired from recent work in the field of amortized variational inference\\n(AVI), in this work we consider treating the context vectors generated by\\nsoft-attention (SA) models as latent variables, with approximate finite mixture\\nmodel posteriors inferred via AVI. We posit that this formulation may yield\\nstronger generalization capacity, in line with the outcomes of existing\\napplications of AVI to deep networks. To illustrate our method, we implement it\\nand experimentally evaluate it considering challenging ADS, VC, and MT\\nbenchmarks. This way, we exhibit its improved effectiveness over\\nstate-of-the-art alternatives.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.09039v9',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.09039v9.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kyriacos Tolias', 'Ioannis Kourouklides', 'Sotirios Chatzis'],\n",
       "  'tasks': ['Document Summarization',\n",
       "   'Variational Inference',\n",
       "   'Video Captioning'],\n",
       "  'date': '2018-05-23',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-challenge-set-for-french-english-machine',\n",
       "  'arxiv_id': '1806.02725',\n",
       "  'title': 'A Challenge Set for French --> English Machine Translation',\n",
       "  'abstract': 'We present a challenge set for French --> English machine translation based\\non the approach introduced in Isabelle, Cherry and Foster (EMNLP 2017). Such\\nchallenge sets are made up of sentences that are expected to be relatively\\ndifficult for machines to translate correctly because their most\\nstraightforward translations tend to be linguistically divergent. We present\\nhere a set of 506 manually constructed French sentences, 307 of which are\\ntargeted to the same kinds of structural divergences as in the paper mentioned\\nabove. The remaining 199 sentences are designed to test the ability of the\\nsystems to correctly translate difficult grammatical words such as\\nprepositions. We report on the results of using this challenge set for testing\\ntwo different systems, namely Google Translate and DEEPL, each on two different\\ndates (October 2017 and January 2018). All the resulting data are made publicly\\navailable.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.02725v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.02725v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Pierre Isabelle', 'Roland Kuhn'],\n",
       "  'tasks': ['Machine Translation', 'Translation'],\n",
       "  'date': '2018-06-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/discovering-user-groups-for-natural-language',\n",
       "  'arxiv_id': '1806.05947',\n",
       "  'title': 'Discovering User Groups for Natural Language Generation',\n",
       "  'abstract': 'We present a model which predicts how individual users of a dialog system\\nunderstand and produce utterances based on user groups. In contrast to previous\\nwork, these user groups are not specified beforehand, but learned in training.\\nWe evaluate on two referring expression (RE) generation tasks; our experiments\\nshow that our model can identify user groups and learn how to most effectively\\ntalk to them, and can dynamically assign unseen users to the correct groups as\\nthey interact with the system.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05947v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05947v1.pdf',\n",
       "  'proceeding': 'WS 2018 7',\n",
       "  'authors': ['Nikos Engonopoulos', 'Christoph Teichmann', 'Alexander Koller'],\n",
       "  'tasks': ['Text Generation'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/efficient-nearest-neighbors-search-for-large',\n",
       "  'arxiv_id': '1806.05946',\n",
       "  'title': 'Efficient Nearest Neighbors Search for Large-Scale Landmark Recognition',\n",
       "  'abstract': 'The problem of landmark recognition has achieved excellent results in\\nsmall-scale datasets. When dealing with large-scale retrieval, issues that were\\nirrelevant with small amount of data, quickly become fundamental for an\\nefficient retrieval phase. In particular, computational time needs to be kept\\nas low as possible, whilst the retrieval accuracy has to be preserved as much\\nas possible. In this paper we propose a novel multi-index hashing method called\\nBag of Indexes (BoI) for Approximate Nearest Neighbors (ANN) search. It allows\\nto drastically reduce the query time and outperforms the accuracy results\\ncompared to the state-of-the-art methods for large-scale landmark recognition.\\nIt has been demonstrated that this family of algorithms can be applied on\\ndifferent embedding techniques like VLAD and R-MAC obtaining excellent results\\nin very short times on different public datasets: Holidays+Flickr1M, Oxford105k\\nand Paris106k.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05946v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05946v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Federico Magliani', 'Tomaso Fontanini', 'Andrea Prati'],\n",
       "  'tasks': ['Landmark Recognition'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['oxford5k', 'oxford105k'],\n",
       "  'datasets_used_full': ['Oxford5k', 'Oxford105k'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/repmet-representative-based-metric-learning',\n",
       "  'arxiv_id': '1806.04728',\n",
       "  'title': 'RepMet: Representative-based metric learning for classification and one-shot object detection',\n",
       "  'abstract': 'Distance metric learning (DML) has been successfully applied to object\\nclassification, both in the standard regime of rich training data and in the\\nfew-shot scenario, where each category is represented by only a few examples.\\nIn this work, we propose a new method for DML that simultaneously learns the\\nbackbone network parameters, the embedding space, and the multi-modal\\ndistribution of each of the training categories in that space, in a single\\nend-to-end training process. Our approach outperforms state-of-the-art methods\\nfor DML-based object classification on a variety of standard fine-grained\\ndatasets. Furthermore, we demonstrate the effectiveness of our approach on the\\nproblem of few-shot object detection, by incorporating the proposed DML\\narchitecture as a classification head into a standard object detection model.\\nWe achieve the best results on the ImageNet-LOC dataset compared to strong\\nbaselines, when only a few training examples are available. We also offer the\\ncommunity a new episodic benchmark based on the ImageNet dataset for the\\nfew-shot object detection task.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04728v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04728v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Leonid Karlinsky',\n",
       "   'Joseph Shtok',\n",
       "   'Sivan Harary',\n",
       "   'Eli Schwartz',\n",
       "   'Amit Aides',\n",
       "   'Rogerio Feris',\n",
       "   'Raja Giryes',\n",
       "   'Alex M. Bronstein'],\n",
       "  'tasks': ['Classification',\n",
       "   'Few-Shot Object Detection',\n",
       "   'General Classification',\n",
       "   'Metric Learning',\n",
       "   'Object Detection',\n",
       "   'One-Shot Object Detection'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['oxford-102-flower'],\n",
       "  'datasets_used_full': ['Oxford 102 Flower'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/query-k-means-clustering-and-the-double-dixie',\n",
       "  'arxiv_id': '1806.05938',\n",
       "  'title': 'Query K-means Clustering and the Double Dixie Cup Problem',\n",
       "  'abstract': 'We consider the problem of approximate $K$-means clustering with outliers and\\nside information provided by same-cluster queries and possibly noisy answers.\\nOur solution shows that, under some mild assumptions on the smallest cluster\\nsize, one can obtain an $(1+\\\\epsilon)$-approximation for the optimal potential\\nwith probability at least $1-\\\\delta$, where $\\\\epsilon>0$ and $\\\\delta\\\\in(0,1)$,\\nusing an expected number of $O(\\\\frac{K^3}{\\\\epsilon \\\\delta})$ noiseless\\nsame-cluster queries and comparison-based clustering of complexity $O(ndK +\\n\\\\frac{K^3}{\\\\epsilon \\\\delta})$, here, $n$ denotes the number of points and $d$\\nthe dimension of space. Compared to a handful of other known approaches that\\nperform importance sampling to account for small cluster sizes, the proposed\\nquery technique reduces the number of queries by a factor of roughly\\n$O(\\\\frac{K^6}{\\\\epsilon^3})$, at the cost of possibly missing very small\\nclusters. We extend this settings to the case where some queries to the oracle\\nproduce erroneous information, and where certain points, termed outliers, do\\nnot belong to any clusters. Our proof techniques differ from previous methods\\nused for $K$-means clustering analysis, as they rely on estimating the sizes of\\nthe clusters and the number of points needed for accurate centroid estimation\\nand subsequent nontrivial generalizations of the double Dixie cup problem. We\\nillustrate the performance of the proposed algorithm both on synthetic and real\\ndatasets, including MNIST and CIFAR $10$.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05938v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05938v2.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['I Chien', 'Chao Pan', 'Olgica Milenkovic'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10'],\n",
       "  'datasets_used_full': ['CIFAR-10'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dynamic-weight-alignment-for-temporal',\n",
       "  'arxiv_id': '1712.06530',\n",
       "  'title': 'Dynamic Weight Alignment for Temporal Convolutional Neural Networks',\n",
       "  'abstract': 'In this paper, we propose a method of improving temporal Convolutional Neural\\nNetworks (CNN) by determining the optimal alignment of weights and inputs using\\ndynamic programming. Conventional CNN convolutions linearly match the shared\\nweights to a window of the input. However, it is possible that there exists a\\nmore optimal alignment of weights. Thus, we propose the use of Dynamic Time\\nWarping (DTW) to dynamically align the weights to the input of the\\nconvolutional layer. Specifically, the dynamic alignment overcomes issues such\\nas temporal distortion by finding the minimal distance matching of the weights\\nand the inputs under constraints. We demonstrate the effectiveness of the\\nproposed architecture on the Unipen online handwritten digit and character\\ndatasets, the UCI Spoken Arabic Digit dataset, and the UCI Activities of Daily\\nLife dataset.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.06530v6',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.06530v6.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Brian Kenji Iwana', 'Seiichi Uchida'],\n",
       "  'tasks': ['Dynamic Time Warping', 'Time Series'],\n",
       "  'date': '2017-12-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-semantic-sentence-embeddings-using-1',\n",
       "  'arxiv_id': '1806.00807',\n",
       "  'title': 'Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator',\n",
       "  'abstract': 'In this paper, we propose a method for obtaining sentence-level embeddings.\\nWhile the problem of securing word-level embeddings is very well studied, we\\npropose a novel method for obtaining sentence-level embeddings. This is\\nobtained by a simple method in the context of solving the paraphrase generation\\ntask. If we use a sequential encoder-decoder model for generating paraphrase,\\nwe would like the generated paraphrase to be semantically close to the original\\nsentence. One way to ensure this is by adding constraints for true paraphrase\\nembeddings to be close and unrelated paraphrase candidate sentence embeddings\\nto be far. This is ensured by using a sequential pair-wise discriminator that\\nshares weights with the encoder that is trained with a suitable loss function.\\nOur loss function penalizes paraphrase sentence embedding distances from being\\ntoo large. This loss is used in combination with a sequential encoder-decoder\\nnetwork. We also validated our method by evaluating the obtained embeddings for\\na sentiment analysis task. The proposed method results in semantic embeddings\\nand outperforms the state-of-the-art on the paraphrase generation and sentiment\\nanalysis task on standard datasets. These results are also shown to be\\nstatistically significant.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.00807v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.00807v5.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Badri N. Patro',\n",
       "   'Vinod K. Kurmi',\n",
       "   'Sandeep Kumar',\n",
       "   'Vinay P. Namboodiri'],\n",
       "  'tasks': ['Paraphrase Generation',\n",
       "   'Sentence Embedding',\n",
       "   'Sentence-Embedding',\n",
       "   'Sentence Embeddings',\n",
       "   'Sentiment Analysis'],\n",
       "  'date': '2018-06-03',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['quora-1'],\n",
       "  'datasets_used_full': ['Quora'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/bayesian-best-arm-identification-for',\n",
       "  'arxiv_id': '1711.06299',\n",
       "  'title': 'Bayesian Best-Arm Identification for Selecting Influenza Mitigation Strategies',\n",
       "  'abstract': 'Pandemic influenza has the epidemic potential to kill millions of people.\\nWhile various preventive measures exist (i.a., vaccination and school\\nclosures), deciding on strategies that lead to their most effective and\\nefficient use remains challenging. To this end, individual-based\\nepidemiological models are essential to assist decision makers in determining\\nthe best strategy to curb epidemic spread. However, individual-based models are\\ncomputationally intensive and it is therefore pivotal to identify the optimal\\nstrategy using a minimal amount of model evaluations. Additionally, as\\nepidemiological modeling experiments need to be planned, a computational budget\\nneeds to be specified a priori. Consequently, we present a new sampling\\ntechnique to optimize the evaluation of preventive strategies using fixed\\nbudget best-arm identification algorithms. We use epidemiological modeling\\ntheory to derive knowledge about the reward distribution which we exploit using\\nBayesian best-arm identification algorithms (i.e., Top-two Thompson sampling\\nand BayesGap). We evaluate these algorithms in a realistic experimental setting\\nand demonstrate that it is possible to identify the optimal strategy using only\\na limited number of model evaluations, i.e., 2-to-3 times faster compared to\\nthe uniform sampling method, the predominant technique used for epidemiological\\ndecision making in the literature. Finally, we contribute and evaluate a\\nstatistic for Top-two Thompson sampling to inform the decision makers about the\\nconfidence of an arm recommendation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.06299v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.06299v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Pieter Libin',\n",
       "   'Timothy Verstraeten',\n",
       "   'Diederik M. Roijers',\n",
       "   'Jelena Grujic',\n",
       "   'Kristof Theys',\n",
       "   'Philippe Lemey',\n",
       "   'Ann Nowé'],\n",
       "  'tasks': ['Decision Making'],\n",
       "  'date': '2017-11-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/robust-bayesian-model-selection-for-variable',\n",
       "  'arxiv_id': '1806.05924',\n",
       "  'title': 'Robust Bayesian Model Selection for Variable Clustering with the Gaussian Graphical Model',\n",
       "  'abstract': 'Variable clustering is important for explanatory analysis. However, only few\\ndedicated methods for variable clustering with the Gaussian graphical model\\nhave been proposed. Even more severe, small insignificant partial correlations\\ndue to noise can dramatically change the clustering result when evaluating for\\nexample with the Bayesian Information Criteria (BIC). In this work, we try to\\naddress this issue by proposing a Bayesian model that accounts for negligible\\nsmall, but not necessarily zero, partial correlations. Based on our model, we\\npropose to evaluate a variable clustering result using the marginal likelihood.\\nTo address the intractable calculation of the marginal likelihood, we propose\\ntwo solutions: one based on a variational approximation, and another based on\\nMCMC. Experiments on simulated data shows that the proposed method is similarly\\naccurate as BIC in the no noise setting, but considerably more accurate when\\nthere are noisy partial correlations. Furthermore, on real data the proposed\\nmethod provides clustering results that are intuitively sensible, which is not\\nalways the case when using BIC or its extensions.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05924v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05924v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Daniel Andrade', 'Akiko Takeda', 'Kenji Fukumizu'],\n",
       "  'tasks': ['Model Selection'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/online-feature-ranking-for-intrusion',\n",
       "  'arxiv_id': '1803.00530',\n",
       "  'title': 'Online Feature Ranking for Intrusion Detection Systems',\n",
       "  'abstract': 'Many current approaches to the design of intrusion detection systems apply\\nfeature selection in a static, non-adaptive fashion. These methods often\\nneglect the dynamic nature of network data which requires to use adaptive\\nfeature selection techniques. In this paper, we present a simple technique\\nbased on incremental learning of support vector machines in order to rank the\\nfeatures in real time within a streaming model for network data. Some\\nillustrative numerical experiments with two popular benchmark datasets show\\nthat our approach allows to adapt to the changes in normal network behaviour\\nand novel attack patterns which have not been experienced before.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.00530v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.00530v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Buse Gul Atli', 'Alexander Jung'],\n",
       "  'tasks': ['Incremental Learning', 'Intrusion Detection'],\n",
       "  'date': '2018-03-01',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/an-empirical-analysis-of-the-correlation-of',\n",
       "  'arxiv_id': '1806.05900',\n",
       "  'title': 'An Empirical Analysis of the Correlation of Syntax and Prosody',\n",
       "  'abstract': 'The relation of syntax and prosody (the syntax--prosody interface) has been\\nan active area of research, mostly in linguistics and typically studied under\\ncontrolled conditions. More recently, prosody has also been successfully used\\nin the data-based training of syntax parsers. However, there is a gap between\\nthe controlled and detailed study of the individual effects between syntax and\\nprosody and the large-scale application of prosody in syntactic parsing with\\nonly a shallow analysis of the respective influences. In this paper, we close\\nthe gap by investigating the significance of correlations of prosodic\\nrealization with specific syntactic functions using linear mixed effects models\\nin a very large corpus of read-out German encyclopedic texts. Using this\\ncorpus, we are able to analyze prosodic structuring performed by a diverse set\\nof speakers while they try to optimize factual content delivery. After\\nnormalization by speaker, we obtain significant effects, e.g. confirming that\\nthe subject function, as compared to the object function, has a positive effect\\non pitch and duration of a word, but a negative effect on loudness.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05900v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05900v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Arne Köhn', 'Timo Baumann', 'Oskar Dörfler'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/magix-model-agnostic-globally-interpretable',\n",
       "  'arxiv_id': '1706.07160',\n",
       "  'title': 'MAGIX: Model Agnostic Globally Interpretable Explanations',\n",
       "  'abstract': 'Explaining the behavior of a black box machine learning model at the instance\\nlevel is useful for building trust. However, it is also important to understand\\nhow the model behaves globally. Such an understanding provides insight into\\nboth the data on which the model was trained and the patterns that it learned.\\nWe present here an approach that learns if-then rules to globally explain the\\nbehavior of black box machine learning models that have been used to solve\\nclassification problems. The approach works by first extracting conditions that\\nwere important at the instance level and then evolving rules through a genetic\\nalgorithm with an appropriate fitness function. Collectively, these rules\\nrepresent the patterns followed by the model for decisioning and are useful for\\nunderstanding its behavior. We demonstrate the validity and usefulness of the\\napproach by interpreting black box models created using publicly available data\\nsets as well as a private digital marketing data set.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1706.07160v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1706.07160v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Nikaash Puri',\n",
       "   'Piyush Gupta',\n",
       "   'Pratiksha Agarwal',\n",
       "   'Sukriti Verma',\n",
       "   'Balaji Krishnamurthy'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-06-22',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/improving-width-based-planning-with-compact',\n",
       "  'arxiv_id': '1806.05898',\n",
       "  'title': 'Improving width-based planning with compact policies',\n",
       "  'abstract': 'Optimal action selection in decision problems characterized by sparse,\\ndelayed rewards is still an open challenge. For these problems, current deep\\nreinforcement learning methods require enormous amounts of data to learn\\ncontrollers that reach human-level performance. In this work, we propose a\\nmethod that interleaves planning and learning to address this issue. The\\nplanning step hinges on the Iterated-Width (IW) planner, a state of the art\\nplanner that makes explicit use of the state representation to perform\\nstructured exploration. IW is able to scale up to problems independently of the\\nsize of the state space. From the state-actions visited by IW, the learning\\nstep estimates a compact policy, which in turn is used to guide the planning\\nstep. The type of exploration used by our method is radically different than\\nthe standard random exploration used in RL. We evaluate our method in simple\\nproblems where we show it to have superior performance than the\\nstate-of-the-art reinforcement learning algorithms A2C and Alpha Zero. Finally,\\nwe present preliminary results in a subset of the Atari games suite.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05898v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05898v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Miquel Junyent', 'Anders Jonsson', 'Vicenç Gómez'],\n",
       "  'tasks': ['Atari Games'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [{'name': 'A2C',\n",
       "    'full_name': 'A2C',\n",
       "    'description': '**A2C**, or **Advantage Actor Critic**, is a synchronous version of the [A3C](https://paperswithcode.com/method/a3c) policy gradient method. As an alternative to the asynchronous implementation of A3C, A2C is a synchronous, deterministic implementation that waits for each actor to finish its segment of experience before updating, averaging over all of the actors. This more effectively uses GPUs due to larger batch sizes.\\r\\n\\r\\nImage Credit: [OpenAI Baselines](https://openai.com/blog/baselines-acktr-a2c/)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1602.01783v2',\n",
       "    'source_title': 'Asynchronous Methods for Deep Reinforcement Learning',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Policy Gradient Methods',\n",
       "     'description': '**Policy Gradient Methods** try to optimize the policy function directly in reinforcement learning. This contrasts with, for example Q-Learning, where the policy manifests itself as maximizing a value function. Below you can find a continuously updating catalogue of policy gradient methods.',\n",
       "     'parent': None,\n",
       "     'area': 'Reinforcement Learning'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/mining-rank-data',\n",
       "  'arxiv_id': '1806.05897',\n",
       "  'title': 'Mining Rank Data',\n",
       "  'abstract': 'The problem of frequent pattern mining has been studied quite extensively for\\nvarious types of data, including sets, sequences, and graphs. Somewhat\\nsurprisingly, another important type of data, namely rank data, has received\\nvery little attention in data mining so far. In this paper, we therefore\\naddresses the problem of mining rank data, that is, data in the form of\\nrankings (total orders) of an underlying set of items. More specifically, two\\ntypes of patterns are considered, namely frequent rankings and dependencies\\nbetween such rankings in the form of association rules. Algorithms for mining\\nfrequent rankings and frequent closed rankings are proposed and tested\\nexperimentally, using both synthetic and real data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05897v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05897v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sascha Henzgen', 'Eyke Hüllermeier'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-front-end-filter-bank-parameters',\n",
       "  'arxiv_id': '1806.05892',\n",
       "  'title': 'Learning Front-end Filter-bank Parameters using Convolutional Neural Networks for Abnormal Heart Sound Detection',\n",
       "  'abstract': 'Automatic heart sound abnormality detection can play a vital role in the\\nearly diagnosis of heart diseases, particularly in low-resource settings. The\\nstate-of-the-art algorithms for this task utilize a set of Finite Impulse\\nResponse (FIR) band-pass filters as a front-end followed by a Convolutional\\nNeural Network (CNN) model. In this work, we propound a novel CNN architecture\\nthat integrates the front-end bandpass filters within the network using\\ntime-convolution (tConv) layers, which enables the FIR filter-bank parameters\\nto become learnable. Different initialization strategies for the learnable\\nfilters, including random parameters and a set of predefined FIR filter-bank\\ncoefficients, are examined. Using the proposed tConv layers, we add constraints\\nto the learnable FIR filters to ensure linear and zero phase responses.\\nExperimental evaluations are performed on a balanced 4-fold cross-validation\\ntask prepared using the PhysioNet/CinC 2016 dataset. Results demonstrate that\\nthe proposed models yield superior performance compared to the state-of-the-art\\nsystem, while the linear phase FIR filterbank method provides an absolute\\nimprovement of 9.54% over the baseline in terms of an overall accuracy metric.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05892v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05892v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ahmed Imtiaz Humayun',\n",
       "   'Shabnam Ghaffarzadegan',\n",
       "   'Zhe Feng',\n",
       "   'Taufiq Hasan'],\n",
       "  'tasks': ['Anomaly Detection'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/structured-low-rank-matrix-learning',\n",
       "  'arxiv_id': '1704.07352',\n",
       "  'title': 'Structured low-rank matrix learning: algorithms and applications',\n",
       "  'abstract': \"We consider the problem of learning a low-rank matrix, constrained to lie in\\na linear subspace, and introduce a novel factorization for modeling such\\nmatrices. A salient feature of the proposed factorization scheme is it\\ndecouples the low-rank and the structural constraints onto separate factors. We\\nformulate the optimization problem on the Riemannian spectrahedron manifold,\\nwhere the Riemannian framework allows to develop computationally efficient\\nconjugate gradient and trust-region algorithms. Experiments on problems such as\\nstandard/robust/non-negative matrix completion, Hankel matrix learning and\\nmulti-task learning demonstrate the efficacy of our approach. A shorter version\\nof this work has been published in ICML'18.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1704.07352v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1704.07352v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Pratik Jawanpuria', 'Bamdev Mishra'],\n",
       "  'tasks': ['Matrix Completion', 'Multi-Task Learning'],\n",
       "  'date': '2017-04-24',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['movielens'],\n",
       "  'datasets_used_full': ['MovieLens'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/hierarchical-novelty-detection-for-visual',\n",
       "  'arxiv_id': '1804.00722',\n",
       "  'title': 'Hierarchical Novelty Detection for Visual Object Recognition',\n",
       "  'abstract': 'Deep neural networks have achieved impressive success in large-scale visual\\nobject recognition tasks with a predefined set of classes. However, recognizing\\nobjects of novel classes unseen during training still remains challenging. The\\nproblem of detecting such novel classes has been addressed in the literature,\\nbut most prior works have focused on providing simple binary or regressive\\ndecisions, e.g., the output would be \"known,\" \"novel,\" or corresponding\\nconfidence intervals. In this paper, we study more informative novelty\\ndetection schemes based on a hierarchical classification framework. For an\\nobject of a novel class, we aim for finding its closest super class in the\\nhierarchical taxonomy of known classes. To this end, we propose two different\\napproaches termed top-down and flatten methods, and their combination as well.\\nThe essential ingredients of our methods are confidence-calibrated classifiers,\\ndata relabeling, and the leave-one-out strategy for modeling novel classes\\nunder the hierarchical taxonomy. Furthermore, our method can generate a\\nhierarchical embedding that leads to improved generalized zero-shot learning\\nperformance in combination with other commonly-used semantic embeddings.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.00722v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.00722v2.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Kibok Lee',\n",
       "   'Kimin Lee',\n",
       "   'Kyle Min',\n",
       "   'Yuting Zhang',\n",
       "   'Jinwoo Shin',\n",
       "   'Honglak Lee'],\n",
       "  'tasks': ['Generalized Zero-Shot Learning',\n",
       "   'Object Recognition',\n",
       "   'Zero-Shot Learning'],\n",
       "  'date': '2018-04-02',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet', 'awa2-1'],\n",
       "  'datasets_used_full': ['ImageNet', 'AwA2'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/automated-image-data-preprocessing-with-deep',\n",
       "  'arxiv_id': '1806.05886',\n",
       "  'title': 'Automated Image Data Preprocessing with Deep Reinforcement Learning',\n",
       "  'abstract': 'Data preparation, i.e. the process of transforming raw data into a format that can be used for training effective machine learning models, is a tedious and time-consuming task. For image data, preprocessing typically involves a sequence of basic transformations such as cropping, filtering, rotating or flipping images. Currently, data scientists decide manually based on their experience which transformations to apply in which particular order to a given image data set. Besides constituting a bottleneck in real-world data science projects, manual image data preprocessing may yield suboptimal results as data scientists need to rely on intuition or trial-and-error approaches when exploring the space of possible image transformations and thus might not be able to discover the most effective ones. To mitigate the inefficiency and potential ineffectiveness of manual data preprocessing, this paper proposes a deep reinforcement learning framework to automatically discover the optimal data preprocessing steps for training an image classifier. The framework takes as input sets of labeled images and predefined preprocessing transformations. It jointly learns the classifier and the optimal preprocessing transformations for individual images. Experimental results show that the proposed approach not only improves the accuracy of image classifiers, but also makes them substantially more robust to noisy inputs at test time.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05886v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05886v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Tran Ngoc Minh',\n",
       "   'Mathieu Sinn',\n",
       "   'Hoang Thanh Lam',\n",
       "   'Martin Wistuba'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-simple-blind-denoising-filter-inspired-by',\n",
       "  'arxiv_id': '1806.05882',\n",
       "  'title': 'A simple blind-denoising filter inspired by electrically coupled photoreceptors in the retina',\n",
       "  'abstract': 'Photoreceptors in the retina are coupled by electrical synapses called \"gap\\njunctions\". It has long been established that gap junctions increase the\\nsignal-to-noise ratio of photoreceptors. Inspired by electrically coupled\\nphotoreceptors, we introduced a simple filter, the PR-filter, with only one\\nvariable. On BSD68 dataset, PR-filter showed outstanding performance in SSIM\\nduring blind denoising tasks. It also significantly improved the performance of\\nstate-of-the-art convolutional neural network blind denosing on non-Gaussian\\nnoise. The performance of keeping more details might be attributed to small\\nreceptive field of the photoreceptors.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05882v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05882v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yang Yue',\n",
       "   'Liuyuan He',\n",
       "   'Gan He',\n",
       "   'Jian. K. Liu',\n",
       "   'Kai Du',\n",
       "   'Yonghong Tian',\n",
       "   'Tiejun Huang'],\n",
       "  'tasks': ['Denoising', 'SSIM'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['bsd'],\n",
       "  'datasets_used_full': ['BSD'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/neural-stethoscopes-unifying-analytic',\n",
       "  'arxiv_id': '1806.05502',\n",
       "  'title': 'Scrutinizing and De-Biasing Intuitive Physics with Neural Stethoscopes',\n",
       "  'abstract': 'Visually predicting the stability of block towers is a popular task in the domain of intuitive physics. While previous work focusses on prediction accuracy, a one-dimensional performance measure, we provide a broader analysis of the learned physical understanding of the final model and how the learning process can be guided. To this end, we introduce neural stethoscopes as a general purpose framework for quantifying the degree of importance of specific factors of influence in deep neural networks as well as for actively promoting and suppressing information as appropriate. In doing so, we unify concepts from multitask learning as well as training with auxiliary and adversarial losses. We apply neural stethoscopes to analyse the state-of-the-art neural network for stability prediction. We show that the baseline model is susceptible to being misled by incorrect visual cues. This leads to a performance breakdown to the level of random guessing when training on scenarios where visual cues are inversely correlated with stability. Using stethoscopes to promote meaningful feature extraction increases performance from 51% to 90% prediction accuracy. Conversely, training on an easy dataset where visual cues are positively correlated with stability, the baseline model learns a bias leading to poor performance on a harder dataset. Using an adversarial stethoscope, the network is successfully de-biased, leading to a performance increase from 66% to 88%.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05502v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05502v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Fabian B. Fuchs',\n",
       "   'Oliver Groth',\n",
       "   'Adam R. Kosiorek',\n",
       "   'Alex Bewley',\n",
       "   'Markus Wulfmeier',\n",
       "   'Andrea Vedaldi',\n",
       "   'Ingmar Posner'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['shapestacks'],\n",
       "  'datasets_used_full': ['ShapeStacks'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/financial-risk-and-returns-prediction-with',\n",
       "  'arxiv_id': '1806.05876',\n",
       "  'title': 'Financial Risk and Returns Prediction with Modular Networked Learning',\n",
       "  'abstract': 'An artificial agent for financial risk and returns\\' prediction is built with\\na modular cognitive system comprised of interconnected recurrent neural\\nnetworks, such that the agent learns to predict the financial returns, and\\nlearns to predict the squared deviation around these predicted returns. These\\ntwo expectations are used to build a volatility-sensitive interval prediction\\nfor financial returns, which is evaluated on three major financial indices and\\nshown to be able to predict financial returns with higher than 80% success rate\\nin interval prediction in both training and testing, raising into question the\\nEfficient Market Hypothesis. The agent is introduced as an example of a class\\nof artificial intelligent systems that are equipped with a Modular Networked\\nLearning cognitive system, defined as an integrated networked system of machine\\nlearning modules, where each module constitutes a functional unit that is\\ntrained for a given specific task that solves a subproblem of a complex main\\nproblem expressed as a network of linked subproblems. In the case of neural\\nnetworks, these systems function as a form of an \"artificial brain\", where each\\nmodule is like a specialized brain region comprised of a neural network with a\\nspecific architecture.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05876v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05876v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Carlos Pedro Gonçalves'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/data-efficient-design-exploration-through',\n",
       "  'arxiv_id': '1806.05865',\n",
       "  'title': 'Data-Efficient Design Exploration through Surrogate-Assisted Illumination',\n",
       "  'abstract': 'Design optimization techniques are often used at the beginning of the design\\nprocess to explore the space of possible designs. In these domains illumination\\nalgorithms, such as MAP-Elites, are promising alternatives to classic\\noptimization algorithms because they produce diverse, high-quality solutions in\\na single run, instead of only a single near-optimal solution. Unfortunately,\\nthese algorithms currently require a large number of function evaluations,\\nlimiting their applicability. In this article we introduce a new illumination\\nalgorithm, Surrogate-Assisted Illumination (SAIL), that leverages surrogate\\nmodeling techniques to create a map of the design space according to\\nuser-defined features while minimizing the number of fitness evaluations. On a\\n2-dimensional airfoil optimization problem SAIL produces hundreds of diverse\\nbut high-performing designs with several orders of magnitude fewer evaluations\\nthan MAP-Elites or CMA-ES. We demonstrate that SAIL is also capable of\\nproducing maps of high-performing designs in realistic 3-dimensional\\naerodynamic tasks with an accurate flow simulation. Data-efficient design\\nexploration with SAIL can help designers understand what is possible, beyond\\nwhat is optimal, by considering more than pure objective-based optimization.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05865v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05865v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Adam Gaier', 'Alexander Asteroth', 'Jean-Baptiste Mouret'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deeplaser-practical-fault-attack-on-deep',\n",
       "  'arxiv_id': '1806.05859',\n",
       "  'title': 'DeepLaser: Practical Fault Attack on Deep Neural Networks',\n",
       "  'abstract': 'As deep learning systems are widely adopted in safety- and security-critical\\napplications, such as autonomous vehicles, banking systems, etc., malicious\\nfaults and attacks become a tremendous concern, which potentially could lead to\\ncatastrophic consequences. In this paper, we initiate the first study of\\nleveraging physical fault injection attacks on Deep Neural Networks (DNNs), by\\nusing laser injection technique on embedded systems. In particular, our\\nexploratory study targets four widely used activation functions in DNNs\\ndevelopment, that are the general main building block of DNNs that creates\\nnon-linear behaviors -- ReLu, softmax, sigmoid, and tanh. Our results show that\\nby targeting these functions, it is possible to achieve a misclassification by\\ninjecting faults into the hidden layer of the network. Such result can have\\npractical implications for real-world applications, where faults can be\\nintroduced by simpler means (such as altering the supply voltage).',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05859v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05859v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jakub Breier',\n",
       "   'Xiaolu Hou',\n",
       "   'Dirmanto Jap',\n",
       "   'Lei Ma',\n",
       "   'Shivam Bhasin',\n",
       "   'Yang Liu'],\n",
       "  'tasks': ['Autonomous Vehicles'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/semantic-variation-in-online-communities-of',\n",
       "  'arxiv_id': '1806.05847',\n",
       "  'title': 'Semantic Variation in Online Communities of Practice',\n",
       "  'abstract': 'We introduce a framework for quantifying semantic variation of common words\\nin Communities of Practice and in sets of topic-related communities. We show\\nthat while some meaning shifts are shared across related communities, others\\nare community-specific, and therefore independent from the discussed topic. We\\npropose such findings as evidence in favour of sociolinguistic theories of\\nsocially-driven semantic variation. Results are evaluated using an independent\\nlanguage modelling task. Furthermore, we investigate extralinguistic features\\nand show that factors such as prominence and dissemination of words are related\\nto semantic variation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05847v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05847v1.pdf',\n",
       "  'proceeding': 'WS 2017 1',\n",
       "  'authors': ['Marco Del Tredici', 'Raquel Fernández'],\n",
       "  'tasks': ['Language Modelling'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-covariance-matrix-self-adaptation-evolution',\n",
       "  'arxiv_id': '1806.05845',\n",
       "  'title': 'A Covariance Matrix Self-Adaptation Evolution Strategy for Optimization under Linear Constraints',\n",
       "  'abstract': 'This paper addresses the development of a covariance matrix self-adaptation\\nevolution strategy (CMSA-ES) for solving optimization problems with linear\\nconstraints. The proposed algorithm is referred to as Linear Constraint CMSA-ES\\n(lcCMSA-ES). It uses a specially built mutation operator together with repair\\nby projection to satisfy the constraints. The lcCMSA-ES evolves itself on a\\nlinear manifold defined by the constraints. The objective function is only\\nevaluated at feasible search points (interior point method). This is a property\\noften required in application domains such as simulation optimization and\\nfinite element methods. The algorithm is tested on a variety of different test\\nproblems revealing considerable results.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05845v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05845v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Patrick Spettel', 'Hans-Georg Beyer', 'Michael Hellwig'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/co-clustering-via-information-theoretic',\n",
       "  'arxiv_id': '1801.00584',\n",
       "  'title': 'Co-Clustering via Information-Theoretic Markov Aggregation',\n",
       "  'abstract': 'We present an information-theoretic cost function for co-clustering, i.e.,\\nfor simultaneous clustering of two sets based on similarities between their\\nelements. By constructing a simple random walk on the corresponding bipartite\\ngraph, our cost function is derived from a recently proposed generalized\\nframework for information-theoretic Markov chain aggregation. The goal of our\\ncost function is to minimize relevant information loss, hence it connects to\\nthe information bottleneck formalism. Moreover, via the connection to Markov\\naggregation, our cost function is not ad hoc, but inherits its justification\\nfrom the operational qualities associated with the corresponding Markov\\naggregation problem. We furthermore show that, for appropriate parameter\\nsettings, our cost function is identical to well-known approaches from the\\nliterature, such as Information-Theoretic Co-Clustering of Dhillon et al.\\nHence, understanding the influence of this parameter admits a deeper\\nunderstanding of the relationship between previously proposed\\ninformation-theoretic cost functions. We highlight some strengths and\\nweaknesses of the cost function for different parameters. We also illustrate\\nthe performance of our cost function, optimized with a simple sequential\\nheuristic, on several synthetic and real-world data sets, including the\\nNewsgroup20 and the MovieLens100k data sets.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1801.00584v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1801.00584v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Clemens Bloechl', 'Rana Ali Amjad', 'Bernhard C. Geiger'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-01-02',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['movielens'],\n",
       "  'datasets_used_full': ['MovieLens'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/real-time-monocular-visual-odometry-for',\n",
       "  'arxiv_id': '1806.05842',\n",
       "  'title': 'Real-time Monocular Visual Odometry for Turbid and Dynamic Underwater Environments',\n",
       "  'abstract': 'In the context of robotic underwater operations, the visual degradations induced by the medium properties make difficult the exclusive use of cameras for localization purpose. Hence, most localization methods are based on expensive navigational sensors associated with acoustic positioning. On the other hand, visual odometry and visual SLAM have been exhaustively studied for aerial or terrestrial applications, but state-of-the-art algorithms fail underwater. In this paper we tackle the problem of using a simple low-cost camera for underwater localization and propose a new monocular visual odometry method dedicated to the underwater environment. We evaluate different tracking methods and show that optical flow based tracking is more suited to underwater images than classical approaches based on descriptors. We also propose a keyframe-based visual odometry approach highly relying on nonlinear optimization. The proposed algorithm has been assessed on both simulated and real underwater datasets and outperforms state-of-the-art visual SLAM methods under many of the most challenging conditions. The main application of this work is the localization of Remotely Operated Vehicles (ROVs) used for underwater archaeological missions but the developed system can be used in any other applications as long as visual information is available.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05842v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05842v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Maxime Ferrera',\n",
       "   'Julien Moras',\n",
       "   'Pauline Trouvé-Peloux',\n",
       "   'Vincent Creuze'],\n",
       "  'tasks': ['Monocular Visual Odometry',\n",
       "   'Optical Flow Estimation',\n",
       "   'Visual Odometry'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-road-to-success-assessing-the-fate-of',\n",
       "  'arxiv_id': '1806.05838',\n",
       "  'title': 'The Road to Success: Assessing the Fate of Linguistic Innovations in Online Communities',\n",
       "  'abstract': 'We investigate the birth and diffusion of lexical innovations in a large\\ndataset of online social communities. We build on sociolinguistic theories and\\nfocus on the relation between the spread of a novel term and the social role of\\nthe individuals who use it, uncovering characteristics of innovators and\\nadopters. Finally, we perform a prediction task that allows us to anticipate\\nwhether an innovation will successfully spread within a community.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05838v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05838v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Marco Del Tredici', 'Raquel Fernández'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-the-exact-minimization-of-saturated-loss',\n",
       "  'arxiv_id': '1806.05833',\n",
       "  'title': 'On the exact minimization of saturated loss functions for robust regression and subspace estimation',\n",
       "  'abstract': 'This paper deals with robust regression and subspace estimation and more\\nprecisely with the problem of minimizing a saturated loss function. In\\nparticular, we focus on computational complexity issues and show that an exact\\nalgorithm with polynomial time-complexity with respect to the number of data\\ncan be devised for robust regression and subspace estimation. This result is\\nobtained by adopting a classification point of view and relating the problems\\nto the search for a linear model that can approximate the maximal number of\\npoints with a given error. Approximate variants of the algorithms based on\\nramdom sampling are also discussed and experiments show that it offers an\\naccuracy gain over the traditional RANSAC for a similar algorithmic simplicity.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05833v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05833v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Fabien Lauer'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/selfless-sequential-learning',\n",
       "  'arxiv_id': '1806.05421',\n",
       "  'title': 'Selfless Sequential Learning',\n",
       "  'abstract': 'Sequential learning, also called lifelong learning, studies the problem of\\nlearning tasks in a sequence with access restricted to only the data of the\\ncurrent task. In this paper we look at a scenario with fixed model capacity,\\nand postulate that the learning process should not be selfish, i.e. it should\\naccount for future tasks to be added and thus leave enough capacity for them.\\nTo achieve Selfless Sequential Learning we study different regularization\\nstrategies and activation functions. We find that imposing sparsity at the\\nlevel of the representation (i.e.~neuron activations) is more beneficial for\\nsequential learning than encouraging parameter sparsity. In particular, we\\npropose a novel regularizer, that encourages representation sparsity by means\\nof neural inhibition. It results in few active neurons which in turn leaves\\nmore free neurons to be utilized by upcoming tasks. As neural inhibition over\\nan entire layer can be too drastic, especially for complex tasks requiring\\nstrong representations, our regularizer only inhibits other neurons in a local\\nneighbourhood, inspired by lateral inhibition processes in the brain. We\\ncombine our novel regularizer, with state-of-the-art lifelong learning methods\\nthat penalize changes to important previously learned parts of the network. We\\nshow that our new regularizer leads to increased sparsity which translates in\\nconsistent performance improvement %over alternative regularizers we studied on\\ndiverse datasets.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05421v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05421v5.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Rahaf Aljundi', 'Marcus Rohrbach', 'Tinne Tuytelaars'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist', 'permuted-mnist'],\n",
       "  'datasets_used_full': ['MNIST', 'Permuted MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/temporal-stability-in-predictive-process',\n",
       "  'arxiv_id': '1712.04165',\n",
       "  'title': 'Temporal Stability in Predictive Process Monitoring',\n",
       "  'abstract': 'Predictive process monitoring is concerned with the analysis of events\\nproduced during the execution of a business process in order to predict as\\nearly as possible the final outcome of an ongoing case. Traditionally,\\npredictive process monitoring methods are optimized with respect to accuracy.\\nHowever, in environments where users make decisions and take actions in\\nresponse to the predictions they receive, it is equally important to optimize\\nthe stability of the successive predictions made for each case. To this end,\\nthis paper defines a notion of temporal stability for binary classification\\ntasks in predictive process monitoring and evaluates existing methods with\\nrespect to both temporal stability and accuracy. We find that methods based on\\nXGBoost and LSTM neural networks exhibit the highest temporal stability. We\\nthen show that temporal stability can be enhanced by hyperparameter-optimizing\\nrandom forests and XGBoost classifiers with respect to inter-run stability.\\nFinally, we show that time series smoothing techniques can further enhance\\ntemporal stability at the expense of slightly lower accuracy.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.04165v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.04165v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Irene Teinemaa',\n",
       "   'Marlon Dumas',\n",
       "   'Anna Leontjeva',\n",
       "   'Fabrizio Maria Maggi'],\n",
       "  'tasks': ['Predictive Process Monitoring', 'Time Series'],\n",
       "  'date': '2017-12-12',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/three-dimensional-deep-learning-approach-for',\n",
       "  'arxiv_id': '1806.05824',\n",
       "  'title': 'Three dimensional Deep Learning approach for remote sensing image classification',\n",
       "  'abstract': \"Recently, a variety of approaches has been enriching the field of Remote\\nSensing (RS) image processing and analysis. Unfortunately, existing methods\\nremain limited faced to the rich spatio-spectral content of today's large\\ndatasets. It would seem intriguing to resort to Deep Learning (DL) based\\napproaches at this stage with regards to their ability to offer accurate\\nsemantic interpretation of the data. However, the specificity introduced by the\\ncoexistence of spectral and spatial content in the RS datasets widens the scope\\nof the challenges presented to adapt DL methods to these contexts. Therefore,\\nthe aim of this paper is firstly to explore the performance of DL architectures\\nfor the RS hyperspectral dataset classification and secondly to introduce a new\\nthree-dimensional DL approach that enables a joint spectral and spatial\\ninformation process. A set of three-dimensional schemes is proposed and\\nevaluated. Experimental results based on well knownhyperspectral datasets\\ndemonstrate that the proposed method is able to achieve a better classification\\nrate than state of the art methods with lower computational costs.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05824v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05824v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Amina Ben Hamida',\n",
       "   'A. Benoit',\n",
       "   'Patrick Lambert',\n",
       "   'Chokri Ben Amar'],\n",
       "  'tasks': ['Classification',\n",
       "   'General Classification',\n",
       "   'Image Classification',\n",
       "   'Remote Sensing Image Classification'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/primal-dual-residual-networks',\n",
       "  'arxiv_id': '1806.05823',\n",
       "  'title': 'Primal-dual residual networks',\n",
       "  'abstract': 'In this work, we propose a deep neural network architecture motivated by\\nprimal-dual splitting methods from convex optimization. We show theoretically\\nthat there exists a close relation between the derived architecture and\\nresidual networks, and further investigate this connection in numerical\\nexperiments. Moreover, we demonstrate how our approach can be used to unroll\\noptimization algorithms for certain problems with hard constraints. Using the\\nexample of speech dequantization, we show that our method can outperform\\nclassical splitting methods when both are applied to the same task.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05823v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05823v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Christoph Brauer', 'Dirk Lorenz'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/bubblerank-safe-online-learning-to-rerank',\n",
       "  'arxiv_id': '1806.05819',\n",
       "  'title': 'BubbleRank: Safe Online Learning to Re-Rank via Implicit Click Feedback',\n",
       "  'abstract': 'In this paper, we study the problem of safe online learning to re-rank, where user feedback is used to improve the quality of displayed lists. Learning to rank has traditionally been studied in two settings. In the offline setting, rankers are typically learned from relevance labels created by judges. This approach has generally become standard in industrial applications of ranking, such as search. However, this approach lacks exploration and thus is limited by the information content of the offline training data. In the online setting, an algorithm can experiment with lists and learn from feedback on them in a sequential fashion. Bandit algorithms are well-suited for this setting but they tend to learn user preferences from scratch, which results in a high initial cost of exploration. This poses an additional challenge of safe exploration in ranked lists. We propose BubbleRank, a bandit algorithm for safe re-ranking that combines the strengths of both the offline and online settings. The algorithm starts with an initial base list and improves it online by gradually exchanging higher-ranked less attractive items for lower-ranked more attractive items. We prove an upper bound on the n-step regret of BubbleRank that degrades gracefully with the quality of the initial base list. Our theoretical findings are supported by extensive experiments on a large-scale real-world click dataset.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05819v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05819v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Chang Li',\n",
       "   'Branislav Kveton',\n",
       "   'Tor Lattimore',\n",
       "   'Ilya Markov',\n",
       "   'Maarten de Rijke',\n",
       "   'Csaba Szepesvari',\n",
       "   'Masrour Zoghi'],\n",
       "  'tasks': ['Learning-To-Rank',\n",
       "   'online learning',\n",
       "   'Re-Ranking',\n",
       "   'Safe Exploration'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/safe-active-feature-selection-for-sparse',\n",
       "  'arxiv_id': '1806.05817',\n",
       "  'title': 'Safe Active Feature Selection for Sparse Learning',\n",
       "  'abstract': 'We present safe active incremental feature selection~(SAIF) to scale up the\\ncomputation of LASSO solutions. SAIF does not require a solution from a heavier\\npenalty parameter as in sequential screening or updating the full model for\\neach iteration as in dynamic screening. Different from these existing screening\\nmethods, SAIF starts from a small number of features and incrementally recruits\\nactive features and updates the significantly reduced model. Hence, it is much\\nmore computationally efficient and scalable with the number of features. More\\ncritically, SAIF has the safe guarantee as it has the convergence guarantee to\\nthe optimal solution to the original full LASSO problem. Such an incremental\\nprocedure and theoretical convergence guarantee can be extended to fused LASSO\\nproblems. Compared with state-of-the-art screening methods as well as working\\nset and homotopy methods, which may not always guarantee the optimal solution,\\nSAIF can achieve superior or comparable efficiency and high scalability with\\nthe safe guarantee when facing extremely high dimensional data sets.\\nExperiments with both synthetic and real-world data sets show that SAIF can be\\nup to 50 times faster than dynamic screening, and hundreds of times faster than\\ncomputing LASSO or fused LASSO solutions without screening.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05817v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05817v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shaogang Ren',\n",
       "   'Jianhua Z. Huang',\n",
       "   'Shuai Huang',\n",
       "   'Xiaoning Qian'],\n",
       "  'tasks': ['Sparse Learning'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/sgm-sequence-generation-model-for-multi-label',\n",
       "  'arxiv_id': '1806.04822',\n",
       "  'title': 'SGM: Sequence Generation Model for Multi-label Classification',\n",
       "  'abstract': 'Multi-label classification is an important yet challenging task in natural\\nlanguage processing. It is more complex than single-label classification in\\nthat the labels tend to be correlated. Existing methods tend to ignore the\\ncorrelations between labels. Besides, different parts of the text can\\ncontribute differently for predicting different labels, which is not considered\\nby existing models. In this paper, we propose to view the multi-label\\nclassification task as a sequence generation problem, and apply a sequence\\ngeneration model with a novel decoder structure to solve it. Extensive\\nexperimental results show that our proposed methods outperform previous work by\\na substantial margin. Further analysis of experimental results demonstrates\\nthat the proposed methods not only capture the correlations between labels, but\\nalso select the most informative words automatically when predicting different\\nlabels.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04822v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04822v3.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Pengcheng Yang',\n",
       "   'Xu sun',\n",
       "   'Wei Li',\n",
       "   'Shuming Ma',\n",
       "   'Wei Wu',\n",
       "   'Houfeng Wang'],\n",
       "  'tasks': ['Classification',\n",
       "   'General Classification',\n",
       "   'Multi-Label Classification'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['rcv1'],\n",
       "  'datasets_used_full': ['RCV1'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/best-sources-forward-domain-generalization',\n",
       "  'arxiv_id': '1806.05810',\n",
       "  'title': 'Best sources forward: domain generalization through source-specific nets',\n",
       "  'abstract': 'A long standing problem in visual object categorization is the ability of\\nalgorithms to generalize across different testing conditions. The problem has\\nbeen formalized as a covariate shift among the probability distributions\\ngenerating the training data (source) and the test data (target) and several\\ndomain adaptation methods have been proposed to address this issue. While these\\napproaches have considered the single source-single target scenario, it is\\nplausible to have multiple sources and require adaptation to any possible\\ntarget domain. This last scenario, named Domain Generalization (DG), is the\\nfocus of our work. Differently from previous DG methods which learn domain\\ninvariant representations from source data, we design a deep network with\\nmultiple domain-specific classifiers, each associated to a source domain. At\\ntest time we estimate the probabilities that a target sample belongs to each\\nsource domain and exploit them to optimally fuse the classifiers predictions.\\nTo further improve the generalization ability of our model, we also introduced\\na domain agnostic component supporting the final classifier. Experiments on two\\npublic benchmarks demonstrate the power of our approach.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05810v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05810v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Massimiliano Mancini',\n",
       "   'Samuel Rota Bulò',\n",
       "   'Barbara Caputo',\n",
       "   'Elisa Ricci'],\n",
       "  'tasks': ['Domain Adaptation', 'Domain Generalization'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist', 'pacs'],\n",
       "  'datasets_used_full': ['MNIST', 'PACS'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/molecular-generative-model-based-on',\n",
       "  'arxiv_id': '1806.05805',\n",
       "  'title': 'Molecular generative model based on conditional variational autoencoder for de novo molecular design',\n",
       "  'abstract': 'We propose a molecular generative model based on the conditional variational\\nautoencoder for de novo molecular design. It is specialized to control multiple\\nmolecular properties simultaneously by imposing them on a latent space. As a\\nproof of concept, we demonstrate that it can be used to generate drug-like\\nmolecules with five target properties. We were also able to adjust a single\\nproperty without changing the others and to manipulate it beyond the range of\\nthe dataset.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05805v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05805v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jaechang Lim', 'Seongok Ryu', 'Jin Woo Kim', 'Woo Youn Kim'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/hybrid-approach-of-relation-network-and',\n",
       "  'arxiv_id': '1711.05859',\n",
       "  'title': 'Hybrid Approach of Relation Network and Localized Graph Convolutional Filtering for Breast Cancer Subtype Classification',\n",
       "  'abstract': 'Network biology has been successfully used to help reveal complex mechanisms\\nof disease, especially cancer. On the other hand, network biology requires\\nin-depth knowledge to construct disease-specific networks, but our current\\nknowledge is very limited even with the recent advances in human cancer\\nbiology. Deep learning has shown a great potential to address the difficult\\nsituation like this. However, deep learning technologies conventionally use\\ngrid-like structured data, thus application of deep learning technologies to\\nthe classification of human disease subtypes is yet to be explored. Recently,\\ngraph based deep learning techniques have emerged, which becomes an opportunity\\nto leverage analyses in network biology. In this paper, we proposed a hybrid\\nmodel, which integrates two key components 1) graph convolution neural network\\n(graph CNN) and 2) relation network (RN). We utilize graph CNN as a component\\nto learn expression patterns of cooperative gene community, and RN as a\\ncomponent to learn associations between learned patterns. The proposed model is\\napplied to the PAM50 breast cancer subtype classification task, the standard\\nbreast cancer subtype classification of clinical utility. In experiments of\\nboth subtype classification and patient survival analysis, our proposed method\\nachieved significantly better performances than existing methods. We believe\\nthat this work is an important starting point to realize the upcoming\\npersonalized medicine.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.05859v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.05859v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sungmin Rhee', 'Seokjun Seo', 'Sun Kim'],\n",
       "  'tasks': ['Classification', 'General Classification', 'Survival Analysis'],\n",
       "  'date': '2017-11-16',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['string'],\n",
       "  'datasets_used_full': ['STRING'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-to-act-properly-predicting-and',\n",
       "  'arxiv_id': '1712.07576',\n",
       "  'title': 'Learning to Act Properly: Predicting and Explaining Affordances from Images',\n",
       "  'abstract': \"We address the problem of affordance reasoning in diverse scenes that appear\\nin the real world. Affordances relate the agent's actions to their effects when\\ntaken on the surrounding objects. In our work, we take the egocentric view of\\nthe scene, and aim to reason about action-object affordances that respect both\\nthe physical world as well as the social norms imposed by the society. We also\\naim to teach artificial agents why some actions should not be taken in certain\\nsituations, and what would likely happen if these actions would be taken. We\\ncollect a new dataset that builds upon ADE20k, referred to as ADE-Affordance,\\nwhich contains annotations enabling such rich visual reasoning. We propose a\\nmodel that exploits Graph Neural Networks to propagate contextual information\\nfrom the scene in order to perform detailed affordance reasoning about each\\nobject. Our model is showcased through various ablation studies, pointing to\\nsuccesses and challenges in this complex task.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.07576v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.07576v2.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Ching-Yao Chuang',\n",
       "   'Jiaman Li',\n",
       "   'Antonio Torralba',\n",
       "   'Sanja Fidler'],\n",
       "  'tasks': ['Visual Reasoning'],\n",
       "  'date': '2017-12-20',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['ade20k'],\n",
       "  'datasets_used_full': ['ADE20K'],\n",
       "  'datasets_introduced_lower': ['ade-affordance'],\n",
       "  'datasets_introduced_full': ['ADE-Affordance']},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/weakly-supervised-deep-image-hashing-through',\n",
       "  'arxiv_id': '1806.05804',\n",
       "  'title': 'Weakly Supervised Deep Image Hashing through Tag Embeddings',\n",
       "  'abstract': 'Many approaches to semantic image hashing have been formulated as supervised\\nlearning problems that utilize images and label information to learn the binary\\nhash codes. However, large-scale labeled image data is expensive to obtain,\\nthus imposing a restriction on the usage of such algorithms. On the other hand,\\nunlabelled image data is abundant due to the existence of many Web image\\nrepositories. Such Web images may often come with images tags that contain\\nuseful information, although raw tags, in general, do not readily lead to\\nsemantic labels. Motivated by this scenario, we formulate the problem of\\nsemantic image hashing as a weakly-supervised learning problem. We utilize the\\ninformation contained in the user-generated tags associated with the images to\\nlearn the hash codes. More specifically, we extract the word2vec semantic\\nembeddings of the tags and use the information contained in them for\\nconstraining the learning. Accordingly, we name our model Weakly Supervised\\nDeep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of\\nsemantic image retrieval and is compared against several state-of-art models.\\nResults show that our approach sets a new state-of-art in the area of weekly\\nsupervised image hashing.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05804v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05804v3.pdf',\n",
       "  'proceeding': 'CVPR 2019 6',\n",
       "  'authors': ['Vijetha Gattupalli', 'Yaoxin Zhuo', 'Baoxin Li'],\n",
       "  'tasks': ['Image Retrieval', 'TAG'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['nus-wide'],\n",
       "  'datasets_used_full': ['NUS-WIDE'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/wikiref-wikilinks-as-a-route-to-recommending',\n",
       "  'arxiv_id': '1806.04092',\n",
       "  'title': 'WikiRef: Wikilinks as a route to recommending appropriate references for scientific Wikipedia pages',\n",
       "  'abstract': 'The exponential increase in the usage of Wikipedia as a key source of\\nscientific knowledge among the researchers is making it absolutely necessary to\\nmetamorphose this knowledge repository into an integral and self-contained\\nsource of information for direct utilization. Unfortunately, the references\\nwhich support the content of each Wikipedia entity page, are far from complete.\\nWhy are the reference section ill-formed for most Wikipedia pages? Is this\\nsection edited as frequently as the other sections of a page? Can there be\\nappropriate surrogates that can automatically enhance the reference section? In\\nthis paper, we propose a novel two step approach -- WikiRef -- that (i)\\nleverages the wikilinks present in a scientific Wikipedia target page and,\\nthereby, (ii) recommends highly relevant references to be included in that\\ntarget page appropriately and automatically borrowed from the reference section\\nof the wikilinks. In the first step, we build a classifier to ascertain whether\\na wikilink is a potential source of reference or not. In the following step, we\\nrecommend references to the target page from the reference section of the\\nwikilinks that are classified as potential sources of references in the first\\nstep. We perform an extensive evaluation of our approach on datasets from two\\ndifferent domains -- Computer Science and Physics. For Computer Science we\\nachieve a notably good performance with a precision@1 of 0.44 for reference\\nrecommendation as opposed to 0.38 obtained from the most competitive baseline.\\nFor the Physics dataset, we obtain a similar performance boost of 10% with\\nrespect to the most competitive baseline.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04092v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04092v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Abhik Jana',\n",
       "   'Pranjal Kanojiya',\n",
       "   'Pawan Goyal',\n",
       "   'Animesh Mukherjee'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/disentangled-person-image-generation',\n",
       "  'arxiv_id': '1712.02621',\n",
       "  'title': 'Disentangled Person Image Generation',\n",
       "  'abstract': 'Generating novel, yet realistic, images of persons is a challenging task due\\nto the complex interplay between the different image factors, such as the\\nforeground, background and pose information. In this work, we aim at generating\\nsuch images based on a novel, two-stage reconstruction pipeline that learns a\\ndisentangled representation of the aforementioned image factors and generates\\nnovel person images at the same time. First, a multi-branched reconstruction\\nnetwork is proposed to disentangle and encode the three factors into embedding\\nfeatures, which are then combined to re-compose the input image itself. Second,\\nthree corresponding mapping functions are learned in an adversarial manner in\\norder to map Gaussian noise to the learned embedding feature space, for each\\nfactor respectively. Using the proposed framework, we can manipulate the\\nforeground, background and pose of the input image, and also sample new\\nembedding features to generate such targeted manipulations, that provide more\\ncontrol over the generation process. Experiments on Market-1501 and Deepfashion\\ndatasets show that our model does not only generate realistic person images\\nwith new foregrounds, backgrounds and poses, but also manipulates the generated\\nfactors and interpolates the in-between states. Another set of experiments on\\nMarket-1501 shows that our model can also be beneficial for the person\\nre-identification task.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.02621v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.02621v4.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Liqian Ma',\n",
       "   'Qianru Sun',\n",
       "   'Stamatios Georgoulis',\n",
       "   'Luc van Gool',\n",
       "   'Bernt Schiele',\n",
       "   'Mario Fritz'],\n",
       "  'tasks': ['Gesture-to-Gesture Translation',\n",
       "   'Image Generation',\n",
       "   'Person Re-Identification',\n",
       "   'Pose Transfer'],\n",
       "  'date': '2017-12-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['market-1501', 'cuhk03', 'deepfashion'],\n",
       "  'datasets_used_full': ['Market-1501', 'CUHK03', 'DeepFashion'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/scalable-factorized-hierarchical-variational',\n",
       "  'arxiv_id': '1804.03201',\n",
       "  'title': 'Scalable Factorized Hierarchical Variational Autoencoder Training',\n",
       "  'abstract': 'Deep generative models have achieved great success in unsupervised learning\\nwith the ability to capture complex nonlinear relationships between latent\\ngenerating factors and observations. Among them, a factorized hierarchical\\nvariational autoencoder (FHVAE) is a variational inference-based model that\\nformulates a hierarchical generative process for sequential data. Specifically,\\nan FHVAE model can learn disentangled and interpretable representations, which\\nhave been proven useful for numerous speech applications, such as speaker\\nverification, robust speech recognition, and voice conversion. However, as we\\nwill elaborate in this paper, the training algorithm proposed in the original\\npaper is not scalable to datasets of thousands of hours, which makes this model\\nless applicable on a larger scale. After identifying limitations in terms of\\nruntime, memory, and hyperparameter optimization, we propose a hierarchical\\nsampling training algorithm to address all three issues. Our proposed method is\\nevaluated comprehensively on a wide variety of datasets, ranging from 3 to\\n1,000 hours and involving different types of generating factors, such as\\nrecording conditions and noise types. In addition, we also present a new\\nvisualization method for qualitatively evaluating the performance with respect\\nto the interpretability and disentanglement. Models trained with our proposed\\nalgorithm demonstrate the desired characteristics on all the datasets.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.03201v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.03201v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Wei-Ning Hsu', 'James Glass'],\n",
       "  'tasks': ['Disentanglement',\n",
       "   'Hyperparameter Optimization',\n",
       "   'Robust Speech Recognition',\n",
       "   'Speaker Verification',\n",
       "   'Speech Recognition',\n",
       "   'Variational Inference',\n",
       "   'Voice Conversion'],\n",
       "  'date': '2018-04-09',\n",
       "  'methods': [{'name': 'Interpretability',\n",
       "    'full_name': 'Interpretability',\n",
       "    'description': 'Please enter a description about the method here',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1310.1533v2',\n",
       "    'source_title': 'CAM: Causal additive models, high-dimensional order search and penalized regression',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Image Models',\n",
       "     'description': '**Image Models** are methods that build representations of images for downstream tasks such as classification and object detection. The most popular subcategory are convolutional neural networks. Below you can find a continuously updated list of image models.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'AutoEncoder',\n",
       "    'full_name': 'AutoEncoder',\n",
       "    'description': 'An **Autoencoder** is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then performs a reconstruction of the input with this latent code (the decoder).\\r\\n\\r\\nImage: [Michael Massi](https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://science.sciencemag.org/content/313/5786/504',\n",
       "    'source_title': 'Reducing the Dimensionality of Data with Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/L1aoXingyu/pytorch-beginner/blob/9c86be785c7c318a09cf29112dd1f1a58613239b/08-AutoEncoder/simple_autoencoder.py#L38',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['librispeech'],\n",
       "  'datasets_used_full': ['LibriSpeech'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-6-dof-grasping-interaction-via-deep',\n",
       "  'arxiv_id': '1708.07303',\n",
       "  'title': 'Learning 6-DOF Grasping Interaction via Deep Geometry-aware 3D Representations',\n",
       "  'abstract': 'This paper focuses on the problem of learning 6-DOF grasping with a parallel\\njaw gripper in simulation. We propose the notion of a geometry-aware\\nrepresentation in grasping based on the assumption that knowledge of 3D\\ngeometry is at the heart of interaction. Our key idea is constraining and\\nregularizing grasping interaction learning through 3D geometry prediction.\\nSpecifically, we formulate the learning of deep geometry-aware grasping model\\nin two steps: First, we learn to build mental geometry-aware representation by\\nreconstructing the scene (i.e., 3D occupancy grid) from RGBD input via\\ngenerative 3D shape modeling. Second, we learn to predict grasping outcome with\\nits internal geometry-aware representation. The learned outcome prediction\\nmodel is used to sequentially propose grasping solutions via\\nanalysis-by-synthesis optimization. Our contributions are fourfold: (1) To best\\nof our knowledge, we are presenting for the first time a method to learn a\\n6-DOF grasping net from RGBD input; (2) We build a grasping dataset from\\ndemonstrations in virtual reality with rich sensory and interaction\\nannotations. This dataset includes 101 everyday objects spread across 7\\ncategories, additionally, we propose a data augmentation strategy for effective\\nlearning; (3) We demonstrate that the learned geometry-aware representation\\nleads to about 10 percent relative performance improvement over the baseline\\nCNN on grasping objects from our dataset. (4) We further demonstrate that the\\nmodel generalizes to novel viewpoints and object instances.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1708.07303v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1708.07303v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xinchen Yan',\n",
       "   'Jasmine Hsu',\n",
       "   'Mohi Khansari',\n",
       "   'Yunfei Bai',\n",
       "   'Arkanath Pathak',\n",
       "   'Abhinav Gupta',\n",
       "   'James Davidson',\n",
       "   'Honglak Lee'],\n",
       "  'tasks': ['3D Geometry Prediction',\n",
       "   '3D Shape Modeling',\n",
       "   'Data Augmentation'],\n",
       "  'date': '2017-08-24',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/satr-dl-improving-surgical-skill-assessment',\n",
       "  'arxiv_id': '1806.05798',\n",
       "  'title': 'SATR-DL: Improving Surgical Skill Assessment and Task Recognition in Robot-assisted Surgery with Deep Neural Networks',\n",
       "  'abstract': 'Purpose: This paper focuses on an automated analysis of surgical motion\\nprofiles for objective skill assessment and task recognition in robot-assisted\\nsurgery. Existing techniques heavily rely on conventional statistic measures or\\nshallow modelings based on hand-engineered features and gesture segmentation.\\nSuch developments require significant expert knowledge, are prone to errors,\\nand are less efficient in online adaptive training systems. Methods: In this\\nwork, we present an efficient analytic framework with a parallel deep learning\\narchitecture, SATR-DL, to assess trainee expertise and recognize surgical\\ntraining activity. Through an end-to-end learning technique, abstract\\ninformation of spatial representations and temporal dynamics is jointly\\nobtained directly from raw motion sequences. Results: By leveraging a shared\\nhigh-level representation learning, the resulting model is successful in the\\nrecognition of trainee skills and surgical tasks, suturing, needle-passing, and\\nknot-tying. Meanwhile, we explore the use of ensemble in classification at the\\ntrial level, where the SATR-DL outperforms state-of-the-art performance by\\nachieving accuracies of 0.960 and 1.000 in skill assessment and task\\nrecognition, respectively. Conclusion: This study highlights the potential of\\nSATR-DL to provide improvements for an efficient data-driven assessment in\\nintelligent robotic surgery.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05798v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05798v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ziheng Wang', 'Ann Majewicz Fey'],\n",
       "  'tasks': ['Representation Learning'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['jigsaws'],\n",
       "  'datasets_used_full': ['JIGSAWS'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/supervised-speech-separation-based-on-deep',\n",
       "  'arxiv_id': '1708.07524',\n",
       "  'title': 'Supervised Speech Separation Based on Deep Learning: An Overview',\n",
       "  'abstract': 'Speech separation is the task of separating target speech from background\\ninterference. Traditionally, speech separation is studied as a signal\\nprocessing problem. A more recent approach formulates speech separation as a\\nsupervised learning problem, where the discriminative patterns of speech,\\nspeakers, and background noise are learned from training data. Over the past\\ndecade, many supervised separation algorithms have been put forward. In\\nparticular, the recent introduction of deep learning to supervised speech\\nseparation has dramatically accelerated progress and boosted separation\\nperformance. This article provides a comprehensive overview of the research on\\ndeep learning based supervised speech separation in the last several years. We\\nfirst introduce the background of speech separation and the formulation of\\nsupervised separation. Then we discuss three main components of supervised\\nseparation: learning machines, training targets, and acoustic features. Much of\\nthe overview is on separation algorithms where we review monaural methods,\\nincluding speech enhancement (speech-nonspeech separation), speaker separation\\n(multi-talker separation), and speech dereverberation, as well as\\nmulti-microphone techniques. The important issue of generalization, unique to\\nsupervised learning, is discussed. This overview provides a historical\\nperspective on how advances are made. In addition, we discuss a number of\\nconceptual issues, including what constitutes the target source.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1708.07524v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1708.07524v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['DeLiang Wang', 'Jitong Chen'],\n",
       "  'tasks': ['Speaker Separation',\n",
       "   'Speech Dereverberation',\n",
       "   'Speech Enhancement',\n",
       "   'Speech Separation'],\n",
       "  'date': '2017-08-24',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-learning-with-convolutional-neural-2',\n",
       "  'arxiv_id': '1806.05796',\n",
       "  'title': 'Deep Learning with Convolutional Neural Network for Objective Skill Evaluation in Robot-assisted Surgery',\n",
       "  'abstract': 'With the advent of robot-assisted surgery, the role of data-driven approaches\\nto integrate statistics and machine learning is growing rapidly with prominent\\ninterests in objective surgical skill assessment. However, most existing work\\nrequires translating robot motion kinematics into intermediate features or\\ngesture segments that are expensive to extract, lack efficiency, and require\\nsignificant domain-specific knowledge. We propose an analytical deep learning\\nframework for skill assessment in surgical training. A deep convolutional\\nneural network is implemented to map multivariate time series data of the\\nmotion kinematics to individual skill levels. We perform experiments on the\\npublic minimally invasive surgical robotic dataset, JHU-ISI Gesture and Skill\\nAssessment Working Set (JIGSAWS). Our proposed learning model achieved a\\ncompetitive accuracy of 92.5%, 95.4%, and 91.3%, in the standard training\\ntasks: Suturing, Needle-passing, and Knot-tying, respectively. Without the need\\nof engineered features or carefully-tuned gesture segmentation, our model can\\nsuccessfully decode skill information from raw motion profiles via end-to-end\\nlearning. Meanwhile, the proposed model is able to reliably interpret skills\\nwithin 1-3 second window, without needing an observation of entire training\\ntrial. This study highlights the potentials of deep architectures for an\\nproficient online skill assessment in modern surgical training.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05796v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05796v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ziheng Wang', 'Ann Majewicz Fey'],\n",
       "  'tasks': ['Time Series'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['jigsaws'],\n",
       "  'datasets_used_full': ['JIGSAWS'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/rapidnn-in-memory-deep-neural-network',\n",
       "  'arxiv_id': '1806.05794',\n",
       "  'title': 'RAPIDNN: In-Memory Deep Neural Network Acceleration Framework',\n",
       "  'abstract': 'Deep neural networks (DNN) have demonstrated effectiveness for various\\napplications such as image processing, video segmentation, and speech\\nrecognition. Running state-of-the-art DNNs on current systems mostly relies on\\neither generalpurpose processors, ASIC designs, or FPGA accelerators, all of\\nwhich suffer from data movements due to the limited onchip memory and data\\ntransfer bandwidth. In this work, we propose a novel framework, called RAPIDNN,\\nwhich processes all DNN operations within the memory to minimize the cost of\\ndata movement. To enable in-memory processing, RAPIDNN reinterprets a DNN model\\nand maps it into a specialized accelerator, which is designed using\\nnon-volatile memory blocks that model four fundamental DNN operations, i.e.,\\nmultiplication, addition, activation functions, and pooling. The framework\\nextracts representative operands of a DNN model, e.g., weights and input\\nvalues, using clustering methods to optimize the model for in-memory\\nprocessing. Then, it maps the extracted operands and their precomputed results\\ninto the accelerator memory blocks. At runtime, the accelerator identifies\\ncomputation results based on efficient in-memory search capability which also\\nprovides tunability of approximation to further improve computation efficiency.\\nOur evaluation shows that RAPIDNN achieves 68.4x, 49.5x energy efficiency\\nimprovement and 48.1x, 10.9x speedup as compared to ISAAC and PipeLayer, the\\nstate-of-the-art DNN accelerators, while ensuring less than 0.3% of quality\\nloss.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05794v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05794v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mohsen Imani',\n",
       "   'Mohammad Samragh',\n",
       "   'Yeseong Kim',\n",
       "   'Saransh Gupta',\n",
       "   'Farinaz Koushanfar',\n",
       "   'Tajana Rosing'],\n",
       "  'tasks': ['Speech Recognition',\n",
       "   'Video Segmentation',\n",
       "   'Video Semantic Segmentation'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/recurrent-multiresolution-convolutional',\n",
       "  'arxiv_id': '1806.05793',\n",
       "  'title': 'Recurrent Multiresolution Convolutional Networks for VHR Image Classification',\n",
       "  'abstract': 'Classification of very high resolution (VHR) satellite images has three major\\nchallenges: 1) inherent low intra-class and high inter-class spectral\\nsimilarities, 2) mismatching resolution of available bands, and 3) the need to\\nregularize noisy classification maps. Conventional methods have addressed these\\nchallenges by adopting separate stages of image fusion, feature extraction, and\\npost-classification map regularization. These processing stages, however, are\\nnot jointly optimizing the classification task at hand. In this study, we\\npropose a single-stage framework embedding the processing stages in a recurrent\\nmultiresolution convolutional network trained in an end-to-end manner. The\\nfeedforward version of the network, called FuseNet, aims to match the\\nresolution of the panchromatic and multispectral bands in a VHR image using\\nconvolutional layers with corresponding downsampling and upsampling operations.\\nContextual label information is incorporated into FuseNet by means of a\\nrecurrent version called ReuseNet. We compared FuseNet and ReuseNet against the\\nuse of separate processing steps for both image fusion, e.g. pansharpening and\\nresampling through interpolation, and map regularization such as conditional\\nrandom fields. We carried out our experiments on a land cover classification\\ntask using a Worldview-03 image of Quezon City, Philippines and the ISPRS 2D\\nsemantic labeling benchmark dataset of Vaihingen, Germany. FuseNet and ReuseNet\\nsurpass the baseline approaches in both quantitative and qualitative results.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05793v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05793v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['John Ray Bergado', 'Claudio Persello', 'Alfred Stein'],\n",
       "  'tasks': ['Classification',\n",
       "   'General Classification',\n",
       "   'Image Classification'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/mobilefacenets-efficient-cnns-for-accurate',\n",
       "  'arxiv_id': '1804.07573',\n",
       "  'title': 'MobileFaceNets: Efficient CNNs for Accurate Real-Time Face Verification on Mobile Devices',\n",
       "  'abstract': 'Face Analysis Project on MXNet',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.07573v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.07573v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sheng Chen', 'Yang Liu', 'Xiang Gao', 'Zhen Han'],\n",
       "  'tasks': ['Face Verification'],\n",
       "  'date': '2018-04-20',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet',\n",
       "   'lfw',\n",
       "   'casia-webface',\n",
       "   'ms-celeb-1m',\n",
       "   'megaface'],\n",
       "  'datasets_used_full': ['ImageNet',\n",
       "   'LFW',\n",
       "   'CASIA-WebFace',\n",
       "   'MS-Celeb-1M',\n",
       "   'MegaFace'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/monaural-source-enhancement-maximizing-source',\n",
       "  'arxiv_id': '1806.05791',\n",
       "  'title': 'Monaural source enhancement maximizing source-to-distortion ratio via automatic differentiation',\n",
       "  'abstract': 'Recently, deep neural network (DNN) has made a breakthrough in monaural\\nsource enhancement. Through a training step by using a large amount of data,\\nDNN estimates a mapping between mixed signals and clean signals. At this time,\\nwe use an objective function that numerically expresses the quality of a\\nmapping by DNN. In the conventional methods, L1 norm, L2 norm, and\\nItakura-Saito divergence are often used as objective functions. Recently, an\\nobjective function based on short-time objective intelligibility (STOI) has\\nalso been proposed. However, these functions only indicate similarity between\\nthe clean signal and the estimated signal by DNN. In other words, they do not\\nshow the quality of noise reduction or source enhancement. Motivated by the\\nfact, this paper adopts signal-to-distortion ratio (SDR) as the objective\\nfunction. Since SDR virtually shows signal-to-noise ratio (SNR), maximizing SDR\\nsolves the above problem. The experimental results revealed that the proposed\\nmethod achieved better performance than the conventional methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05791v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05791v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hiroaki Nakajima',\n",
       "   'Yu Takahashi',\n",
       "   'Kazunobu Kondo',\n",
       "   'Yuji Hisaminato'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/image-classification-and-retrieval-with',\n",
       "  'arxiv_id': '1806.05789',\n",
       "  'title': 'Image classification and retrieval with random depthwise signed convolutional neural networks',\n",
       "  'abstract': \"We propose a random convolutional neural network to generate a feature space\\nin which we study image classification and retrieval performance. Put briefly\\nwe apply random convolutional blocks followed by global average pooling to\\ngenerate a new feature, and we repeat this k times to produce a k-dimensional\\nfeature space. This can be interpreted as partitioning the space of image\\npatches with random hyperplanes which we formalize as a random depthwise\\nconvolutional neural network. In the network's final layer we perform image\\nclassification and retrieval with the linear support vector machine and\\nk-nearest neighbor classifiers and study other empirical properties. We show\\nthat the ratio of image pixel distribution similarity across classes to within\\nclasses is higher in our network's final layer compared to the input space.\\nWhen we apply the linear support vector machine for image classification we see\\nthat the accuracy is higher than if we were to train just the final layer of\\nVGG16, ResNet18, and DenseNet40 with random weights. In the same setting we\\ncompare it to an unsupervised feature learning method and find our accuracy to\\nbe comparable on CIFAR10 but higher on CIFAR100 and STL10. We see that the\\naccuracy is not far behind that of trained networks, particularly in the top-k\\nsetting. For example the top-2 accuracy of our network is near 90% on both\\nCIFAR10 and a 10-class mini ImageNet, and 85% on STL10. We find that k-nearest\\nneighbor gives a comparable precision on the Corel Princeton Image Similarity\\nBenchmark than if we were to use the final layer of trained networks. As with\\nother networks we find that our network fails to a black box attack even though\\nwe lack a gradient and use the sign activation. We highlight sensitivity of our\\nnetwork to background as a potential pitfall and an advantage. Overall our work\\npushes the boundary of what can be achieved with random weights.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05789v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05789v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yunzhe Xue', 'Usman Roshan'],\n",
       "  'tasks': ['General Classification', 'Image Classification'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [{'name': 'Global Average Pooling',\n",
       "    'full_name': 'Global Average Pooling',\n",
       "    'description': '**Global Average Pooling** is a pooling operation designed to replace fully connected layers in classical CNNs. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the [softmax](https://paperswithcode.com/method/softmax) layer. \\r\\n\\r\\nOne advantage of global [average pooling](https://paperswithcode.com/method/average-pooling) over the fully connected layers is that it is more native to the [convolution](https://paperswithcode.com/method/convolution) structure by enforcing correspondences between feature maps and categories. Thus the feature maps can be easily interpreted as categories confidence maps. Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. Furthermore, global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/baa592b215804927e28638f6a7f3318cbc411d49/torchvision/models/resnet.py#L157',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Average Pooling',\n",
       "    'full_name': 'Average Pooling',\n",
       "    'description': '**Average Pooling** is a pooling operation that calculates the average value for patches of a feature map, and uses it to create a downsampled (pooled) feature map. It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs. It extracts features more smoothly than [Max Pooling](https://paperswithcode.com/method/max-pooling), whereas max pooling extracts more pronounced features like edges.\\r\\n\\r\\nImage Source: [here](https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['cifar-10', 'mnist', 'cifar-100', 'stl-10'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'MNIST', 'CIFAR-100', 'STL-10'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-survey-of-automatic-facial-micro-expression',\n",
       "  'arxiv_id': '1806.05781',\n",
       "  'title': 'A Survey of Automatic Facial Micro-expression Analysis: Databases, Methods and Challenges',\n",
       "  'abstract': 'Over the last few years, automatic facial micro-expression analysis has\\ngarnered increasing attention from experts across different disciplines because\\nof its potential applications in various fields such as clinical diagnosis,\\nforensic investigation and security systems. Advances in computer algorithms\\nand video acquisition technology have rendered machine analysis of facial\\nmicro-expressions possible today, in contrast to decades ago when it was\\nprimarily the domain of psychiatrists where analysis was largely manual.\\nIndeed, although the study of facial micro-expressions is a well-established\\nfield in psychology, it is still relatively new from the computational\\nperspective with many interesting problems. In this survey, we present a\\ncomprehensive review of state-of-the-art databases and methods for\\nmicro-expressions spotting and recognition. Individual stages involved in the\\nautomation of these tasks are also described and reviewed at length. In\\naddition, we also deliberate on the challenges and future directions in this\\ngrowing field of automatic facial micro-expression analysis.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05781v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05781v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yee-Hui Oh',\n",
       "   'John See',\n",
       "   'Anh Cat Le Ngo',\n",
       "   'Raphael Chung-Wei Phan',\n",
       "   'Vishnu Monn Baskaran'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/surprising-negative-results-for-generative',\n",
       "  'arxiv_id': '1806.05780',\n",
       "  'title': 'Surprising Negative Results for Generative Adversarial Tree Search',\n",
       "  'abstract': 'While many recent advances in deep reinforcement learning (RL) rely on model-free methods, model-based approaches remain an alluring prospect for their potential to exploit unsupervised data to learn environment model. In this work, we provide an extensive study on the design of deep generative models for RL environments and propose a sample efficient and robust method to learn the model of Atari environments. We deploy this model and propose generative adversarial tree search (GATS) a deep RL algorithm that learns the environment model and implements Monte Carlo tree search (MCTS) on the learned model for planning. While MCTS on the learned model is computationally expensive, similar to AlphaGo, GATS follows depth limited MCTS. GATS employs deep Q network (DQN) and learns a Q-function to assign values to the leaves of the tree in MCTS. We theoretical analyze GATS vis-a-vis the bias-variance trade-off and show GATS is able to mitigate the worst-case error in the Q-estimate. While we were expecting GATS to enjoy a better sample complexity and faster converges to better policies, surprisingly, GATS fails to outperform DQN. We provide a study on which we show why depth limited MCTS fails to perform desirably.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05780v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05780v4.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Kamyar Azizzadenesheli',\n",
       "   'Brandon Yang',\n",
       "   'Weitang Liu',\n",
       "   'Zachary C. Lipton',\n",
       "   'Animashree Anandkumar'],\n",
       "  'tasks': ['Atari Games'],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Dense Connections',\n",
       "    'full_name': 'Dense Connections',\n",
       "    'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\\\_{\\\\text{inputs}}*n\\\\_{\\\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\\r\\n\\r\\n$$h\\\\_{l} = g\\\\left(\\\\textbf{W}^{T}h\\\\_{l-1}\\\\right)$$\\r\\n\\r\\nwhere $g$ is an activation function.\\r\\n\\r\\nImage Source: Deep Learning by Goodfellow, Bengio and Courville',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Q-Learning',\n",
       "    'full_name': 'Q-Learning',\n",
       "    'description': '**Q-Learning** is an off-policy temporal difference control algorithm:\\r\\n\\r\\n$$Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) \\\\leftarrow Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) + \\\\alpha\\\\left[R_{t+1} + \\\\gamma\\\\max\\\\_{a}Q\\\\left(S\\\\_{t+1}, a\\\\right) - Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right)\\\\right] $$\\r\\n\\r\\nThe learned action-value function $Q$ directly approximates $q\\\\_{*}$, the optimal action-value function, independent of the policy being followed.\\r\\n\\r\\nSource: Sutton and Barto, Reinforcement Learning, 2nd Edition',\n",
       "    'introduced_year': 1984,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Off-Policy TD Control',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Reinforcement Learning'}},\n",
       "   {'name': 'DQN',\n",
       "    'full_name': 'Deep Q-Network',\n",
       "    'description': 'A **DQN**, or Deep Q-Network, approximates a state-value function in a [Q-Learning](https://paperswithcode.com/method/q-learning) framework with a neural network. In the Atari Games case, they take in several frames of the game as an input and output state values for each action as an output. \\r\\n\\r\\nIt is usually used in conjunction with [Experience Replay](https://paperswithcode.com/method/experience-replay), for storing the episode steps in memory for off-policy learning, where samples are drawn from the replay memory at random. Additionally, the Q-Network is usually optimized towards a frozen target network that is periodically updated with the latest weights every $k$ steps (where $k$ is a hyperparameter). The latter makes training more stable by preventing short-term oscillations from a moving target. The former tackles autocorrelation that would occur from on-line learning, and having a replay memory makes the problem more like a supervised learning problem.\\r\\n\\r\\nImage Source: [here](https://www.researchgate.net/publication/319643003_Autonomous_Quadrotor_Landing_using_Deep_Reinforcement_Learning)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.5602v1',\n",
       "    'source_title': 'Playing Atari with Deep Reinforcement Learning',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Q-Learning Networks',\n",
       "     'description': '',\n",
       "     'parent': 'Off-Policy TD Control',\n",
       "     'area': 'Reinforcement Learning'}}],\n",
       "  'datasets_used_lower': ['arcade-learning-environment'],\n",
       "  'datasets_used_full': ['Arcade Learning Environment'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-learning-approximation-zero-shot-neural',\n",
       "  'arxiv_id': '1806.05779',\n",
       "  'title': 'Deep Learning Approximation: Zero-Shot Neural Network Speedup',\n",
       "  'abstract': 'Neural networks offer high-accuracy solutions to a range of problems, but are\\ncostly to run in production systems because of computational and memory\\nrequirements during a forward pass. Given a trained network, we propose a\\ntechique called Deep Learning Approximation to build a faster network in a tiny\\nfraction of the time required for training by only manipulating the network\\nstructure and coefficients without requiring re-training or access to the\\ntraining data. Speedup is achieved by by applying a sequential series of\\nindependent optimizations that reduce the floating-point operations (FLOPs)\\nrequired to perform a forward pass. First, lossless optimizations are applied,\\nfollowed by lossy approximations using singular value decomposition (SVD) and\\nlow-rank matrix decomposition. The optimal approximation is chosen by weighing\\nthe relative accuracy loss and FLOP reduction according to a single parameter\\nspecified by the user. On PASCAL VOC 2007 with the YOLO network, we show an\\nend-to-end 2x speedup in a network forward pass with a 5% drop in mAP that can\\nbe re-gained by finetuning.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05779v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05779v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Michele Pratusevich'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/gradient-descent-learns-one-hidden-layer-cnn',\n",
       "  'arxiv_id': '1712.00779',\n",
       "  'title': \"Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of Spurious Local Minima\",\n",
       "  'abstract': 'We consider the problem of learning a one-hidden-layer neural network with\\nnon-overlapping convolutional layer and ReLU activation, i.e., $f(\\\\mathbf{Z},\\n\\\\mathbf{w}, \\\\mathbf{a}) = \\\\sum_j a_j\\\\sigma(\\\\mathbf{w}^T\\\\mathbf{Z}_j)$, in which\\nboth the convolutional weights $\\\\mathbf{w}$ and the output weights $\\\\mathbf{a}$\\nare parameters to be learned. When the labels are the outputs from a teacher\\nnetwork of the same architecture with fixed weights $(\\\\mathbf{w}^*,\\n\\\\mathbf{a}^*)$, we prove that with Gaussian input $\\\\mathbf{Z}$, there is a\\nspurious local minimizer. Surprisingly, in the presence of the spurious local\\nminimizer, gradient descent with weight normalization from randomly initialized\\nweights can still be proven to recover the true parameters with constant\\nprobability, which can be boosted to probability $1$ with multiple restarts. We\\nalso show that with constant probability, the same procedure could also\\nconverge to the spurious local minimum, showing that the local minimum plays a\\nnon-trivial role in the dynamics of gradient descent. Furthermore, a\\nquantitative analysis shows that the gradient descent dynamics has two phases:\\nit starts off slow, but converges much faster after several iterations.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.00779v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.00779v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Simon S. Du',\n",
       "   'Jason D. Lee',\n",
       "   'Yuandong Tian',\n",
       "   'Barnabas Poczos',\n",
       "   'Aarti Singh'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-12-03',\n",
       "  'methods': [{'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Weight Normalization',\n",
       "    'full_name': 'Weight Normalization',\n",
       "    'description': \"**Weight Normalization** is a normalization method for training neural networks. It is inspired by [batch normalization](https://paperswithcode.com/method/batch-normalization), but it is a deterministic method that does not share batch normalization's property of adding noise to the gradients. It reparameterizes each weight vector $\\\\textbf{w}$ in terms of a parameter vector $\\\\textbf{v}$ and a scalar parameter $g$ and to perform stochastic gradient descent with respect to those parameters instead. Weight vectors are expressed in terms of the new parameters using:\\r\\n\\r\\n$$ \\\\textbf{w} = \\\\frac{g}{\\\\Vert\\\\\\\\textbf{v}\\\\Vert}\\\\textbf{v}$$\\r\\n\\r\\nwhere $\\\\textbf{v}$ is a $k$-dimensional vector, $g$ is a scalar, and $\\\\Vert\\\\textbf{v}\\\\Vert$ denotes the Euclidean norm of $\\\\textbf{v}$. This reparameterization has the effect of fixing the Euclidean norm of the weight vector $\\\\textbf{w}$: we now have $\\\\Vert\\\\textbf{w}\\\\Vert = g$, independent of the parameters $\\\\textbf{v}$.\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1602.07868v3',\n",
       "    'source_title': 'Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/1c5c289b6218eb1026dcb5fd9738231401cfccea/torch/nn/utils/weight_norm.py#L8',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-the-power-of-over-parametrization-in-1',\n",
       "  'arxiv_id': '1803.01206',\n",
       "  'title': 'On the Power of Over-parametrization in Neural Networks with Quadratic Activation',\n",
       "  'abstract': 'We provide new theoretical insights on why over-parametrization is effective\\nin learning neural networks. For a $k$ hidden node shallow network with\\nquadratic activation and $n$ training data points, we show as long as $ k \\\\ge\\n\\\\sqrt{2n}$, over-parametrization enables local search algorithms to find a\\n\\\\emph{globally} optimal solution for general smooth and convex loss functions.\\nFurther, despite that the number of parameters may exceed the sample size,\\nusing theory of Rademacher complexity, we show with weight decay, the solution\\nalso generalizes well if the data is sampled from a regular distribution such\\nas Gaussian. To prove when $k\\\\ge \\\\sqrt{2n}$, the loss function has benign\\nlandscape properties, we adopt an idea from smoothed analysis, which may have\\nother applications in studying loss surfaces of neural networks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.01206v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.01206v2.pdf',\n",
       "  'proceeding': 'ICML 2018',\n",
       "  'authors': ['Simon S. Du', 'Jason D. Lee'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-03-03',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/hardware-trojan-attacks-on-neural-networks',\n",
       "  'arxiv_id': '1806.05768',\n",
       "  'title': 'Hardware Trojan Attacks on Neural Networks',\n",
       "  'abstract': 'With the rising popularity of machine learning and the ever increasing demand\\nfor computational power, there is a growing need for hardware optimized\\nimplementations of neural networks and other machine learning models. As the\\ntechnology evolves, it is also plausible that machine learning or artificial\\nintelligence will soon become consumer electronic products and military\\nequipment, in the form of well-trained models. Unfortunately, the modern\\nfabless business model of manufacturing hardware, while economic, leads to\\ndeficiencies in security through the supply chain. In this paper, we illuminate\\nthese security issues by introducing hardware Trojan attacks on neural\\nnetworks, expanding the current taxonomy of neural network security to\\nincorporate attacks of this nature. To aid in this, we develop a novel\\nframework for inserting malicious hardware Trojans in the implementation of a\\nneural network classifier. We evaluate the capabilities of the adversary in\\nthis setting by implementing the attack algorithm on convolutional neural\\nnetworks while controlling a variety of parameters available to the adversary.\\nOur experimental results show that the proposed algorithm could effectively\\nclassify a selected input trigger as a specified class on the MNIST dataset by\\ninjecting hardware Trojans into $0.03\\\\%$, on average, of neurons in the 5th\\nhidden layer of arbitrary 7-layer convolutional neural networks, while\\nundetectable under the test data. Finally, we discuss the potential defenses to\\nprotect neural networks against hardware Trojan attacks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05768v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05768v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Joseph Clements', 'Yingjie Lao'],\n",
       "  'tasks': ['Neural Network Security'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/motion-planning-networks',\n",
       "  'arxiv_id': '1806.05767',\n",
       "  'title': 'Motion Planning Networks',\n",
       "  'abstract': 'Fast and efficient motion planning algorithms are crucial for many\\nstate-of-the-art robotics applications such as self-driving cars. Existing\\nmotion planning methods become ineffective as their computational complexity\\nincreases exponentially with the dimensionality of the motion planning problem.\\nTo address this issue, we present Motion Planning Networks (MPNet), a neural\\nnetwork-based novel planning algorithm. The proposed method encodes the given\\nworkspaces directly from a point cloud measurement and generates the end-to-end\\ncollision-free paths for the given start and goal configurations. We evaluate\\nMPNet on various 2D and 3D environments including the planning of a 7 DOF\\nBaxter robot manipulator. The results show that MPNet is not only consistently\\ncomputationally efficient in all environments but also generalizes to\\ncompletely unseen environments. The results also show that the computation time\\nof MPNet consistently remains less than 1 second in all presented experiments,\\nwhich is significantly lower than existing state-of-the-art motion planning\\nalgorithms.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05767v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05767v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ahmed H. Qureshi',\n",
       "   'Anthony Simeonov',\n",
       "   'Mayur J. Bency',\n",
       "   'Michael C. Yip'],\n",
       "  'tasks': ['Motion Planning', 'Self-Driving Cars', 'Transfer Learning'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/generative-adversarial-networks-and',\n",
       "  'arxiv_id': '1806.05764',\n",
       "  'title': 'Generative Adversarial Networks and Perceptual Losses for Video Super-Resolution',\n",
       "  'abstract': 'Video super-resolution (VSR) has become one of the most critical problems in\\nvideo processing. In the deep learning literature, recent works have shown the\\nbenefits of using adversarial-based and perceptual losses to improve the\\nperformance on various image restoration tasks; however, these have yet to be\\napplied for video super-resolution. In this work, we propose a Generative\\nAdversarial Network(GAN)-based formulation for VSR. We introduce a new\\ngenerator network optimized for the VSR problem, named VSRResNet, along with a\\nnew discriminator architecture to properly guide VSRResNet during the GAN\\ntraining. We further enhance our VSR GAN formulation with two regularizers, a\\ndistance loss in feature-space and pixel-space, to obtain our final\\nVSRResFeatGAN model. We show that pre-training our generator with the\\nMean-Squared-Error loss only quantitatively surpasses the current\\nstate-of-the-art VSR models. Finally, we employ the PercepDist metric (Zhang et\\nal., 2018) to compare state-of-the-art VSR models. We show that this metric\\nmore accurately evaluates the perceptual quality of SR solutions obtained from\\nneural networks, compared with the commonly used PSNR/SSIM metrics. Finally, we\\nshow that our proposed model, the VSRResFeatGAN model, outperforms current\\nstate-of-the-art SR models, both quantitatively and qualitatively.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05764v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05764v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alice Lucas',\n",
       "   'Santiago Lopez Tapia',\n",
       "   'Rafael Molina',\n",
       "   'Aggelos K. Katsaggelos'],\n",
       "  'tasks': ['Image Restoration',\n",
       "   'SSIM',\n",
       "   'Super-Resolution',\n",
       "   'Video Super-Resolution'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/pac-bayes-control-synthesizing-controllers',\n",
       "  'arxiv_id': '1806.04225',\n",
       "  'title': 'PAC-Bayes Control: Learning Policies that Provably Generalize to Novel Environments',\n",
       "  'abstract': 'Our goal is to learn control policies for robots that provably generalize well to novel environments given a dataset of example environments. The key technical idea behind our approach is to leverage tools from generalization theory in machine learning by exploiting a precise analogy (which we present in the form of a reduction) between generalization of control policies to novel environments and generalization of hypotheses in the supervised learning setting. In particular, we utilize the Probably Approximately Correct (PAC)-Bayes framework, which allows us to obtain upper bounds that hold with high probability on the expected cost of (stochastic) control policies across novel environments. We propose policy learning algorithms that explicitly seek to minimize this upper bound. The corresponding optimization problem can be solved using convex optimization (Relative Entropy Programming in particular) in the setting where we are optimizing over a finite policy space. In the more general setting of continuously parameterized policies (e.g., neural network policies), we minimize this upper bound using stochastic gradient descent. We present simulated results of our approach applied to learning (1) reactive obstacle avoidance policies and (2) neural network-based grasping policies. We also present hardware results for the Parrot Swing drone navigating through different obstacle environments. Our examples demonstrate the potential of our approach to provide strong generalization guarantees for robotic systems with continuous state and action spaces, complicated (e.g., nonlinear) dynamics, rich sensory inputs (e.g., depth images), and neural network-based policies.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04225v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04225v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Anirudha Majumdar', 'Alec Farid', 'Anoopkumar Sonar'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet', 'shapenet'],\n",
       "  'datasets_used_full': ['ImageNet', 'ShapeNet'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/insights-on-representational-similarity-in',\n",
       "  'arxiv_id': '1806.05759',\n",
       "  'title': 'Insights on representational similarity in neural networks with canonical correlation',\n",
       "  'abstract': 'Comparing different neural network representations and determining how\\nrepresentations evolve over time remain challenging open questions in our\\nunderstanding of the function of neural networks. Comparing representations in\\nneural networks is fundamentally difficult as the structure of representations\\nvaries greatly, even across groups of networks trained on identical tasks, and\\nover the course of training. Here, we develop projection weighted CCA\\n(Canonical Correlation Analysis) as a tool for understanding neural networks,\\nbuilding off of SVCCA, a recently proposed method (Raghu et al., 2017). We\\nfirst improve the core method, showing how to differentiate between signal and\\nnoise, and then apply this technique to compare across a group of CNNs,\\ndemonstrating that networks which generalize converge to more similar\\nrepresentations than networks which memorize, that wider networks converge to\\nmore similar solutions than narrow networks, and that trained networks with\\nidentical topology but different learning rates converge to distinct clusters\\nwith diverse representations. We also investigate the representational dynamics\\nof RNNs, across both training and sequential timesteps, finding that RNNs\\nconverge in a bottom-up pattern over the course of training and that the hidden\\nstate is highly variable over the course of a sequence, even when accounting\\nfor linear transforms. Together, these results provide new insights into the\\nfunction of CNNs and RNNs, and demonstrate the utility of using CCA to\\nunderstand representations.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05759v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05759v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Ari S. Morcos', 'Maithra Raghu', 'Samy Bengio'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/enhanced-local-binary-patterns-for-automatic',\n",
       "  'arxiv_id': '1702.03349',\n",
       "  'title': 'Enhanced Local Binary Patterns for Automatic Face Recognition',\n",
       "  'abstract': 'This paper presents a novel automatic face recognition approach based on\\nlocal binary patterns. This descriptor considers a local neighbourhood of a\\npixel to compute the feature vector values. This method is not very robust to\\nhandle image noise, variances and different illumination conditions. We address\\nthese issues by proposing a novel descriptor which considers more pixels and\\ndifferent neighbourhoods to compute the feature vector values. The proposed\\nmethod is evaluated on two benchmark corpora, namely UFI and FERET face\\ndatasets. We experimentally show that our approach outperforms state-of-the-art\\nmethods and is efficient particularly in the real conditions where the above\\nmentioned issues are obvious. We further show that the proposed method handles\\nwell one training sample issue and is also robust to the image resolution.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1702.03349v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1702.03349v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Pavel Král', 'Ladislav Lenc', 'Antonín Vrba'],\n",
       "  'tasks': ['Face Recognition'],\n",
       "  'date': '2017-02-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/sliced-wasserstein-autoencoder-an',\n",
       "  'arxiv_id': '1804.01947',\n",
       "  'title': 'Sliced-Wasserstein Autoencoder: An Embarrassingly Simple Generative Model',\n",
       "  'abstract': 'In this paper we study generative modeling via autoencoders while using the\\nelegant geometric properties of the optimal transport (OT) problem and the\\nWasserstein distances. We introduce Sliced-Wasserstein Autoencoders (SWAE),\\nwhich are generative models that enable one to shape the distribution of the\\nlatent space into any samplable probability distribution without the need for\\ntraining an adversarial network or defining a closed-form for the distribution.\\nIn short, we regularize the autoencoder loss with the sliced-Wasserstein\\ndistance between the distribution of the encoded training samples and a\\npredefined samplable distribution. We show that the proposed formulation has an\\nefficient numerical solution that provides similar capabilities to Wasserstein\\nAutoencoders (WAE) and Variational Autoencoders (VAE), while benefiting from an\\nembarrassingly simple implementation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.01947v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.01947v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Soheil Kolouri',\n",
       "   'Phillip E. Pope',\n",
       "   'Charles E. Martin',\n",
       "   'Gustavo K. Rohde'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-04-05',\n",
       "  'methods': [{'name': 'AutoEncoder',\n",
       "    'full_name': 'AutoEncoder',\n",
       "    'description': 'An **Autoencoder** is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then performs a reconstruction of the input with this latent code (the decoder).\\r\\n\\r\\nImage: [Michael Massi](https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://science.sciencemag.org/content/313/5786/504',\n",
       "    'source_title': 'Reducing the Dimensionality of Data with Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/L1aoXingyu/pytorch-beginner/blob/9c86be785c7c318a09cf29112dd1f1a58613239b/08-AutoEncoder/simple_autoencoder.py#L38',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['celeba'],\n",
       "  'datasets_used_full': ['CelebA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/detecting-speech-act-types-in-developer',\n",
       "  'arxiv_id': '1806.05130',\n",
       "  'title': 'Detecting Speech Act Types in Developer Question/Answer Conversations During Bug Repair',\n",
       "  'abstract': 'This paper targets the problem of speech act detection in conversations about\\nbug repair. We conduct a \"Wizard of Oz\" experiment with 30 professional\\nprogrammers, in which the programmers fix bugs for two hours, and use a\\nsimulated virtual assistant for help. Then, we use an open coding manual\\nannotation procedure to identify the speech act types in the conversations.\\nFinally, we train and evaluate a supervised learning algorithm to automatically\\ndetect the speech act types in the conversations. In 30 two-hour conversations,\\nwe made 2459 annotations and uncovered 26 speech act types. Our automated\\ndetection achieved 69% precision and 50% recall. The key application of this\\nwork is to advance the state of the art for virtual assistants in software\\nengineering. Virtual assistant technology is growing rapidly, though\\napplications in software engineering are behind those in other areas, largely\\ndue to a lack of relevant data and experiments. This paper targets this problem\\nin the area of developer Q/A conversations about bug repair.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05130v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05130v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Andrew Wood',\n",
       "   'Paige Rodeghero',\n",
       "   'Ameer Armaly',\n",
       "   'Collin McMillan'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/probabilistic-tools-for-the-analysis-of',\n",
       "  'arxiv_id': '1801.06733',\n",
       "  'title': 'Probabilistic Tools for the Analysis of Randomized Optimization Heuristics',\n",
       "  'abstract': 'This chapter collects several probabilistic tools that proved to be useful in the analysis of randomized search heuristics. This includes classic material like Markov, Chebyshev and Chernoff inequalities, but also lesser known topics like stochastic domination and coupling or Chernoff bounds for geometrically distributed random variables and for negatively correlated random variables. Most of the results presented here have appeared previously, some, however, only in recent conference publications. While the focus is on collecting tools for the analysis of randomized search heuristics, many of these may be useful as well in the analysis of classic randomized algorithms or discrete random structures.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1801.06733v6',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1801.06733v6.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Benjamin Doerr'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-01-20',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/using-clinical-narratives-and-structured-data',\n",
       "  'arxiv_id': '1806.04818',\n",
       "  'title': 'Using Clinical Narratives and Structured Data to Identify Distant Recurrences in Breast Cancer',\n",
       "  'abstract': 'Accurately identifying distant recurrences in breast cancer from the\\nElectronic Health Records (EHR) is important for both clinical care and\\nsecondary analysis. Although multiple applications have been developed for\\ncomputational phenotyping in breast cancer, distant recurrence identification\\nstill relies heavily on manual chart review. In this study, we aim to develop a\\nmodel that identifies distant recurrences in breast cancer using clinical\\nnarratives and structured data from EHR. We apply MetaMap to extract features\\nfrom clinical narratives and also retrieve structured clinical data from EHR.\\nUsing these features, we train a support vector machine model to identify\\ndistant recurrences in breast cancer patients. We train the model using 1,396\\ndouble-annotated subjects and validate the model using 599 double-annotated\\nsubjects. In addition, we validate the model on a set of 4,904 single-annotated\\nsubjects as a generalization test. We obtained a high area under curve (AUC)\\nscore of 0.92 (SD=0.01) in the cross-validation using the training dataset,\\nthen obtained AUC scores of 0.95 and 0.93 in the held-out test and\\ngeneralization test using 599 and 4,904 samples respectively. Our model can\\naccurately and efficiently identify distant recurrences in breast cancer by\\ncombining features extracted from unstructured clinical narratives and\\nstructured clinical data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04818v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04818v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Zexian Zeng',\n",
       "   'Ankita Roy',\n",
       "   'Xiaoyu Li',\n",
       "   'Sasa Espino',\n",
       "   'Susan Clare',\n",
       "   'Seema Khan',\n",
       "   'Yuan Luo'],\n",
       "  'tasks': ['Computational Phenotyping'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/natural-language-processing-for-ehr-based',\n",
       "  'arxiv_id': '1806.04820',\n",
       "  'title': 'Natural Language Processing for EHR-Based Computational Phenotyping',\n",
       "  'abstract': 'This article reviews recent advances in applying natural language processing\\n(NLP) to Electronic Health Records (EHRs) for computational phenotyping.\\nNLP-based computational phenotyping has numerous applications including\\ndiagnosis categorization, novel phenotype discovery, clinical trial screening,\\npharmacogenomics, drug-drug interaction (DDI) and adverse drug event (ADE)\\ndetection, as well as genome-wide and phenome-wide association studies.\\nSignificant progress has been made in algorithm development and resource\\nconstruction for computational phenotyping. Among the surveyed methods,\\nwell-designed keyword search and rule-based systems often achieve good\\nperformance. However, the construction of keyword and rule lists requires\\nsignificant manual effort, which is difficult to scale. Supervised machine\\nlearning models have been favored because they are capable of acquiring both\\nclassification patterns and structures from data. Recently, deep learning and\\nunsupervised learning have received growing attention, with the former favored\\nfor its performance and the latter for its ability to find novel phenotypes.\\nIntegrating heterogeneous data sources have become increasingly important and\\nhave shown promise in improving model performance. Often better performance is\\nachieved by combining multiple modalities of information. Despite these many\\nadvances, challenges and opportunities remain for NLP-based computational\\nphenotyping, including better model interpretability and generalizability, and\\nproper characterization of feature relations in clinical narratives',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04820v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04820v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Zexian Zeng',\n",
       "   'Yu Deng',\n",
       "   'Xiaoyu Li',\n",
       "   'Tristan Naumann',\n",
       "   'Yuan Luo'],\n",
       "  'tasks': ['Computational Phenotyping'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'Interpretability',\n",
       "    'full_name': 'Interpretability',\n",
       "    'description': 'Please enter a description about the method here',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1310.1533v2',\n",
       "    'source_title': 'CAM: Causal additive models, high-dimensional order search and penalized regression',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Image Models',\n",
       "     'description': '**Image Models** are methods that build representations of images for downstream tasks such as classification and object detection. The most popular subcategory are convolutional neural networks. Below you can find a continuously updated list of image models.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/to-understand-deep-learning-we-need-to',\n",
       "  'arxiv_id': '1802.01396',\n",
       "  'title': 'To understand deep learning we need to understand kernel learning',\n",
       "  'abstract': 'Generalization performance of classifiers in deep learning has recently\\nbecome a subject of intense study. Deep models, typically over-parametrized,\\ntend to fit the training data exactly. Despite this \"overfitting\", they perform\\nwell on test data, a phenomenon not yet fully understood.\\n  The first point of our paper is that strong performance of overfitted\\nclassifiers is not a unique feature of deep learning. Using six real-world and\\ntwo synthetic datasets, we establish experimentally that kernel machines\\ntrained to have zero classification or near zero regression error perform very\\nwell on test data, even when the labels are corrupted with a high level of\\nnoise. We proceed to give a lower bound on the norm of zero loss solutions for\\nsmooth kernels, showing that they increase nearly exponentially with data size.\\nWe point out that this is difficult to reconcile with the existing\\ngeneralization bounds. Moreover, none of the bounds produce non-trivial results\\nfor interpolating solutions.\\n  Second, we show experimentally that (non-smooth) Laplacian kernels easily fit\\nrandom labels, a finding that parallels results for ReLU neural networks. In\\ncontrast, fitting noisy data requires many more epochs for smooth Gaussian\\nkernels. Similar performance of overfitted Laplacian and Gaussian classifiers\\non test, suggests that generalization is tied to the properties of the kernel\\nfunction rather than the optimization process.\\n  Certain key phenomena of deep learning are manifested similarly in kernel\\nmethods in the modern \"overfitted\" regime. The combination of the experimental\\nand theoretical results presented in this paper indicates a need for new\\ntheoretical ideas for understanding properties of classical kernel methods. We\\nargue that progress on understanding deep learning will be difficult until more\\ntractable \"shallow\" kernel methods are better understood.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.01396v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.01396v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Mikhail Belkin', 'Siyuan Ma', 'Soumik Mandal'],\n",
       "  'tasks': ['Generalization Bounds'],\n",
       "  'date': '2018-02-05',\n",
       "  'methods': [{'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['mnist', 'svhn'],\n",
       "  'datasets_used_full': ['MNIST', 'SVHN'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/age-and-gender-classification-from-ear-images',\n",
       "  'arxiv_id': '1806.05742',\n",
       "  'title': 'Age and Gender Classification From Ear Images',\n",
       "  'abstract': 'In this paper, we present a detailed analysis on extracting soft biometric\\ntraits, age and gender, from ear images. Although there have been a few\\nprevious work on gender classification using ear images, to the best of our\\nknowledge, this study is the first work on age classification from ear images.\\nIn the study, we have utilized both geometric features and appearance-based\\nfeatures for ear representation. The utilized geometric features are based on\\neight anthropometric landmarks and consist of 14 distance measurements and two\\narea calculations. The appearance-based methods employ deep convolutional\\nneural networks for representation and classification. The well-known\\nconvolutional neural network models, namely, AlexNet, VGG-16, GoogLeNet, and\\nSqueezeNet have been adopted for the study. They have been fine-tuned on a\\nlarge-scale ear dataset that has been built from the profile and\\nclose-to-profile face images in the Multi-PIE face dataset. This way, we have\\nperformed a domain adaptation. The updated models have been fine-tuned once\\nmore time on the small-scale target ear dataset, which contains only around 270\\near images for training. According to the experimental results,\\nappearance-based methods have been found to be superior to the methods based on\\ngeometric features. We have achieved 94\\\\% accuracy for gender classification,\\nwhereas 52\\\\% accuracy has been obtained for age classification. These results\\nindicate that ear images provide useful cues for age and gender classification,\\nhowever, further work is required for age estimation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05742v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05742v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Dogucan Yaman',\n",
       "   'Fevziye Irem Eyiokur',\n",
       "   'Nurdan Sezgin',\n",
       "   'Hazim Kemal Ekenel'],\n",
       "  'tasks': ['Age And Gender Classification',\n",
       "   'Age Estimation',\n",
       "   'Classification',\n",
       "   'Domain Adaptation',\n",
       "   'General Classification'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'VGG-16',\n",
       "    'full_name': 'VGG-16',\n",
       "    'description': '',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1409.1556v6',\n",
       "    'source_title': 'Very Deep Convolutional Networks for Large-Scale Image Recognition',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': '1x1 Convolution',\n",
       "    'full_name': '1x1 Convolution',\n",
       "    'description': 'A **1 x 1 Convolution** is a [convolution](https://paperswithcode.com/method/convolution) with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an [MLP](https://paperswithcode.com/method/feedforward-network) looking at a particular pixel location.\\r\\n\\r\\nImage Credit: [http://deeplearning.ai](http://deeplearning.ai)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://www.healthnutra.org/es/maxup/',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Average Pooling',\n",
       "    'full_name': 'Average Pooling',\n",
       "    'description': '**Average Pooling** is a pooling operation that calculates the average value for patches of a feature map, and uses it to create a downsampled (pooled) feature map. It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs. It extracts features more smoothly than [Max Pooling](https://paperswithcode.com/method/max-pooling), whereas max pooling extracts more pronounced features like edges.\\r\\n\\r\\nImage Source: [here](https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Local Response Normalization',\n",
       "    'full_name': 'Local Response Normalization',\n",
       "    'description': \"**Local Response Normalization** is a normalization layer that implements the idea of lateral inhibition. Lateral inhibition is a concept in neurobiology that refers to the phenomenon of an excited neuron inhibiting its neighbours: this leads to a peak in the form of a local maximum, creating contrast in that area and increasing sensory perception. In practice, we can either normalize within the same channel or normalize across channels when we apply LRN to convolutional neural networks.\\r\\n\\r\\n$$ b_{c} = a_{c}\\\\left(k + \\\\frac{\\\\alpha}{n}\\\\sum_{c'=\\\\max(0, c-n/2)}^{\\\\min(N-1,c+n/2)}a_{c'}^2\\\\right)^{-\\\\beta} $$\\r\\n\\r\\nWhere the size is the number of neighbouring channels used for normalization, $\\\\alpha$ is multiplicative factor, $\\\\beta$ an exponent and $k$ an additive factor\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks',\n",
       "    'source_title': 'ImageNet Classification with Deep Convolutional Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/1c5c289b6218eb1026dcb5fd9738231401cfccea/torch/nn/modules/normalization.py#L13',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Auxiliary Classifier',\n",
       "    'full_name': 'Auxiliary Classifier',\n",
       "    'description': '**Auxiliary Classifiers** are type of architectural component that seek to improve the convergence of very deep networks. They are classifier heads we attach to layers before the end of the network. The motivation is to push useful gradients to the lower layers to make them immediately useful and improve the convergence during training by combatting the vanishing gradient problem. They are notably used in the Inception family of convolutional neural networks.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Miscellaneous Components',\n",
       "     'description': 'The following is a list of miscellaneous components used in neural networks.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Inception Module',\n",
       "    'full_name': 'Inception Module',\n",
       "    'description': 'An **Inception Module** is an image model block that aims to approximate an optimal local sparse structure in a CNN. Put simply, it allows for us to use multiple types of filter size, instead of being restricted to a single filter size, in a single image block, which we then concatenate and pass onto the next layer.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1409.4842v1',\n",
       "    'source_title': 'Going Deeper with Convolutions',\n",
       "    'code_snippet_url': 'https://github.com/hskang9/Googlenet/blob/654d126b6a2cd3ac944cf5613419deb73da5311e/keras/googlenet.py#L39',\n",
       "    'main_collection': {'name': 'Image Model Blocks',\n",
       "     'description': '**Image Model Blocks** are building blocks used in image models such as convolutional neural networks. Below you can find a continuously updating list of image model blocks.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Grouped Convolution',\n",
       "    'full_name': 'Grouped Convolution',\n",
       "    'description': 'A **Grouped Convolution** uses a group of convolutions - multiple kernels per layer - resulting in multiple channel outputs per layer. This leads to wider networks helping a network learn a varied set of low level and high level features. The original motivation of using Grouped Convolutions in [AlexNet](https://paperswithcode.com/method/alexnet) was to distribute the model over multiple GPUs as an engineering compromise. But later, with models such as [ResNeXt](https://paperswithcode.com/method/resnext), it was shown this module could be used to improve classification accuracy. Specifically by exposing a new dimension through grouped convolutions, *cardinality* (the size of set of transformations), we can increase accuracy by increasing it.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks',\n",
       "    'source_title': 'ImageNet Classification with Deep Convolutional Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/prlz77/ResNeXt.pytorch/blob/39fb8d03847f26ec02fb9b880ecaaa88db7a7d16/models/model.py#L42',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Dropout',\n",
       "    'full_name': 'Dropout',\n",
       "    'description': '**Dropout** is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p=0.5$). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).\\r\\n\\r\\nThe idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://jmlr.org/papers/v15/srivastava14a.html',\n",
       "    'source_title': 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/7f3078b70d0ed9bea6228efa420879c56f72ef69/jax/experimental/stax.py#L271-L275',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Dense Connections',\n",
       "    'full_name': 'Dense Connections',\n",
       "    'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\\\_{\\\\text{inputs}}*n\\\\_{\\\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\\r\\n\\r\\n$$h\\\\_{l} = g\\\\left(\\\\textbf{W}^{T}h\\\\_{l-1}\\\\right)$$\\r\\n\\r\\nwhere $g$ is an activation function.\\r\\n\\r\\nImage Source: Deep Learning by Goodfellow, Bengio and Courville',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'AlexNet',\n",
       "    'full_name': 'AlexNet',\n",
       "    'description': '**AlexNet** is a classic convolutional neural network architecture. It consists of convolutions, [max pooling](https://paperswithcode.com/method/max-pooling) and dense layers as the basic building blocks. Grouped convolutions are used in order to fit the model across two GPUs.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks',\n",
       "    'source_title': 'ImageNet Classification with Deep Convolutional Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/dansuh17/alexnet-pytorch/blob/d0c1b1c52296ffcbecfbf5b17e1d1685b4ca6744/model.py#L40',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GoogLeNet',\n",
       "    'full_name': 'GoogLeNet',\n",
       "    'description': '**GoogLeNet** is a type of convolutional neural network based on the [Inception](https://paperswithcode.com/method/inception-module) architecture. It utilises Inception modules, which allow the network to choose between multiple convolutional filter sizes in each block. An Inception network stacks these modules on top of each other, with occasional max-pooling layers with stride 2 to halve the resolution of the grid.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1409.4842v1',\n",
       "    'source_title': 'Going Deeper with Convolutions',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/googlenet.py#L62',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['multi-pie'],\n",
       "  'datasets_used_full': ['Multi-PIE'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/psyphy-a-psychophysics-driven-evaluation',\n",
       "  'arxiv_id': '1611.06448',\n",
       "  'title': 'PsyPhy: A Psychophysics Driven Evaluation Framework for Visual Recognition',\n",
       "  'abstract': 'By providing substantial amounts of data and standardized evaluation\\nprotocols, datasets in computer vision have helped fuel advances across all\\nareas of visual recognition. But even in light of breakthrough results on\\nrecent benchmarks, it is still fair to ask if our recognition algorithms are\\ndoing as well as we think they are. The vision sciences at large make use of a\\nvery different evaluation regime known as Visual Psychophysics to study visual\\nperception. Psychophysics is the quantitative examination of the relationships\\nbetween controlled stimuli and the behavioral responses they elicit in\\nexperimental test subjects. Instead of using summary statistics to gauge\\nperformance, psychophysics directs us to construct item-response curves made up\\nof individual stimulus responses to find perceptual thresholds, thus allowing\\none to identify the exact point at which a subject can no longer reliably\\nrecognize the stimulus class. In this article, we introduce a comprehensive\\nevaluation framework for visual recognition models that is underpinned by this\\nmethodology. Over millions of procedurally rendered 3D scenes and 2D images, we\\ncompare the performance of well-known convolutional neural networks. Our\\nresults bring into question recent claims of human-like performance, and\\nprovide a path forward for correcting newly surfaced algorithmic deficiencies.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1611.06448v6',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1611.06448v6.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Brandon RichardWebster',\n",
       "   'Samuel E. Anthony',\n",
       "   'Walter J. Scheirer'],\n",
       "  'tasks': [],\n",
       "  'date': '2016-11-19',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/using-search-queries-to-understand-health',\n",
       "  'arxiv_id': '1806.05740',\n",
       "  'title': 'Using Search Queries to Understand Health Information Needs in Africa',\n",
       "  'abstract': \"The lack of comprehensive, high-quality health data in developing nations\\ncreates a roadblock for combating the impacts of disease. One key challenge is\\nunderstanding the health information needs of people in these nations. Without\\nunderstanding people's everyday needs, concerns, and misconceptions, health\\norganizations and policymakers lack the ability to effectively target education\\nand programming efforts. In this paper, we propose a bottom-up approach that\\nuses search data from individuals to uncover and gain insight into health\\ninformation needs in Africa. We analyze Bing searches related to HIV/AIDS,\\nmalaria, and tuberculosis from all 54 African nations. For each disease, we\\nautomatically derive a set of common search themes or topics, revealing a\\nwide-spread interest in various types of information, including disease\\nsymptoms, drugs, concerns about breastfeeding, as well as stigma, beliefs in\\nnatural cures, and other topics that may be hard to uncover through traditional\\nsurveys. We expose the different patterns that emerge in health information\\nneeds by demographic groups (age and sex) and country. We also uncover\\ndiscrepancies in the quality of content returned by search engines to users by\\ntopic. Combined, our results suggest that search data can help illuminate\\nhealth information needs in Africa and inform discussions on health policy and\\ntargeted education efforts both on- and offline.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05740v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05740v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Rediet Abebe',\n",
       "   'Shawndra Hill',\n",
       "   'Jennifer Wortman Vaughan',\n",
       "   'Peter M. Small',\n",
       "   'H. Andrew Schwartz'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-sauer-shelah-perles-lemma-for-sumsets',\n",
       "  'arxiv_id': '1806.05737',\n",
       "  'title': 'A Sauer-Shelah-Perles Lemma for Sumsets',\n",
       "  'abstract': \"We show that any family of subsets $A\\\\subseteq 2^{[n]}$ satisfies $\\\\lvert\\nA\\\\rvert \\\\leq O\\\\bigl(n^{\\\\lceil{d}/{2}\\\\rceil}\\\\bigr)$, where $d$ is the VC\\ndimension of $\\\\{S\\\\triangle T \\\\,\\\\vert\\\\, S,T\\\\in A\\\\}$, and $\\\\triangle$ is the\\nsymmetric difference operator. We also observe that replacing $\\\\triangle$ by\\neither $\\\\cup$ or $\\\\cap$ fails to satisfy an analogous statement. Our proof is\\nbased on the polynomial method; specifically, on an argument due to [Croot,\\nLev, Pach '17].\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05737v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05737v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Zeev Dvir', 'Shay Moran'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/efficient-sampling-for-gaussian-linear',\n",
       "  'arxiv_id': '1806.05738',\n",
       "  'title': 'Efficient sampling for Gaussian linear regression with arbitrary priors',\n",
       "  'abstract': 'This paper develops a slice sampler for Bayesian linear regression models\\nwith arbitrary priors. The new sampler has two advantages over current\\napproaches. One, it is faster than many custom implementations that rely on\\nauxiliary latent variables, if the number of regressors is large. Two, it can\\nbe used with any prior with a density function that can be evaluated up to a\\nnormalizing constant, making it ideal for investigating the properties of new\\nshrinkage priors without having to develop custom sampling algorithms. The new\\nsampler takes advantage of the special structure of the linear regression\\nlikelihood, allowing it to produce better effective sample size per second than\\ncommon alternative approaches.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05738v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05738v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['P. Richard Hahn', 'Jingyu He', 'Hedibert Lopes'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'Linear Regression',\n",
       "    'full_name': 'Linear Regression',\n",
       "    'description': '**Linear Regression** is a method for modelling a relationship between a dependent variable and independent variables. These models can be fit with numerous approaches. The most common is *least squares*, where we minimize the mean square error between the predicted values $\\\\hat{y} = \\\\textbf{X}\\\\hat{\\\\beta}$ and actual values $y$: $\\\\left(y-\\\\textbf{X}\\\\beta\\\\right)^{2}$.\\r\\n\\r\\nWe can also define the problem in probabilistic terms as a generalized linear model (GLM) where the pdf is a Gaussian distribution, and then perform maximum likelihood estimation to estimate $\\\\hat{\\\\beta}$.\\r\\n\\r\\nImage Source: [Wikipedia](https://en.wikipedia.org/wiki/Linear_regression)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Generalized Linear Models',\n",
       "     'description': '**Generalized Linear Models (GLMs)** are a class of models that generalize upon linear regression by allowing many more distributions to be modeled for the response variable via a link function. Below you can find a continuously updating list of GLMs.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-influence-receptivity-network',\n",
       "  'arxiv_id': '1806.05730',\n",
       "  'title': 'Learning Influence-Receptivity Network Structure with Guarantee',\n",
       "  'abstract': 'Traditional works on community detection from observations of information\\ncascade assume that a single adjacency matrix parametrizes all the observed\\ncascades. However, in reality the connection structure usually does not stay\\nthe same across cascades. For example, different people have different topics\\nof interest, therefore the connection structure depends on the\\ninformation/topic content of the cascade. In this paper we consider the case\\nwhere we observe a sequence of noisy adjacency matrices triggered by\\ninformation/event with different topic distributions. We propose a novel latent\\nmodel using the intuition that a connection is more likely to exist between two\\nnodes if they are interested in similar topics, which are common with the\\ninformation/event. Specifically, we endow each node with two node-topic\\nvectors: an influence vector that measures how influential/authoritative they\\nare on each topic; and a receptivity vector that measures how\\nreceptive/susceptible they are to each topic. We show how these two node-topic\\nstructures can be estimated from observed adjacency matrices with theoretical\\nguarantee on estimation error, in cases where the topic distributions of the\\ninformation/event are known, as well as when they are unknown. Experiments on\\nsynthetic and real data demonstrate the effectiveness of our model and superior\\nperformance compared to state-of-the-art methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05730v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05730v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ming Yu', 'Varun Gupta', 'Mladen Kolar'],\n",
       "  'tasks': ['Community Detection'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/action-learning-for-3d-point-cloud-based',\n",
       "  'arxiv_id': '1806.05724',\n",
       "  'title': 'Action Learning for 3D Point Cloud Based Organ Segmentation',\n",
       "  'abstract': 'We propose a novel point cloud based 3D organ segmentation pipeline utilizing\\ndeep Q-learning. In order to preserve shape properties, the learning process is\\nguided using a statistical shape model. The trained agent directly predicts\\npiece-wise linear transformations for all vertices in each iteration. This\\nmapping between the ideal transformation for an object outline estimation is\\nlearned based on image features. To this end, we introduce aperture features\\nthat extract gray values by sampling the 3D volume within the cone centered\\naround the associated vertex and its normal vector. Our approach is also\\ncapable of estimating a hierarchical pyramid of non rigid deformations for\\nmulti-resolution meshes. In the application phase, we use a marginal approach\\nto gradually estimate affine as well as non-rigid transformations. We performed\\nextensive evaluations to highlight the robust performance of our approach on a\\nvariety of challenge data as well as clinical data. Additionally, our method\\nhas a run time ranging from 0.3 to 2.7 seconds to segment each organ. In\\naddition, we show that the proposed method can be applied to different organs,\\nX-ray based modalities, and scanning protocols without the need of transfer\\nlearning. As we learn actions, even unseen reference meshes can be processed as\\ndemonstrated in an example with the Visible Human. From this we conclude that\\nour method is robust, and we believe that our method can be successfully\\napplied to many more applications, in particular, in the interventional imaging\\nspace.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05724v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05724v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xia Zhong',\n",
       "   'Mario Amrehn',\n",
       "   'Nishant Ravikumar',\n",
       "   'Shuqing Chen',\n",
       "   'Norbert Strobel',\n",
       "   'Annette Birkhold',\n",
       "   'Markus Kowarschik',\n",
       "   'Rebecca Fahrig',\n",
       "   'Andreas Maier'],\n",
       "  'tasks': ['Q-Learning', 'Transfer Learning'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/differentiable-submodular-maximization',\n",
       "  'arxiv_id': '1803.01785',\n",
       "  'title': 'Differentiable Submodular Maximization',\n",
       "  'abstract': 'We consider learning of submodular functions from data. These functions are\\nimportant in machine learning and have a wide range of applications, e.g. data\\nsummarization, feature selection and active learning. Despite their\\ncombinatorial nature, submodular functions can be maximized approximately with\\nstrong theoretical guarantees in polynomial time. Typically, learning the\\nsubmodular function and optimization of that function are treated separately,\\ni.e. the function is first learned using a proxy objective and subsequently\\nmaximized. In contrast, we show how to perform learning and optimization\\njointly. By interpreting the output of greedy maximization algorithms as\\ndistributions over sequences of items and smoothening these distributions, we\\nobtain a differentiable objective. In this way, we can differentiate through\\nthe maximization algorithms and optimize the model to work well with the\\noptimization algorithm. We theoretically characterize the error made by our\\napproach, yielding insights into the tradeoff of smoothness and accuracy. We\\ndemonstrate the effectiveness of our approach for jointly learning and\\noptimizing on synthetic maximum cut data, and on real world applications such\\nas product recommendation and image collection summarization.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.01785v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.01785v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sebastian Tschiatschek', 'Aytunc Sahin', 'Andreas Krause'],\n",
       "  'tasks': ['Active Learning', 'Data Summarization', 'Product Recommendation'],\n",
       "  'date': '2018-03-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/non-asymptotic-identification-of-lti-systems',\n",
       "  'arxiv_id': '1806.05722',\n",
       "  'title': 'Non-asymptotic Identification of LTI Systems from a Single Trajectory',\n",
       "  'abstract': \"We consider the problem of learning a realization for a linear time-invariant\\n(LTI) dynamical system from input/output data. Given a single input/output\\ntrajectory, we provide finite time analysis for learning the system's Markov\\nparameters, from which a balanced realization is obtained using the classical\\nHo-Kalman algorithm. By proving a stability result for the Ho-Kalman algorithm\\nand combining it with the sample complexity results for Markov parameters, we\\nshow how much data is needed to learn a balanced realization of the system up\\nto a desired accuracy with high probability.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05722v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05722v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Samet Oymak', 'Necmiye Ozay'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/safe-policy-improvement-with-baseline',\n",
       "  'arxiv_id': '1712.06924',\n",
       "  'title': 'Safe Policy Improvement with Baseline Bootstrapping',\n",
       "  'abstract': 'This paper considers Safe Policy Improvement (SPI) in Batch Reinforcement Learning (Batch RL): from a fixed dataset and without direct access to the true environment, train a policy that is guaranteed to perform at least as well as the baseline policy used to collect the data. Our approach, called SPI with Baseline Bootstrapping (SPIBB), is inspired by the knows-what-it-knows paradigm: it bootstraps the trained policy with the baseline when the uncertainty is high. Our first algorithm, $\\\\Pi_b$-SPIBB, comes with SPI theoretical guarantees. We also implement a variant, $\\\\Pi_{\\\\leq b}$-SPIBB, that is even more efficient in practice. We apply our algorithms to a motivational stochastic gridworld domain and further demonstrate on randomly generated MDPs the superiority of SPIBB with respect to existing algorithms, not only in safety but also in mean performance. Finally, we implement a model-free version of SPIBB and show its benefits on a navigation task with deep RL implementation called SPIBB-DQN, which is, to the best of our knowledge, the first RL algorithm relying on a neural network representation able to train efficiently and reliably from batch data, without any interaction with the environment.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1712.06924v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1712.06924v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Romain Laroche', 'Paul Trichelair', 'Rémi Tachet des Combes'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-12-19',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/data-driven-decentralized-optimal-power-flow',\n",
       "  'arxiv_id': '1806.06790',\n",
       "  'title': 'Towards Distributed Energy Services: Decentralizing Optimal Power Flow with Machine Learning',\n",
       "  'abstract': 'The implementation of optimal power flow (OPF) methods to perform voltage and power flow regulation in electric networks is generally believed to require extensive communication. We consider distribution systems with multiple controllable Distributed Energy Resources (DERs) and present a data-driven approach to learn control policies for each DER to reconstruct and mimic the solution to a centralized OPF problem from solely locally available information. Collectively, all local controllers closely match the centralized OPF solution, providing near optimal performance and satisfaction of system constraints. A rate distortion framework enables the analysis of how well the resulting fully decentralized control policies are able to reconstruct the OPF solution. The methodology provides a natural extension to decide what nodes a DER should communicate with to improve the reconstruction of its individual policy. The method is applied on both single- and three-phase test feeder networks using data from real loads and distributed generators, focusing on DERs that do not exhibit inter-temporal dependencies. It provides a framework for Distribution System Operators to efficiently plan and operate the contributions of DERs to achieve Distributed Energy Services in distribution networks.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.06790v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.06790v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Roel Dobbe',\n",
       "   'Oscar Sondermeijer',\n",
       "   'David Fridovich-Keil',\n",
       "   'Daniel Arnold',\n",
       "   'Duncan Callaway',\n",
       "   'Claire Tomlin'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multilevel-artificial-neural-network-training',\n",
       "  'arxiv_id': '1806.05703',\n",
       "  'title': 'Multilevel Artificial Neural Network Training for Spatially Correlated Learning',\n",
       "  'abstract': 'Multigrid modeling algorithms are a technique used to accelerate relaxation models running on a hierarchy of similar graphlike structures. We introduce and demonstrate a new method for training neural networks which uses multilevel methods. Using an objective function derived from a graph-distance metric, we perform orthogonally-constrained optimization to find optimal prolongation and restriction maps between graphs. We compare and contrast several methods for performing this numerical optimization, and additionally present some new theoretical results on upper bounds of this type of objective function. Once calculated, these optimal maps between graphs form the core of Multiscale Artificial Neural Network (MsANN) training, a new procedure we present which simultaneously trains a hierarchy of neural network models of varying spatial resolution. Parameter information is passed between members of this hierarchy according to standard coarsening and refinement schedules from the multiscale modelling literature. In our machine learning experiments, these models are able to learn faster than default training, achieving a comparable level of error in an order of magnitude fewer training examples.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05703v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05703v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['C. B. Scott', 'Eric Mjolsness'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist'],\n",
       "  'datasets_used_full': ['MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/detection-limits-in-the-high-dimensional',\n",
       "  'arxiv_id': '1802.07309',\n",
       "  'title': 'Detection limits in the high-dimensional spiked rectangular model',\n",
       "  'abstract': 'We study the problem of detecting the presence of a single unknown spike in a\\nrectangular data matrix, in a high-dimensional regime where the spike has fixed\\nstrength and the aspect ratio of the matrix converges to a finite limit. This\\nsetup includes Johnstone\\'s spiked covariance model. We analyze the likelihood\\nratio of the spiked model against an \"all noise\" null model of reference, and\\nshow it has asymptotically Gaussian fluctuations in a region below---but in\\ngeneral not up to---the so-called BBP threshold from random matrix theory. Our\\nresult parallels earlier findings of Onatski et al.\\\\ (2013) and\\nJohnstone-Onatski (2015) for spherical spikes. We present a probabilistic\\napproach capable of treating generic product priors. In particular, sparsity in\\nthe spike is allowed. Our approach is based on Talagrand\\'s interpretation of\\nthe cavity method from spin-glass theory. The question of the maximal parameter\\nregion where asymptotic normality is expected to hold is left open. This region\\nis shaped by the prior in a non-trivial way. We conjecture that this is the\\nentire paramagnetic phase of an associated spin-glass model, and is defined by\\nthe vanishing of the replica-symmetric solution of Lesieur et al.\\\\ (2015).',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.07309v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.07309v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ahmed El Alaoui', 'Michael. I. Jordan'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-20',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/neural-generative-models-for-global',\n",
       "  'arxiv_id': '1805.08594',\n",
       "  'title': 'Neural Generative Models for Global Optimization with Gradients',\n",
       "  'abstract': 'The aim of global optimization is to find the global optimum of arbitrary\\nclasses of functions, possibly highly multimodal ones. In this paper we focus\\non the subproblem of global optimization for differentiable functions and we\\npropose an Evolutionary Search-inspired solution where we model point search\\ndistributions via Generative Neural Networks. This approach enables us to model\\ndiverse and complex search distributions based on which we can efficiently\\nexplore complicated objective landscapes. In our experiments we show the\\npractical superiority of our algorithm versus classical Evolutionary Search and\\ngradient-based solutions on a benchmark set of multimodal functions, and\\ndemonstrate how it can be used to accelerate Bayesian Optimization with\\nGaussian Processes.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.08594v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.08594v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Louis Faury',\n",
       "   'Flavian vasile',\n",
       "   'Clément Calauzènes',\n",
       "   'Olivier Fercoq'],\n",
       "  'tasks': ['Gaussian Processes'],\n",
       "  'date': '2018-05-22',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/evolving-simple-programs-for-playing-atari',\n",
       "  'arxiv_id': '1806.05695',\n",
       "  'title': 'Evolving simple programs for playing Atari games',\n",
       "  'abstract': 'Cartesian Genetic Programming (CGP) has previously shown capabilities in\\nimage processing tasks by evolving programs with a function set specialized for\\ncomputer vision. A similar approach can be applied to Atari playing. Programs\\nare evolved using mixed type CGP with a function set suited for matrix\\noperations, including image processing, but allowing for controller behavior to\\nemerge. While the programs are relatively small, many controllers are\\ncompetitive with state of the art methods for the Atari benchmark set and\\nrequire less training time. By evaluating the programs of the best evolved\\nindividuals, simple but effective strategies can be found.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05695v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05695v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Dennis G Wilson',\n",
       "   'Sylvain Cussat-Blanc',\n",
       "   'Hervé Luga',\n",
       "   'Julian F. Miller'],\n",
       "  'tasks': ['Atari Games'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['arcade-learning-environment'],\n",
       "  'datasets_used_full': ['Arcade Learning Environment'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/discovering-latent-patterns-of-urban-cultural',\n",
       "  'arxiv_id': '1806.05694',\n",
       "  'title': 'Discovering Latent Patterns of Urban Cultural Interactions in WeChat for Modern City Planning',\n",
       "  'abstract': 'Cultural activity is an inherent aspect of urban life and the success of a\\nmodern city is largely determined by its capacity to offer generous cultural\\nentertainment to its citizens. To this end, the optimal allocation of cultural\\nestablishments and related resources across urban regions becomes of vital\\nimportance, as it can reduce financial costs in terms of planning and improve\\nquality of life in the city, more generally. In this paper, we make use of a\\nlarge longitudinal dataset of user location check-ins from the online social\\nnetwork WeChat to develop a data-driven framework for cultural planning in the\\ncity of Beijing. We exploit rich spatio-temporal representations on user\\nactivity at cultural venues and use a novel extended version of the traditional\\nlatent Dirichlet allocation model that incorporates temporal information to\\nidentify latent patterns of urban cultural interactions. Using the\\ncharacteristic typologies of mobile user cultural activities emitted by the\\nmodel, we determine the levels of demand for different types of cultural\\nresources across urban areas. We then compare those with the corresponding\\nlevels of supply as driven by the presence and spatial reach of cultural venues\\nin local areas to obtain high resolution maps that indicate urban regions with\\nlack of cultural resources, and thus give suggestions for further urban\\ncultural planning and investment optimisation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05694v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05694v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xiao Zhou',\n",
       "   'Anastasios Noulas',\n",
       "   'Cecilia Mascoloo',\n",
       "   'Zhongxiang Zhao'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-power-of-interpolation-understanding-the',\n",
       "  'arxiv_id': '1712.06559',\n",
       "  'title': 'The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning',\n",
       "  'abstract': 'In this paper we aim to formally explain the phenomenon of fast convergence\\nof SGD observed in modern machine learning. The key observation is that most\\nmodern learning architectures are over-parametrized and are trained to\\ninterpolate the data by driving the empirical loss (classification and\\nregression) close to zero. While it is still unclear why these interpolated\\nsolutions perform well on test data, we show that these regimes allow for fast\\nconvergence of SGD, comparable in number of iterations to full gradient\\ndescent.\\n  For convex loss functions we obtain an exponential convergence bound for {\\\\it\\nmini-batch} SGD parallel to that for full gradient descent. We show that there\\nis a critical batch size $m^*$ such that: (a) SGD iteration with mini-batch\\nsize $m\\\\leq m^*$ is nearly equivalent to $m$ iterations of mini-batch size $1$\\n(\\\\emph{linear scaling regime}). (b) SGD iteration with mini-batch $m> m^*$ is\\nnearly equivalent to a full gradient descent iteration (\\\\emph{saturation\\nregime}).\\n  Moreover, for the quadratic loss, we derive explicit expressions for the\\noptimal mini-batch and step size and explicitly characterize the two regimes\\nabove. The critical mini-batch size can be viewed as the limit for effective\\nmini-batch parallelization. It is also nearly independent of the data size,\\nimplying $O(n)$ acceleration over GD per unit of computation. We give\\nexperimental evidence on real data which closely follows our theoretical\\nanalyses.\\n  Finally, we show how our results fit in the recent developments in training\\ndeep neural networks and discuss connections to adaptive rates for SGD and\\nvariance reduction.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.06559v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.06559v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Siyuan Ma', 'Raef Bassily', 'Mikhail Belkin'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-12-18',\n",
       "  'methods': [{'name': 'SGD',\n",
       "    'full_name': 'Stochastic Gradient Descent',\n",
       "    'description': '**Stochastic Gradient Descent** is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:\\r\\n\\r\\n$$ w\\\\_{t+1} = w\\\\_{t} - \\\\eta\\\\hat{\\\\nabla}\\\\_{w}{L(w\\\\_{t})} $$\\r\\n\\r\\nWhere $\\\\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.\\r\\n\\r\\n(Image Source: [here](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/))',\n",
       "    'introduced_year': 1951,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/4e0ac120e9a8b096069c2f892488d630a5c8f358/torch/optim/sgd.py#L97-L112',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['mnist'],\n",
       "  'datasets_used_full': ['MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/ivus-net-an-intravascular-ultrasound',\n",
       "  'arxiv_id': '1806.03583',\n",
       "  'title': 'IVUS-Net: An Intravascular Ultrasound Segmentation Network',\n",
       "  'abstract': 'IntraVascular UltraSound (IVUS) is one of the most effective imaging\\nmodalities that provides assistance to experts in order to diagnose and treat\\ncardiovascular diseases. We address a central problem in IVUS image analysis\\nwith Fully Convolutional Network (FCN): automatically delineate the lumen and\\nmedia-adventitia borders in IVUS images, which is crucial to shorten the\\ndiagnosis process or benefits a faster and more accurate 3D reconstruction of\\nthe artery. Particularly, we propose an FCN architecture, called IVUS-Net,\\nfollowed by a post-processing contour extraction step, in order to\\nautomatically segments the interior (lumen) and exterior (media-adventitia)\\nregions of the human arteries. We evaluated our IVUS-Net on the test set of a\\nstandard publicly available dataset containing 326 IVUS B-mode images with two\\nmeasurements, namely Jaccard Measure (JM) and Hausdorff Distances (HD). The\\nevaluation result shows that IVUS-Net outperforms the state-of-the-art lumen\\nand media segmentation methods by 4% to 20% in terms of HD distance. IVUS-Net\\nperforms well on images in the test set that contain a significant amount of\\nmajor artifacts such as bifurcations, shadows, and side branches that are not\\ncommon in the training set. Furthermore, using a modern GPU, IVUS-Net segments\\neach IVUS frame only in 0.15 seconds. The proposed work, to the best of our\\nknowledge, is the first deep learning based method for segmentation of both the\\nlumen and the media vessel walls in 20 MHz IVUS B-mode images that achieves the\\nbest results without any manual intervention. Code is available at\\nhttps://github.com/Kulbear/ivus-segmentation-icsm2018',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03583v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03583v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ji Yang', 'Lin Tong', 'Mehdi Faraji', 'Anup Basu'],\n",
       "  'tasks': ['3D Reconstruction'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [{'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'FCN',\n",
       "    'full_name': 'Fully Convolutional Network',\n",
       "    'description': '**Fully Convolutional Networks**, or **FCNs**, are an architecture used mainly for semantic segmentation. They employ solely locally connected layers, such as [convolution](https://paperswithcode.com/method/convolution), pooling and upsampling. Avoiding the use of dense layers means less parameters (making the networks faster to train). It also means an FCN can work for variable image sizes given all connections are local.\\r\\n\\r\\nThe network consists of a downsampling path, used to extract and interpret the context, and an upsampling path, which allows for localization. \\r\\n\\r\\nFCNs also employ skip connections to recover the fine-grained spatial information lost in the downsampling path.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1605.06211v1',\n",
       "    'source_title': 'Fully Convolutional Networks for Semantic Segmentation',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/bf843c664b8ba0ff49d2921237500c77d82f2d04/torchvision/models/segmentation/fcn.py#L9',\n",
       "    'main_collection': {'name': 'Semantic Segmentation Models',\n",
       "     'description': '**Semantic Segmentation Models** are a class of methods that address the task of semantically segmenting an image into different object classes. Below you can find a continuously updating list of semantic segmentation models. ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-human-optical-flow',\n",
       "  'arxiv_id': '1806.05666',\n",
       "  'title': 'Learning Human Optical Flow',\n",
       "  'abstract': 'The optical flow of humans is well known to be useful for the analysis of\\nhuman action. Given this, we devise an optical flow algorithm specifically for\\nhuman motion and show that it is superior to generic flow methods. Designing a\\nmethod by hand is impractical, so we develop a new training database of image\\nsequences with ground truth optical flow. For this we use a 3D model of the\\nhuman body and motion capture data to synthesize realistic flow fields. We then\\ntrain a convolutional neural network to estimate human flow fields from pairs\\nof images. Since many applications in human motion analysis depend on speed,\\nand we anticipate mobile applications, we base our method on SpyNet with\\nseveral modifications. We demonstrate that our trained network is more accurate\\nthan a wide range of top methods on held-out test data and that it generalizes\\nwell to real image sequences. When combined with a person detector/tracker, the\\napproach provides a full solution to the problem of 2D human flow estimation.\\nBoth the code and the dataset are available for research.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05666v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05666v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Anurag Ranjan', 'Javier Romero', 'Michael J. Black'],\n",
       "  'tasks': ['Optical Flow Estimation'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['kitti',\n",
       "   'lsun',\n",
       "   'human3-6m',\n",
       "   'flyingchairs',\n",
       "   'virtual-kitti'],\n",
       "  'datasets_used_full': ['KITTI',\n",
       "   'LSUN',\n",
       "   'Human3.6M',\n",
       "   'FlyingChairs',\n",
       "   'Virtual KITTI'],\n",
       "  'datasets_introduced_lower': ['coma-1'],\n",
       "  'datasets_introduced_full': ['Human Optical Flow']},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-deep-resnet-blocks-sequentially',\n",
       "  'arxiv_id': '1706.04964',\n",
       "  'title': 'Learning Deep ResNet Blocks Sequentially using Boosting Theory',\n",
       "  'abstract': 'Deep neural networks are known to be difficult to train due to the\\ninstability of back-propagation. A deep \\\\emph{residual network} (ResNet) with\\nidentity loops remedies this by stabilizing gradient computations. We prove a\\nboosting theory for the ResNet architecture. We construct $T$ weak module\\nclassifiers, each contains two of the $T$ layers, such that the combined strong\\nlearner is a ResNet. Therefore, we introduce an alternative Deep ResNet\\ntraining algorithm, \\\\emph{BoostResNet}, which is particularly suitable in\\nnon-differentiable architectures. Our proposed algorithm merely requires a\\nsequential training of $T$ \"shallow ResNets\" which are inexpensive. We prove\\nthat the training error decays exponentially with the depth $T$ if the\\n\\\\emph{weak module classifiers} that we train perform slightly better than some\\nweak baseline. In other words, we propose a weak learning condition and prove a\\nboosting theory for ResNet under the weak learning condition. Our results apply\\nto general multi-class ResNets. A generalization error bound based on margin\\ntheory is proved and suggests ResNet\\'s resistant to overfitting under network\\nwith $l_1$ norm bounded weights.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1706.04964v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1706.04964v4.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Furong Huang',\n",
       "   'Jordan Ash',\n",
       "   'John Langford',\n",
       "   'Robert Schapire'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-06-15',\n",
       "  'methods': [{'name': 'Average Pooling',\n",
       "    'full_name': 'Average Pooling',\n",
       "    'description': '**Average Pooling** is a pooling operation that calculates the average value for patches of a feature map, and uses it to create a downsampled (pooled) feature map. It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs. It extracts features more smoothly than [Max Pooling](https://paperswithcode.com/method/max-pooling), whereas max pooling extracts more pronounced features like edges.\\r\\n\\r\\nImage Source: [here](https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': '1x1 Convolution',\n",
       "    'full_name': '1x1 Convolution',\n",
       "    'description': 'A **1 x 1 Convolution** is a [convolution](https://paperswithcode.com/method/convolution) with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an [MLP](https://paperswithcode.com/method/feedforward-network) looking at a particular pixel location.\\r\\n\\r\\nImage Credit: [http://deeplearning.ai](http://deeplearning.ai)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://www.healthnutra.org/es/maxup/',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Batch Normalization',\n",
       "    'full_name': 'Batch Normalization',\n",
       "    'description': '**Batch Normalization** aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for [Dropout](https://paperswithcode.com/method/dropout).\\r\\n\\r\\nWe apply a batch normalization layer as follows for a minibatch $\\\\mathcal{B}$:\\r\\n\\r\\n$$ \\\\mu\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}x\\\\_{i} $$\\r\\n\\r\\n$$ \\\\sigma^{2}\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}\\\\left(x\\\\_{i}-\\\\mu\\\\_{\\\\mathcal{B}}\\\\right)^{2} $$\\r\\n\\r\\n$$ \\\\hat{x}\\\\_{i} = \\\\frac{x\\\\_{i} - \\\\mu\\\\_{\\\\mathcal{B}}}{\\\\sqrt{\\\\sigma^{2}\\\\_{\\\\mathcal{B}}+\\\\epsilon}} $$\\r\\n\\r\\n$$ y\\\\_{i} = \\\\gamma\\\\hat{x}\\\\_{i} + \\\\beta = \\\\text{BN}\\\\_{\\\\gamma, \\\\beta}\\\\left(x\\\\_{i}\\\\right) $$\\r\\n\\r\\nWhere $\\\\gamma$ and $\\\\beta$ are learnable parameters.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.03167v3',\n",
       "    'source_title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/36f91261099b00194922bd93ed1286fe1c199724/jax/experimental/stax.py#L116',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Bottleneck Residual Block',\n",
       "    'full_name': 'Bottleneck Residual Block',\n",
       "    'description': 'A **Bottleneck Residual Block** is a variant of the [residual block](https://paperswithcode.com/method/residual-block) that utilises 1x1 convolutions to create a bottleneck. The use of a bottleneck reduces the number of parameters and matrix multiplications. The idea is to make residual blocks as thin as possible to increase depth and have less parameters. They were introduced as part of the [ResNet](https://paperswithcode.com/method/resnet) architecture, and are used as part of deeper ResNets such as ResNet-50 and ResNet-101.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L75',\n",
       "    'main_collection': {'name': 'Skip Connection Blocks',\n",
       "     'description': \"**Skip Connection Blocks** are building blocks for neural networks that feature skip connections. These skip connections 'skip' some layers allowing gradients to better flow through the network. Below you will find a continuously updating list of skip connection blocks:\",\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Global Average Pooling',\n",
       "    'full_name': 'Global Average Pooling',\n",
       "    'description': '**Global Average Pooling** is a pooling operation designed to replace fully connected layers in classical CNNs. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the [softmax](https://paperswithcode.com/method/softmax) layer. \\r\\n\\r\\nOne advantage of global [average pooling](https://paperswithcode.com/method/average-pooling) over the fully connected layers is that it is more native to the [convolution](https://paperswithcode.com/method/convolution) structure by enforcing correspondences between feature maps and categories. Thus the feature maps can be easily interpreted as categories confidence maps. Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. Furthermore, global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/baa592b215804927e28638f6a7f3318cbc411d49/torchvision/models/resnet.py#L157',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Residual Block',\n",
       "    'full_name': 'Residual Block',\n",
       "    'description': \"**Residual Blocks** are skip-connection blocks that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. They were introduced as part of the [ResNet](https://paperswithcode.com/method/resnet) architecture.\\r\\n \\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$. The $\\\\mathcal{F}({x})$ acts like a residual, hence the name 'residual block'.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. Having skip connections allows the network to more easily learn identity-like mappings.\\r\\n\\r\\nNote that in practice, [Bottleneck Residual Blocks](https://paperswithcode.com/method/bottleneck-residual-block) are used for deeper ResNets, such as ResNet-50 and ResNet-101, as these bottleneck blocks are less computationally intensive.\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L35',\n",
       "    'main_collection': {'name': 'Skip Connection Blocks',\n",
       "     'description': \"**Skip Connection Blocks** are building blocks for neural networks that feature skip connections. These skip connections 'skip' some layers allowing gradients to better flow through the network. Below you will find a continuously updating list of skip connection blocks:\",\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Kaiming Initialization',\n",
       "    'full_name': 'Kaiming Initialization',\n",
       "    'description': '**Kaiming Initialization**, or **He Initialization**, is an initialization method for neural networks that takes into account the non-linearity of activation functions, such as [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nA proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. Using a derivation they work out that the condition to stop this happening is:\\r\\n\\r\\n$$\\\\frac{1}{2}n\\\\_{l}\\\\text{Var}\\\\left[w\\\\_{l}\\\\right] = 1 $$\\r\\n\\r\\nThis implies an initialization scheme of:\\r\\n\\r\\n$$ w\\\\_{l} \\\\sim \\\\mathcal{N}\\\\left(0,  2/n\\\\_{l}\\\\right)$$\\r\\n\\r\\nThat is, a zero-centered Gaussian with standard deviation of $\\\\sqrt{2/{n}\\\\_{l}}$ (variance shown in equation above). Biases are initialized at $0$.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.01852v1',\n",
       "    'source_title': 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/0adb5843766092fba584791af76383125fd0d01c/torch/nn/init.py#L389',\n",
       "    'main_collection': {'name': 'Initialization',\n",
       "     'description': '**Initialization** methods are used to initialize the weights in a neural network. Below can you find a continuously updating list of initialization methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Residual Connection',\n",
       "    'full_name': 'Residual Connection',\n",
       "    'description': '**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'ResNet',\n",
       "    'full_name': 'Residual Network',\n",
       "    'description': '**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack [residual blocks](https://paperswithcode.com/method/residual-block) ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}(x)$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}(x):=\\\\mathcal{H}(x)-x$. The original mapping is recast into $\\\\mathcal{F}(x)+x$.\\r\\n\\r\\nThere is empirical evidence that these types of network are easier to optimize, and can gain accuracy from considerably increased depth.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/resnet.py#L124',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['cifar-10', 'mnist', 'svhn'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'MNIST', 'SVHN'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/interactive-classification-for-deep-learning',\n",
       "  'arxiv_id': '1806.05660',\n",
       "  'title': 'Interactive Classification for Deep Learning Interpretation',\n",
       "  'abstract': 'We present an interactive system enabling users to manipulate images to\\nexplore the robustness and sensitivity of deep learning image classifiers.\\nUsing modern web technologies to run in-browser inference, users can remove\\nimage features using inpainting algorithms and obtain new classifications in\\nreal time, which allows them to ask a variety of \"what if\" questions by\\nexperimentally modifying images and seeing how the model reacts. Our system\\nallows users to compare and contrast what image regions humans and machine\\nlearning models use for classification, revealing a wide range of surprising\\nresults ranging from spectacular failures (e.g., a \"water bottle\" image becomes\\na \"concert\" when removing a person) to impressive resilience (e.g., a \"baseball\\nplayer\" image remains correctly classified even without a glove or base). We\\ndemonstrate our system at The 2018 Conference on Computer Vision and Pattern\\nRecognition (CVPR) for the audience to try it live. Our system is open-sourced\\nat https://github.com/poloclub/interactive-classification. A video demo is\\navailable at https://youtu.be/llub5GcOF6w.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05660v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05660v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ángel Alexander Cabrera',\n",
       "   'Fred Hohman',\n",
       "   'Jason Lin',\n",
       "   'Duen Horng Chau'],\n",
       "  'tasks': ['Classification', 'General Classification'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'GloVe',\n",
       "    'full_name': 'GloVe Embeddings',\n",
       "    'description': '**GloVe Embeddings** are a type of word embedding that encode the co-occurrence probability ratio between two words as vector differences. GloVe uses a weighted least squares objective $J$ that minimizes the difference between the dot product of the vectors of two words and the logarithm of their number of co-occurrences:\\r\\n\\r\\n$$ J=\\\\sum\\\\_{i, j=1}^{V}f\\\\left(𝑋\\\\_{i j}\\\\right)(w^{T}\\\\_{i}\\\\tilde{w}_{j} + b\\\\_{i} + \\\\tilde{b}\\\\_{j} - \\\\log{𝑋}\\\\_{ij})^{2} $$\\r\\n\\r\\nwhere $w\\\\_{i}$ and $b\\\\_{i}$ are the word vector and bias respectively of word $i$, $\\\\tilde{w}_{j}$ and $b\\\\_{j}$ are the context word vector and bias respectively of word $j$, $X\\\\_{ij}$ is the number of times word $i$ occurs in the context of word $j$, and $f$ is a weighting function that assigns lower weights to rare and frequent co-occurrences.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://aclanthology.org/D14-1162',\n",
       "    'source_title': 'GloVe: Global Vectors for Word Representation',\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Word Embeddings',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Natural Language Processing'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/structure-infused-copy-mechanisms-for',\n",
       "  'arxiv_id': '1806.05658',\n",
       "  'title': 'Structure-Infused Copy Mechanisms for Abstractive Summarization',\n",
       "  'abstract': 'Seq2seq learning has produced promising results on summarization. However, in\\nmany cases, system summaries still struggle to keep the meaning of the original\\nintact. They may miss out important words or relations that play critical roles\\nin the syntactic structure of source sentences. In this paper, we present\\nstructure-infused copy mechanisms to facilitate copying important words and\\nrelations from the source sentence to summary sentence. The approach naturally\\ncombines source dependency structure with the copy mechanism of an abstractive\\nsentence summarizer. Experimental results demonstrate the effectiveness of\\nincorporating source-side syntactic information in the system, and our proposed\\napproach compares favorably to state-of-the-art methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05658v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05658v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Kaiqiang Song', 'Lin Zhao', 'Fei Liu'],\n",
       "  'tasks': ['Abstractive Text Summarization'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/adding-new-tasks-to-a-single-network-with',\n",
       "  'arxiv_id': '1805.11119',\n",
       "  'title': 'Adding New Tasks to a Single Network with Weight Transformations using Binary Masks',\n",
       "  'abstract': 'Visual recognition algorithms are required today to exhibit adaptive\\nabilities. Given a deep model trained on a specific, given task, it would be\\nhighly desirable to be able to adapt incrementally to new tasks, preserving\\nscalability as the number of new tasks increases, while at the same time\\navoiding catastrophic forgetting issues. Recent work has shown that masking the\\ninternal weights of a given original conv-net through learned binary variables\\nis a promising strategy. We build upon this intuition and take into account\\nmore elaborated affine transformations of the convolutional weights that\\ninclude learned binary masks. We show that with our generalization it is\\npossible to achieve significantly higher levels of adaptation to new tasks,\\nenabling the approach to compete with fine tuning strategies by requiring\\nslightly more than 1 bit per network parameter per additional task. Experiments\\non two popular benchmarks showcase the power of our approach, that achieves the\\nnew state of the art on the Visual Decathlon Challenge.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.11119v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.11119v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Massimiliano Mancini',\n",
       "   'Elisa Ricci',\n",
       "   'Barbara Caputo',\n",
       "   'Samuel Rota Bulò'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-28',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-100',\n",
       "   'svhn',\n",
       "   'ucf101',\n",
       "   'oxford-102-flower',\n",
       "   'omniglot-1',\n",
       "   'dtd',\n",
       "   'sketch',\n",
       "   'wikiart'],\n",
       "  'datasets_used_full': ['CIFAR-100',\n",
       "   'SVHN',\n",
       "   'UCF101',\n",
       "   'Oxford 102 Flower',\n",
       "   'Omniglot',\n",
       "   'DTD',\n",
       "   'Sketch',\n",
       "   'WikiArt'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/abstract-meaning-representation-for-multi',\n",
       "  'arxiv_id': '1806.05655',\n",
       "  'title': 'Abstract Meaning Representation for Multi-Document Summarization',\n",
       "  'abstract': 'Generating an abstract from a collection of documents is a desirable\\ncapability for many real-world applications. However, abstractive approaches to\\nmulti-document summarization have not been thoroughly investigated. This paper\\nstudies the feasibility of using Abstract Meaning Representation (AMR), a\\nsemantic representation of natural language grounded in linguistic theory, as a\\nform of content representation. Our approach condenses source documents to a\\nset of summary graphs following the AMR formalism. The summary graphs are then\\ntransformed to a set of summary sentences in a surface realization step. The\\nframework is fully data-driven and flexible. Each component can be optimized\\nindependently using small-scale, in-domain training data. We perform\\nexperiments on benchmark summarization datasets and report promising results.\\nWe also describe opportunities and challenges for advancing this line of\\nresearch.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05655v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05655v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Kexin Liao', 'Logan Lebanoff', 'Fei Liu'],\n",
       "  'tasks': ['Document Summarization', 'Multi-Document Summarization'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/hgr-net-a-fusion-network-for-hand-gesture',\n",
       "  'arxiv_id': '1806.05653',\n",
       "  'title': 'HGR-Net: A Fusion Network for Hand Gesture Segmentation and Recognition',\n",
       "  'abstract': 'We propose a two-stage convolutional neural network (CNN) architecture for robust recognition of hand gestures, called HGR-Net, where the first stage performs accurate semantic segmentation to determine hand regions, and the second stage identifies the gesture. The segmentation stage architecture is based on the combination of fully convolutional residual network and atrous spatial pyramid pooling. Although the segmentation sub-network is trained without depth information, it is particularly robust against challenges such as illumination variations and complex backgrounds. The recognition stage deploys a two-stream CNN, which fuses the information from the red-green-blue and segmented images by combining their deep representations in a fully connected layer before classification. Extensive experiments on public datasets show that our architecture achieves almost as good as state-of-the-art performance in segmentation and recognition of static hand gestures, at a fraction of training time, run time, and model size. Our method can operate at an average of 23 ms per frame.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05653v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05653v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Amirhossein Dadashzadeh',\n",
       "   'Alireza Tavakoli Targhi',\n",
       "   'Maryam Tahmasbi',\n",
       "   'Majid Mirmehdi'],\n",
       "  'tasks': ['Gesture Recognition',\n",
       "   'Hand Gesture Recognition',\n",
       "   'Hand-Gesture Recognition',\n",
       "   'Hand Gesture Segmentation',\n",
       "   'Hand Segmentation',\n",
       "   'Semantic Segmentation'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-learning-to-represent-sub-grid-processes',\n",
       "  'arxiv_id': '1806.04731',\n",
       "  'title': 'Deep learning to represent sub-grid processes in climate models',\n",
       "  'abstract': 'The representation of nonlinear sub-grid processes, especially clouds, has\\nbeen a major source of uncertainty in climate models for decades.\\nCloud-resolving models better represent many of these processes and can now be\\nrun globally but only for short-term simulations of at most a few years because\\nof computational limitations. Here we demonstrate that deep learning can be\\nused to capture many advantages of cloud-resolving modeling at a fraction of\\nthe computational cost. We train a deep neural network to represent all\\natmospheric sub-grid processes in a climate model by learning from a\\nmulti-scale model in which convection is treated explicitly. The trained neural\\nnetwork then replaces the traditional sub-grid parameterizations in a global\\ngeneral circulation model in which it freely interacts with the resolved\\ndynamics and the surface-flux scheme. The prognostic multi-year simulations are\\nstable and closely reproduce not only the mean climate of the cloud-resolving\\nsimulation but also key aspects of variability, including precipitation\\nextremes and the equatorial wave spectrum. Furthermore, the neural network\\napproximately conserves energy despite not being explicitly instructed to.\\nFinally, we show that the neural network parameterization generalizes to new\\nsurface forcing patterns but struggles to cope with temperatures far outside\\nits training manifold. Our results show the feasibility of using deep learning\\nfor climate model parameterization. In a broader context, we anticipate that\\ndata-driven Earth System Model development could play a key role in reducing\\nclimate prediction uncertainty in the coming decade.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04731v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04731v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Stephan Rasp', 'Michael S. Pritchard', 'Pierre Gentine'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/grounded-textual-entailment',\n",
       "  'arxiv_id': '1806.05645',\n",
       "  'title': 'Grounded Textual Entailment',\n",
       "  'abstract': 'Capturing semantic relations between sentences, such as entailment, is a\\nlong-standing challenge for computational semantics. Logic-based models analyse\\nentailment in terms of possible worlds (interpretations, or situations) where a\\npremise P entails a hypothesis H iff in all worlds where P is true, H is also\\ntrue. Statistical models view this relationship probabilistically, addressing\\nit in terms of whether a human would likely infer H from P. In this paper, we\\nwish to bridge these two perspectives, by arguing for a visually-grounded\\nversion of the Textual Entailment task. Specifically, we ask whether models can\\nperform better if, in addition to P and H, there is also an image\\n(corresponding to the relevant \"world\" or \"situation\"). We use a multimodal\\nversion of the SNLI dataset (Bowman et al., 2015) and we compare \"blind\" and\\nvisually-augmented models of textual entailment. We show that visual\\ninformation is beneficial, but we also conduct an in-depth error analysis that\\nreveals that current multimodal models are not performing \"grounding\" in an\\noptimal fashion.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05645v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05645v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Hoa Trong Vu',\n",
       "   'Claudio Greco',\n",
       "   'Aliia Erofeeva',\n",
       "   'Somayeh Jafaritazehjan',\n",
       "   'Guido Linders',\n",
       "   'Marc Tanti',\n",
       "   'Alberto Testoni',\n",
       "   'Raffaella Bernardi',\n",
       "   'Albert Gatt'],\n",
       "  'tasks': ['Natural Language Inference'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['penn-treebank',\n",
       "   'visual-question-answering',\n",
       "   'snli',\n",
       "   'visual-genome',\n",
       "   'flickr30k',\n",
       "   'sick'],\n",
       "  'datasets_used_full': ['Penn Treebank',\n",
       "   'Visual Question Answering',\n",
       "   'SNLI',\n",
       "   'Visual Genome',\n",
       "   'Flickr30k',\n",
       "   'SICK'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/evaluation-of-unsupervised-compositional',\n",
       "  'arxiv_id': '1806.04713',\n",
       "  'title': 'Evaluation of Unsupervised Compositional Representations',\n",
       "  'abstract': 'We evaluated various compositional models, from bag-of-words representations\\nto compositional RNN-based models, on several extrinsic supervised and\\nunsupervised evaluation benchmarks. Our results confirm that weighted vector\\naveraging can outperform context-sensitive models in most benchmarks, but\\nstructural features encoded in RNN models can also be useful in certain\\nclassification tasks. We analyzed some of the evaluation datasets to identify\\nthe aspects of meaning they measure and the characteristics of the various\\nmodels that explain their performance variance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04713v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04713v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Hanan Aldarmaki', 'Mona Diab'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imdb-movie-reviews', 'mpqa-opinion-corpus', 'sick'],\n",
       "  'datasets_used_full': ['IMDb Movie Reviews', 'MPQA Opinion Corpus', 'SICK'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/self-imitation-learning',\n",
       "  'arxiv_id': '1806.05635',\n",
       "  'title': 'Self-Imitation Learning',\n",
       "  'abstract': \"This paper proposes Self-Imitation Learning (SIL), a simple off-policy\\nactor-critic algorithm that learns to reproduce the agent's past good\\ndecisions. This algorithm is designed to verify our hypothesis that exploiting\\npast good experiences can indirectly drive deep exploration. Our empirical\\nresults show that SIL significantly improves advantage actor-critic (A2C) on\\nseveral hard exploration Atari games and is competitive to the state-of-the-art\\ncount-based exploration methods. We also show that SIL improves proximal policy\\noptimization (PPO) on MuJoCo tasks.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05635v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05635v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Junhyuk Oh', 'Yijie Guo', 'Satinder Singh', 'Honglak Lee'],\n",
       "  'tasks': ['Atari Games', 'Imitation Learning'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['openai-gym',\n",
       "   'mujoco',\n",
       "   'arcade-learning-environment'],\n",
       "  'datasets_used_full': ['OpenAI Gym',\n",
       "   'MuJoCo',\n",
       "   'Arcade Learning Environment'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/teaching-multiple-concepts-to-a-forgetful',\n",
       "  'arxiv_id': '1805.08322',\n",
       "  'title': 'Teaching Multiple Concepts to a Forgetful Learner',\n",
       "  'abstract': \"How can we help a forgetful learner learn multiple concepts within a limited time frame? While there have been extensive studies in designing optimal schedules for teaching a single concept given a learner's memory model, existing approaches for teaching multiple concepts are typically based on heuristic scheduling techniques without theoretical guarantees. In this paper, we look at the problem from the perspective of discrete optimization and introduce a novel algorithmic framework for teaching multiple concepts with strong performance guarantees. Our framework is both generic, allowing the design of teaching schedules for different memory models, and also interactive, allowing the teacher to adapt the schedule to the underlying forgetting mechanisms of the learner. Furthermore, for a well-known memory model, we are able to identify a regime of model parameters where our framework is guaranteed to achieve high performance. We perform extensive evaluations using simulations along with real user studies in two concrete applications: (i) an educational app for online vocabulary teaching; and (ii) an app for teaching novices how to recognize animal species from images. Our results demonstrate the effectiveness of our algorithm compared to popular heuristic approaches.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1805.08322v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1805.08322v4.pdf',\n",
       "  'proceeding': 'NeurIPS 2019 12',\n",
       "  'authors': ['Anette Hunziker',\n",
       "   'Yuxin Chen',\n",
       "   'Oisin Mac Aodha',\n",
       "   'Manuel Gomez Rodriguez',\n",
       "   'Andreas Krause',\n",
       "   'Pietro Perona',\n",
       "   'Yisong Yue',\n",
       "   'Adish Singla'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-21',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-in-pomdps-with-monte-carlo-tree',\n",
       "  'arxiv_id': '1806.05631',\n",
       "  'title': 'Learning in POMDPs with Monte Carlo Tree Search',\n",
       "  'abstract': 'The POMDP is a powerful framework for reasoning under outcome and information\\nuncertainty, but constructing an accurate POMDP model is difficult.\\nBayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs)\\nextend POMDPs to allow the model to be learned during execution. BA-POMDPs are\\na Bayesian RL approach that, in principle, allows for an optimal trade-off\\nbetween exploitation and exploration. Unfortunately, BA-POMDPs are currently\\nimpractical to solve for any non-trivial domain. In this paper, we extend the\\nMonte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting\\nmethod, which we call BA-POMCP, is able to tackle problems that previous\\nsolution methods have been unable to solve. Additionally, we introduce several\\ntechniques that exploit the BA-POMDP structure to improve the efficiency of\\nBA-POMCP along with proof of their convergence.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05631v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05631v1.pdf',\n",
       "  'proceeding': 'ICML 2017 8',\n",
       "  'authors': ['Sammie Katt', 'Frans A. Oliehoek', 'Christopher Amato'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/voxceleb2-deep-speaker-recognition',\n",
       "  'arxiv_id': '1806.05622',\n",
       "  'title': 'VoxCeleb2: Deep Speaker Recognition',\n",
       "  'abstract': 'The objective of this paper is speaker recognition under noisy and\\nunconstrained conditions.\\n  We make two key contributions. First, we introduce a very large-scale\\naudio-visual speaker recognition dataset collected from open-source media.\\nUsing a fully automated pipeline, we curate VoxCeleb2 which contains over a\\nmillion utterances from over 6,000 speakers. This is several times larger than\\nany publicly available speaker recognition dataset.\\n  Second, we develop and compare Convolutional Neural Network (CNN) models and\\ntraining strategies that can effectively recognise identities from voice under\\nvarious conditions. The models trained on the VoxCeleb2 dataset surpass the\\nperformance of previous works on a benchmark dataset by a significant margin.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05622v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05622v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Joon Son Chung', 'Arsha Nagrani', 'Andrew Zisserman'],\n",
       "  'tasks': ['Speaker Recognition', 'Speaker Verification'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['vggface2-1'],\n",
       "  'datasets_used_full': ['VGGFace2'],\n",
       "  'datasets_introduced_lower': ['voxceleb2'],\n",
       "  'datasets_introduced_full': ['VoxCeleb2']},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dynaslam-tracking-mapping-and-inpainting-in',\n",
       "  'arxiv_id': '1806.05620',\n",
       "  'title': 'DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes',\n",
       "  'abstract': 'The assumption of scene rigidity is typical in SLAM algorithms. Such a strong\\nassumption limits the use of most visual SLAM systems in populated real-world\\nenvironments, which are the target of several relevant applications like\\nservice robotics or autonomous vehicles. In this paper we present DynaSLAM, a\\nvisual SLAM system that, building over ORB-SLAM2 [1], adds the capabilities of\\ndynamic object detection and background inpainting. DynaSLAM is robust in\\ndynamic scenarios for monocular, stereo and RGB-D configurations. We are\\ncapable of detecting the moving objects either by multi-view geometry, deep\\nlearning or both. Having a static map of the scene allows inpainting the frame\\nbackground that has been occluded by such dynamic objects. We evaluate our\\nsystem in public monocular, stereo and RGB-D datasets. We study the impact of\\nseveral accuracy/speed trade-offs to assess the limits of the proposed\\nmethodology. DynaSLAM outperforms the accuracy of standard visual SLAM\\nbaselines in highly dynamic scenarios. And it also estimates a map of the\\nstatic parts of the scene, which is a must for long-term applications in\\nreal-world environments.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05620v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05620v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Berta Bescos', 'José M. Fácil', 'Javier Civera', 'José Neira'],\n",
       "  'tasks': ['Autonomous Vehicles', 'Object Detection'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'ORB-SLAM2',\n",
       "    'full_name': 'ORB-Simultaneous localization and mapping',\n",
       "    'description': 'ORB-SLAM2 is a complete SLAM system for monocular, stereo and RGB-D cameras, including map reuse, loop closing and relocalization capabilities. The system works in real-time on standard CPUs in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city.\\r\\n\\r\\nSource: [Mur-Artal and Tardos](https://arxiv.org/pdf/1610.06475v2.pdf)\\r\\n\\r\\nImage source: [Mur-Artal and Tardos](https://arxiv.org/pdf/1610.06475v2.pdf)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1610.06475v2',\n",
       "    'source_title': 'ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras',\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Localization Models',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/stochastic-variance-reduced-policy-gradient',\n",
       "  'arxiv_id': '1806.05618',\n",
       "  'title': 'Stochastic Variance-Reduced Policy Gradient',\n",
       "  'abstract': 'In this paper, we propose a novel reinforcement- learning algorithm\\nconsisting in a stochastic variance-reduced version of policy gradient for\\nsolving Markov Decision Processes (MDPs). Stochastic variance-reduced gradient\\n(SVRG) methods have proven to be very successful in supervised learning.\\nHowever, their adaptation to policy gradient is not straightforward and needs\\nto account for I) a non-concave objective func- tion; II) approximations in the\\nfull gradient com- putation; and III) a non-stationary sampling pro- cess. The\\nresult is SVRPG, a stochastic variance- reduced policy gradient algorithm that\\nleverages on importance weights to preserve the unbiased- ness of the gradient\\nestimate. Under standard as- sumptions on the MDP, we provide convergence\\nguarantees for SVRPG with a convergence rate that is linear under increasing\\nbatch sizes. Finally, we suggest practical variants of SVRPG, and we\\nempirically evaluate them on continuous MDPs.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05618v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05618v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Matteo Papini',\n",
       "   'Damiano Binaghi',\n",
       "   'Giuseppe Canonaco',\n",
       "   'Matteo Pirotta',\n",
       "   'Marcello Restelli'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/from-self-ception-to-image-self-ception-a',\n",
       "  'arxiv_id': '1806.05610',\n",
       "  'title': 'From Self-ception to Image Self-ception: A method to represent an image with its own approximations',\n",
       "  'abstract': \"A concept of defining images based on its own approximate ones is proposed\\nhere, which is called 'Self-ception'. In this regard, an algorithm is proposed\\nto implement the self-ception for images, which we call it 'Image Self-ception'\\nsince we use it for images. We can control the accuracy of this self-ception\\nrepresentation by deciding how many segments or regions we want to use for the\\nrepresentation. Some self-ception images are included in the paper. The video\\nversions of the proposed image self-ception algorithm in action are shown in a\\nYouTube channel (find it by Googling image self-ception).\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05610v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05610v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hamed Shah-Hosseini'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/gender-prediction-in-english-hindi-code-mixed',\n",
       "  'arxiv_id': '1806.05600',\n",
       "  'title': 'Gender Prediction in English-Hindi Code-Mixed Social Media Content : Corpus and Baseline System',\n",
       "  'abstract': \"The rapid expansion in the usage of social media networking sites leads to a\\nhuge amount of unprocessed user generated data which can be used for text\\nmining. Author profiling is the problem of automatically determining profiling\\naspects like the author's gender and age group through a text is gaining much\\npopularity in computational linguistics. Most of the past research in author\\nprofiling is concentrated on English texts \\\\cite{1,2}. However many users often\\nchange the language while posting on social media which is called code-mixing,\\nand it develops some challenges in the field of text classification and author\\nprofiling like variations in spelling, non-grammatical structure and\\ntransliteration \\\\cite{3}. There are very few English-Hindi code-mixed annotated\\ndatasets of social media content present online \\\\cite{4}. In this paper, we\\nanalyze the task of author's gender prediction in code-mixed content and\\npresent a corpus of English-Hindi texts collected from Twitter which is\\nannotated with author's gender. We also explore language identification of\\nevery word in this corpus. We present a supervised classification baseline\\nsystem which uses various machine learning algorithms to identify the gender of\\nan author using a text, based on character and word level features.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05600v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05600v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ankush Khandelwal',\n",
       "   'Sahil Swami',\n",
       "   'Syed Sarfaraz Akhtar',\n",
       "   'Manish Shrivastava'],\n",
       "  'tasks': ['Gender Prediction',\n",
       "   'General Classification',\n",
       "   'Language Identification',\n",
       "   'Text Classification',\n",
       "   'Transliteration'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-survey-on-open-information-extraction',\n",
       "  'arxiv_id': '1806.05599',\n",
       "  'title': 'A Survey on Open Information Extraction',\n",
       "  'abstract': 'We provide a detailed overview of the various approaches that were proposed\\nto date to solve the task of Open Information Extraction. We present the major\\nchallenges that such systems face, show the evolution of the suggested\\napproaches over time and depict the specific issues they address. In addition,\\nwe provide a critique of the commonly applied evaluation procedures for\\nassessing the performance of Open IE systems and highlight some directions for\\nfuture work.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05599v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05599v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Christina Niklaus',\n",
       "   'Matthias Cetto',\n",
       "   'André Freitas',\n",
       "   'Siegfried Handschuh'],\n",
       "  'tasks': ['Open Information Extraction'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/there-are-many-consistent-explanations-of',\n",
       "  'arxiv_id': '1806.05594',\n",
       "  'title': 'There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average',\n",
       "  'abstract': 'Presently the most successful approaches to semi-supervised learning are\\nbased on consistency regularization, whereby a model is trained to be robust to\\nsmall perturbations of its inputs and parameters. To understand consistency\\nregularization, we conceptually explore how loss geometry interacts with\\ntraining procedures. The consistency loss dramatically improves generalization\\nperformance over supervised-only training; however, we show that SGD struggles\\nto converge on the consistency loss and continues to make large steps that lead\\nto changes in predictions on the test data. Motivated by these observations, we\\npropose to train consistency-based methods with Stochastic Weight Averaging\\n(SWA), a recent approach which averages weights along the trajectory of SGD\\nwith a modified learning rate schedule. We also propose fast-SWA, which further\\naccelerates convergence by averaging multiple points within each cycle of a\\ncyclical learning rate schedule. With weight averaging, we achieve the best\\nknown semi-supervised results on CIFAR-10 and CIFAR-100, over many different\\nquantities of labeled training data. For example, we achieve 5.0% error on\\nCIFAR-10 with only 4000 labels, compared to the previous best result in the\\nliterature of 6.3%.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05594v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05594v3.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Ben Athiwaratkun',\n",
       "   'Marc Finzi',\n",
       "   'Pavel Izmailov',\n",
       "   'Andrew Gordon Wilson'],\n",
       "  'tasks': ['Domain Adaptation', 'Semi-Supervised Image Classification'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'SGD',\n",
       "    'full_name': 'Stochastic Gradient Descent',\n",
       "    'description': '**Stochastic Gradient Descent** is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:\\r\\n\\r\\n$$ w\\\\_{t+1} = w\\\\_{t} - \\\\eta\\\\hat{\\\\nabla}\\\\_{w}{L(w\\\\_{t})} $$\\r\\n\\r\\nWhere $\\\\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.\\r\\n\\r\\n(Image Source: [here](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/))',\n",
       "    'introduced_year': 1951,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/4e0ac120e9a8b096069c2f892488d630a5c8f358/torch/optim/sgd.py#L97-L112',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['cifar-10', 'cifar-100', 'tiny-images'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'CIFAR-100', 'Tiny Images'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/sparsely-grouped-multi-task-generative',\n",
       "  'arxiv_id': '1805.07509',\n",
       "  'title': 'Sparsely Grouped Multi-task Generative Adversarial Networks for Facial Attribute Manipulation',\n",
       "  'abstract': 'Recent Image-to-Image Translation algorithms have achieved significant progress in neural style transfer and image attribute manipulation tasks. However, existing approaches require exhaustively labelling training data, which is labor demanding, difficult to scale up, and hard to migrate into new domains. To overcome such a key limitation, we propose Sparsely Grouped Generative Adversarial Networks (SG-GAN) as a novel approach that can translate images on sparsely grouped datasets where only a few samples for training are labelled. Using a novel one-input multi-output architecture, SG-GAN is well-suited for tackling sparsely grouped learning and multi-task learning. The proposed model can translate images among multiple groups using only a single commonly trained model. To experimentally validate advantages of the new model, we apply the proposed method to tackle a series of attribute manipulation tasks for facial images. Experimental results demonstrate that SG-GAN can generate image translation results of comparable quality with baselines methods on adequately labelled datasets and results of superior quality on sparsely grouped datasets. The official implementation is publicly available:https://github.com/zhangqianhui/Sparsely-Grouped-GAN.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1805.07509v7',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1805.07509v7.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jichao Zhang',\n",
       "   'Yezhi Shu',\n",
       "   'Songhua Xu',\n",
       "   'Gongze Cao',\n",
       "   'Fan Zhong',\n",
       "   'Meng Liu',\n",
       "   'Xueying Qin'],\n",
       "  'tasks': ['Image-to-Image Translation',\n",
       "   'Multi-Task Learning',\n",
       "   'Style Transfer',\n",
       "   'Translation'],\n",
       "  'date': '2018-05-19',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet', 'celeba'],\n",
       "  'datasets_used_full': ['ImageNet', 'CelebA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/tract-orientation-mapping-for-bundle-specific',\n",
       "  'arxiv_id': '1806.05580',\n",
       "  'title': 'Tract orientation mapping for bundle-specific tractography',\n",
       "  'abstract': 'While the major white matter tracts are of great interest to numerous studies\\nin neuroscience and medicine, their manual dissection in larger cohorts from\\ndiffusion MRI tractograms is time-consuming, requires expert knowledge and is\\nhard to reproduce. Tract orientation mapping (TOM) is a novel concept that\\nfacilitates bundle-specific tractography based on a learned mapping from the\\noriginal fiber orientation distribution function (fODF) peaks to a list of\\ntract orientation maps (also abbr. TOM). Each TOM represents one of the known\\ntracts with each voxel containing no more than one orientation vector. TOMs can\\nact as a prior or even as direct input for tractography. We use an\\nencoder-decoder fully-convolutional neural network architecture to learn the\\nrequired mapping. In comparison to previous concepts for the reconstruction of\\nspecific bundles, the presented one avoids various cumbersome processing steps\\nlike whole brain tractography, atlas registration or clustering. We compare it\\nto four state of the art bundle recognition methods on 20 different bundles in\\na total of 105 subjects from the Human Connectome Project. Results are\\nanatomically convincing even for difficult tracts, while reaching low angular\\nerrors, unprecedented runtimes and top accuracy values (Dice). Our code and our\\ndata are openly available.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05580v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05580v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jakob Wasserthal', 'Peter F. Neher', 'Klaus H. Maier-Hein'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/lap-a-linearize-and-project-method-for',\n",
       "  'arxiv_id': '1705.09992',\n",
       "  'title': 'LAP: a Linearize and Project Method for Solving Inverse Problems with Coupled Variables',\n",
       "  'abstract': 'Many inverse problems involve two or more sets of variables that represent\\ndifferent physical quantities but are tightly coupled with each other. For\\nexample, image super-resolution requires joint estimation of the image and\\nmotion parameters from noisy measurements. Exploiting this structure is key for\\nefficiently solving these large-scale optimization problems, which are often\\nill-conditioned.\\n  In this paper, we present a new method called Linearize And Project (LAP)\\nthat offers a flexible framework for solving inverse problems with coupled\\nvariables. LAP is most promising for cases when the subproblem corresponding to\\none of the variables is considerably easier to solve than the other. LAP is\\nbased on a Gauss-Newton method, and thus after linearizing the residual, it\\neliminates one block of variables through projection. Due to the linearization,\\nthis block can be chosen freely. Further, LAP supports direct, iterative, and\\nhybrid regularization as well as constraints. Therefore LAP is attractive,\\ne.g., for ill-posed imaging problems. These traits differentiate LAP from\\ncommon alternatives for this type of problem such as variable projection\\n(VarPro) and block coordinate descent (BCD). Our numerical experiments compare\\nthe performance of LAP to BCD and VarPro using three coupled problems whose\\nforward operators are linear with respect to one block and nonlinear for the\\nother set of variables.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1705.09992v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1705.09992v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['James Herring', 'James Nagy', 'Lars Ruthotto'],\n",
       "  'tasks': ['Image Super-Resolution', 'Super-Resolution'],\n",
       "  'date': '2017-05-28',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fruit-recognition-from-images-using-deep',\n",
       "  'arxiv_id': '1712.00580',\n",
       "  'title': 'Fruit recognition from images using deep learning',\n",
       "  'abstract': 'In this paper we introduce a new, high-quality, dataset of images containing\\nfruits. We also present the results of some numerical experiment for training a\\nneural network to detect fruits. We discuss the reason why we chose to use\\nfruits in this project by proposing a few applications that could use this kind\\nof neural network.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.00580v9',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.00580v9.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Horea Mureşan', 'Mihai Oltean'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-12-02',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['fruits-360-1'],\n",
       "  'datasets_used_full': ['Fruits 360'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/autoregressive-quantile-networks-for',\n",
       "  'arxiv_id': '1806.05575',\n",
       "  'title': 'Autoregressive Quantile Networks for Generative Modeling',\n",
       "  'abstract': 'We introduce autoregressive implicit quantile networks (AIQN), a\\nfundamentally different approach to generative modeling than those commonly\\nused, that implicitly captures the distribution using quantile regression. AIQN\\nis able to achieve superior perceptual quality and improvements in evaluation\\nmetrics, without incurring a loss of sample diversity. The method can be\\napplied to many existing models and architectures. In this work we extend the\\nPixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using\\nInception score, FID, non-cherry-picked samples, and inpainting results. We\\nconsistently observe that AIQN yields a highly stable algorithm that improves\\nperceptual quality while maintaining a highly diverse distribution.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05575v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05575v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Georg Ostrovski', 'Will Dabney', 'Rémi Munos'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10'],\n",
       "  'datasets_used_full': ['CIFAR-10'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/weakly-supervised-learning-for-tool',\n",
       "  'arxiv_id': '1806.05573',\n",
       "  'title': 'Weakly-Supervised Learning for Tool Localization in Laparoscopic Videos',\n",
       "  'abstract': 'Surgical tool localization is an essential task for the automatic analysis of\\nendoscopic videos. In the literature, existing methods for tool localization,\\ntracking and segmentation require training data that is fully annotated,\\nthereby limiting the size of the datasets that can be used and the\\ngeneralization of the approaches. In this work, we propose to circumvent the\\nlack of annotated data with weak supervision. We propose a deep architecture,\\ntrained solely on image level annotations, that can be used for both tool\\npresence detection and localization in surgical videos. Our architecture relies\\non a fully convolutional neural network, trained end-to-end, enabling us to\\nlocalize surgical tools without explicit spatial annotations. We demonstrate\\nthe benefits of our approach on a large public dataset, Cholec80, which is\\nfully annotated with binary tool presence information and of which 5 videos\\nhave been fully annotated with bounding boxes and tool centers for the\\nevaluation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05573v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05573v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Armine Vardazaryan',\n",
       "   'Didier Mutter',\n",
       "   'Jacques Marescaux',\n",
       "   'Nicolas Padoy'],\n",
       "  'tasks': ['Surgical tool detection'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cholec80'],\n",
       "  'datasets_used_full': ['Cholec80'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/direct-automated-quantitative-measurement-of',\n",
       "  'arxiv_id': '1806.05570',\n",
       "  'title': 'Direct Automated Quantitative Measurement of Spine via Cascade Amplifier Regression Network',\n",
       "  'abstract': 'Automated quantitative measurement of the spine (i.e., multiple indices\\nestimation of heights, widths, areas, and so on for the vertebral body and\\ndisc) is of the utmost importance in clinical spinal disease diagnoses, such as\\nosteoporosis, intervertebral disc degeneration, and lumbar disc herniation, yet\\nstill an unprecedented challenge due to the variety of spine structure and the\\nhigh dimensionality of indices to be estimated. In this paper, we propose a\\nnovel cascade amplifier regression network (CARN), which includes the CARN\\narchitecture and local shape-constrained manifold regularization (LSCMR) loss\\nfunction, to achieve accurate direct automated multiple indices estimation. The\\nCARN architecture is composed of a cascade amplifier network (CAN) for\\nexpressive feature embedding and a linear regression model for multiple indices\\nestimation. The CAN consists of cascade amplifier units (AUs), which are used\\nfor selective feature reuse by stimulating effective feature and suppressing\\nredundant feature during propagating feature map between adjacent layers, thus\\nan expressive feature embedding is obtained. During training, the LSCMR is\\nutilized to alleviate overfitting and generate realistic estimation by learning\\nthe multiple indices distribution. Experiments on MR images of 195 subjects\\nshow that the proposed CARN achieves impressive performance with mean absolute\\nerrors of 1.2496 mm, 1.2887 mm, and 1.2692 mm for estimation of 15 heights of\\ndiscs, 15 heights of vertebral bodies, and total indices respectively. The\\nproposed method has great potential in clinical spinal disease diagnoses.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05570v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05570v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shumao Pang',\n",
       "   'Stephanie Leung',\n",
       "   'Ilanit Ben Nachum',\n",
       "   'Qianjin Feng',\n",
       "   'Shuo Li'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'Linear Regression',\n",
       "    'full_name': 'Linear Regression',\n",
       "    'description': '**Linear Regression** is a method for modelling a relationship between a dependent variable and independent variables. These models can be fit with numerous approaches. The most common is *least squares*, where we minimize the mean square error between the predicted values $\\\\hat{y} = \\\\textbf{X}\\\\hat{\\\\beta}$ and actual values $y$: $\\\\left(y-\\\\textbf{X}\\\\beta\\\\right)^{2}$.\\r\\n\\r\\nWe can also define the problem in probabilistic terms as a generalized linear model (GLM) where the pdf is a Gaussian distribution, and then perform maximum likelihood estimation to estimate $\\\\hat{\\\\beta}$.\\r\\n\\r\\nImage Source: [Wikipedia](https://en.wikipedia.org/wiki/Linear_regression)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Generalized Linear Models',\n",
       "     'description': '**Generalized Linear Models (GLMs)** are a class of models that generalize upon linear regression by allowing many more distributions to be modeled for the response variable via a link function. Below you can find a continuously updating list of GLMs.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/cardiac-motion-scoring-with-segment-and',\n",
       "  'arxiv_id': '1806.05569',\n",
       "  'title': 'Cardiac Motion Scoring with Segment- and Subject-level Non-Local Modeling',\n",
       "  'abstract': 'Motion scoring of cardiac myocardium is of paramount importance for early\\ndetection and diagnosis of various cardiac disease. It aims at identifying\\nregional wall motions into one of the four types including normal, hypokinetic,\\nakinetic, and dyskinetic, and is extremely challenging due to the complex\\nmyocardium deformation and subtle inter-class difference of motion patterns.\\nAll existing work on automated motion analysis are focused on binary\\nabnormality detection to avoid the much more demanding motion scoring, which is\\nurgently required in real clinical practice yet has never been investigated\\nbefore. In this work, we propose Cardiac-MOS, the first powerful method for\\ncardiac motion scoring from cardiac MR sequences based on deep convolution\\nneural network. Due to the locality of convolution, the relationship between\\ndistant non-local responses of the feature map cannot be explored, which is\\nclosely related to motion difference between segments. In Cardiac-MOS, such\\nnon-local relationship is modeled with non-local neural network within each\\nsegment and across all segments of one subject, i.e., segment- and\\nsubject-level non-local modeling, and lead to obvious performance improvement.\\nBesides, Cardiac-MOS can effectively extract motion information from MR\\nsequences of various lengths by interpolating the convolution kernel along the\\ntemporal dimension, therefore can be applied to MR sequences of multiple\\nsources. Experiments on 1440 myocardium segments of 90 subjects from short axis\\nMR sequences of multiple lengths prove that Cardiac-MOS achieves reliable\\nperformance, with correlation of 0.926 for motion score index estimation and\\naccuracy of 77.4\\\\% for motion scoring. Cardiac-MOS also outperforms all\\nexisting work for binary abnormality detection. As the first automatic motion\\nscoring solution, Cardiac-MOS demonstrates great potential in future clinical\\napplication.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05569v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05569v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Wufeng Xue',\n",
       "   'Gary Brahm',\n",
       "   'Stephanie Leung',\n",
       "   'Ogla Shmuilovich',\n",
       "   'Shuo Li'],\n",
       "  'tasks': ['Anomaly Detection'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/adaptive-shooting-for-bots-in-first-person',\n",
       "  'arxiv_id': '1806.05554',\n",
       "  'title': 'Adaptive Shooting for Bots in First Person Shooter Games Using Reinforcement Learning',\n",
       "  'abstract': 'In current state-of-the-art commercial first person shooter games, computer\\ncontrolled bots, also known as non player characters, can often be easily\\ndistinguishable from those controlled by humans. Tell-tale signs such as failed\\nnavigation, \"sixth sense\" knowledge of human players\\' whereabouts and\\ndeterministic, scripted behaviors are some of the causes of this. We propose,\\nhowever, that one of the biggest indicators of non humanlike behavior in these\\ngames can be found in the weapon shooting capability of the bot. Consistently\\nperfect accuracy and \"locking on\" to opponents in their visual field from any\\ndistance are indicative capabilities of bots that are not found in human\\nplayers. Traditionally, the bot is handicapped in some way with either a timed\\nreaction delay or a random perturbation to its aim, which doesn\\'t adapt or\\nimprove its technique over time. We hypothesize that enabling the bot to learn\\nthe skill of shooting through trial and error, in the same way a human player\\nlearns, will lead to greater variation in game-play and produce less\\npredictable non player characters. This paper describes a reinforcement\\nlearning shooting mechanism for adapting shooting over time based on a dynamic\\nreward signal from the amount of damage caused to opponents.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05554v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05554v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Frank G. Glavin', 'Michael G. Madden'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/apple-picker-automatic-particle-picking-a-low',\n",
       "  'arxiv_id': '1802.00469',\n",
       "  'title': 'APPLE Picker: Automatic Particle Picking, a Low-Effort Cryo-EM Framework',\n",
       "  'abstract': 'Particle picking is a crucial first step in the computational pipeline of\\nsingle-particle cryo-electron microscopy (cryo-EM). Selecting particles from\\nthe micrographs is difficult especially for small particles with low contrast.\\nAs high-resolution reconstruction typically requires hundreds of thousands of\\nparticles, manually picking that many particles is often too time-consuming.\\nWhile semi-automated particle picking is currently a popular approach, it may\\nsuffer from introducing manual bias into the selection process. In addition,\\nsemi-automated particle picking is still somewhat time-consuming. This paper\\npresents the APPLE (Automatic Particle Picking with Low user Effort) picker, a\\nsimple and novel approach for fast, accurate, and fully automatic particle\\npicking. While our approach was inspired by template matching, it is completely\\ntemplate-free. This approach is evaluated on publicly available datasets\\ncontaining micrographs of $\\\\beta$-galactosidase and keyhole limpet hemocyanin\\nprojections.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.00469v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.00469v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ayelet Heimowitz', 'Joakim andén', 'Amit Singer'],\n",
       "  'tasks': ['Template Matching'],\n",
       "  'date': '2018-02-01',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/correlation-tracking-via-robust-region',\n",
       "  'arxiv_id': '1806.05530',\n",
       "  'title': 'Correlation Tracking via Robust Region Proposals',\n",
       "  'abstract': 'Recently, correlation filter-based trackers have received extensive attention\\ndue to their simplicity and superior speed. However, such trackers perform\\npoorly when the target undergoes occlusion, viewpoint change or other\\nchallenging attributes due to pre-defined sampling strategy. To tackle these\\nissues, in this paper, we propose an adaptive region proposal scheme to\\nfacilitate visual tracking. To be more specific, a novel tracking monitoring\\nindicator is advocated to forecast tracking failure. Afterwards, we incorporate\\ndetection and scale proposals respectively, to recover from model drift as well\\nas handle aspect ratio variation. We test the proposed algorithm on several\\nchallenging sequences, which have demonstrated that the proposed tracker\\nperforms favourably against state-of-the-art trackers.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05530v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05530v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yuqi Han',\n",
       "   'Jinghong Nan',\n",
       "   'Zengshuo Zhang',\n",
       "   'Jingjing Wang',\n",
       "   'Baojun Zhao'],\n",
       "  'tasks': ['Region Proposal', 'Visual Tracking'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/variational-message-passing-with-structured',\n",
       "  'arxiv_id': '1803.05589',\n",
       "  'title': 'Variational Message Passing with Structured Inference Networks',\n",
       "  'abstract': 'Recent efforts on combining deep models with probabilistic graphical models\\nare promising in providing flexible models that are also easy to interpret. We\\npropose a variational message-passing algorithm for variational inference in\\nsuch models. We make three contributions. First, we propose structured\\ninference networks that incorporate the structure of the graphical model in the\\ninference network of variational auto-encoders (VAE). Second, we establish\\nconditions under which such inference networks enable fast amortized inference\\nsimilar to VAE. Finally, we derive a variational message passing algorithm to\\nperform efficient natural-gradient inference while retaining the efficiency of\\nthe amortized inference. By simultaneously enabling structured, amortized, and\\nnatural-gradient inference for deep structured models, our method simplifies\\nand generalizes existing methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.05589v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.05589v2.pdf',\n",
       "  'proceeding': 'ICLR 2018 1',\n",
       "  'authors': ['Wu Lin', 'Nicolas Hubacher', 'Mohammad Emtiyaz Khan'],\n",
       "  'tasks': ['Variational Inference'],\n",
       "  'date': '2018-03-15',\n",
       "  'methods': [{'name': 'VAE',\n",
       "    'full_name': 'Variational Autoencoder',\n",
       "    'description': 'A **Variational Autoencoder** is a type of likelihood-based generative model. It consists of an encoder, that takes in data $x$ as input and transforms this into a latent representation $z$,  and a decoder, that takes a latent representation $z$ and returns a reconstruction $\\\\hat{x}$. Inference is performed via variational inference to approximate the posterior of the model.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.6114v10',\n",
       "    'source_title': 'Auto-Encoding Variational Bayes',\n",
       "    'code_snippet_url': 'https://github.com/AntixK/PyTorch-VAE/blob/8700d245a9735640dda458db4cf40708caf2e77f/models/vanilla_vae.py#L8',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/el-gan-embedding-loss-driven-generative',\n",
       "  'arxiv_id': '1806.05525',\n",
       "  'title': 'EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane Detection',\n",
       "  'abstract': \"Convolutional neural networks have been successfully applied to semantic\\nsegmentation problems. However, there are many problems that are inherently not\\npixel-wise classification problems but are nevertheless frequently formulated\\nas semantic segmentation. This ill-posed formulation consequently necessitates\\nhand-crafted scenario-specific and computationally expensive post-processing\\nmethods to convert the per pixel probability maps to final desired outputs.\\nGenerative adversarial networks (GANs) can be used to make the semantic\\nsegmentation network output to be more realistic or better\\nstructure-preserving, decreasing the dependency on potentially complex\\npost-processing. In this work, we propose EL-GAN: a GAN framework to mitigate\\nthe discussed problem using an embedding loss. With EL-GAN, we discriminate\\nbased on learned embeddings of both the labels and the prediction at the same\\ntime. This results in more stable training due to having better discriminative\\ninformation, benefiting from seeing both `fake' and `real' predictions at the\\nsame time. This substantially stabilizes the adversarial training process. We\\nuse the TuSimple lane marking challenge to demonstrate that with our proposed\\nframework it is viable to overcome the inherent anomalies of posing it as a\\nsemantic segmentation problem. Not only is the output considerably more similar\\nto the labels when compared to conventional methods, the subsequent\\npost-processing is also simpler and crosses the competitive 96% accuracy\\nthreshold.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05525v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05525v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mohsen Ghafoorian',\n",
       "   'Cedric Nugteren',\n",
       "   'Nóra Baka',\n",
       "   'Olaf Booij',\n",
       "   'Michael Hofmann'],\n",
       "  'tasks': ['Lane Detection', 'Semantic Segmentation'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['tusimple'],\n",
       "  'datasets_used_full': ['TuSimple'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/improved-density-based-spatio-textual',\n",
       "  'arxiv_id': '1806.05522',\n",
       "  'title': 'Improved Density-Based Spatio--Textual Clustering on Social Media',\n",
       "  'abstract': 'DBSCAN may not be sufficient when the input data type is heterogeneous in\\nterms of textual description. When we aim to discover clusters of geo-tagged\\nrecords relevant to a particular point-of-interest (POI) on social media,\\nexamining only one type of input data (e.g., the tweets relevant to a POI) may\\ndraw an incomplete picture of clusters due to noisy regions. To overcome this\\nproblem, we introduce DBSTexC, a newly defined density-based clustering\\nalgorithm using spatio--textual information. We first characterize POI-relevant\\nand POI-irrelevant tweets as the texts that include and do not include a POI\\nname or its semantically coherent variations, respectively. By leveraging the\\nproportion of POI-relevant and POI-irrelevant tweets, the proposed algorithm\\ndemonstrates much higher clustering performance than the DBSCAN case in terms\\nof $\\\\mathcal{F}_1$ score and its variants. While DBSTexC performs exactly as\\nDBSCAN with the textually homogeneous inputs, it far outperforms DBSCAN with\\nthe textually heterogeneous inputs. Furthermore, to further improve the\\nclustering quality by fully capturing the geographic distribution of tweets, we\\npresent fuzzy DBSTexC (F-DBSTexC), an extension of DBSTexC, which incorporates\\nthe notion of fuzzy clustering into the DBSTexC. We then demonstrate the\\nrobustness of F-DBSTexC via intensive experiments. The computational complexity\\nof our algorithms is also analytically and numerically shown.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05522v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05522v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Minh D. Nguyen', 'Won-Yong Shin'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/semaxis-a-lightweight-framework-to',\n",
       "  'arxiv_id': '1806.05521',\n",
       "  'title': 'SemAxis: A Lightweight Framework to Characterize Domain-Specific Word Semantics Beyond Sentiment',\n",
       "  'abstract': 'Because word semantics can substantially change across communities and\\ncontexts, capturing domain-specific word semantics is an important challenge.\\nHere, we propose SEMAXIS, a simple yet powerful framework to characterize word\\nsemantics using many semantic axes in word- vector spaces beyond sentiment. We\\ndemonstrate that SEMAXIS can capture nuanced semantic representations in\\nmultiple online communities. We also show that, when the sentiment axis is\\nexamined, SEMAXIS outperforms the state-of-the-art approaches in building\\ndomain-specific sentiment lexicons.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05521v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05521v1.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Jisun An', 'Haewoon Kwak', 'Yong-Yeol Ahn'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['conceptnet'],\n",
       "  'datasets_used_full': ['ConceptNet'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/features-projections-and-representation',\n",
       "  'arxiv_id': '1801.10055',\n",
       "  'title': 'Features, Projections, and Representation Change for Generalized Planning',\n",
       "  'abstract': 'Generalized planning is concerned with the characterization and computation\\nof plans that solve many instances at once. In the standard formulation, a\\ngeneralized plan is a mapping from feature or observation histories into\\nactions, assuming that the instances share a common pool of features and\\nactions. This assumption, however, excludes the standard relational planning\\ndomains where actions and objects change across instances. In this work, we\\nextend the standard formulation of generalized planning to such domains. This\\nis achieved by projecting the actions over the features, resulting in a common\\nset of abstract actions which can be tested for soundness and completeness, and\\nwhich can be used for generating general policies such as \"if the gripper is\\nempty, pick the clear block above x and place it on the table\" that achieve the\\ngoal clear(x) in any Blocksworld instance. In this policy, \"pick the clear\\nblock above x\" is an abstract action that may represent the action Unstack(a,\\nb) in one situation and the action Unstack(b, c) in another. Transformations\\nare also introduced for computing such policies by means of fully observable\\nnon-deterministic (FOND) planners. The value of generalized representations for\\nlearning general policies is also discussed.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1801.10055v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1801.10055v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Blai Bonet', 'Hector Geffner'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-01-30',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/split-door-criterion-identification-of-causal',\n",
       "  'arxiv_id': '1611.09414',\n",
       "  'title': 'Split-door criterion: Identification of causal effects through auxiliary outcomes',\n",
       "  'abstract': \"We present a method for estimating causal effects in time series data when\\nfine-grained information about the outcome of interest is available.\\nSpecifically, we examine what we call the split-door setting, where the outcome\\nvariable can be split into two parts: one that is potentially affected by the\\ncause being studied and another that is independent of it, with both parts\\nsharing the same (unobserved) confounders. We show that under these conditions,\\nthe problem of identification reduces to that of testing for independence among\\nobserved variables, and present a method that uses this approach to\\nautomatically find subsets of the data that are causally identified. We\\ndemonstrate the method by estimating the causal impact of Amazon's recommender\\nsystem on traffic to product pages, finding thousands of examples within the\\ndataset that satisfy the split-door criterion. Unlike past studies based on\\nnatural experiments that were limited to a single product category, our method\\napplies to a large and representative sample of products viewed on the site. In\\nline with previous work, we find that the widely-used click-through rate (CTR)\\nmetric overestimates the causal impact of recommender systems; depending on the\\nproduct category, we estimate that 50-80\\\\% of the traffic attributed to\\nrecommender systems would have happened even without any recommendations. We\\nconclude with guidelines for using the split-door criterion as well as a\\ndiscussion of other contexts where the method can be applied.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1611.09414v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1611.09414v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Amit Sharma', 'Jake M. Hofman', 'Duncan J. Watts'],\n",
       "  'tasks': ['Recommendation Systems', 'Time Series'],\n",
       "  'date': '2016-11-28',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/real-time-cardiovascular-mr-with-spatio',\n",
       "  'arxiv_id': '1803.05192',\n",
       "  'title': 'Real-time Cardiovascular MR with Spatio-temporal Artifact Suppression using Deep Learning - Proof of Concept in Congenital Heart Disease',\n",
       "  'abstract': \"PURPOSE: Real-time assessment of ventricular volumes requires high\\nacceleration factors. Residual convolutional neural networks (CNN) have shown\\npotential for removing artifacts caused by data undersampling. In this study we\\ninvestigated the effect of different radial sampling patterns on the accuracy\\nof a CNN. We also acquired actual real-time undersampled radial data in\\npatients with congenital heart disease (CHD), and compare CNN reconstruction to\\nCompressed Sensing (CS).\\n  METHODS: A 3D (2D plus time) CNN architecture was developed, and trained\\nusing 2276 gold-standard paired 3D data sets, with 14x radial undersampling.\\nFour sampling schemes were tested, using 169 previously unseen 3D 'synthetic'\\ntest data sets. Actual real-time tiny Golden Angle (tGA) radial SSFP data was\\nacquired in 10 new patients (122 3D data sets), and reconstructed using the 3D\\nCNN as well as a CS algorithm; GRASP.\\n  RESULTS: Sampling pattern was shown to be important for image quality, and\\naccurate visualisation of cardiac structures. For actual real-time data,\\noverall reconstruction time with CNN (including creation of aliased images) was\\nshown to be more than 5x faster than GRASP. Additionally, CNN image quality and\\naccuracy of biventricular volumes was observed to be superior to GRASP for the\\nsame raw data.\\n  CONCLUSION: This paper has demonstrated the potential for the use of a 3D CNN\\nfor deep de-aliasing of real-time radial data, within the clinical setting.\\nClinical measures of ventricular volumes using real-time data with CNN\\nreconstruction are not statistically significantly different from the\\ngold-standard, cardiac gated, BH techniques.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.05192v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.05192v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Andreas Hauptmann',\n",
       "   'Simon Arridge',\n",
       "   'Felix Lucka',\n",
       "   'Vivek Muthurangu',\n",
       "   'Jennifer A. Steeden'],\n",
       "  'tasks': ['De-aliasing'],\n",
       "  'date': '2018-03-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/translations-as-additional-contexts-for',\n",
       "  'arxiv_id': '1806.05516',\n",
       "  'title': 'Translations as Additional Contexts for Sentence Classification',\n",
       "  'abstract': 'In sentence classification tasks, additional contexts, such as the\\nneighboring sentences, may improve the accuracy of the classifier. However,\\nsuch contexts are domain-dependent and thus cannot be used for another\\nclassification task with an inappropriate domain. In contrast, we propose the\\nuse of translated sentences as context that is always available regardless of\\nthe domain. We find that naive feature expansion of translations gains only\\nmarginal improvements and may decrease the performance of the classifier, due\\nto possible inaccurate translations thus producing noisy sentence vectors. To\\nthis end, we present multiple context fixing attachment (MCFA), a series of\\nmodules attached to multiple sentence vectors to fix the noise in the vectors\\nusing the other sentence vectors as context. We show that our method performs\\ncompetitively compared to previous models, achieving best classification\\nperformance on multiple data sets. We are the first to use translations as\\ndomain-free contexts for sentence classification.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05516v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05516v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Reinald Kim Amplayo',\n",
       "   'Kyungjae Lee',\n",
       "   'Jinyeong Yeo',\n",
       "   'Seung-won Hwang'],\n",
       "  'tasks': ['Classification',\n",
       "   'General Classification',\n",
       "   'Sentence Classification',\n",
       "   'Subjectivity Analysis',\n",
       "   'Text Classification'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['subj'],\n",
       "  'datasets_used_full': ['SUBJ'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-exact-equivalence-of-distance-and-kernel',\n",
       "  'arxiv_id': '1806.05514',\n",
       "  'title': 'The Exact Equivalence of Distance and Kernel Methods for Hypothesis Testing',\n",
       "  'abstract': 'Distance-based tests, also called \"energy statistics\", are leading methods for two-sample and independence tests from the statistics community. Kernel-based tests, developed from \"kernel mean embeddings\", are leading methods for two-sample and independence tests from the machine learning community. A fixed-point transformation was previously proposed to connect the distance methods and kernel methods for the population statistics. In this paper, we propose a new bijective transformation between metrics and kernels. It simplifies the fixed-point transformation, inherits similar theoretical properties, allows distance methods to be exactly the same as kernel methods for sample statistics and p-value, and better preserves the data structure upon transformation. Our results further advance the understanding in distance and kernel-based tests, streamline the code base for implementing these tests, and enable a rich literature of distance-based and kernel-based methodologies to directly communicate with each other.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05514v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05514v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Cencheng Shen', 'Joshua T. Vogelstein'],\n",
       "  'tasks': ['Two-sample testing'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/humor-detection-in-english-hindi-code-mixed',\n",
       "  'arxiv_id': '1806.05513',\n",
       "  'title': 'Humor Detection in English-Hindi Code-Mixed Social Media Content : Corpus and Baseline System',\n",
       "  'abstract': 'The tremendous amount of user generated data through social networking sites\\nled to the gaining popularity of automatic text classification in the field of\\ncomputational linguistics over the past decade. Within this domain, one problem\\nthat has drawn the attention of many researchers is automatic humor detection\\nin texts. In depth semantic understanding of the text is required to detect\\nhumor which makes the problem difficult to automate. With increase in the\\nnumber of social media users, many multilingual speakers often interchange\\nbetween languages while posting on social media which is called code-mixing. It\\nintroduces some challenges in the field of linguistic analysis of social media\\ncontent (Barman et al., 2014), like spelling variations and non-grammatical\\nstructures in a sentence. Past researches include detecting puns in texts (Kao\\net al., 2016) and humor in one-lines (Mihalcea et al., 2010) in a single\\nlanguage, but with the tremendous amount of code-mixed data available online,\\nthere is a need to develop techniques which detects humor in code-mixed tweets.\\nIn this paper, we analyze the task of humor detection in texts and describe a\\nfreely available corpus containing English-Hindi code-mixed tweets annotated\\nwith humorous(H) or non-humorous(N) tags. We also tagged the words in the\\ntweets with Language tags (English/Hindi/Others). Moreover, we describe the\\nexperiments carried out on the corpus and provide a baseline classification\\nsystem which distinguishes between humorous and non-humorous texts.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05513v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05513v1.pdf',\n",
       "  'proceeding': 'LREC 2018 5',\n",
       "  'authors': ['Ankush Khandelwal',\n",
       "   'Sahil Swami',\n",
       "   'Syed S. Akhtar',\n",
       "   'Manish Shrivastava'],\n",
       "  'tasks': ['General Classification',\n",
       "   'Humor Detection',\n",
       "   'Text Classification'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/netscore-towards-universal-metrics-for-large',\n",
       "  'arxiv_id': '1806.05512',\n",
       "  'title': 'NetScore: Towards Universal Metrics for Large-scale Performance Analysis of Deep Neural Networks for Practical On-Device Edge Usage',\n",
       "  'abstract': 'Much of the focus in the design of deep neural networks has been on improving\\naccuracy, leading to more powerful yet highly complex network architectures\\nthat are difficult to deploy in practical scenarios, particularly on edge\\ndevices such as mobile and other consumer devices given their high\\ncomputational and memory requirements. As a result, there has been a recent\\ninterest in the design of quantitative metrics for evaluating deep neural\\nnetworks that accounts for more than just model accuracy as the sole indicator\\nof network performance. In this study, we continue the conversation towards\\nuniversal metrics for evaluating the performance of deep neural networks for\\npractical on-device edge usage. In particular, we propose a new balanced metric\\ncalled NetScore, which is designed specifically to provide a quantitative\\nassessment of the balance between accuracy, computational complexity, and\\nnetwork architecture complexity of a deep neural network, which is important\\nfor on-device edge operation. In what is one of the largest comparative\\nanalysis between deep neural networks in literature, the NetScore metric, the\\ntop-1 accuracy metric, and the popular information density metric were compared\\nacross a diverse set of 60 different deep convolutional neural networks for\\nimage classification on the ImageNet Large Scale Visual Recognition Challenge\\n(ILSVRC 2012) dataset. The evaluation results across these three metrics for\\nthis diverse set of networks are presented in this study to act as a reference\\nguide for practitioners in the field. The proposed NetScore metric, along with\\nthe other tested metrics, are by no means perfect, but the hope is to push the\\nconversation towards better universal metrics for evaluating deep neural\\nnetworks for use in practical on-device edge scenarios to help guide\\npractitioners in model design for such scenarios.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05512v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05512v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alexander Wong'],\n",
       "  'tasks': ['Image Classification', 'Object Recognition'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/cold-start-aware-user-and-product-attention',\n",
       "  'arxiv_id': '1806.05507',\n",
       "  'title': 'Cold-Start Aware User and Product Attention for Sentiment Classification',\n",
       "  'abstract': 'The use of user/product information in sentiment analysis is important,\\nespecially for cold-start users/products, whose number of reviews are very\\nlimited. However, current models do not deal with the cold-start problem which\\nis typical in review websites. In this paper, we present Hybrid Contextualized\\nSentiment Classifier (HCSC), which contains two modules: (1) a fast word\\nencoder that returns word vectors embedded with short and long range dependency\\nfeatures; and (2) Cold-Start Aware Attention (CSAA), an attention mechanism\\nthat considers the existence of cold-start problem when attentively pooling the\\nencoded word vectors. HCSC introduces shared vectors that are constructed from\\nsimilar users/products, and are used when the original distinct vectors do not\\nhave sufficient information (i.e. cold-start). This is decided by a\\nfrequency-guided selective gate vector. Our experiments show that in terms of\\nRMSE, HCSC performs significantly better when compared with on famous datasets,\\ndespite having less complexity, and thus can be trained much faster. More\\nimportantly, our model performs significantly better than previous models when\\nthe training data is sparse and has cold-start problems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05507v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05507v1.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Reinald Kim Amplayo',\n",
       "   'Jihyeok Kim',\n",
       "   'Sua Sung',\n",
       "   'Seung-won Hwang'],\n",
       "  'tasks': ['Classification', 'General Classification', 'Sentiment Analysis'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imdb-movie-reviews'],\n",
       "  'datasets_used_full': ['IMDb Movie Reviews'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dense-light-field-reconstruction-from-sparse',\n",
       "  'arxiv_id': '1806.05506',\n",
       "  'title': 'Dense Light Field Reconstruction From Sparse Sampling Using Residual Network',\n",
       "  'abstract': 'A light field records numerous light rays from a real-world scene. However,\\ncapturing a dense light field by existing devices is a time-consuming process.\\nBesides, reconstructing a large amount of light rays equivalent to multiple\\nlight fields using sparse sampling arises a severe challenge for existing\\nmethods. In this paper, we present a learning based method to reconstruct\\nmultiple novel light fields between two mutually independent light fields. We\\nindicate that light rays distributed in different light fields have the same\\nconsistent constraints under a certain condition. The most significant\\nconstraint is a depth related correlation between angular and spatial\\ndimensions. Our method avoids working out the error-sensitive constraint by\\nemploying a deep neural network. We solve residual values of pixels on epipolar\\nplane image (EPI) to reconstruct novel light fields. Our method is able to\\nreconstruct 2 to 4 novel light fields between two mutually independent input\\nlight fields. We also compare our results with those yielded by a number of\\nalternatives elsewhere in the literature, which shows our reconstructed light\\nfields have better structure similarity and occlusion relationship.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05506v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05506v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mantang Guo', 'Hao Zhu', 'Guoqing Zhou', 'Qing Wang'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/entity-commonsense-representation-for-neural',\n",
       "  'arxiv_id': '1806.05504',\n",
       "  'title': 'Entity Commonsense Representation for Neural Abstractive Summarization',\n",
       "  'abstract': \"A major proportion of a text summary includes important entities found in the\\noriginal text. These entities build up the topic of the summary. Moreover, they\\nhold commonsense information once they are linked to a knowledge base. Based on\\nthese observations, this paper investigates the usage of linked entities to\\nguide the decoder of a neural text summarizer to generate concise and better\\nsummaries. To this end, we leverage on an off-the-shelf entity linking system\\n(ELS) to extract linked entities and propose Entity2Topic (E2T), a module\\neasily attachable to a sequence-to-sequence model that transforms a list of\\nentities into a vector representation of the topic of the summary. Current\\navailable ELS's are still not sufficiently effective, possibly introducing\\nunresolved ambiguities and irrelevant entities. We resolve the imperfections of\\nthe ELS by (a) encoding entities with selective disambiguation, and (b) pooling\\nentity vectors using firm attention. By applying E2T to a simple\\nsequence-to-sequence model with attention mechanism as base model, we see\\nsignificant improvements of the performance in the Gigaword (sentence to title)\\nand CNN (long document to multi-sentence highlights) summarization datasets by\\nat least 2 ROUGE points.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05504v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05504v1.pdf',\n",
       "  'proceeding': 'NAACL 2018 6',\n",
       "  'authors': ['Reinald Kim Amplayo', 'Seonjae Lim', 'Seung-won Hwang'],\n",
       "  'tasks': ['Abstractive Text Summarization', 'Entity Linking'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/gradient-based-meta-learning-with-learned',\n",
       "  'arxiv_id': '1801.05558',\n",
       "  'title': 'Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace',\n",
       "  'abstract': \"Gradient-based meta-learning methods leverage gradient descent to learn the\\ncommonalities among various tasks. While previous such methods have been\\nsuccessful in meta-learning tasks, they resort to simple gradient descent\\nduring meta-testing. Our primary contribution is the {\\\\em MT-net}, which\\nenables the meta-learner to learn on each layer's activation space a subspace\\nthat the task-specific learner performs gradient descent on. Additionally, a\\ntask-specific learner of an {\\\\em MT-net} performs gradient descent with respect\\nto a meta-learned distance metric, which warps the activation space to be more\\nsensitive to task identity. We demonstrate that the dimension of this learned\\nsubspace reflects the complexity of the task-specific learner's adaptation\\ntask, and also that our model is less sensitive to the choice of initial\\nlearning rates than previous gradient-based meta-learning methods. Our method\\nachieves state-of-the-art or comparable performance on few-shot classification\\nand regression tasks.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1801.05558v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1801.05558v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Yoonho Lee', 'Seungjin Choi'],\n",
       "  'tasks': ['Few-Shot Image Classification', 'Meta-Learning'],\n",
       "  'date': '2018-01-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['miniimagenet-1', 'omniglot-1'],\n",
       "  'datasets_used_full': ['miniImageNet', 'Omniglot'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/aspect-sentiment-model-for-micro-reviews',\n",
       "  'arxiv_id': '1806.05499',\n",
       "  'title': 'Aspect Sentiment Model for Micro Reviews',\n",
       "  'abstract': 'This paper aims at an aspect sentiment model for aspect-based sentiment\\nanalysis (ABSA) focused on micro reviews. This task is important in order to\\nunderstand short reviews majority of the users write, while existing topic\\nmodels are targeted for expert-level long reviews with sufficient co-occurrence\\npatterns to observe. Current methods on aggregating micro reviews using\\nmetadata information may not be effective as well due to metadata absence,\\ntopical heterogeneity, and cold start problems. To this end, we propose a model\\ncalled Micro Aspect Sentiment Model (MicroASM). MicroASM is based on the\\nobservation that short reviews 1) are viewed with sentiment-aspect word pairs\\nas building blocks of information, and 2) can be clustered into larger reviews.\\nWhen compared to the current state-of-the-art aspect sentiment models,\\nexperiments show that our model provides better performance on aspect-level\\ntasks such as aspect term extraction and document-level tasks such as sentiment\\nclassification.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05499v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05499v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Reinald Kim Amplayo', 'Seung-won Hwang'],\n",
       "  'tasks': ['Aspect-Based Sentiment Analysis',\n",
       "   'Sentiment Analysis',\n",
       "   'Term Extraction',\n",
       "   'Topic Models'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-accurate-evaluation-of-gans-for-language',\n",
       "  'arxiv_id': '1806.04936',\n",
       "  'title': 'On Accurate Evaluation of GANs for Language Generation',\n",
       "  'abstract': 'Generative Adversarial Networks (GANs) are a promising approach to language generation. The latest works introducing novel GAN models for language generation use n-gram based metrics for evaluation and only report single scores of the best run. In this paper, we argue that this often misrepresents the true picture and does not tell the full story, as GAN models can be extremely sensitive to the random initialization and small deviations from the best hyperparameter choice. In particular, we demonstrate that the previously used BLEU score is not sensitive to semantic deterioration of generated texts and propose alternative metrics that better capture the quality and diversity of the generated samples. We also conduct a set of experiments comparing a number of GAN models for text with a conventional Language Model (LM) and find that neither of the considered models performs convincingly better than the LM.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04936v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04936v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Stanislau Semeniuta', 'Aliaksei Severyn', 'Sylvain Gelly'],\n",
       "  'tasks': ['Language Modelling', 'Text Generation'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['snli', 'multinli'],\n",
       "  'datasets_used_full': ['SNLI', 'MultiNLI'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dynamic-video-segmentation-network',\n",
       "  'arxiv_id': '1804.00931',\n",
       "  'title': 'Dynamic Video Segmentation Network',\n",
       "  'abstract': 'In this paper, we present a detailed design of dynamic video segmentation\\nnetwork (DVSNet) for fast and efficient semantic video segmentation. DVSNet\\nconsists of two convolutional neural networks: a segmentation network and a\\nflow network. The former generates highly accurate semantic segmentations, but\\nis deeper and slower. The latter is much faster than the former, but its output\\nrequires further processing to generate less accurate semantic segmentations.\\nWe explore the use of a decision network to adaptively assign different frame\\nregions to different networks based on a metric called expected confidence\\nscore. Frame regions with a higher expected confidence score traverse the flow\\nnetwork. Frame regions with a lower expected confidence score have to pass\\nthrough the segmentation network. We have extensively performed experiments on\\nvarious configurations of DVSNet, and investigated a number of variants for the\\nproposed decision network. The experimental results show that our DVSNet is\\nable to achieve up to 70.4% mIoU at 19.8 fps on the Cityscape dataset. A high\\nspeed version of DVSNet is able to deliver an fps of 30.4 with 63.2% mIoU on\\nthe same dataset. DVSNet is also able to reduce up to 95% of the computational\\nworkloads.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.00931v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.00931v2.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Yu-Syuan Xu', 'Tsu-Jui Fu', 'Hsuan-Kung Yang', 'Chun-Yi Lee'],\n",
       "  'tasks': ['Video Segmentation', 'Video Semantic Segmentation'],\n",
       "  'date': '2018-04-03',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cityscapes'],\n",
       "  'datasets_used_full': ['Cityscapes'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-a-tree-structured-ising-model-in',\n",
       "  'arxiv_id': '1604.06749',\n",
       "  'title': 'Learning a Tree-Structured Ising Model in Order to Make Predictions',\n",
       "  'abstract': 'We study the problem of learning a tree Ising model from samples such that\\nsubsequent predictions made using the model are accurate. The prediction task\\nconsidered in this paper is that of predicting the values of a subset of\\nvariables given values of some other subset of variables. Virtually all\\nprevious work on graphical model learning has focused on recovering the true\\nunderlying graph. We define a distance (\"small set TV\" or ssTV) between\\ndistributions $P$ and $Q$ by taking the maximum, over all subsets $\\\\mathcal{S}$\\nof a given size, of the total variation between the marginals of $P$ and $Q$ on\\n$\\\\mathcal{S}$; this distance captures the accuracy of the prediction task of\\ninterest. We derive non-asymptotic bounds on the number of samples needed to\\nget a distribution (from the same class) with small ssTV relative to the one\\ngenerating the samples. One of the main messages of this paper is that far\\nfewer samples are needed than for recovering the underlying tree, which means\\nthat accurate predictions are possible using the wrong tree.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1604.06749v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1604.06749v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Guy Bresler', 'Mina Karzand'],\n",
       "  'tasks': [],\n",
       "  'date': '2016-04-22',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/inference-in-deep-gaussian-processes-using',\n",
       "  'arxiv_id': '1806.05490',\n",
       "  'title': 'Inference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo',\n",
       "  'abstract': 'Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian\\nProcesses that combine well calibrated uncertainty estimates with the high\\nflexibility of multilayer models. One of the biggest challenges with these\\nmodels is that exact inference is intractable. The current state-of-the-art\\ninference method, Variational Inference (VI), employs a Gaussian approximation\\nto the posterior distribution. This can be a potentially poor unimodal\\napproximation of the generally multimodal posterior. In this work, we provide\\nevidence for the non-Gaussian nature of the posterior and we apply the\\nStochastic Gradient Hamiltonian Monte Carlo method to generate samples. To\\nefficiently optimize the hyperparameters, we introduce the Moving Window MCEM\\nalgorithm. This results in significantly better predictions at a lower\\ncomputational cost than its VI counterpart. Thus our method establishes a new\\nstate-of-the-art for inference in DGPs.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05490v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05490v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Marton Havasi',\n",
       "   'José Miguel Hernández-Lobato',\n",
       "   'Juan José Murillo-Fuentes'],\n",
       "  'tasks': ['Gaussian Processes', 'Variational Inference'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/nearly-zero-shot-learning-for-semantic',\n",
       "  'arxiv_id': '1806.05484',\n",
       "  'title': 'Nearly Zero-Shot Learning for Semantic Decoding in Spoken Dialogue Systems',\n",
       "  'abstract': 'This paper presents two ways of dealing with scarce data in semantic decoding\\nusing N-Best speech recognition hypotheses. First, we learn features by using a\\ndeep learning architecture in which the weights for the unknown and known\\ncategories are jointly optimised. Second, an unsupervised method is used for\\nfurther tuning the weights. Sharing weights injects prior knowledge to unknown\\ncategories. The unsupervised tuning (i.e. the risk minimisation) improves the\\nF-Measure when recognising nearly zero-shot data on the DSTC3 corpus. This\\nunsupervised method can be applied subject to two assumptions: the rank of the\\nclass marginal is assumed to be known and the class-conditional scores of the\\nclassifier are assumed to follow a Gaussian distribution.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05484v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05484v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Lina M. Rojas-Barahona',\n",
       "   'Stefan Ultes',\n",
       "   'Pawel Budzianowski',\n",
       "   'Iñigo Casanueva',\n",
       "   'Milica Gasic',\n",
       "   'Bo-Hsiang Tseng',\n",
       "   'Steve Young'],\n",
       "  'tasks': ['Speech Recognition',\n",
       "   'Spoken Dialogue Systems',\n",
       "   'Zero-Shot Learning'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/morphological-and-language-agnostic-word',\n",
       "  'arxiv_id': '1806.05482',\n",
       "  'title': 'Morphological and Language-Agnostic Word Segmentation for NMT',\n",
       "  'abstract': 'The state of the art of handling rich morphology in neural machine\\ntranslation (NMT) is to break word forms into subword units, so that the\\noverall vocabulary size of these units fits the practical limits given by the\\nNMT model and GPU memory capacity. In this paper, we compare two common but\\nlinguistically uninformed methods of subword construction (BPE and STE, the\\nmethod implemented in Tensor2Tensor toolkit) and two linguistically-motivated\\nmethods: Morfessor and one novel method, based on a derivational dictionary.\\nOur experiments with German-to-Czech translation, both morphologically rich,\\ndocument that so far, the non-motivated methods perform better. Furthermore, we\\niden- tify a critical difference between BPE and STE and show a simple pre-\\nprocessing step for BPE that considerably increases translation quality as\\nevaluated by automatic measures.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05482v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05482v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Dominik Macháček', 'Jonáš Vidra', 'Ondřej Bojar'],\n",
       "  'tasks': ['Machine Translation', 'Translation'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'BPE',\n",
       "    'full_name': 'Byte Pair Encoding',\n",
       "    'description': '**Byte Pair Encoding**, or **BPE**, is a subword segmentation algorithm that encodes rare and unknown words as sequences of subword units. The intuition is that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).\\r\\n\\r\\n[Lei Mao](https://leimao.github.io/blog/Byte-Pair-Encoding/) has a detailed blog post that explains how this works.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1508.07909v5',\n",
       "    'source_title': 'Neural Machine Translation of Rare Words with Subword Units',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Subword Segmentation',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Natural Language Processing'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/automatic-language-identification-for-romance',\n",
       "  'arxiv_id': '1806.05480',\n",
       "  'title': 'Automatic Language Identification for Romance Languages using Stop Words and Diacritics',\n",
       "  'abstract': 'Automatic language identification is a natural language processing problem\\nthat tries to determine the natural language of a given content. In this paper\\nwe present a statistical method for automatic language identification of\\nwritten text using dictionaries containing stop words and diacritics. We\\npropose different approaches that combine the two dictionaries to accurately\\ndetermine the language of textual corpora. This method was chosen because stop\\nwords and diacritics are very specific to a language, although some languages\\nhave some similar words and special characters they are not all common. The\\nlanguages taken into account were romance languages because they are very\\nsimilar and usually it is hard to distinguish between them from a computational\\npoint of view. We have tested our method using a Twitter corpus and a news\\narticle corpus. Both corpora consists of UTF-8 encoded text, so the diacritics\\ncould be taken into account, in the case that the text has no diacritics only\\nthe stop words are used to determine the language of the text. The experimental\\nresults show that the proposed method has an accuracy of over 90% for small\\ntexts and over 99.8% for',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05480v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05480v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ciprian-Octavian Truică', 'Julien Velcin', 'Alexandru Boicea'],\n",
       "  'tasks': ['Language Identification'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/copycat-cnn-stealing-knowledge-by-persuading',\n",
       "  'arxiv_id': '1806.05476',\n",
       "  'title': 'Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data',\n",
       "  'abstract': 'In the past few years, Convolutional Neural Networks (CNNs) have been\\nachieving state-of-the-art performance on a variety of problems. Many companies\\nemploy resources and money to generate these models and provide them as an API,\\ntherefore it is in their best interest to protect them, i.e., to avoid that\\nsomeone else copies them. Recent studies revealed that state-of-the-art CNNs\\nare vulnerable to adversarial examples attacks, and this weakness indicates\\nthat CNNs do not need to operate in the problem domain (PD). Therefore, we\\nhypothesize that they also do not need to be trained with examples of the PD in\\norder to operate in it.\\n  Given these facts, in this paper, we investigate if a target black-box CNN\\ncan be copied by persuading it to confess its knowledge through random\\nnon-labeled data. The copy is two-fold: i) the target network is queried with\\nrandom data and its predictions are used to create a fake dataset with the\\nknowledge of the network; and ii) a copycat network is trained with the fake\\ndataset and should be able to achieve similar performance as the target\\nnetwork.\\n  This hypothesis was evaluated locally in three problems (facial expression,\\nobject, and crosswalk classification) and against a cloud-based API. In the\\ncopy attacks, images from both non-problem domain and PD were used. All copycat\\nnetworks achieved at least 93.7% of the performance of the original models with\\nnon-problem domain data, and at least 98.6% using additional data from the PD.\\nAdditionally, the copycat CNN successfully copied at least 97.3% of the\\nperformance of the Microsoft Azure Emotion API. Our results show that it is\\npossible to create a copycat CNN by simply querying a target network as\\nblack-box with random non-labeled data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05476v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05476v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jacson Rodrigues Correia-Silva',\n",
       "   'Rodrigo F. Berriel',\n",
       "   'Claudine Badue',\n",
       "   'Alberto F. de Souza',\n",
       "   'Thiago Oliveira-Santos'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'stl-10'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'STL-10'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/efficient-active-learning-for-image',\n",
       "  'arxiv_id': '1806.05473',\n",
       "  'title': 'Efficient Active Learning for Image Classification and Segmentation using a Sample Selection and Conditional Generative Adversarial Network',\n",
       "  'abstract': 'Training robust deep learning (DL) systems for medical image classification or segmentation is challenging due to limited images covering different disease types and severity. We propose an active learning (AL) framework to select most informative samples and add to the training data. We use conditional generative adversarial networks (cGANs) to generate realistic chest xray images with different disease characteristics by conditioning its generation on a real image sample. Informative samples to add to the training set are identified using a Bayesian neural network. Experiments show our proposed AL framework is able to achieve state of the art performance by using about 35% of the full dataset, thus saving significant time and effort over conventional methods.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05473v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05473v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Dwarikanath Mahapatra',\n",
       "   'Behzad Bozorgtabar',\n",
       "   'Jean-Philippe Thiran',\n",
       "   'Mauricio Reyes'],\n",
       "  'tasks': ['Active Learning',\n",
       "   'General Classification',\n",
       "   'Image Classification'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/relation-networks-for-object-detection',\n",
       "  'arxiv_id': '1711.11575',\n",
       "  'title': 'Relation Networks for Object Detection',\n",
       "  'abstract': 'Although it is well believed for years that modeling relations between\\nobjects would help object recognition, there has not been evidence that the\\nidea is working in the deep learning era. All state-of-the-art object detection\\nsystems still rely on recognizing object instances individually, without\\nexploiting their relations during learning.\\n  This work proposes an object relation module. It processes a set of objects\\nsimultaneously through interaction between their appearance feature and\\ngeometry, thus allowing modeling of their relations. It is lightweight and\\nin-place. It does not require additional supervision and is easy to embed in\\nexisting networks. It is shown effective on improving object recognition and\\nduplicate removal steps in the modern object detection pipeline. It verifies\\nthe efficacy of modeling object relations in CNN based detection. It gives rise\\nto the first fully end-to-end object detector.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.11575v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.11575v2.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Han Hu',\n",
       "   'Jiayuan Gu',\n",
       "   'Zheng Zhang',\n",
       "   'Jifeng Dai',\n",
       "   'Yichen Wei'],\n",
       "  'tasks': ['Object Detection', 'Object Recognition'],\n",
       "  'date': '2017-11-30',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['coco'],\n",
       "  'datasets_used_full': ['COCO'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dice-the-infinitely-differentiable-monte',\n",
       "  'arxiv_id': '1802.05098',\n",
       "  'title': 'DiCE: The Infinitely Differentiable Monte-Carlo Estimator',\n",
       "  'abstract': 'The score function estimator is widely used for estimating gradients of\\nstochastic objectives in stochastic computation graphs (SCG), eg, in\\nreinforcement learning and meta-learning. While deriving the first-order\\ngradient estimators by differentiating a surrogate loss (SL) objective is\\ncomputationally and conceptually simple, using the same approach for\\nhigher-order derivatives is more challenging. Firstly, analytically deriving\\nand implementing such estimators is laborious and not compliant with automatic\\ndifferentiation. Secondly, repeatedly applying SL to construct new objectives\\nfor each order derivative involves increasingly cumbersome graph manipulations.\\nLastly, to match the first-order gradient under differentiation, SL treats part\\nof the cost as a fixed sample, which we show leads to missing and wrong terms\\nfor estimators of higher-order derivatives. To address all these shortcomings\\nin a unified way, we introduce DiCE, which provides a single objective that can\\nbe differentiated repeatedly, generating correct estimators of derivatives of\\nany order in SCGs. Unlike SL, DiCE relies on automatic differentiation for\\nperforming the requisite graph manipulations. We verify the correctness of DiCE\\nboth through a proof and numerical evaluation of the DiCE derivative estimates.\\nWe also use DiCE to propose and evaluate a novel approach for multi-agent\\nlearning. Our code is available at https://www.github.com/alshedivat/lola.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.05098v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.05098v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jakob Foerster',\n",
       "   'Gregory Farquhar',\n",
       "   'Maruan Al-Shedivat',\n",
       "   'Tim Rocktäschel',\n",
       "   'Eric P. Xing',\n",
       "   'Shimon Whiteson'],\n",
       "  'tasks': ['Meta-Learning'],\n",
       "  'date': '2018-02-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-cross-lingual-distributed-logical',\n",
       "  'arxiv_id': '1806.05461',\n",
       "  'title': 'Learning Cross-lingual Distributed Logical Representations for Semantic Parsing',\n",
       "  'abstract': 'With the development of several multilingual datasets used for semantic\\nparsing, recent research efforts have looked into the problem of learning\\nsemantic parsers in a multilingual setup. However, how to improve the\\nperformance of a monolingual semantic parser for a specific language by\\nleveraging data annotated in different languages remains a research question\\nthat is under-explored. In this work, we present a study to show how learning\\ndistributed representations of the logical forms from data annotated in\\ndifferent languages can be used for improving the performance of a monolingual\\nsemantic parser. We extend two existing monolingual semantic parsers to\\nincorporate such cross-lingual distributed logical representations as features.\\nExperiments show that our proposed approach is able to yield improved semantic\\nparsing results on the standard multilingual GeoQuery dataset.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05461v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05461v1.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Yanyan Zou', 'Wei Lu'],\n",
       "  'tasks': ['Semantic Parsing'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/gradient-layer-enhancing-the-convergence-of',\n",
       "  'arxiv_id': '1801.02227',\n",
       "  'title': 'Gradient Layer: Enhancing the Convergence of Adversarial Training for Generative Models',\n",
       "  'abstract': 'We propose a new technique that boosts the convergence of training generative\\nadversarial networks. Generally, the rate of training deep models reduces\\nseverely after multiple iterations. A key reason for this phenomenon is that a\\ndeep network is expressed using a highly non-convex finite-dimensional model,\\nand thus the parameter gets stuck in a local optimum. Because of this, methods\\noften suffer not only from degeneration of the convergence speed but also from\\nlimitations in the representational power of the trained network. To overcome\\nthis issue, we propose an additional layer called the gradient layer to seek a\\ndescent direction in an infinite-dimensional space. Because the layer is\\nconstructed in the infinite-dimensional space, we are not restricted by the\\nspecific model structure of finite-dimensional models. As a result, we can get\\nout of the local optima in finite-dimensional models and move towards the\\nglobal optimal function more directly. In this paper, this phenomenon is\\nexplained from the functional gradient method perspective of the gradient\\nlayer. Interestingly, the optimization procedure using the gradient layer\\nnaturally constructs the deep structure of the network. Moreover, we\\ndemonstrate that this procedure can be regarded as a discretization method of\\nthe gradient flow that naturally reduces the objective function. Finally, the\\nmethod is tested using several numerical experiments, which show its fast\\nconvergence.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1801.02227v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1801.02227v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Atsushi Nitanda', 'Taiji Suzuki'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-01-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/analysis-of-the-effect-of-unexpected-outliers',\n",
       "  'arxiv_id': '1806.05455',\n",
       "  'title': 'Analysis of the Effect of Unexpected Outliers in the Classification of Spectroscopy Data',\n",
       "  'abstract': 'Multi-class classification algorithms are very widely used, but we argue that\\nthey are not always ideal from a theoretical perspective, because they assume\\nall classes are characterized by the data, whereas in many applications,\\ntraining data for some classes may be entirely absent, rare, or statistically\\nunrepresentative. We evaluate one-sided classifiers as an alternative, since\\nthey assume that only one class (the target) is well characterized. We consider\\na task of identifying whether a substance contains a chlorinated solvent, based\\non its chemical spectrum. For this application, it is not really feasible to\\ncollect a statistically representative set of outliers, since that group may\\ncontain \\\\emph{anything} apart from the target chlorinated solvents. Using a new\\none-sided classification toolkit, we compare a One-Sided k-NN algorithm with\\ntwo well-known binary classification algorithms, and conclude that the\\none-sided classifier is more robust to unexpected outliers.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05455v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05455v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Frank G. Glavin', 'Michael G. Madden'],\n",
       "  'tasks': ['Classification',\n",
       "   'General Classification',\n",
       "   'Multi-class Classification'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'k-NN',\n",
       "    'full_name': 'k-Nearest Neighbors',\n",
       "    'description': '**$k$-Nearest Neighbors** is a clustering-based algorithm for classification and regression. It is a a type of instance-based learning as it does not attempt to construct a general internal model, but simply stores instances of the training data. Prediction is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\\r\\n\\r\\nSource of Description and Image: [scikit-learn](https://scikit-learn.org/stable/modules/neighbors.html#classification)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Non-Parametric Classification',\n",
       "     'description': '**Non-Parametric Classification** methods perform classification where we use non-parametric methods to approximate the functional form of the relationship. Below you can find a continuously updating list of non-parametric classification methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/low-rank-geometric-mean-metric-learning',\n",
       "  'arxiv_id': '1806.05454',\n",
       "  'title': 'Low-rank geometric mean metric learning',\n",
       "  'abstract': 'We propose a low-rank approach to learning a Mahalanobis metric from data.\\nInspired by the recent geometric mean metric learning (GMML) algorithm, we\\npropose a low-rank variant of the algorithm. This allows to jointly learn a\\nlow-dimensional subspace where the data reside and the Mahalanobis metric that\\nappropriately fits the data. Our results show that we compete effectively with\\nGMML at lower ranks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05454v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05454v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mukul Bhutani',\n",
       "   'Pratik Jawanpuria',\n",
       "   'Hiroyuki Kasai',\n",
       "   'Bamdev Mishra'],\n",
       "  'tasks': ['Metric Learning'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/4dfab-a-large-scale-4d-facial-expression',\n",
       "  'arxiv_id': '1712.01443',\n",
       "  'title': '4DFAB: A Large Scale 4D Facial Expression Database for Biometric Applications',\n",
       "  'abstract': 'The progress we are currently witnessing in many computer vision\\napplications, including automatic face analysis, would not be made possible\\nwithout tremendous efforts in collecting and annotating large scale visual\\ndatabases. To this end, we propose 4DFAB, a new large scale database of dynamic\\nhigh-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings\\nof 180 subjects captured in four different sessions spanning over a five-year\\nperiod. It contains 4D videos of subjects displaying both spontaneous and posed\\nfacial behaviours. The database can be used for both face and facial expression\\nrecognition, as well as behavioural biometrics. It can also be used to learn\\nvery powerful blendshapes for parametrising facial behaviour. In this paper, we\\nconduct several experiments and demonstrate the usefulness of the database for\\nvarious applications. The database will be made publicly available for research\\npurposes.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.01443v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.01443v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shiyang Cheng',\n",
       "   'Irene Kotsia',\n",
       "   'Maja Pantic',\n",
       "   'Stefanos Zafeiriou'],\n",
       "  'tasks': ['Facial Expression Recognition'],\n",
       "  'date': '2017-12-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['facewarehouse', 'bp4d'],\n",
       "  'datasets_used_full': ['FaceWarehouse', 'BP4D'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/masked-autoregressive-flow-for-density',\n",
       "  'arxiv_id': '1705.07057',\n",
       "  'title': 'Masked Autoregressive Flow for Density Estimation',\n",
       "  'abstract': 'Autoregressive models are among the best performing neural density\\nestimators. We describe an approach for increasing the flexibility of an\\nautoregressive model, based on modelling the random numbers that the model uses\\ninternally when generating data. By constructing a stack of autoregressive\\nmodels, each modelling the random numbers of the next model in the stack, we\\nobtain a type of normalizing flow suitable for density estimation, which we\\ncall Masked Autoregressive Flow. This type of flow is closely related to\\nInverse Autoregressive Flow and is a generalization of Real NVP. Masked\\nAutoregressive Flow achieves state-of-the-art performance in a range of\\ngeneral-purpose density estimation tasks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1705.07057v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1705.07057v4.pdf',\n",
       "  'proceeding': 'NeurIPS 2017 12',\n",
       "  'authors': ['George Papamakarios', 'Theo Pavlakou', 'Iain Murray'],\n",
       "  'tasks': ['Density Estimation'],\n",
       "  'date': '2017-05-19',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10',\n",
       "   'mnist',\n",
       "   'bsd',\n",
       "   'uci-machine-learning-repository'],\n",
       "  'datasets_used_full': ['CIFAR-10',\n",
       "   'MNIST',\n",
       "   'BSD',\n",
       "   'UCI Machine Learning Repository'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-generative-models-in-the-real-world-an',\n",
       "  'arxiv_id': '1806.05452',\n",
       "  'title': 'Deep Generative Models in the Real-World: An Open Challenge from Medical Imaging',\n",
       "  'abstract': 'Recent advances in deep learning led to novel generative modeling techniques\\nthat achieve unprecedented quality in generated samples and performance in\\nlearning complex distributions in imaging data. These new models in medical\\nimage computing have important applications that form clinically relevant and\\nvery challenging unsupervised learning problems. In this paper, we explore the\\nfeasibility of using state-of-the-art auto-encoder-based deep generative\\nmodels, such as variational and adversarial auto-encoders, for one such task:\\nabnormality detection in medical imaging. We utilize typical, publicly\\navailable datasets with brain scans from healthy subjects and patients with\\nstroke lesions and brain tumors. We use the data from healthy subjects to train\\ndifferent auto-encoder based models to learn the distribution of healthy images\\nand detect pathologies as outliers. Models that can better learn the data\\ndistribution should be able to detect outliers more accurately. We evaluate the\\ndetection performance of deep generative models and compare them with non-deep\\nlearning based approaches to provide a benchmark of the current state of\\nresearch. We conclude that abnormality detection is a challenging task for deep\\ngenerative models and large room exists for improvement. In order to facilitate\\nfurther research, we aim to provide carefully pre-processed imaging data\\navailable to the research community.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05452v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05452v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xiaoran Chen',\n",
       "   'Nick Pawlowski',\n",
       "   'Martin Rajchl',\n",
       "   'Ben Glocker',\n",
       "   'Ender Konukoglu'],\n",
       "  'tasks': ['Anomaly Detection'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-committee-machine-computational-to',\n",
       "  'arxiv_id': '1806.05451',\n",
       "  'title': 'The committee machine: Computational to statistical gaps in learning a two-layers neural network',\n",
       "  'abstract': 'Heuristic tools from statistical physics have been used in the past to locate the phase transitions and compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine. We also introduce a version of the approximate message passing (AMP) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes in which a low generalization error is information-theoretically achievable while the AMP algorithm fails to deliver it, strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large computational gap.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05451v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05451v2.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Benjamin Aubin',\n",
       "   'Antoine Maillard',\n",
       "   'Jean Barbier',\n",
       "   'Florent Krzakala',\n",
       "   'Nicolas Macris',\n",
       "   'Lenka Zdeborová'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/stochastic-gradient-descent-with-exponential',\n",
       "  'arxiv_id': '1806.05438',\n",
       "  'title': 'Stochastic Gradient Descent with Exponential Convergence Rates of Expected Classification Errors',\n",
       "  'abstract': 'We consider stochastic gradient descent and its averaging variant for binary classification problems in a reproducing kernel Hilbert space. In the traditional analysis using a consistency property of loss functions, it is known that the expected classification error converges more slowly than the expected risk even when assuming a low-noise condition on the conditional label probabilities. Consequently, the resulting rate is sublinear. Therefore, it is important to consider whether much faster convergence of the expected classification error can be achieved. In recent research, an exponential convergence rate for stochastic gradient descent was shown under a strong low-noise condition but provided theoretical analysis was limited to the squared loss function, which is somewhat inadequate for binary classification tasks. In this paper, we show an exponential convergence of the expected classification error in the final phase of the stochastic gradient descent for a wide class of differentiable convex loss functions under similar assumptions. As for the averaged stochastic gradient descent, we show that the same convergence rate holds from the early phase of training. In experiments, we verify our analyses on the $L_2$-regularized logistic regression.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05438v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05438v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Atsushi Nitanda', 'Taiji Suzuki'],\n",
       "  'tasks': ['Classification', 'General Classification'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/servenet-a-deep-neural-network-for-web',\n",
       "  'arxiv_id': '1806.05437',\n",
       "  'title': 'ServeNet: A Deep Neural Network for Web Services Classification',\n",
       "  'abstract': 'Automated service classification plays a crucial role in service discovery, selection, and composition. Machine learning has been widely used for service classification in recent years. However, the performance of conventional machine learning methods highly depends on the quality of manual feature engineering. In this paper, we present a novel deep neural network to automatically abstract low-level representation of both service name and service description to high-level merged features without feature engineering and the length limitation, and then predict service classification on 50 service categories. To demonstrate the effectiveness of our approach, we conduct a comprehensive experimental study by comparing 10 machine learning methods on 10,000 real-world web services. The result shows that the proposed deep neural network can achieve higher accuracy in classification and more robust than other machine learning methods.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05437v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05437v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yilong Yang',\n",
       "   'Nafees Qamar',\n",
       "   'Peng Liu',\n",
       "   'Katarina Grolinger',\n",
       "   'Weiru Wang',\n",
       "   'Zhi Li',\n",
       "   'Zhifang Liao'],\n",
       "  'tasks': ['Classification', 'Feature Engineering', 'General Classification'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/transfer-learning-for-context-aware-question',\n",
       "  'arxiv_id': '1806.05434',\n",
       "  'title': 'Transfer Learning for Context-Aware Question Matching in Information-seeking Conversations in E-commerce',\n",
       "  'abstract': 'Building multi-turn information-seeking conversation systems is an important\\nand challenging research topic. Although several advanced neural text matching\\nmodels have been proposed for this task, they are generally not efficient for\\nindustrial applications. Furthermore, they rely on a large amount of labeled\\ndata, which may not be available in real-world applications. To alleviate these\\nproblems, we study transfer learning for multi-turn information seeking\\nconversations in this paper. We first propose an efficient and effective\\nmulti-turn conversation model based on convolutional neural networks. After\\nthat, we extend our model to adapt the knowledge learned from a resource-rich\\ndomain to enhance the performance. Finally, we deployed our model in an\\nindustrial chatbot called AliMe Assist\\n(https://consumerservice.taobao.com/online-help) and observed a significant\\nimprovement over the existing online model.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05434v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05434v1.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Minghui Qiu',\n",
       "   'Liu Yang',\n",
       "   'Feng Ji',\n",
       "   'Weipeng Zhao',\n",
       "   'Wei Zhou',\n",
       "   'Jun Huang',\n",
       "   'Haiqing Chen',\n",
       "   'W. Bruce Croft',\n",
       "   'Wei. Lin'],\n",
       "  'tasks': ['Chatbot', 'Text Matching', 'Transfer Learning'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['ubuntu-dialogue-corpus'],\n",
       "  'datasets_used_full': ['UDC'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/urdu-word-segmentation-using-conditional',\n",
       "  'arxiv_id': '1806.05432',\n",
       "  'title': 'Urdu Word Segmentation using Conditional Random Fields (CRFs)',\n",
       "  'abstract': 'State-of-the-art Natural Language Processing algorithms rely heavily on\\nefficient word segmentation. Urdu is amongst languages for which word\\nsegmentation is a complex task as it exhibits space omission as well as space\\ninsertion issues. This is partly due to the Arabic script which although\\ncursive in nature, consists of characters that have inherent joining and\\nnon-joining attributes regardless of word boundary. This paper presents a word\\nsegmentation system for Urdu which uses a Conditional Random Field sequence\\nmodeler with orthographic, linguistic and morphological features. Our proposed\\nmodel automatically learns to predict white space as word boundary as well as\\nZero Width Non-Joiner (ZWNJ) as sub-word boundary. Using a manually annotated\\ncorpus, our model achieves F1 score of 0.97 for word boundary identification\\nand 0.85 for sub-word boundary identification tasks. We have made our code and\\ncorpus publicly available to make our results reproducible.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05432v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05432v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Haris Bin Zia', 'Agha Ali Raza', 'Awais Athar'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/only-bayes-should-learn-a-manifold-on-the',\n",
       "  'arxiv_id': '1806.04994',\n",
       "  'title': 'Only Bayes should learn a manifold (on the estimation of differential geometric structure from data)',\n",
       "  'abstract': 'We investigate learning of the differential geometric structure of a data manifold embedded in a high-dimensional Euclidean space. We first analyze kernel-based algorithms and show that under the usual regularizations, non-probabilistic methods cannot recover the differential geometric structure, but instead find mostly linear manifolds or spaces equipped with teleports. To properly learn the differential geometric structure, non-probabilistic methods must apply regularizations that enforce large gradients, which go against common wisdom. We repeat the analysis for probabilistic methods and find that under reasonable priors, the geometric structure can be recovered. Fully exploiting the recovered structure, however, requires the development of stochastic extensions to classic Riemannian geometry. We take early steps in that regard. Finally, we partly extend the analysis to modern models based on neural networks, thereby highlighting geometric and probabilistic shortcomings of current deep generative models.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04994v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04994v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Søren Hauberg'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/modeling-coherence-for-neural-machine',\n",
       "  'arxiv_id': '1711.11221',\n",
       "  'title': 'Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches',\n",
       "  'abstract': 'Sentences in a well-formed text are connected to each other via various links\\nto form the cohesive structure of the text. Current neural machine translation\\n(NMT) systems translate a text in a conventional sentence-by-sentence fashion,\\nignoring such cross-sentence links and dependencies. This may lead to generate\\nan incoherent target text for a coherent source text. In order to handle this\\nissue, we propose a cache-based approach to modeling coherence for neural\\nmachine translation by capturing contextual information either from recently\\ntranslated sentences or the entire document. Particularly, we explore two types\\nof caches: a dynamic cache, which stores words from the best translation\\nhypotheses of preceding sentences, and a topic cache, which maintains a set of\\ntarget-side topical words that are semantically related to the document to be\\ntranslated. On this basis, we build a new layer to score target words in these\\ntwo caches with a cache-based neural model. Here the estimated probabilities\\nfrom the cache-based neural model are combined with NMT probabilities into the\\nfinal word prediction probabilities via a gating mechanism. Finally, the\\nproposed cache-based neural model is trained jointly with NMT system in an\\nend-to-end manner. Experiments and analysis presented in this paper demonstrate\\nthat the proposed cache-based model achieves substantial improvements over\\nseveral state-of-the-art SMT and NMT baselines.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.11221v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.11221v3.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Shaohui Kuang', 'Deyi Xiong', 'Weihua Luo', 'Guodong Zhou'],\n",
       "  'tasks': ['Machine Translation', 'Translation'],\n",
       "  'date': '2017-11-30',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/cross-modal-hallucination-for-few-shot-fine',\n",
       "  'arxiv_id': '1806.05147',\n",
       "  'title': 'Cross-modal Hallucination for Few-shot Fine-grained Recognition',\n",
       "  'abstract': 'State-of-the-art deep learning algorithms generally require large amounts of\\ndata for model training. Lack thereof can severely deteriorate the performance,\\nparticularly in scenarios with fine-grained boundaries between categories. To\\nthis end, we propose a multimodal approach that facilitates bridging the\\ninformation gap by means of meaningful joint embeddings. Specifically, we\\npresent a benchmark that is multimodal during training (i.e. images and texts)\\nand single-modal in testing time (i.e. images), with the associated task to\\nutilize multimodal data in base classes (with many samples), to learn explicit\\nvisual classifiers for novel classes (with few samples). Next, we propose a\\nframework built upon the idea of cross-modal data hallucination. In this\\nregard, we introduce a discriminative text-conditional GAN for sample\\ngeneration with a simple self-paced strategy for sample selection. We show the\\nresults of our proposed discriminative hallucinated method for 1-, 2-, and 5-\\nshot learning on the CUB dataset, where the accuracy is improved by employing\\nmultimodal data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05147v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05147v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Frederik Pahde',\n",
       "   'Patrick Jähnichen',\n",
       "   'Tassilo Klein',\n",
       "   'Moin Nabi'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['cub-200-2011'],\n",
       "  'datasets_used_full': ['CUB-200-2011'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/ranking-recovery-from-limited-comparisons',\n",
       "  'arxiv_id': '1806.05419',\n",
       "  'title': 'Ranking Recovery from Limited Comparisons using Low-Rank Matrix Completion',\n",
       "  'abstract': 'This paper proposes a new method for solving the well-known rank aggregation\\nproblem from pairwise comparisons using the method of low-rank matrix\\ncompletion. The partial and noisy data of pairwise comparisons is transformed\\ninto a matrix form. We then use tools from matrix completion, which has served\\nas a major component in the low-rank completion solution of the Netflix\\nchallenge, to construct the preference of the different objects. In our\\napproach, the data of multiple comparisons is used to create an estimate of the\\nprobability of object i to win (or be chosen) over object j, where only a\\npartial set of comparisons between N objects is known. The data is then\\ntransformed into a matrix form for which the noiseless solution has a known\\nrank of one. An alternating minimization algorithm, in which the target matrix\\ntakes a bilinear form, is then used in combination with maximum likelihood\\nestimation for both factors. The reconstructed matrix is used to obtain the\\ntrue underlying preference intensity. This work demonstrates the improvement of\\nour proposed algorithm over the current state-of-the-art in both simulated\\nscenarios and real data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05419v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05419v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Tal Levy', 'Alireza Vahid', 'Raja Giryes'],\n",
       "  'tasks': ['Low-Rank Matrix Completion', 'Matrix Completion'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/configurable-markov-decision-processes',\n",
       "  'arxiv_id': '1806.05415',\n",
       "  'title': 'Configurable Markov Decision Processes',\n",
       "  'abstract': 'In many real-world problems, there is the possibility to configure, to a\\nlimited extent, some environmental parameters to improve the performance of a\\nlearning agent. In this paper, we propose a novel framework, Configurable\\nMarkov Decision Processes (Conf-MDPs), to model this new type of interaction\\nwith the environment. Furthermore, we provide a new learning algorithm, Safe\\nPolicy-Model Iteration (SPMI), to jointly and adaptively optimize the policy\\nand the environment configuration. After having introduced our approach and\\nderived some theoretical results, we present the experimental evaluation in two\\nexplicative problems to show the benefits of the environment configurability on\\nthe performance of the learned policy.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05415v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05415v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Alberto Maria Metelli', 'Mirco Mutti', 'Marcello Restelli'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-dynamics-of-linear-denoising',\n",
       "  'arxiv_id': '1806.05413',\n",
       "  'title': 'Learning Dynamics of Linear Denoising Autoencoders',\n",
       "  'abstract': 'Denoising autoencoders (DAEs) have proven useful for unsupervised\\nrepresentation learning, but a thorough theoretical understanding is still\\nlacking of how the input noise influences learning. Here we develop theory for\\nhow noise influences learning in DAEs. By focusing on linear DAEs, we are able\\nto derive analytic expressions that exactly describe their learning dynamics.\\nWe verify our theoretical predictions with simulations as well as experiments\\non MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise\\nallows DAEs to ignore low variance directions in the inputs while learning to\\nreconstruct them. Furthermore, in a comparison of the learning dynamics of DAEs\\nto standard regularised autoencoders, we show that noise has a similar\\nregularisation effect to weight decay, but with faster training dynamics. We\\nalso show that our theoretical predictions approximate learning dynamics on\\nreal-world data and qualitatively match observed dynamics in nonlinear DAEs.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05413v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05413v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Arnu Pretorius', 'Steve Kroon', 'Herman Kamper'],\n",
       "  'tasks': ['Denoising', 'Representation Learning'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/an-approach-to-vehicle-trajectory-prediction',\n",
       "  'arxiv_id': '1802.08632',\n",
       "  'title': 'An Approach to Vehicle Trajectory Prediction Using Automatically Generated Traffic Maps',\n",
       "  'abstract': 'Trajectory and intention prediction of traffic participants is an important\\ntask in automated driving and crucial for safe interaction with the\\nenvironment. In this paper, we present a new approach to vehicle trajectory\\nprediction based on automatically generated maps containing statistical\\ninformation about the behavior of traffic participants in a given area. These\\nmaps are generated based on trajectory observations using image processing and\\nmap matching techniques and contain all typical vehicle movements and\\nprobabilities in the considered area. Our prediction approach matches an\\nobserved trajectory to a behavior contained in the map and uses this\\ninformation to generate a prediction. We evaluated our approach on a dataset\\ncontaining over 14000 trajectories and found that it produces significantly\\nmore precise mid-term predictions compared to motion model-based prediction\\napproaches.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.08632v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.08632v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jannik Quehl', 'Haohao Hu', 'Sascha Wirges', 'Martin Lauer'],\n",
       "  'tasks': ['Trajectory Prediction'],\n",
       "  'date': '2018-02-23',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/incremental-natural-language-processing-1',\n",
       "  'arxiv_id': '1805.12518',\n",
       "  'title': 'Incremental Natural Language Processing: Challenges, Strategies, and Evaluation',\n",
       "  'abstract': 'Incrementality is ubiquitous in human-human interaction and beneficial for\\nhuman-computer interaction. It has been a topic of research in different parts\\nof the NLP community, mostly with focus on the specific topic at hand even\\nthough incremental systems have to deal with similar challenges regardless of\\ndomain. In this survey, I consolidate and categorize the approaches,\\nidentifying similarities and differences in the computation and data, and show\\ntrade-offs that have to be considered. A focus lies on evaluating incremental\\nsystems because the standard metrics often fail to capture the incremental\\nproperties of a system and coming up with a suitable evaluation scheme is\\nnon-trivial.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.12518v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.12518v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Arne Köhn'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-31',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-the-perceptrons-compression',\n",
       "  'arxiv_id': '1806.05403',\n",
       "  'title': \"On the Perceptron's Compression\",\n",
       "  'abstract': \"We study and provide exposition to several phenomena that are related to the\\nperceptron's compression. One theme concerns modifications of the perceptron\\nalgorithm that yield better guarantees on the margin of the hyperplane it\\noutputs. These modifications can be useful in training neural networks as well,\\nand we demonstrate them with some experimental data. In a second theme, we\\ndeduce conclusions from the perceptron's compression in various contexts.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05403v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05403v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shay Moran', 'Ido Nachum', 'Itai Panasoff', 'Amir Yehudayoff'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/aggregating-predictions-on-multiple-non',\n",
       "  'arxiv_id': '1806.04000',\n",
       "  'title': 'Aggregating Predictions on Multiple Non-disclosed Datasets using Conformal Prediction',\n",
       "  'abstract': 'Conformal Prediction is a machine learning methodology that produces valid\\nprediction regions under mild conditions. In this paper, we explore the\\napplication of making predictions over multiple data sources of different sizes\\nwithout disclosing data between the sources. We propose that each data source\\napplies a transductive conformal predictor independently using the local data,\\nand that the individual predictions are then aggregated to form a combined\\nprediction region. We demonstrate the method on several data sets, and show\\nthat the proposed method produces conservatively valid predictions and reduces\\nthe variance in the aggregated predictions. We also study the effect that the\\nnumber of data sources and size of each source has on aggregated predictions,\\nas compared with equally sized sources and pooled data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04000v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04000v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ola Spjuth', 'Lars Carlsson', 'Niharika Gauraha'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dynamical-isometry-and-a-mean-field-theory-of',\n",
       "  'arxiv_id': '1806.05394',\n",
       "  'title': 'Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks',\n",
       "  'abstract': 'Recurrent neural networks have gained widespread use in modeling sequence\\ndata across various domains. While many successful recurrent architectures\\nemploy a notion of gating, the exact mechanism that enables such remarkable\\nperformance is not well understood. We develop a theory for signal propagation\\nin recurrent networks after random initialization using a combination of mean\\nfield theory and random matrix theory. To simplify our discussion, we introduce\\na new RNN cell with a simple gating mechanism that we call the minimalRNN and\\ncompare it with vanilla RNNs. Our theory allows us to define a maximum\\ntimescale over which RNNs can remember an input. We show that this theory\\npredicts trainability for both recurrent architectures. We show that gated\\nrecurrent networks feature a much broader, more robust, trainable region than\\nvanilla RNNs, which corroborates recent experimental findings. Finally, we\\ndevelop a closed-form critical initialization scheme that achieves dynamical\\nisometry in both vanilla RNNs and minimalRNNs. We show that this results in\\nsignificantly improvement in training dynamics. Finally, we demonstrate that\\nthe minimalRNN achieves comparable performance to its more complex\\ncounterparts, such as LSTMs or GRUs, on a language modeling task.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05394v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05394v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Minmin Chen', 'Jeffrey Pennington', 'Samuel S. Schoenholz'],\n",
       "  'tasks': ['Language Modelling'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist', 'penn-treebank'],\n",
       "  'datasets_used_full': ['MNIST', 'Penn Treebank'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dynamical-isometry-and-a-mean-field-theory-of-2',\n",
       "  'arxiv_id': '1806.05393',\n",
       "  'title': 'Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks',\n",
       "  'abstract': 'In recent years, state-of-the-art methods in computer vision have utilized\\nincreasingly deep convolutional neural network architectures (CNNs), with some\\nof the most successful models employing hundreds or even thousands of layers. A\\nvariety of pathologies such as vanishing/exploding gradients make training such\\ndeep networks challenging. While residual connections and batch normalization\\ndo enable training at these depths, it has remained unclear whether such\\nspecialized architecture designs are truly necessary to train deep CNNs. In\\nthis work, we demonstrate that it is possible to train vanilla CNNs with ten\\nthousand layers or more simply by using an appropriate initialization scheme.\\nWe derive this initialization scheme theoretically by developing a mean field\\ntheory for signal propagation and by characterizing the conditions for\\ndynamical isometry, the equilibration of singular values of the input-output\\nJacobian matrix. These conditions require that the convolution operator be an\\northogonal transformation in the sense that it is norm-preserving. We present\\nan algorithm for generating such random initial orthogonal convolution kernels\\nand demonstrate empirically that they enable efficient training of extremely\\ndeep architectures.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05393v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05393v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Lechao Xiao',\n",
       "   'Yasaman Bahri',\n",
       "   'Jascha Sohl-Dickstein',\n",
       "   'Samuel S. Schoenholz',\n",
       "   'Jeffrey Pennington'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/how-do-source-side-monolingual-word',\n",
       "  'arxiv_id': '1806.01515',\n",
       "  'title': 'How Do Source-side Monolingual Word Embeddings Impact Neural Machine Translation?',\n",
       "  'abstract': 'Using pre-trained word embeddings as input layer is a common practice in many\\nnatural language processing (NLP) tasks, but it is largely neglected for neural\\nmachine translation (NMT). In this paper, we conducted a systematic analysis on\\nthe effect of using pre-trained source-side monolingual word embedding in NMT.\\nWe compared several strategies, such as fixing or updating the embeddings\\nduring NMT training on varying amounts of data, and we also proposed a novel\\nstrategy called dual-embedding that blends the fixing and updating strategies.\\nOur results suggest that pre-trained embeddings can be helpful if properly\\nincorporated into NMT, especially when parallel data is limited or additional\\nin-domain monolingual data is readily available.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.01515v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.01515v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shuoyang Ding', 'Kevin Duh'],\n",
       "  'tasks': ['Machine Translation', 'Translation', 'Word Embeddings'],\n",
       "  'date': '2018-06-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/theory-of-estimation-of-distribution',\n",
       "  'arxiv_id': '1806.05392',\n",
       "  'title': 'Theory of Estimation-of-Distribution Algorithms',\n",
       "  'abstract': 'Estimation-of-distribution algorithms (EDAs) are general metaheuristics used\\nin optimization that represent a more recent alternative to classical\\napproaches like evolutionary algorithms. In a nutshell, EDAs typically do not\\ndirectly evolve populations of search points but build probabilistic models of\\npromising solutions by repeatedly sampling and selecting points from the\\nunderlying search space. Recently, there has been made significant progress in\\nthe theoretical understanding of EDAs. This article provides an up-to-date\\noverview of the most commonly analyzed EDAs and the most recent theoretical\\nresults in this area. In particular, emphasis is put on the runtime analysis of\\nsimple univariate EDAs, including a description of typical benchmark functions\\nand tools for the analysis. Along the way, open problems and directions for\\nfuture research are described.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05392v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05392v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Martin S. Krejca', 'Carsten Witt'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/convex-coupled-matrix-and-tensor-completion',\n",
       "  'arxiv_id': '1705.05197',\n",
       "  'title': 'Convex Coupled Matrix and Tensor Completion',\n",
       "  'abstract': 'We propose a set of convex low rank inducing norms for a coupled matrices and\\ntensors (hereafter coupled tensors), which shares information between matrices\\nand tensors through common modes. More specifically, we propose a mixture of\\nthe overlapped trace norm and the latent norms with the matrix trace norm, and\\nthen, we propose a new completion algorithm based on the proposed norms. A key\\nadvantage of the proposed norms is that it is convex and can find a globally\\noptimal solution, while existing methods for coupled learning are non-convex.\\nFurthermore, we analyze the excess risk bounds of the completion model\\nregularized by our proposed norms which show that our proposed norms can\\nexploit the low rankness of coupled tensors leading to better bounds compared\\nto uncoupled norms. Through synthetic and real-world data experiments, we show\\nthat the proposed completion algorithm compares favorably with existing\\ncompletion algorithms.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1705.05197v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1705.05197v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kishan Wimalawarne', 'Makoto Yamada', 'Hiroshi Mamitsuka'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-05-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/parameter-learning-and-change-detection-using',\n",
       "  'arxiv_id': '1806.05387',\n",
       "  'title': 'Parameter Learning and Change Detection Using a Particle Filter With Accelerated Adaptation',\n",
       "  'abstract': 'This paper presents the construction of a particle filter, which incorporates\\nelements inspired by genetic algorithms, in order to achieve accelerated\\nadaptation of the estimated posterior distribution to changes in model\\nparameters. Specifically, the filter is designed for the situation where the\\nsubsequent data in online sequential filtering does not match the model\\nposterior filtered based on data up to a current point in time. The examples\\nconsidered encompass parameter regime shifts and stochastic volatility. The\\nfilter adapts to regime shifts extremely rapidly and delivers a clear heuristic\\nfor distinguishing between regime shifts and stochastic volatility, even though\\nthe model dynamics assumed by the filter exhibit neither of those features.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05387v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05387v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Karol Gellert', 'Erik Schlögl'],\n",
       "  'tasks': ['Change Detection'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/joint-blind-motion-deblurring-and-depth',\n",
       "  'arxiv_id': '1711.10918',\n",
       "  'title': 'Joint Blind Motion Deblurring and Depth Estimation of Light Field',\n",
       "  'abstract': 'Removing camera motion blur from a single light field is a challenging task\\nsince it is highly ill-posed inverse problem. The problem becomes even worse\\nwhen blur kernel varies spatially due to scene depth variation and high-order\\ncamera motion. In this paper, we propose a novel algorithm to estimate all blur\\nmodel variables jointly, including latent sub-aperture image, camera motion,\\nand scene depth from the blurred 4D light field. Exploiting multi-view nature\\nof a light field relieves the inverse property of the optimization by utilizing\\nstrong depth cues and multi-view blur observation. The proposed joint\\nestimation achieves high quality light field deblurring and depth estimation\\nsimultaneously under arbitrary 6-DOF camera motion and unconstrained scene\\ndepth. Intensive experiment on real and synthetic blurred light field confirms\\nthat the proposed algorithm outperforms the state-of-the-art light field\\ndeblurring and depth estimation methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.10918v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.10918v2.pdf',\n",
       "  'proceeding': 'ECCV 2018 9',\n",
       "  'authors': ['Dongwoo Lee', 'Haesol Park', 'In Kyu Park', 'Kyoung Mu Lee'],\n",
       "  'tasks': ['Deblurring', 'Depth Estimation'],\n",
       "  'date': '2017-11-29',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/pcas-pruning-channels-with-attention',\n",
       "  'arxiv_id': '1806.05382',\n",
       "  'title': 'PCAS: Pruning Channels with Attention Statistics for Deep Network Compression',\n",
       "  'abstract': 'Compression techniques for deep neural networks are important for implementing them on small embedded devices. In particular, channel-pruning is a useful technique for realizing compact networks. However, many conventional methods require manual setting of compression ratios in each layer. It is difficult to analyze the relationships between all layers, especially for deeper models. To address these issues, we propose a simple channel-pruning technique based on attention statistics that enables to evaluate the importance of channels. We improved the method by means of a criterion for automatic channel selection, using a single compression ratio for the entire model in place of per-layer model analysis. The proposed approach achieved superior performance over conventional methods with respect to accuracy and the computational costs for various models and datasets. We provide analysis results for behavior of the proposed criterion on different datasets to demonstrate its favorable properties for channel pruning.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05382v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05382v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kohei Yamamoto', 'Kurato Maeno'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'imagenet', 'cifar-100', 'camvid'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'ImageNet', 'CIFAR-100', 'CamVid'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/single-image-reflection-separation-with',\n",
       "  'arxiv_id': '1806.05376',\n",
       "  'title': 'Single Image Reflection Separation with Perceptual Losses',\n",
       "  'abstract': 'We present an approach to separating reflection from a single image. The\\napproach uses a fully convolutional network trained end-to-end with losses that\\nexploit low-level and high-level image information. Our loss function includes\\ntwo perceptual losses: a feature loss from a visual perception network, and an\\nadversarial loss that encodes characteristics of images in the transmission\\nlayers. We also propose a novel exclusion loss that enforces pixel-level layer\\nseparation. We create a dataset of real-world images with reflection and\\ncorresponding ground-truth transmission layers for quantitative evaluation and\\nmodel training. We validate our method through comprehensive quantitative\\nexperiments and show that our approach outperforms state-of-the-art reflection\\nremoval methods in PSNR, SSIM, and perceptual user study. We also extend our\\nmethod to two other image enhancement tasks to demonstrate the generality of\\nour approach.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05376v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05376v1.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Xuaner Zhang', 'Ren Ng', 'Qifeng Chen'],\n",
       "  'tasks': ['Image Enhancement', 'Reflection Removal', 'SSIM'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multi-attention-multi-class-constraint-for',\n",
       "  'arxiv_id': '1806.05372',\n",
       "  'title': 'Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition',\n",
       "  'abstract': 'Attention-based learning for fine-grained image recognition remains a\\nchallenging task, where most of the existing methods treat each object part in\\nisolation, while neglecting the correlations among them. In addition, the\\nmulti-stage or multi-scale mechanisms involved make the existing methods less\\nefficient and hard to be trained end-to-end. In this paper, we propose a novel\\nattention-based convolutional neural network (CNN) which regulates multiple\\nobject parts among different input images. Our method first learns multiple\\nattention region features of each input image through the one-squeeze\\nmulti-excitation (OSME) module, and then apply the multi-attention multi-class\\nconstraint (MAMC) in a metric learning framework. For each anchor feature, the\\nMAMC functions by pulling same-attention same-class features closer, while\\npushing different-attention or different-class features away. Our method can be\\neasily trained end-to-end, and is highly efficient which requires only one\\ntraining stage. Moreover, we introduce Dogs-in-the-Wild, a comprehensive dog\\nspecies dataset that surpasses similar existing datasets by category coverage,\\ndata volume and annotation quality. This dataset will be released upon\\nacceptance to facilitate the research of fine-grained image recognition.\\nExtensive experiments are conducted to show the substantial improvements of our\\nmethod on four benchmark datasets.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05372v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05372v1.pdf',\n",
       "  'proceeding': 'ECCV 2018 9',\n",
       "  'authors': ['Ming Sun', 'Yuchen Yuan', 'Feng Zhou', 'Errui Ding'],\n",
       "  'tasks': ['Fine-Grained Image Recognition', 'Metric Learning'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet',\n",
       "   'cub-200-2011',\n",
       "   'stanford-cars',\n",
       "   'goldfinch'],\n",
       "  'datasets_used_full': ['ImageNet',\n",
       "   'CUB-200-2011',\n",
       "   'Stanford Cars',\n",
       "   'Goldfinch'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-fast-proximal-point-method-for-computing',\n",
       "  'arxiv_id': '1802.04307',\n",
       "  'title': 'A Fast Proximal Point Method for Computing Exact Wasserstein Distance',\n",
       "  'abstract': 'Wasserstein distance plays increasingly important roles in machine learning, stochastic programming and image processing. Major efforts have been under way to address its high computational complexity, some leading to approximate or regularized variations such as Sinkhorn distance. However, as we will demonstrate, regularized variations with large regularization parameter will degradate the performance in several important machine learning applications, and small regularization parameter will fail due to numerical stability issues with existing algorithms. We address this challenge by developing an Inexact Proximal point method for exact Optimal Transport problem (IPOT) with the proximal operator approximately evaluated at each iteration using projections to the probability simplex. The algorithm (a) converges to exact Wasserstein distance with theoretical guarantee and robust regularization parameter selection, (b) alleviates numerical stability issue, (c) has similar computational complexity to Sinkhorn, and (d) avoids the shrinking problem when apply to generative models. Furthermore, a new algorithm is proposed based on IPOT to obtain sharper Wasserstein barycenter.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1802.04307v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1802.04307v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yujia Xie', 'Xiangfeng Wang', 'Ruijia Wang', 'Hongyuan Zha'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fire-ssd-wide-fire-modules-based-single-shot',\n",
       "  'arxiv_id': '1806.05363',\n",
       "  'title': 'Fire SSD: Wide Fire Modules based Single Shot Detector on Edge Device',\n",
       "  'abstract': 'With the emergence of edge computing, there is an increasing need for running\\nconvolutional neural network based object detection on small form factor edge\\ncomputing devices with limited compute and thermal budget for applications such\\nas video surveillance. To address this problem, efficient object detection\\nframeworks such as YOLO and SSD were proposed. However, SSD based object\\ndetection that uses VGG16 as backend network is insufficient to achieve real\\ntime speed on edge devices. To further improve the detection speed, the backend\\nnetwork is replaced by more efficient networks such as SqueezeNet and\\nMobileNet. Although the speed is greatly improved, it comes with a price of\\nlower accuracy. In this paper, we propose an efficient SSD named Fire SSD. Fire\\nSSD achieves 70.7mAP on Pascal VOC 2007 test set. Fire SSD achieves the speed\\nof 30.6FPS on low power mainstream CPU and is about 6 times faster than SSD300\\nand has about 4 times smaller model size. Fire SSD also achieves 22.2FPS on\\nintegrated GPU.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05363v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05363v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hengfui Liau', 'Nimmagadda Yamini', 'YengLiong Wong'],\n",
       "  'tasks': ['Edge-computing', 'Object Detection'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Residual Connection',\n",
       "    'full_name': 'Residual Connection',\n",
       "    'description': '**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Average Pooling',\n",
       "    'full_name': 'Average Pooling',\n",
       "    'description': '**Average Pooling** is a pooling operation that calculates the average value for patches of a feature map, and uses it to create a downsampled (pooled) feature map. It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs. It extracts features more smoothly than [Max Pooling](https://paperswithcode.com/method/max-pooling), whereas max pooling extracts more pronounced features like edges.\\r\\n\\r\\nImage Source: [here](https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Fire Module',\n",
       "    'full_name': 'Fire Module',\n",
       "    'description': 'A **Fire Module** is a building block for convolutional neural networks, notably used as part of [SqueezeNet](https://paperswithcode.com/method/squeezenet). A Fire module is comprised of: a squeeze [convolution](https://paperswithcode.com/method/convolution) layer (which has only 1x1 filters), feeding into an expand layer that has a mix of 1x1 and 3x3 convolution filters.  We expose three tunable dimensions (hyperparameters) in a Fire module: $s\\\\_{1x1}$, $e\\\\_{1x1}$, and $e\\\\_{3x3}$. In a Fire module, $s\\\\_{1x1}$ is the number of filters in the squeeze layer (all 1x1), $e\\\\_{1x1}$ is the number of 1x1 filters in the expand layer, and $e\\\\_{3x3}$ is the number of 3x3 filters in the expand layer. When we use Fire modules we set $s\\\\_{1x1}$ to be less than ($e\\\\_{1x1}$ + $e\\\\_{3x3}$), so the squeeze layer helps to limit the number of input channels to the 3x3 filters.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1602.07360v4',\n",
       "    'source_title': 'SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/squeezenet.py#L14',\n",
       "    'main_collection': {'name': 'Image Model Blocks',\n",
       "     'description': '**Image Model Blocks** are building blocks used in image models such as convolutional neural networks. Below you can find a continuously updating list of image model blocks.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Non Maximum Suppression',\n",
       "    'full_name': 'Non Maximum Suppression',\n",
       "    'description': '**Non Maximum Suppression** is a computer vision method that selects a single entity out of many overlapping entities (for example bounding boxes in object detection). The criteria is usually discarding entities that are below a given probability bound. With remaining entities we repeatedly pick the entity with the highest probability, output that as the prediction, and discard any remaining box where a $\\\\text{IoU} \\\\geq 0.5$ with the box output in the previous step.\\r\\n\\r\\nImage Credit: [Martin Kersner](https://github.com/martinkersner/non-maximum-suppression-cpp)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Proposal Filtering',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Global Average Pooling',\n",
       "    'full_name': 'Global Average Pooling',\n",
       "    'description': '**Global Average Pooling** is a pooling operation designed to replace fully connected layers in classical CNNs. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the [softmax](https://paperswithcode.com/method/softmax) layer. \\r\\n\\r\\nOne advantage of global [average pooling](https://paperswithcode.com/method/average-pooling) over the fully connected layers is that it is more native to the [convolution](https://paperswithcode.com/method/convolution) structure by enforcing correspondences between feature maps and categories. Thus the feature maps can be easily interpreted as categories confidence maps. Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. Furthermore, global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/baa592b215804927e28638f6a7f3318cbc411d49/torchvision/models/resnet.py#L157',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': '1x1 Convolution',\n",
       "    'full_name': '1x1 Convolution',\n",
       "    'description': 'A **1 x 1 Convolution** is a [convolution](https://paperswithcode.com/method/convolution) with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an [MLP](https://paperswithcode.com/method/feedforward-network) looking at a particular pixel location.\\r\\n\\r\\nImage Credit: [http://deeplearning.ai](http://deeplearning.ai)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://www.healthnutra.org/es/maxup/',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Dropout',\n",
       "    'full_name': 'Dropout',\n",
       "    'description': '**Dropout** is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p=0.5$). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).\\r\\n\\r\\nThe idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://jmlr.org/papers/v15/srivastava14a.html',\n",
       "    'source_title': 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/7f3078b70d0ed9bea6228efa420879c56f72ef69/jax/experimental/stax.py#L271-L275',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Xavier Initialization',\n",
       "    'full_name': 'Xavier Initialization',\n",
       "    'description': '**Xavier Initialization**, or **Glorot Initialization**, is an initialization scheme for neural networks. Biases are initialized be 0 and the weights $W\\\\_{ij}$ at each layer are initialized as:\\r\\n\\r\\n$$ W\\\\_{ij} \\\\sim U\\\\left[-\\\\frac{1}{\\\\sqrt{n}}, \\\\frac{1}{\\\\sqrt{n}}\\\\right] $$\\r\\n\\r\\nWhere $U$ is a uniform distribution and $n$ is the size of the previous layer (number of columns in $W$).',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/0adb5843766092fba584791af76383125fd0d01c/torch/nn/init.py#L289',\n",
       "    'main_collection': {'name': 'Initialization',\n",
       "     'description': '**Initialization** methods are used to initialize the weights in a neural network. Below can you find a continuously updating list of initialization methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'SSD',\n",
       "    'full_name': 'SSD',\n",
       "    'description': '**SSD** is a single-stage object detection method that discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. \\r\\n\\r\\nThe fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage. Improvements over competing single-stage methods include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.02325v5',\n",
       "    'source_title': 'SSD: Single Shot MultiBox Detector',\n",
       "    'code_snippet_url': 'https://github.com/amdegroot/ssd.pytorch/blob/5b0b77faa955c1917b0c710d770739ba8fbff9b7/ssd.py#L10',\n",
       "    'main_collection': {'name': 'Object Detection Models',\n",
       "     'description': '**Object Detection Models** are architectures used to perform the task of object detection. Below you can find a continuously updating list of object detection models.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'SqueezeNet',\n",
       "    'full_name': 'SqueezeNet',\n",
       "    'description': '**SqueezeNet** is a convolutional neural network that employs design strategies to reduce the number of parameters, notably with the use of fire modules that \"squeeze\" parameters using 1x1 convolutions.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1602.07360v4',\n",
       "    'source_title': 'SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/squeezenet.py#L37',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/adagrad-stepsizes-sharp-convergence-over',\n",
       "  'arxiv_id': '1806.01811',\n",
       "  'title': 'AdaGrad stepsizes: Sharp convergence over nonconvex landscapes',\n",
       "  'abstract': \"Adaptive gradient methods such as AdaGrad and its variants update the stepsize in stochastic gradient descent on the fly according to the gradients received along the way; such methods have gained widespread use in large-scale optimization for their ability to converge robustly, without the need to fine-tune the stepsize schedule. Yet, the theoretical guarantees to date for AdaGrad are for online and convex optimization. We bridge this gap by providing theoretical guarantees for the convergence of AdaGrad for smooth, nonconvex functions. We show that the norm version of AdaGrad (AdaGrad-Norm) converges to a stationary point at the $\\\\mathcal{O}(\\\\log(N)/\\\\sqrt{N})$ rate in the stochastic setting, and at the optimal $\\\\mathcal{O}(1/N)$ rate in the batch (non-stochastic) setting -- in this sense, our convergence guarantees are 'sharp'. In particular, the convergence of AdaGrad-Norm is robust to the choice of all hyper-parameters of the algorithm, in contrast to stochastic gradient descent whose convergence depends crucially on tuning the step-size to the (generally unknown) Lipschitz smoothness constant and level of stochastic noise on the gradient. Extensive numerical experiments are provided to corroborate our theory; moreover, the experiments suggest that the robustness of AdaGrad-Norm extends to state-of-the-art models in deep learning, without sacrificing generalization.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.01811v8',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.01811v8.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Rachel Ward', 'Xiaoxia Wu', 'Leon Bottou'],\n",
       "  'tasks': ['Stochastic Optimization'],\n",
       "  'date': '2018-06-05',\n",
       "  'methods': [{'name': 'AdaGrad',\n",
       "    'full_name': 'AdaGrad',\n",
       "    'description': '**AdaGrad** is a stochastic optimization method that adapts the learning rate to the parameters. It performs smaller updates for parameters associated with frequently occurring features, and larger updates for parameters associated with infrequently occurring features. In its update rule, Adagrad modifies the general learning rate $\\\\eta$ at each time step $t$ for every parameter $\\\\theta\\\\_{i}$ based on the past gradients for $\\\\theta\\\\_{i}$: \\r\\n\\r\\n$$ \\\\theta\\\\_{t+1, i} = \\\\theta\\\\_{t, i} - \\\\frac{\\\\eta}{\\\\sqrt{G\\\\_{t, ii} + \\\\epsilon}}g\\\\_{t, i} $$\\r\\n\\r\\nThe benefit of AdaGrad is that it eliminates the need to manually tune the learning rate; most leave it at a default value of $0.01$. Its main weakness is the accumulation of the squared gradients in the denominator. Since every added term is positive, the accumulated sum keeps growing during training, causing the learning rate to shrink and becoming infinitesimally small.\\r\\n\\r\\nImage: [Alec Radford](https://twitter.com/alecrad)',\n",
       "    'introduced_year': 2011,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['cifar-10', 'imagenet', 'mnist'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'ImageNet', 'MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/view-volume-network-for-semantic-scene',\n",
       "  'arxiv_id': '1806.05361',\n",
       "  'title': 'View-volume Network for Semantic Scene Completion from a Single Depth Image',\n",
       "  'abstract': 'We introduce a View-Volume convolutional neural network (VVNet) for inferring\\nthe occupancy and semantic labels of a volumetric 3D scene from a single depth\\nimage. The VVNet concatenates a 2D view CNN and a 3D volume CNN with a\\ndifferentiable projection layer. Given a single RGBD image, our method extracts\\nthe detailed geometric features from the input depth image with a 2D view CNN\\nand then projects the features into a 3D volume according to the input depth\\nmap via a projection layer. After that, we learn the 3D context information of\\nthe scene with a 3D volume CNN for computing the result volumetric occupancy\\nand semantic labels. With combined 2D and 3D representations, the VVNet\\nefficiently reduces the computational cost, enables feature extraction from\\nmulti-channel high resolution inputs, and thus significantly improves the\\nresult accuracy. We validate our method and demonstrate its efficiency and\\neffectiveness on both synthetic SUNCG and real NYU dataset.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05361v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05361v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yu-Xiao Guo', 'Xin Tong'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['suncg'],\n",
       "  'datasets_used_full': ['SUNCG'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/defending-against-saddle-point-attack-in',\n",
       "  'arxiv_id': '1806.05358',\n",
       "  'title': 'Defending Against Saddle Point Attack in Byzantine-Robust Distributed Learning',\n",
       "  'abstract': 'We study robust distributed learning that involves minimizing a non-convex loss function with saddle points. We consider the Byzantine setting where some worker machines have abnormal or even arbitrary and adversarial behavior. In this setting, the Byzantine machines may create fake local minima near a saddle point that is far away from any true local minimum, even when robust gradient estimators are used. We develop ByzantinePGD, a robust first-order algorithm that can provably escape saddle points and fake local minima, and converge to an approximate true local minimizer with low iteration complexity. As a by-product, we give a simpler algorithm and analysis for escaping saddle points in the usual non-Byzantine setting. We further discuss three robust gradient estimators that can be used in ByzantinePGD, including median, trimmed mean, and iterative filtering. We characterize their performance in concrete statistical settings, and argue for their near-optimality in low and high dimensional regimes.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05358v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05358v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Dong Yin',\n",
       "   'Yudong Chen',\n",
       "   'Kannan Ramchandran',\n",
       "   'Peter Bartlett'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-multi-output-forecasting-learning-to',\n",
       "  'arxiv_id': '1806.05357',\n",
       "  'title': 'Deep Multi-Output Forecasting: Learning to Accurately Predict Blood Glucose Trajectories',\n",
       "  'abstract': \"In many forecasting applications, it is valuable to predict not only the\\nvalue of a signal at a certain time point in the future, but also the values\\nleading up to that point. This is especially true in clinical applications,\\nwhere the future state of the patient can be less important than the patient's\\noverall trajectory. This requires multi-step forecasting, a forecasting variant\\nwhere one aims to predict multiple values in the future simultaneously.\\nStandard methods to accomplish this can propagate error from prediction to\\nprediction, reducing quality over the long term. In light of these challenges,\\nwe propose multi-output deep architectures for multi-step forecasting in which\\nwe explicitly model the distribution of future values of the signal over a\\nprediction horizon. We apply these techniques to the challenging and clinically\\nrelevant task of blood glucose forecasting. Through a series of experiments on\\na real-world dataset consisting of 550K blood glucose measurements, we\\ndemonstrate the effectiveness of our proposed approaches in capturing the\\nunderlying signal dynamics. Compared to existing shallow and deep methods, we\\nfind that our proposed approaches improve performance individually and capture\\ncomplementary information, leading to a large improvement over the baseline\\nwhen combined (4.87 vs. 5.31 absolute percentage error (APE)). Overall, the\\nresults suggest the efficacy of our proposed approach in predicting blood\\nglucose level and multi-step forecasting more generally.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05357v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05357v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ian Fox',\n",
       "   'Lynn Ang',\n",
       "   'Mamta Jaiswal',\n",
       "   'Rodica Pop-Busui',\n",
       "   'Jenna Wiens'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/finding-gems-multi-scale-dictionaries-for',\n",
       "  'arxiv_id': '1806.05356',\n",
       "  'title': 'Finding GEMS: Multi-Scale Dictionaries for High-Dimensional Graph Signals',\n",
       "  'abstract': 'Modern data introduces new challenges to classic signal processing\\napproaches, leading to a growing interest in the field of graph signal\\nprocessing. A powerful and well established model for real world signals in\\nvarious domains is sparse representation over a dictionary, combined with the\\nability to train the dictionary from signal examples. This model has been\\nsuccessfully applied to graph signals as well by integrating the underlying\\ngraph topology into the learned dictionary. Nonetheless, dictionary learning\\nmethods for graph signals are typically restricted to small dimensions due to\\nthe computational constraints that the dictionary learning problem entails, and\\ndue to the direct use of the graph Laplacian matrix. In this paper, we propose\\na dictionary learning algorithm that applies to a broader class of graph\\nsignals, and is capable of handling much higher dimensional data. We\\nincorporate the underlying graph topology both implicitly, by forcing the\\nlearned dictionary atoms to be sparse combinations of graph-wavelet functions,\\nand explicitly, by adding direct graph constraints to promote smoothness in\\nboth the feature and manifold domains. The resulting atoms are thus adapted to\\nthe data of interest while adhering to the underlying graph structure and\\npossessing a desired multi-scale property. Experimental results on several\\ndatasets, representing both synthetic and real network data of different\\nnature, demonstrate the effectiveness of the proposed algorithm for graph\\nsignal processing even in high dimensions.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05356v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05356v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yael Yankelevsky', 'Michael Elad'],\n",
       "  'tasks': ['Dictionary Learning'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/scalable-neural-network-compression-and',\n",
       "  'arxiv_id': '1806.05355',\n",
       "  'title': 'Scalable Neural Network Compression and Pruning Using Hard Clustering and L1 Regularization',\n",
       "  'abstract': 'We propose a simple and easy to implement neural network compression\\nalgorithm that achieves results competitive with more complicated\\nstate-of-the-art methods. The key idea is to modify the original optimization\\nproblem by adding K independent Gaussian priors (corresponding to the k-means\\nobjective) over the network parameters to achieve parameter quantization, as\\nwell as an L1 penalty to achieve pruning. Unlike many existing\\nquantization-based methods, our method uses hard clustering assignments of\\nnetwork parameters, which adds minimal change or overhead to standard network\\ntraining. We also demonstrate experimentally that tying neural network\\nparameters provides less gain in generalization performance than changing\\nnetwork architecture and connectivity patterns entirely.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05355v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05355v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yibo Yang', 'Nicholas Ruozzi', 'Vibhav Gogate'],\n",
       "  'tasks': ['Neural Network Compression', 'Quantization'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist'],\n",
       "  'datasets_used_full': ['MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/q-neurons-neuron-activations-based-on',\n",
       "  'arxiv_id': '1806.00149',\n",
       "  'title': \"q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators\",\n",
       "  'abstract': \"We propose a new generic type of stochastic neurons, called $q$-neurons, that\\nconsiders activation functions based on Jackson's $q$-derivatives with\\nstochastic parameters $q$. Our generalization of neural network architectures\\nwith $q$-neurons is shown to be both scalable and very easy to implement. We\\ndemonstrate experimentally consistently improved performances over\\nstate-of-the-art standard activation functions, both on training and testing\\nloss functions.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.00149v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.00149v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Frank Nielsen', 'Ke Sun'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-01',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-to-explain-an-information-theoretic',\n",
       "  'arxiv_id': '1802.07814',\n",
       "  'title': 'Learning to Explain: An Information-Theoretic Perspective on Model Interpretation',\n",
       "  'abstract': 'We introduce instancewise feature selection as a methodology for model\\ninterpretation. Our method is based on learning a function to extract a subset\\nof features that are most informative for each given example. This feature\\nselector is trained to maximize the mutual information between selected\\nfeatures and the response variable, where the conditional distribution of the\\nresponse variable given the input is the model to be explained. We develop an\\nefficient variational approximation to the mutual information, and show the\\neffectiveness of our method on a variety of synthetic and real data sets using\\nboth quantitative metrics and human evaluation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.07814v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.07814v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Jianbo Chen',\n",
       "   'Le Song',\n",
       "   'Martin J. Wainwright',\n",
       "   'Michael. I. Jordan'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-21',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imdb-movie-reviews'],\n",
       "  'datasets_used_full': ['IMDb Movie Reviews'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/stingray-detection-of-aerial-images-using',\n",
       "  'arxiv_id': '1805.04262',\n",
       "  'title': 'Stingray Detection of Aerial Images Using Augmented Training Images Generated by A Conditional Generative Model',\n",
       "  'abstract': 'In this paper, we present an object detection method that tackles the\\nstingray detection problem based on aerial images. In this problem, the images\\nare aerially captured on a sea-surface area by using an Unmanned Aerial Vehicle\\n(UAV), and the stingrays swimming under (but close to) the sea surface are the\\ntarget we want to detect and locate. To this end, we use a deep object\\ndetection method, faster RCNN, to train a stingray detector based on a limited\\ntraining set of images. To boost the performance, we develop a new generative\\napproach, conditional GLO, to increase the training samples of stingray, which\\nis an extension of the Generative Latent Optimization (GLO) approach. Unlike\\ntraditional data augmentation methods that generate new data only for image\\nclassification, our proposed method that mixes foreground and background\\ntogether can generate new data for an object detection task, and thus improve\\nthe training efficacy of a CNN detector. Experimental results show that\\nsatisfiable performance can be obtained by using our approach on stingray\\ndetection in aerial images.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.04262v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.04262v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yi-Min Chou',\n",
       "   'Chien-Hung Chen',\n",
       "   'Keng-Hao Liu',\n",
       "   'Chu-Song Chen'],\n",
       "  'tasks': ['Data Augmentation', 'Image Classification', 'Object Detection'],\n",
       "  'date': '2018-05-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/convex-class-model-on-symmetric-positive',\n",
       "  'arxiv_id': '1806.05343',\n",
       "  'title': 'Convex Class Model on Symmetric Positive Definite Manifolds',\n",
       "  'abstract': 'The effectiveness of Symmetric Positive Definite (SPD) manifold features has been proven in various computer vision tasks. However, due to the non-Euclidean geometry of these features, existing Euclidean machineries cannot be directly used. In this paper, we tackle the classification tasks with limited training data on SPD manifolds. Our proposed framework, named Manifold Convex Class Model, represents each class on SPD manifolds using a convex model, and classification can be performed by computing distances to the convex models. We provide three methods based on different metrics to address the optimization problem of the smallest distance of a point to the convex model on SPD manifold. The efficacy of our proposed framework is demonstrated both on synthetic data and several computer vision tasks including object recognition, texture classification, person re-identification and traffic scene classification.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05343v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05343v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kun Zhao', 'Arnold Wiliem', 'Shaokang Chen', 'Brian C. Lovell'],\n",
       "  'tasks': ['Classification',\n",
       "   'General Classification',\n",
       "   'Object Recognition',\n",
       "   'Person Re-Identification',\n",
       "   'Scene Classification',\n",
       "   'Texture Classification'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['eth'],\n",
       "  'datasets_used_full': ['ETH'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/context-aware-policy-reuse',\n",
       "  'arxiv_id': '1806.03793',\n",
       "  'title': 'Context-Aware Policy Reuse',\n",
       "  'abstract': 'Transfer learning can greatly speed up reinforcement learning for a new task\\nby leveraging policies of relevant tasks.\\n  Existing works of policy reuse either focus on only selecting a single best\\nsource policy for transfer without considering contexts, or cannot guarantee to\\nlearn an optimal policy for a target task.\\n  To improve transfer efficiency and guarantee optimality, we develop a novel\\npolicy reuse method, called Context-Aware Policy reuSe (CAPS), that enables\\nmulti-policy transfer. Our method learns when and which source policy is best\\nfor reuse, as well as when to terminate its reuse. CAPS provides theoretical\\nguarantees in convergence and optimality for both source policy selection and\\ntarget task learning. Empirical results on a grid-based navigation domain and\\nthe Pygame Learning Environment demonstrate that CAPS significantly outperforms\\nother state-of-the-art policy reuse methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03793v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03793v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Siyuan Li', 'Fangda Gu', 'Guangxiang Zhu', 'Chongjie Zhang'],\n",
       "  'tasks': ['Transfer Learning'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/from-trailers-to-storylines-an-efficient-way',\n",
       "  'arxiv_id': '1806.05341',\n",
       "  'title': 'From Trailers to Storylines: An Efficient Way to Learn from Movies',\n",
       "  'abstract': 'The millions of movies produced in the human history are valuable resources\\nfor computer vision research. However, learning a vision model from movie data\\nwould meet with serious difficulties. A major obstacle is the computational\\ncost -- the length of a movie is often over one hour, which is substantially\\nlonger than the short video clips that previous study mostly focuses on. In\\nthis paper, we explore an alternative approach to learning vision models from\\nmovies. Specifically, we consider a framework comprised of a visual module and\\na temporal analysis module. Unlike conventional learning methods, the proposed\\napproach learns these modules from different sets of data -- the former from\\ntrailers while the latter from movies. This allows distinctive visual features\\nto be learned within a reasonable budget while still preserving long-term\\ntemporal structures across an entire movie. We construct a large-scale dataset\\nfor this study and define a series of tasks on top. Experiments on this dataset\\nshowed that the proposed method can substantially reduce the training time\\nwhile obtaining highly effective features and coherent temporal structures.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05341v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05341v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Qingqiu Huang',\n",
       "   'Yuanjun Xiong',\n",
       "   'Yu Xiong',\n",
       "   'Yuqi Zhang',\n",
       "   'Dahua Lin'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['movieqa'],\n",
       "  'datasets_used_full': ['MovieQA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/hierarchical-interpretations-for-neural',\n",
       "  'arxiv_id': '1806.05337',\n",
       "  'title': 'Hierarchical interpretations for neural network predictions',\n",
       "  'abstract': \"Deep neural networks (DNNs) have achieved impressive predictive performance\\ndue to their ability to learn complex, non-linear relationships between\\nvariables. However, the inability to effectively visualize these relationships\\nhas led to DNNs being characterized as black boxes and consequently limited\\ntheir applications. To ameliorate this problem, we introduce the use of\\nhierarchical interpretations to explain DNN predictions through our proposed\\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\\ntrained DNN, ACD produces a hierarchical clustering of the input features,\\nalong with the contribution of each cluster to the final prediction. This\\nhierarchy is optimized to identify clusters of features that the DNN learned\\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\\nwe show that ACD is effective at diagnosing incorrect predictions and\\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\\nenables users both to identify the more accurate of two DNNs and to better\\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\\nadversarial perturbations, implying that it captures fundamental aspects of the\\ninput and ignores spurious noise.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05337v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05337v2.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Chandan Singh', 'W. James Murdoch', 'Bin Yu'],\n",
       "  'tasks': ['Feature Importance', 'Interpretable Machine Learning'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'Agglomerative Contextual Decomposition',\n",
       "    'full_name': 'Agglomerative Contextual Decomposition',\n",
       "    'description': '**Agglomerative Contextual Decomposition (ACD)** is an interpretability method that produces hierarchical interpretations for a single prediction made by a neural network, by scoring interactions and building them into a tree. Given a prediction from a trained neural network, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1806.05337v2',\n",
       "    'source_title': 'Hierarchical interpretations for neural network predictions',\n",
       "    'code_snippet_url': 'https://github.com/csinva/hierarchical-dnn-interpretations',\n",
       "    'main_collection': {'name': 'Interpretability',\n",
       "     'description': '**Interpretability Methods** seek to explain the predictions made by neural networks by introducing mechanisms to enduce or enforce interpretability. For example, LIME approximates the neural network with a locally interpretable model. Below you can find a continuously updating list of interpretability methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['sst'],\n",
       "  'datasets_used_full': ['SST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/adversarial-learning-with-local-coordinate',\n",
       "  'arxiv_id': '1806.04895',\n",
       "  'title': 'Adversarial Learning with Local Coordinate Coding',\n",
       "  'abstract': 'Generative adversarial networks (GANs) aim to generate realistic data from\\nsome prior distribution (e.g., Gaussian noises). However, such prior\\ndistribution is often independent of real data and thus may lose semantic\\ninformation (e.g., geometric structure or content in images) of data. In\\npractice, the semantic information might be represented by some latent\\ndistribution learned from data, which, however, is hard to be used for sampling\\nin GANs. In this paper, rather than sampling from the pre-defined prior\\ndistribution, we propose a Local Coordinate Coding (LCC) based sampling method\\nto improve GANs. We derive a generalization bound for LCC based GANs and prove\\nthat a small dimensional input is sufficient to achieve good generalization.\\nExtensive experiments on various real-world datasets demonstrate the\\neffectiveness of the proposed method.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04895v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04895v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Jiezhang Cao',\n",
       "   'Yong Guo',\n",
       "   'Qingyao Wu',\n",
       "   'Chunhua Shen',\n",
       "   'Junzhou Huang',\n",
       "   'Mingkui Tan'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'LCC',\n",
       "    'full_name': 'Lipschitz Constant Constraint',\n",
       "    'description': 'Please enter a description about the method here',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1804.04368v3',\n",
       "    'source_title': 'Regularisation of Neural Networks by Enforcing Lipschitz Continuity',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['celeba', 'oxford-102-flower'],\n",
       "  'datasets_used_full': ['CelebA', 'Oxford 102 Flower'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/talakat-bullet-hell-generation-through',\n",
       "  'arxiv_id': '1806.04718',\n",
       "  'title': 'Talakat: Bullet Hell Generation through Constrained Map-Elites',\n",
       "  'abstract': 'We describe a search-based approach to generating new levels for bullet hell\\ngames, which are action games characterized by and requiring avoidance of a\\nvery large amount of projectiles. Levels are represented using a\\ndomain-specific description language, and search in the space defined by this\\nlanguage is performed by a novel variant of the Map-Elites algorithm which\\nincorporates a feasible- infeasible approach to constraint satisfaction.\\nSimulation-based evaluation is used to gauge the fitness of levels, using an\\nagent based on best-first search. The performance of the agent can be tuned\\naccording to the two dimensions of strategy and dexterity, making it possible\\nto search for level configurations that require a specific combination of both.\\nAs far as we know, this paper describes the first generator for this game\\ngenre, and includes several algorithmic innovations.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04718v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04718v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ahmed Khalifa', 'Scott Lee', 'Andy Nealen', 'Julian Togelius'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-generative-modeling-approach-to-limited',\n",
       "  'arxiv_id': '1802.06458',\n",
       "  'title': 'A Generative Modeling Approach to Limited Channel ECG Classification',\n",
       "  'abstract': 'Processing temporal sequences is central to a variety of applications in\\nhealth care, and in particular multi-channel Electrocardiogram (ECG) is a\\nhighly prevalent diagnostic modality that relies on robust sequence modeling.\\nWhile Recurrent Neural Networks (RNNs) have led to significant advances in\\nautomated diagnosis with time-series data, they perform poorly when models are\\ntrained using a limited set of channels. A crucial limitation of existing\\nsolutions is that they rely solely on discriminative models, which tend to\\ngeneralize poorly in such scenarios. In order to combat this limitation, we\\ndevelop a generative modeling approach to limited channel ECG classification.\\nThis approach first uses a Seq2Seq model to implicitly generate the missing\\nchannel information, and then uses the latent representation to perform the\\nactual supervisory task. This decoupling enables the use of unsupervised data\\nand also provides highly robust metric spaces for subsequent discriminative\\nlearning. Our experiments with the Physionet dataset clearly evidence the\\neffectiveness of our approach over standard RNNs in disease prediction.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.06458v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.06458v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Deepta Rajan', 'Jayaraman J. Thiagarajan'],\n",
       "  'tasks': ['Classification',\n",
       "   'Disease Prediction',\n",
       "   'ECG Classification',\n",
       "   'General Classification',\n",
       "   'Time Series'],\n",
       "  'date': '2018-02-18',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}},\n",
       "   {'name': 'Seq2Seq',\n",
       "    'full_name': 'Sequence to Sequence',\n",
       "    'description': '**Seq2Seq**, or **Sequence To Sequence**, is a model used in sequence prediction tasks, such as language modelling and machine translation. The idea is to use one [LSTM](https://paperswithcode.com/method/lstm), the *encoder*, to read the input sequence one timestep at a time, to obtain a large fixed dimensional vector representation (a context vector), and then to use another LSTM, the *decoder*, to extract the output sequence\\r\\nfrom that vector. The second LSTM is essentially a recurrent neural network language model except that it is conditioned on the input sequence.\\r\\n\\r\\n(Note that this page refers to the original seq2seq not general sequence-to-sequence models)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1409.3215v3',\n",
       "    'source_title': 'Sequence to Sequence Learning with Neural Networks',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Sequence To Sequence Models',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/scsp-spectral-clustering-filter-pruning-with',\n",
       "  'arxiv_id': '1806.05320',\n",
       "  'title': 'SCSP: Spectral Clustering Filter Pruning with Soft Self-adaption Manners',\n",
       "  'abstract': 'Deep Convolutional Neural Networks (CNN) has achieved significant success in\\ncomputer vision field. However, the high computational cost of the deep complex\\nmodels prevents the deployment on edge devices with limited memory and\\ncomputational resource. In this paper, we proposed a novel filter pruning for\\nconvolutional neural networks compression, namely spectral clustering filter\\npruning with soft self-adaption manners (SCSP). We first apply spectral\\nclustering on filters layer by layer to explore their intrinsic connections and\\nonly count on efficient groups. By self-adaption manners, the pruning\\noperations can be done in few epochs to let the network gradually choose\\nmeaningful groups. According to this strategy, we not only achieve model\\ncompression while keeping considerable performance, but also find a novel angle\\nto interpret the model compression process.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05320v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05320v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Huiyuan Zhuo',\n",
       "   'Xuelin Qian',\n",
       "   'Yanwei Fu',\n",
       "   'Heng Yang',\n",
       "   'xiangyang xue'],\n",
       "  'tasks': ['Model Compression'],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [{'name': 'Spectral Clustering',\n",
       "    'full_name': 'Spectral Clustering',\n",
       "    'description': 'Spectral clustering has attracted increasing attention due to\\r\\nthe promising ability in dealing with nonlinearly separable datasets [15], [16]. In spectral clustering, the spectrum of the graph Laplacian is used to reveal the cluster structure. The spectral clustering algorithm mainly consists of two steps: 1) constructs the low dimensional embedded representation of the data based on the eigenvectors of the graph Laplacian, 2) applies k-means on the constructed low dimensional data to obtain the clustering result. Thus,',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/0711.0189v1',\n",
       "    'source_title': 'A Tutorial on Spectral Clustering',\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Clustering',\n",
       "     'description': '**Clustering** methods cluster a dataset so that similar datapoints are located in the same group. Below you can find a continuously updating list of clustering methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['mnist'],\n",
       "  'datasets_used_full': ['MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/interpretable-partitioned-embedding-for',\n",
       "  'arxiv_id': '1806.04845',\n",
       "  'title': 'Interpretable Partitioned Embedding for Customized Fashion Outfit Composition',\n",
       "  'abstract': \"Intelligent fashion outfit composition becomes more and more popular in these\\nyears. Some deep learning based approaches reveal competitive composition\\nrecently. However, the unexplainable characteristic makes such deep learning\\nbased approach cannot meet the the designer, businesses and consumers' urge to\\ncomprehend the importance of different attributes in an outfit composition. To\\nrealize interpretable and customized fashion outfit compositions, we propose a\\npartitioned embedding network to learn interpretable representations from\\nclothing items. The overall network architecture consists of three components:\\nan auto-encoder module, a supervised attributes module and a multi-independent\\nmodule. The auto-encoder module serves to encode all useful information into\\nthe embedding. In the supervised attributes module, multiple attributes labels\\nare adopted to ensure that different parts of the overall embedding correspond\\nto different attributes. In the multi-independent module, adversarial operation\\nare adopted to fulfill the mutually independent constraint. With the\\ninterpretable and partitioned embedding, we then construct an outfit\\ncomposition graph and an attribute matching map. Given specified attributes\\ndescription, our model can recommend a ranked list of outfit composition with\\ninterpretable matching scores. Extensive experiments demonstrate that 1) the\\npartitioned embedding have unmingled parts which corresponding to different\\nattributes and 2) outfits recommended by our model are more desirable in\\ncomparison with the existing methods.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04845v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04845v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Zunlei Feng',\n",
       "   'Zhenyun Yu',\n",
       "   'Yezhou Yang',\n",
       "   'Yongcheng Jing',\n",
       "   'Junxiao Jiang',\n",
       "   'Mingli Song'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/projection-free-online-optimization-with',\n",
       "  'arxiv_id': '1802.08183',\n",
       "  'title': 'Projection-Free Online Optimization with Stochastic Gradient: From Convexity to Submodularity',\n",
       "  'abstract': 'Online optimization has been a successful framework for solving large-scale\\nproblems under computational constraints and partial information. Current\\nmethods for online convex optimization require either a projection or exact\\ngradient computation at each step, both of which can be prohibitively expensive\\nfor large-scale applications. At the same time, there is a growing trend of\\nnon-convex optimization in machine learning community and a need for online\\nmethods. Continuous DR-submodular functions, which exhibit a natural\\ndiminishing returns condition, have recently been proposed as a broad class of\\nnon-convex functions which may be efficiently optimized. Although online\\nmethods have been introduced, they suffer from similar problems. In this work,\\nwe propose Meta-Frank-Wolfe, the first online projection-free algorithm that\\nuses stochastic gradient estimates. The algorithm relies on a careful sampling\\nof gradients in each round and achieves the optimal $O( \\\\sqrt{T})$ adversarial\\nregret bounds for convex and continuous submodular optimization. We also\\npropose One-Shot Frank-Wolfe, a simpler algorithm which requires only a single\\nstochastic gradient estimate in each round and achieves an $O(T^{2/3})$\\nstochastic regret bound for convex and continuous submodular optimization. We\\napply our methods to develop a novel \"lifting\" framework for the online\\ndiscrete submodular maximization and also see that they outperform current\\nstate-of-the-art techniques on various experiments.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.08183v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.08183v4.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Lin Chen',\n",
       "   'Christopher Harshaw',\n",
       "   'Hamed Hassani',\n",
       "   'Amin Karbasi'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-22',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['jester'],\n",
       "  'datasets_used_full': ['Jester'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multilingual-end-to-end-speech-recognition',\n",
       "  'arxiv_id': '1806.05059',\n",
       "  'title': 'Multilingual End-to-End Speech Recognition with A Single Transformer on Low-Resource Languages',\n",
       "  'abstract': 'Sequence-to-sequence attention-based models integrate an acoustic,\\npronunciation and language model into a single neural network, which make them\\nvery suitable for multilingual automatic speech recognition (ASR). In this\\npaper, we are concerned with multilingual speech recognition on low-resource\\nlanguages by a single Transformer, one of sequence-to-sequence attention-based\\nmodels. Sub-words are employed as the multilingual modeling unit without using\\nany pronunciation lexicon. First, we show that a single multilingual ASR\\nTransformer performs well on low-resource languages despite of some language\\nconfusion. We then look at incorporating language information into the model by\\ninserting the language symbol at the beginning or at the end of the original\\nsub-words sequence under the condition of language information being known\\nduring training. Experiments on CALLHOME datasets demonstrate that the\\nmultilingual ASR Transformer with the language symbol at the end performs\\nbetter and can obtain relatively 10.5\\\\% average word error rate (WER) reduction\\ncompared to SHL-MLSTM with residual learning. We go on to show that, assuming\\nthe language information being known during training and testing, about\\nrelatively 12.4\\\\% average WER reduction can be observed compared to SHL-MLSTM\\nwith residual learning through giving the language symbol as the sentence start\\ntoken.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05059v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05059v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shiyu Zhou', 'Shuang Xu', 'Bo Xu'],\n",
       "  'tasks': ['Automatic Speech Recognition',\n",
       "   'Language Modelling',\n",
       "   'Speech Recognition'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [{'name': 'Absolute Position Encodings',\n",
       "    'full_name': 'Absolute Position Encodings',\n",
       "    'description': '**Absolute Position Encodings** are a type of position embeddings for [[Transformer](https://paperswithcode.com/method/transformer)-based models] where positional encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension $d\\\\_{model}$ as the embeddings, so that the two can be summed. In the original implementation, sine and cosine functions of different frequencies are used:\\r\\n\\r\\n$$ \\\\text{PE}\\\\left(pos, 2i\\\\right) = \\\\sin\\\\left(pos/10000^{2i/d\\\\_{model}}\\\\right) $$\\r\\n\\r\\n$$ \\\\text{PE}\\\\left(pos, 2i+1\\\\right) = \\\\cos\\\\left(pos/10000^{2i/d\\\\_{model}}\\\\right) $$\\r\\n\\r\\nwhere $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\\\pi$ to $10000 \\\\dot 2\\\\pi$. This function was chosen because the authors hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$,  $\\\\text{PE}\\\\_{pos+k}$ can be represented as a linear function of $\\\\text{PE}\\\\_{pos}$.\\r\\n\\r\\nImage Source: [D2L.ai](https://d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Position Embeddings',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Position-Wise Feed-Forward Layer',\n",
       "    'full_name': 'Position-Wise Feed-Forward Layer',\n",
       "    'description': '**Position-Wise Feed-Forward Layer** is a type of [feedforward layer](https://www.paperswithcode.com/method/category/feedforwad-networks) consisting of two [dense layers](https://www.paperswithcode.com/method/dense-connections) that applies to the last dimension, which means the same dense layers are used for each position item in the sequence, so called position-wise.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Residual Connection',\n",
       "    'full_name': 'Residual Connection',\n",
       "    'description': '**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'BPE',\n",
       "    'full_name': 'Byte Pair Encoding',\n",
       "    'description': '**Byte Pair Encoding**, or **BPE**, is a subword segmentation algorithm that encodes rare and unknown words as sequences of subword units. The intuition is that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).\\r\\n\\r\\n[Lei Mao](https://leimao.github.io/blog/Byte-Pair-Encoding/) has a detailed blog post that explains how this works.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1508.07909v5',\n",
       "    'source_title': 'Neural Machine Translation of Rare Words with Subword Units',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Subword Segmentation',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Natural Language Processing'}},\n",
       "   {'name': 'Dense Connections',\n",
       "    'full_name': 'Dense Connections',\n",
       "    'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\\\_{\\\\text{inputs}}*n\\\\_{\\\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\\r\\n\\r\\n$$h\\\\_{l} = g\\\\left(\\\\textbf{W}^{T}h\\\\_{l-1}\\\\right)$$\\r\\n\\r\\nwhere $g$ is an activation function.\\r\\n\\r\\nImage Source: Deep Learning by Goodfellow, Bengio and Courville',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Label Smoothing',\n",
       "    'full_name': 'Label Smoothing',\n",
       "    'description': '**Label Smoothing** is a regularization technique that introduces noise for the labels. This accounts for the fact that datasets may have mistakes in them, so maximizing the likelihood of $\\\\log{p}\\\\left(y\\\\mid{x}\\\\right)$ directly can be harmful. Assume for a small constant $\\\\epsilon$, the training set label $y$ is correct with probability $1-\\\\epsilon$ and incorrect otherwise. Label Smoothing regularizes a model based on a [softmax](https://paperswithcode.com/method/softmax) with $k$ output values by replacing the hard $0$ and $1$ classification targets with targets of $\\\\frac{\\\\epsilon}{k-1}$ and $1-\\\\epsilon$ respectively.\\r\\n\\r\\nSource: Deep Learning, Goodfellow et al\\r\\n\\r\\nImage Source: [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629)',\n",
       "    'introduced_year': 1985,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Adam',\n",
       "    'full_name': 'Adam',\n",
       "    'description': '**Adam** is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of [RMSProp](https://paperswithcode.com/method/rmsprop) and [SGD w/th Momentum](https://paperswithcode.com/method/sgd-with-momentum). The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. \\r\\n\\r\\nThe weight updates are performed as:\\r\\n\\r\\n$$ w_{t} = w_{t-1} - \\\\eta\\\\frac{\\\\hat{m}\\\\_{t}}{\\\\sqrt{\\\\hat{v}\\\\_{t}} + \\\\epsilon}  $$\\r\\n\\r\\nwith\\r\\n\\r\\n$$ \\\\hat{m}\\\\_{t} = \\\\frac{m_{t}}{1-\\\\beta^{t}_{1}} $$\\r\\n\\r\\n$$ \\\\hat{v}\\\\_{t} = \\\\frac{v_{t}}{1-\\\\beta^{t}_{2}} $$\\r\\n\\r\\n$$ m_{t} = \\\\beta_{1}m_{t-1} + (1-\\\\beta_{1})g_{t} $$\\r\\n\\r\\n$$ v_{t} = \\\\beta_{2}v_{t-1} + (1-\\\\beta_{2})g_{t}^{2}  $$\\r\\n\\r\\n\\r\\n$ \\\\eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \\\\epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \\\\beta_{1} $ and $ \\\\beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1412.6980v9',\n",
       "    'source_title': 'Adam: A Method for Stochastic Optimization',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Dropout',\n",
       "    'full_name': 'Dropout',\n",
       "    'description': '**Dropout** is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p=0.5$). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).\\r\\n\\r\\nThe idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://jmlr.org/papers/v15/srivastava14a.html',\n",
       "    'source_title': 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/7f3078b70d0ed9bea6228efa420879c56f72ef69/jax/experimental/stax.py#L271-L275',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Multi-Head Attention',\n",
       "    'full_name': 'Multi-Head Attention',\n",
       "    'description': '**Multi-head Attention** is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies). \\r\\n\\r\\n$$ \\\\text{MultiHead}\\\\left(\\\\textbf{Q}, \\\\textbf{K}, \\\\textbf{V}\\\\right) = \\\\left[\\\\text{head}\\\\_{1},\\\\dots,\\\\text{head}\\\\_{h}\\\\right]\\\\textbf{W}_{0}$$\\r\\n\\r\\n$$\\\\text{where} \\\\text{ head}\\\\_{i} = \\\\text{Attention} \\\\left(\\\\textbf{Q}\\\\textbf{W}\\\\_{i}^{Q}, \\\\textbf{K}\\\\textbf{W}\\\\_{i}^{K}, \\\\textbf{V}\\\\textbf{W}\\\\_{i}^{V} \\\\right) $$\\r\\n\\r\\nAbove $\\\\textbf{W}$ are all learnable parameter matrices.\\r\\n\\r\\nNote that [scaled dot-product attention](https://paperswithcode.com/method/scaled) is most commonly used in this module, although in principle it can be swapped out for other types of attention mechanism.\\r\\n\\r\\nSource: [Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': 'https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/fec78a687210851f055f792d45300d27cc60ae41/transformer/SubLayers.py#L9',\n",
       "    'main_collection': {'name': 'Attention Modules',\n",
       "     'description': '**Attention Modules** refer to modules that incorporate attention mechanisms. For example, multi-head attention is a module that incorporates multiple attention heads. Below you can find a continuously updating list of attention modules.',\n",
       "     'parent': 'Attention',\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Layer Normalization',\n",
       "    'full_name': 'Layer Normalization',\n",
       "    'description': 'Unlike [batch normalization](https://paperswithcode.com/method/batch-normalization), **Layer Normalization** directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. It works well for [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) and improves both the training time and the generalization performance of several existing RNN models. More recently, it has been used with [Transformer](https://paperswithcode.com/methods/category/transformers) models.\\r\\n\\r\\nWe compute the layer normalization statistics over all the hidden units in the same layer as follows:\\r\\n\\r\\n$$ \\\\mu^{l} = \\\\frac{1}{H}\\\\sum^{H}\\\\_{i=1}a\\\\_{i}^{l} $$\\r\\n\\r\\n$$ \\\\sigma^{l} = \\\\sqrt{\\\\frac{1}{H}\\\\sum^{H}\\\\_{i=1}\\\\left(a\\\\_{i}^{l}-\\\\mu^{l}\\\\right)^{2}}  $$\\r\\n\\r\\nwhere $H$ denotes the number of hidden units in a layer. Under layer normalization, all the hidden units in a layer share the same normalization terms $\\\\mu$ and $\\\\sigma$, but different training cases have different normalization terms. Unlike batch normalization, layer normalization does not impose any constraint on the size of the mini-batch and it can be used in the pure online regime with batch size 1.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1607.06450v1',\n",
       "    'source_title': 'Layer Normalization',\n",
       "    'code_snippet_url': 'https://github.com/CyberZHG/torch-layer-normalization/blob/89f405b60f53f85da6f03fe685c190ef394ce50c/torch_layer_normalization/layer_normalization.py#L8',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Scaled Dot-Product Attention',\n",
       "    'full_name': 'Scaled Dot-Product Attention',\n",
       "    'description': '**Scaled dot-product attention** is an attention mechanism where the dot products are scaled down by $\\\\sqrt{d_k}$. Formally we have a query $Q$, a key $K$ and a value $V$ and calculate the attention as:\\r\\n\\r\\n$$ {\\\\text{Attention}}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^{T}}{\\\\sqrt{d_k}}\\\\right)V $$\\r\\n\\r\\nIf we assume that $q$ and $k$ are $d_k$-dimensional vectors whose components are independent random variables with mean $0$ and variance $1$, then their dot product, $q \\\\cdot k = \\\\sum_{i=1}^{d_k} u_iv_i$, has mean $0$ and variance $d_k$.  Since we would prefer these values to have variance $1$, we divide by $\\\\sqrt{d_k}$.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': 'https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/5c0264915ab43485adc576f88971fc3d42b10445/transformer/Modules.py#L7',\n",
       "    'main_collection': {'name': 'Attention Mechanisms',\n",
       "     'description': '**Attention Mechanisms** are a component used in neural networks to model long-range interaction, for example across a text in NLP. The key idea is to build shortcuts between a context vector and the input, to allow a model to attend to different parts. Below you can find a continuously updating list of attention mechanisms.',\n",
       "     'parent': 'Attention',\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Transformer',\n",
       "    'full_name': 'Transformer',\n",
       "    'description': 'A **Transformer** is a model architecture that eschews recurrence and instead relies entirely on an [attention mechanism](https://paperswithcode.com/methods/category/attention-mechanisms-1) to draw global dependencies between input and output. Before Transformers, the dominant sequence transduction models were based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The Transformer also employs an encoder and decoder, but removing recurrence in favor of [attention mechanisms](https://paperswithcode.com/methods/category/attention-mechanisms-1) allows for significantly more parallelization than methods like [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) and [CNNs](https://paperswithcode.com/methods/category/convolutional-neural-networks).',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': 'https://github.com/tunz/transformer-pytorch/blob/e7266679f0b32fd99135ea617213f986ceede056/model/transformer.py#L201',\n",
       "    'main_collection': {'name': 'Transformers',\n",
       "     'description': '**Transformers** are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embedidngs.',\n",
       "     'parent': 'Language Models',\n",
       "     'area': 'Natural Language Processing'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-reinforcement-learning-for-dynamic-urban',\n",
       "  'arxiv_id': '1806.05310',\n",
       "  'title': 'Deep Reinforcement Learning for Dynamic Urban Transportation Problems',\n",
       "  'abstract': 'We explore the use of deep learning and deep reinforcement learning for\\noptimization problems in transportation. Many transportation system analysis\\ntasks are formulated as an optimization problem - such as optimal control\\nproblems in intelligent transportation systems and long term urban planning.\\nOften transportation models used to represent dynamics of a transportation\\nsystem involve large data sets with complex input-output interactions and are\\ndifficult to use in the context of optimization. Use of deep learning\\nmetamodels can produce a lower dimensional representation of those relations\\nand allow to implement optimization and reinforcement learning algorithms in an\\nefficient manner. In particular, we develop deep learning models for\\ncalibrating transportation simulators and for reinforcement learning to solve\\nthe problem of optimal scheduling of travelers on the network.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05310v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05310v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Laura Schultz', 'Vadim Sokolov'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/stress-test-evaluation-for-natural-language',\n",
       "  'arxiv_id': '1806.00692',\n",
       "  'title': 'Stress Test Evaluation for Natural Language Inference',\n",
       "  'abstract': 'Natural language inference (NLI) is the task of determining if a natural\\nlanguage hypothesis can be inferred from a given premise in a justifiable\\nmanner. NLI was proposed as a benchmark task for natural language\\nunderstanding. Existing models perform well at standard datasets for NLI,\\nachieving impressive results across different genres of text. However, the\\nextent to which these models understand the semantic content of sentences is\\nunclear. In this work, we propose an evaluation methodology consisting of\\nautomatically constructed \"stress tests\" that allow us to examine whether\\nsystems have the ability to make real inferential decisions. Our evaluation of\\nsix sentence-encoder models on these stress tests reveals strengths and\\nweaknesses of these models with respect to challenging linguistic phenomena,\\nand suggests important directions for future work in this area.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.00692v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.00692v3.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Aakanksha Naik',\n",
       "   'Abhilasha Ravichander',\n",
       "   'Norman Sadeh',\n",
       "   'Carolyn Rose',\n",
       "   'Graham Neubig'],\n",
       "  'tasks': ['Natural Language Inference', 'Natural Language Understanding'],\n",
       "  'date': '2018-06-02',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['multinli', 'sick', 'aqua-rat'],\n",
       "  'datasets_used_full': ['MultiNLI', 'SICK', 'AQUA-RAT'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/geometric-shape-features-extraction-using-a',\n",
       "  'arxiv_id': '1806.05299',\n",
       "  'title': 'Geometric Shape Features Extraction Using a Steady State Partial Differential Equation System',\n",
       "  'abstract': 'A unified method for extracting geometric shape features from binary image\\ndata using a steady state partial differential equation (PDE) system as a\\nboundary value problem is presented in this paper. The PDE and functions are\\nformulated to extract the thickness, orientation, and skeleton simultaneously.\\nThe main advantages of the proposed method is that the orientation is defined\\nwithout derivatives and thickness computation is not imposed a topological\\nconstraint on the target shape. A one-dimensional analytical solution is\\nprovided to validate the proposed method. In addition, two-dimensional\\nnumerical examples are presented to confirm the usefulness of the proposed\\nmethod.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05299v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05299v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Takayuki Yamada'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/apuntes-de-redes-neuronales-artificiales',\n",
       "  'arxiv_id': '1806.05298',\n",
       "  'title': 'Apuntes de Redes Neuronales Artificiales',\n",
       "  'abstract': \"These handouts are designed for people who is just starting involved with the\\ntopic artificial neural networks. We show how it works a single artificial\\nneuron (McCulloch & Pitt model), mathematically and graphically. We do explain\\nthe delta rule, a learning algorithm to find the neuron weights. We also\\npresent some examples in MATLAB/Octave. There are examples for classification\\ntask for lineal and non-lineal problems. At the end, we present an artificial\\nneural network, a feed-forward neural network along its learning algorithm\\nbackpropagation.\\n  -----\\n  Estos apuntes est\\\\'an dise\\\\~nados para personas que por primera vez se\\nintroducen en el tema de las redes neuronales artificiales. Se muestra el\\nfuncionamiento b\\\\'asico de una neurona, matem\\\\'aticamente y gr\\\\'aficamente. Se\\nexplica la Regla Delta, algoritmo deaprendizaje para encontrar los pesos de una\\nneurona. Tambi\\\\'en se muestran ejemplos en MATLAB/Octave. Hay ejemplos para\\nproblemas de clasificaci\\\\'on, para problemas lineales y no-lineales. En la\\nparte final se muestra la arquitectura de red neuronal artificial conocida como\\nbackpropagation.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05298v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05298v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['J. C. Cuevas-Tello'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/pattern-dependence-detection-using-n-tarp',\n",
       "  'arxiv_id': '1806.05297',\n",
       "  'title': 'Pattern Dependence Detection using n-TARP Clustering',\n",
       "  'abstract': 'Consider an experiment involving a potentially small number of subjects. Some\\nrandom variables are observed on each subject: a high-dimensional one called\\nthe \"observed\" random variable, and a one-dimensional one called the \"outcome\"\\nrandom variable. We are interested in the dependencies between the observed\\nrandom variable and the outcome random variable. We propose a method to\\nquantify and validate the dependencies of the outcome random variable on the\\nvarious patterns contained in the observed random variable. Different degrees\\nof relationship are explored (linear, quadratic, cubic, ...). This work is\\nmotivated by the need to analyze educational data, which often involves\\nhigh-dimensional data representing a small number of students. Thus our\\nimplementation is designed for a small number of subjects; however, it can be\\neasily modified to handle a very large dataset. As an illustration, the\\nproposed method is used to study the influence of certain skills on the course\\ngrade of students in a signal processing class. A valid dependency of the grade\\non the different skill patterns is observed in the data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05297v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05297v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Tarun Yellamraju', 'Mireille Boutin'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/projecting-embeddings-for-domain-adaptation',\n",
       "  'arxiv_id': '1806.04381',\n",
       "  'title': 'Projecting Embeddings for Domain Adaptation: Joint Modeling of Sentiment Analysis in Diverse Domains',\n",
       "  'abstract': 'Domain adaptation for sentiment analysis is challenging due to the fact that\\nsupervised classifiers are very sensitive to changes in domain. The two most\\nprominent approaches to this problem are structural correspondence learning and\\nautoencoders. However, they either require long training times or suffer\\ngreatly on highly divergent domains. Inspired by recent advances in\\ncross-lingual sentiment analysis, we provide a novel perspective and cast the\\ndomain adaptation problem as an embedding projection task. Our model takes as\\ninput two mono-domain embedding spaces and learns to project them to a\\nbi-domain space, which is jointly optimized to (1) project across domains and\\nto (2) predict sentiment. We perform domain adaptation experiments on 20\\nsource-target domain pairs for sentiment classification and report novel\\nstate-of-the-art results on 11 domain pairs, including the Amazon domain\\nadaptation datasets and SemEval 2013 and 2016 datasets. Our analysis shows that\\nour model performs comparably to state-of-the-art approaches on domains that\\nare similar, while performing significantly better on highly divergent domains.\\nOur code is available at https://github.com/jbarnesspain/domain_blse',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04381v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04381v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jeremy Barnes', 'Roman Klinger', 'Sabine Schulte im Walde'],\n",
       "  'tasks': ['Domain Adaptation', 'Sentiment Analysis'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/patternnet-visual-pattern-mining-with-deep',\n",
       "  'arxiv_id': '1703.06339',\n",
       "  'title': 'PatternNet: Visual Pattern Mining with Deep Neural Network',\n",
       "  'abstract': 'Visual patterns represent the discernible regularity in the visual world.\\nThey capture the essential nature of visual objects or scenes. Understanding\\nand modeling visual patterns is a fundamental problem in visual recognition\\nthat has wide ranging applications. In this paper, we study the problem of\\nvisual pattern mining and propose a novel deep neural network architecture\\ncalled PatternNet for discovering these patterns that are both discriminative\\nand representative. The proposed PatternNet leverages the filters in the last\\nconvolution layer of a convolutional neural network to find locally consistent\\nvisual patches, and by combining these filters we can effectively discover\\nunique visual patterns. In addition, PatternNet can discover visual patterns\\nefficiently without performing expensive image patch sampling, and this\\nadvantage provides an order of magnitude speedup compared to most other\\napproaches. We evaluate the proposed PatternNet subjectively by showing\\nrandomly selected visual patterns which are discovered by our method and\\nquantitatively by performing image classification with the identified visual\\npatterns and comparing our performance with the current state-of-the-art. We\\nalso directly evaluate the quality of the discovered visual patterns by\\nleveraging the identified patterns as proposed objects in an image and compare\\nwith other relevant methods. Our proposed network and procedure, PatterNet, is\\nable to outperform competing methods for the tasks described.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1703.06339v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1703.06339v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hongzhi Li', 'Joseph G. Ellis', 'Lei Zhang', 'Shih-Fu Chang'],\n",
       "  'tasks': ['Image Classification'],\n",
       "  'date': '2017-03-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/automatic-formation-of-the-structure-of',\n",
       "  'arxiv_id': '1806.05292',\n",
       "  'title': 'Automatic formation of the structure of abstract machines in hierarchical reinforcement learning with state clustering',\n",
       "  'abstract': 'We introduce a new approach to hierarchy formation and task decomposition in\\nhierarchical reinforcement learning. Our method is based on the Hierarchy Of\\nAbstract Machines (HAM) framework because HAM approach is able to design\\nefficient controllers that will realize specific behaviors in real robots. The\\nkey to our algorithm is the introduction of the internal or \"mental\"\\nenvironment in which the state represents the structure of the HAM hierarchy.\\nThe internal action in this environment leads to changes the hierarchy of HAMs.\\nWe propose the classical Q-learning procedure in the internal environment which\\nallows the agent to obtain an optimal hierarchy. We extends the HAM framework\\nby adding on-model approach to select the appropriate sub-machine to execute\\naction sequences for certain class of external environment states. Preliminary\\nexperiments demonstrated the prospects of the method.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05292v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05292v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Aleksandr I. Panov', 'Aleksey Skrynnik'],\n",
       "  'tasks': ['Hierarchical Reinforcement Learning', 'Q-Learning'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'Q-Learning',\n",
       "    'full_name': 'Q-Learning',\n",
       "    'description': '**Q-Learning** is an off-policy temporal difference control algorithm:\\r\\n\\r\\n$$Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) \\\\leftarrow Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) + \\\\alpha\\\\left[R_{t+1} + \\\\gamma\\\\max\\\\_{a}Q\\\\left(S\\\\_{t+1}, a\\\\right) - Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right)\\\\right] $$\\r\\n\\r\\nThe learned action-value function $Q$ directly approximates $q\\\\_{*}$, the optimal action-value function, independent of the policy being followed.\\r\\n\\r\\nSource: Sutton and Barto, Reinforcement Learning, 2nd Edition',\n",
       "    'introduced_year': 1984,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Off-Policy TD Control',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Reinforcement Learning'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/natural-language-to-structured-query',\n",
       "  'arxiv_id': '1803.02400',\n",
       "  'title': 'Natural Language to Structured Query Generation via Meta-Learning',\n",
       "  'abstract': 'In conventional supervised training, a model is trained to fit all the\\ntraining examples. However, having a monolithic model may not always be the\\nbest strategy, as examples could vary widely. In this work, we explore a\\ndifferent learning protocol that treats each example as a unique pseudo-task,\\nby reducing the original learning problem to a few-shot meta-learning scenario\\nwith the help of a domain-dependent relevance function. When evaluated on the\\nWikiSQL dataset, our approach leads to faster convergence and achieves\\n1.1%-5.4% absolute accuracy gains over the non-meta-learning counterparts.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.02400v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.02400v4.pdf',\n",
       "  'proceeding': 'NAACL 2018 6',\n",
       "  'authors': ['Po-Sen Huang',\n",
       "   'Chenglong Wang',\n",
       "   'Rishabh Singh',\n",
       "   'Wen-tau Yih',\n",
       "   'Xiaodong He'],\n",
       "  'tasks': ['Meta-Learning'],\n",
       "  'date': '2018-03-02',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['wikisql'],\n",
       "  'datasets_used_full': ['WikiSQL'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-flexible-convolutional-solver-with',\n",
       "  'arxiv_id': '1806.05285',\n",
       "  'title': 'A Flexible Convolutional Solver with Application to Photorealistic Style Transfer',\n",
       "  'abstract': 'We propose a new flexible deep convolutional neural network (convnet) to\\nperform fast visual style transfer. In contrast to existing convnets that\\naddress the same task, our architecture derives directly from the structure of\\nthe gradient descent originally used to solve the style transfer problem [Gatys\\net al., 2016]. Like existing convnets, ours approximately solves the original\\nproblem much faster than the gradient descent. However, our network is uniquely\\nflexible by design: it can be manipulated at runtime to enforce new constraints\\non the final solution. In particular, we show how to modify it to obtain a\\nphotorealistic result with no retraining. We study the modifications made by\\n[Luan et al., 2017] to the original cost function of [Gatys et al., 2016] to\\nachieve photorealistic style transfer. These modifications affect directly the\\ngradient descent and can be reported on-the-fly in our network. These\\nmodifications are possible as the proposed architecture stems from unrolling\\nthe gradient descent.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05285v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05285v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Gilles Puy', 'Patrick Pérez'],\n",
       "  'tasks': ['Style Transfer'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['coco'],\n",
       "  'datasets_used_full': ['COCO'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/how-predictable-is-your-state-leveraging',\n",
       "  'arxiv_id': '1806.05284',\n",
       "  'title': 'How Predictable is Your State? Leveraging Lexical and Contextual Information for Predicting Legislative Floor Action at the State Level',\n",
       "  'abstract': 'Modeling U.S. Congressional legislation and roll-call votes has received\\nsignificant attention in previous literature. However, while legislators across\\n50 state governments and D.C. propose over 100,000 bills each year, and on\\naverage enact over 30% of them, state level analysis has received relatively\\nless attention due in part to the difficulty in obtaining the necessary data.\\nSince each state legislature is guided by their own procedures, politics and\\nissues, however, it is difficult to qualitatively asses the factors that affect\\nthe likelihood of a legislative initiative succeeding. Herein, we present\\nseveral methods for modeling the likelihood of a bill receiving floor action\\nacross all 50 states and D.C. We utilize the lexical content of over 1 million\\nbills, along with contextual legislature and legislator derived features to\\nbuild our predictive models, allowing a comparison of the factors that are\\nimportant to the lawmaking process. Furthermore, we show that these signals\\nhold complementary predictive power, together achieving an average improvement\\nin accuracy of 18% over state specific baselines.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05284v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05284v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Vlad Eidelman', 'Anastassia Kornilova', 'Daniel Argyle'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/solving-the-steiner-tree-problem-in-graphs',\n",
       "  'arxiv_id': '1806.06685',\n",
       "  'title': 'Solving the Steiner Tree Problem in graphs with Variable Neighborhood Descent',\n",
       "  'abstract': 'The Steiner Tree Problem (STP) in graphs is an important problem with various\\napplications in many areas such as design of integrated circuits, evolution\\ntheory, networking, etc. In this paper, we propose an algorithm to solve the\\nSTP. The algorithm includes a reducer and a solver using Variable Neighborhood\\nDescent (VND), interacting with each other during the search. New constructive\\nheuristics and a vertex score system for intensification purpose are proposed.\\nThe algorithm is tested on a set of benchmarks which shows encouraging results.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06685v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06685v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Matthieu De Laere', 'San Tu Pham', 'Patrick De Causmaecker'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-privacy-preserving-encodings-through',\n",
       "  'arxiv_id': '1802.05214',\n",
       "  'title': 'Learning Privacy Preserving Encodings through Adversarial Training',\n",
       "  'abstract': 'We present a framework to learn privacy-preserving encodings of images that\\ninhibit inference of chosen private attributes, while allowing recovery of\\nother desirable information. Rather than simply inhibiting a given fixed\\npre-trained estimator, our goal is that an estimator be unable to learn to\\naccurately predict the private attributes even with knowledge of the encoding\\nfunction. We use a natural adversarial optimization-based formulation for\\nthis---training the encoding function against a classifier for the private\\nattribute, with both modeled as deep neural networks. The key contribution of\\nour work is a stable and convergent optimization approach that is successful at\\nlearning an encoder with our desired properties---maintaining utility while\\ninhibiting inference of private attributes, not just within the adversarial\\noptimization, but also by classifiers that are trained after the encoder is\\nfixed. We adopt a rigorous experimental protocol for verification wherein\\nclassifiers are trained exhaustively till saturation on the fixed encoders. We\\nevaluate our approach on tasks of real-world complexity---learning\\nhigh-dimensional encodings that inhibit detection of different scene\\ncategories---and find that it yields encoders that are resilient at maintaining\\nprivacy.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.05214v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.05214v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Francesco Pittaluga', 'Sanjeev J. Koppal', 'Ayan Chakrabarti'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['places'],\n",
       "  'datasets_used_full': ['Places'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/online-learning-over-a-finite-action-set-with',\n",
       "  'arxiv_id': '1803.01548',\n",
       "  'title': 'Online learning over a finite action set with limited switching',\n",
       "  'abstract': 'This paper studies the value of switching actions in the Prediction From\\nExperts (PFE) problem and Adversarial Multi-Armed Bandits (MAB) problem. First,\\nwe revisit the well-studied and practically motivated setting of PFE with\\nswitching costs. Many algorithms are known to achieve the minimax optimal order\\nof $O(\\\\sqrt{T \\\\log n})$ in expectation for both regret and number of switches,\\nwhere $T$ is the number of iterations and $n$ the number of actions. However,\\nno high probability (h.p.) guarantees are known. Our main technical\\ncontribution is the first algorithms which with h.p. achieve this optimal order\\nfor both regret and switches. This settles an open problem of [Devroye et al.,\\n2015], and directly implies the first h.p. guarantees for several problems of\\ninterest.\\n  Next, to investigate the value of switching actions at a more granular level,\\nwe introduce the setting of switching budgets, in which algorithms are limited\\nto $S \\\\leq T$ switches between actions. This entails a limited number of free\\nswitches, in contrast to the unlimited number of expensive switches in the\\nswitching cost setting. Using the above result and several reductions, we unify\\nprevious work and completely characterize the complexity of this switching\\nbudget setting up to small polylogarithmic factors: for both PFE and MAB, for\\nall switching budgets $S \\\\leq T$, and for both expectation and h.p. guarantees.\\nFor PFE, we show the optimal rate is $\\\\tilde{\\\\Theta}(\\\\sqrt{T\\\\log n})$ for $S =\\n\\\\Omega(\\\\sqrt{T\\\\log n})$, and $\\\\min(\\\\tilde{\\\\Theta}(\\\\tfrac{T\\\\log n}{S}), T)$ for\\n$S = O(\\\\sqrt{T \\\\log n})$. Interestingly, the bandit setting does not exhibit\\nsuch a phase transition; instead we show the minimax rate decays steadily as\\n$\\\\min(\\\\tilde{\\\\Theta}(\\\\tfrac{T\\\\sqrt{n}}{\\\\sqrt{S}}), T)$ for all ranges of $S\\n\\\\leq T$. These results recover and generalize the known minimax rates for the\\n(arbitrary) switching cost setting.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.01548v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.01548v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jason Altschuler', 'Kunal Talwar'],\n",
       "  'tasks': ['Multi-Armed Bandits', 'online learning'],\n",
       "  'date': '2018-03-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/benchmarks-for-image-classification-and-other',\n",
       "  'arxiv_id': '1806.05272',\n",
       "  'title': 'Benchmarks for Image Classification and Other High-dimensional Pattern Recognition Problems',\n",
       "  'abstract': 'A good classification method should yield more accurate results than simple\\nheuristics. But there are classification problems, especially high-dimensional\\nones like the ones based on image/video data, for which simple heuristics can\\nwork quite accurately; the structure of the data in such problems is easy to\\nuncover without any sophisticated or computationally expensive method. On the\\nother hand, some problems have a structure that can only be found with\\nsophisticated pattern recognition methods. We are interested in quantifying the\\ndifficulty of a given high-dimensional pattern recognition problem. We consider\\nthe case where the patterns come from two pre-determined classes and where the\\nobjects are represented by points in a high-dimensional vector space. However,\\nthe framework we propose is extendable to an arbitrarily large number of\\nclasses. We propose classification benchmarks based on simple random projection\\nheuristics. Our benchmarks are 2D curves parameterized by the classification\\nerror and computational cost of these simple heuristics. Each curve divides the\\nplane into a \"positive- gain\" and a \"negative-gain\" region. The latter contains\\nmethods that are ill-suited for the given classification problem. The former is\\ndivided into two by the curve asymptote; methods that lie in the small region\\nunder the curve but right of the asymptote merely provide a computational gain\\nbut no structural advantage over the random heuristics. We prove that the curve\\nasymptotes are optimal (i.e. at Bayes error) in some cases, and thus no\\nsophisticated method can provide a structural advantage over the random\\nheuristics. Such classification problems, an example of which we present in our\\nnumerical experiments, provide poor ground for testing new pattern\\nclassification methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05272v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05272v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Tarun Yellamraju', 'Jonas Hepp', 'Mireille Boutin'],\n",
       "  'tasks': ['Classification',\n",
       "   'General Classification',\n",
       "   'Image Classification'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/structured-variational-learning-of-bayesian',\n",
       "  'arxiv_id': '1806.05975',\n",
       "  'title': 'Structured Variational Learning of Bayesian Neural Networks with Horseshoe Priors',\n",
       "  'abstract': 'Bayesian Neural Networks (BNNs) have recently received increasing attention\\nfor their ability to provide well-calibrated posterior uncertainties. However,\\nmodel selection---even choosing the number of nodes---remains an open question.\\nRecent work has proposed the use of a horseshoe prior over node pre-activations\\nof a Bayesian neural network, which effectively turns off nodes that do not\\nhelp explain the data. In this work, we propose several modeling and inference\\nadvances that consistently improve the compactness of the model learned while\\nmaintaining predictive performance, especially in smaller-sample settings\\nincluding reinforcement learning.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05975v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05975v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Soumya Ghosh', 'Jiayu Yao', 'Finale Doshi-Velez'],\n",
       "  'tasks': ['Model Selection'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/online-self-supervised-scene-segmentation-for',\n",
       "  'arxiv_id': '1806.05269',\n",
       "  'title': 'Online Self-supervised Scene Segmentation for Micro Aerial Vehicles',\n",
       "  'abstract': 'Recently, there have been numerous advances in the development of payload and\\npower constrained lightweight Micro Aerial Vehicles (MAVs). As these robots\\naspire for high-speed autonomous flights in complex dynamic environments,\\nrobust scene understanding at long-range becomes critical. The problem is\\nheavily characterized by either the limitations imposed by sensor capabilities\\nfor geometry-based methods, or the need for large-amounts of manually annotated\\ntraining data required by data-driven methods. This motivates the need to build\\nsystems that have the capability to alleviate these problems by exploiting the\\ncomplimentary strengths of both geometry and data-driven methods. In this\\npaper, we take a step in this direction and propose a generic framework for\\nadaptive scene segmentation using self-supervised online learning. We present\\nthis in the context of vision-based autonomous MAV flight, and demonstrate the\\nefficacy of our proposed system through extensive experiments on benchmark\\ndatasets and real-world field tests.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05269v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05269v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shreyansh Daftry', 'Yashasvi Agrawal', 'Larry Matthies'],\n",
       "  'tasks': ['online learning', 'Scene Segmentation', 'Scene Understanding'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-framework-for-validating-models-of-evasion',\n",
       "  'arxiv_id': '1708.08327',\n",
       "  'title': 'Improving Robustness of ML Classifiers against Realizable Evasion Attacks Using Conserved Features',\n",
       "  'abstract': 'Machine learning (ML) techniques are increasingly common in security applications, such as malware and intrusion detection. However, ML models are often susceptible to evasion attacks, in which an adversary makes changes to the input (such as malware) in order to avoid being detected. A conventional approach to evaluate ML robustness to such attacks, as well as to design robust ML, is by considering simplified feature-space models of attacks, where the attacker changes ML features directly to effect evasion, while minimizing or constraining the magnitude of this change. We investigate the effectiveness of this approach to designing robust ML in the face of attacks that can be realized in actual malware (realizable attacks). We demonstrate that in the context of structure-based PDF malware detection, such techniques appear to have limited effectiveness, but they are effective with content-based detectors. In either case, we show that augmenting the feature space models with conserved features (those that cannot be unilaterally modified without compromising malicious functionality) significantly improves performance. Finally, we show that feature space models enable generalized robustness when faced with a variety of realizable attacks, as compared to classifiers which are tuned to be robust to a specific realizable attack.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1708.08327v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1708.08327v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Liang Tong',\n",
       "   'Bo Li',\n",
       "   'Chen Hajaj',\n",
       "   'Chaowei Xiao',\n",
       "   'Ning Zhang',\n",
       "   'Yevgeniy Vorobeychik'],\n",
       "  'tasks': ['Intrusion Detection', 'Malware Detection'],\n",
       "  'date': '2017-08-28',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-the-connection-between-learning-two-layers',\n",
       "  'arxiv_id': '1802.07301',\n",
       "  'title': 'On the Connection Between Learning Two-Layers Neural Networks and Tensor Decomposition',\n",
       "  'abstract': \"We establish connections between the problem of learning a two-layer neural\\nnetwork and tensor decomposition. We consider a model with feature vectors\\n$\\\\boldsymbol x \\\\in \\\\mathbb R^d$, $r$ hidden units with weights $\\\\{\\\\boldsymbol\\nw_i\\\\}_{1\\\\le i \\\\le r}$ and output $y\\\\in \\\\mathbb R$, i.e., $y=\\\\sum_{i=1}^r\\n\\\\sigma( \\\\boldsymbol w_i^{\\\\mathsf T}\\\\boldsymbol x)$, with activation functions\\ngiven by low-degree polynomials. In particular, if $\\\\sigma(x) =\\na_0+a_1x+a_3x^3$, we prove that no polynomial-time learning algorithm can\\noutperform the trivial predictor that assigns to each example the response\\nvariable $\\\\mathbb E(y)$, when $d^{3/2}\\\\ll r\\\\ll d^2$. Our conclusion holds for a\\n`natural data distribution', namely standard Gaussian feature vectors\\n$\\\\boldsymbol x$, and output distributed according to a two-layer neural network\\nwith random isotropic weights, and under a certain complexity-theoretic\\nassumption on tensor decomposition. Roughly speaking, we assume that no\\npolynomial-time algorithm can substantially outperform current methods for\\ntensor decomposition based on the sum-of-squares hierarchy.\\n  We also prove generalizations of this statement for higher degree polynomial\\nactivations, and non-random weight vectors. Remarkably, several existing\\nalgorithms for learning two-layer networks with rigorous guarantees are based\\non tensor decomposition. Our results support the idea that this is indeed the\\ncore computational difficulty in learning such networks, under the stated\\ngenerative model for the data. As a side result, we show that under this model\\nlearning the network requires accurate learning of its weights, a property that\\ndoes not hold in a more general setting.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.07301v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.07301v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Marco Mondelli', 'Andrea Montanari'],\n",
       "  'tasks': ['Tensor Decomposition'],\n",
       "  'date': '2018-02-20',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-comparison-of-methods-for-model-selection',\n",
       "  'arxiv_id': '1804.05146',\n",
       "  'title': 'A comparison of methods for model selection when estimating individual treatment effects',\n",
       "  'abstract': 'Practitioners in medicine, business, political science, and other fields are\\nincreasingly aware that decisions should be personalized to each patient,\\ncustomer, or voter. A given treatment (e.g. a drug or advertisement) should be\\nadministered only to those who will respond most positively, and certainly not\\nto those who will be harmed by it. Individual-level treatment effects can be\\nestimated with tools adapted from machine learning, but different models can\\nyield contradictory estimates. Unlike risk prediction models, however,\\ntreatment effect models cannot be easily evaluated against each other using a\\nheld-out test set because the true treatment effect itself is never directly\\nobserved. Besides outcome prediction accuracy, several metrics that can\\nleverage held-out data to evaluate treatment effects models have been proposed,\\nbut they are not widely used. We provide a didactic framework that elucidates\\nthe relationships between the different approaches and compare them all using a\\nvariety of simulations of both randomized and observational data. Our results\\nshow that researchers estimating heterogenous treatment effects need not limit\\nthemselves to a single model-fitting algorithm. Instead of relying on a single\\nmethod, multiple models fit by a diverse set of algorithms should be evaluated\\nagainst each other using an objective function learned from the validation set.\\nThe model minimizing that objective should be used for estimating the\\nindividual treatment effect for future individuals.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.05146v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.05146v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alejandro Schuler',\n",
       "   'Michael Baiocchi',\n",
       "   'Robert Tibshirani',\n",
       "   'Nigam Shah'],\n",
       "  'tasks': ['Model Selection'],\n",
       "  'date': '2018-04-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/embarrassingly-parallel-inference-for',\n",
       "  'arxiv_id': '1702.08420',\n",
       "  'title': 'Embarrassingly Parallel Inference for Gaussian Processes',\n",
       "  'abstract': 'Training Gaussian process-based models typically involves an $ O(N^3)$ computational bottleneck due to inverting the covariance matrix. Popular methods for overcoming this matrix inversion problem cannot adequately model all types of latent functions, and are often not parallelizable. However, judicious choice of model structure can ameliorate this problem. A mixture-of-experts model that uses a mixture of $K$ Gaussian processes offers modeling flexibility and opportunities for scalable inference. Our embarrassingly parallel algorithm combines low-dimensional matrix inversions with importance sampling to yield a flexible, scalable mixture-of-experts model that offers comparable performance to Gaussian process regression at a much lower computational cost.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1702.08420v9',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1702.08420v9.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Michael Minyi Zhang', 'Sinead A. Williamson'],\n",
       "  'tasks': ['Gaussian Processes'],\n",
       "  'date': '2017-02-27',\n",
       "  'methods': [{'name': 'Gaussian Process',\n",
       "    'full_name': 'Gaussian Process',\n",
       "    'description': '**Gaussian Processes** are non-parametric models for approximating functions. They rely upon a measure of similarity between points (the kernel function) to predict the value for an unseen point from training data. The models are fully probabilistic so uncertainty bounds are baked in with the model.\\r\\n\\r\\nImage Source: Gaussian Processes for Machine Learning, C. E. Rasmussen & C. K. I. Williams',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Non-Parametric Classification',\n",
       "     'description': '**Non-Parametric Classification** methods perform classification where we use non-parametric methods to approximate the functional form of the relationship. Below you can find a continuously updating list of non-parametric classification methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/smhd-a-large-scale-resource-for-exploring',\n",
       "  'arxiv_id': '1806.05258',\n",
       "  'title': 'SMHD: A Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions',\n",
       "  'abstract': \"Mental health is a significant and growing public health concern. As language\\nusage can be leveraged to obtain crucial insights into mental health\\nconditions, there is a need for large-scale, labeled, mental health-related\\ndatasets of users who have been diagnosed with one or more of such conditions.\\nIn this paper, we investigate the creation of high-precision patterns to\\nidentify self-reported diagnoses of nine different mental health conditions,\\nand obtain high-quality labeled data without the need for manual labelling. We\\nintroduce the SMHD (Self-reported Mental Health Diagnoses) dataset and make it\\navailable. SMHD is a novel large dataset of social media posts from users with\\none or multiple mental health conditions along with matched control users. We\\nexamine distinctions in users' language, as measured by linguistic and\\npsychological variables. We further explore text classification methods to\\nidentify individuals with mental conditions through their language.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05258v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05258v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Arman Cohan',\n",
       "   'Bart Desmet',\n",
       "   'Andrew Yates',\n",
       "   'Luca Soldaini',\n",
       "   'Sean MacAvaney',\n",
       "   'Nazli Goharian'],\n",
       "  'tasks': ['Text Classification'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': ['smhd'],\n",
       "  'datasets_introduced_full': ['SMHD']},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/finding-your-lookalike-measuring-face',\n",
       "  'arxiv_id': '1806.05252',\n",
       "  'title': 'Finding your Lookalike: Measuring Face Similarity Rather than Face Identity',\n",
       "  'abstract': 'Face images are one of the main areas of focus for computer vision, receiving\\non a wide variety of tasks. Although face recognition is probably the most\\nwidely researched, many other tasks such as kinship detection, facial\\nexpression classification and facial aging have been examined. In this work we\\npropose the new, subjective task of quantifying perceived face similarity\\nbetween a pair of faces. That is, we predict the perceived similarity between\\nfacial images, given that they are not of the same person. Although this task\\nis clearly correlated with face recognition, it is different and therefore\\njustifies a separate investigation. Humans often remark that two persons look\\nalike, even in cases where the persons are not actually confused with one\\nanother. In addition, because face similarity is different than traditional\\nimage similarity, there are challenges in data collection and labeling, and\\ndealing with diverging subjective opinions between human labelers. We present\\nevidence that finding facial look-alikes and recognizing faces are two distinct\\ntasks. We propose a new dataset for facial similarity and introduce the\\nLookalike network, directed towards similar face classification, which\\noutperforms the ad hoc usage of a face recognition network directed at the same\\ntask.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05252v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05252v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Amir Sadovnik',\n",
       "   'Wassim Gharbi',\n",
       "   'Thanh Vu',\n",
       "   'Andrew Gallagher'],\n",
       "  'tasks': ['Face Recognition', 'General Classification'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['celeba', 'lfw', 'vgg-face-1', 'color-feret'],\n",
       "  'datasets_used_full': ['CelebA', 'LFW', 'VGG Face', 'Color FERET'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/what-about-applied-fairness',\n",
       "  'arxiv_id': '1806.05250',\n",
       "  'title': 'What About Applied Fairness?',\n",
       "  'abstract': 'Machine learning practitioners are often ambivalent about the ethical aspects\\nof their products. We believe anything that gets us from that current state to\\none in which our systems are achieving some degree of fairness is an\\nimprovement that should be welcomed. This is true even when that progress does\\nnot get us 100% of the way to the goal of \"complete\" fairness or perfectly\\nalign with our personal belief on which measure of fairness is used. Some\\nmeasure of fairness being built would still put us in a better position than\\nthe status quo. Impediments to getting fairness and ethical concerns applied in\\nreal applications, whether they are abstruse philosophical debates or technical\\noverhead such as the introduction of ever more hyper-parameters, should be\\navoided. In this paper we further elaborate on our argument for this viewpoint\\nand its importance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05250v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05250v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jared Sylvester', 'Edward Raff'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/boosted-training-of-convolutional-neural',\n",
       "  'arxiv_id': '1806.05974',\n",
       "  'title': 'Boosted Training of Convolutional Neural Networks for Multi-Class Segmentation',\n",
       "  'abstract': 'Training deep neural networks on large and sparse datasets is still\\nchallenging and can require large amounts of computation and memory. In this\\nwork, we address the task of performing semantic segmentation on large\\nvolumetric data sets, such as CT scans. Our contribution is threefold: 1) We\\npropose a boosted sampling scheme that uses a-posterior error maps, generated\\nthroughout training, to focus sampling on difficult regions, resulting in a\\nmore informative loss. This results in a significant training speed up and\\nimproves learning performance for image segmentation. 2) We propose a novel\\nalgorithm for boosting the SGD learning rate schedule by adaptively increasing\\nand lowering the learning rate, avoiding the need for extensive hyperparameter\\ntuning. 3) We show that our method is able to attain new state-of-the-art\\nresults on the VISCERAL Anatomy benchmark.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05974v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05974v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Lorenz Berger',\n",
       "   'Eoin Hyde',\n",
       "   'Matt Gibb',\n",
       "   'Nevil Pavithran',\n",
       "   'Garin Kelly',\n",
       "   'Faiz Mumtaz',\n",
       "   'Sébastien Ourselin'],\n",
       "  'tasks': ['Semantic Segmentation'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'SGD',\n",
       "    'full_name': 'Stochastic Gradient Descent',\n",
       "    'description': '**Stochastic Gradient Descent** is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:\\r\\n\\r\\n$$ w\\\\_{t+1} = w\\\\_{t} - \\\\eta\\\\hat{\\\\nabla}\\\\_{w}{L(w\\\\_{t})} $$\\r\\n\\r\\nWhere $\\\\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.\\r\\n\\r\\n(Image Source: [here](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/))',\n",
       "    'introduced_year': 1951,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/4e0ac120e9a8b096069c2f892488d630a5c8f358/torch/optim/sgd.py#L97-L112',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/manifold-mixup-better-representations-by',\n",
       "  'arxiv_id': '1806.05236',\n",
       "  'title': 'Manifold Mixup: Better Representations by Interpolating Hidden States',\n",
       "  'abstract': 'Deep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose Manifold Mixup, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. Manifold Mixup leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with Manifold Mixup learn class-representations with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it on practical situations, and connect it to previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, Manifold Mixup improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05236v7',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05236v7.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Vikas Verma',\n",
       "   'Alex Lamb',\n",
       "   'Christopher Beckham',\n",
       "   'Amir Najafi',\n",
       "   'Ioannis Mitliagkas',\n",
       "   'Aaron Courville',\n",
       "   'David Lopez-Paz',\n",
       "   'Yoshua Bengio'],\n",
       "  'tasks': ['Image Classification'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'Manifold Mixup',\n",
       "    'full_name': 'Manifold Mixup',\n",
       "    'description': \"**Manifold Mixup** is a regularization method that encourages neural networks to predict less confidently on interpolations of hidden representations. It leverages semantic interpolations as an additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with Manifold Mixup learn class-representations with fewer directions of variance.\\r\\n\\r\\nConsider training a deep neural network $f\\\\left(x\\\\right) = f\\\\_{k}\\\\left(g\\\\_{k}\\\\left(x\\\\right)\\\\right)$, where $g\\\\_{k}$ denotes the part of the neural network mapping the input data to the hidden representation at layer $k$, and $f\\\\_{k}$ denotes the\\r\\npart mapping such hidden representation to the output $f\\\\left(x\\\\right)$. Training $f$ using Manifold Mixup is performed in five steps:\\r\\n\\r\\n(1) Select a random layer $k$ from a set of eligible layers $S$ in the neural network. This set may include the input layer $g\\\\_{0}\\\\left(x\\\\right)$.\\r\\n\\r\\n(2) Process two random data minibatches $\\\\left(x, y\\\\right)$ and $\\\\left(x', y'\\\\right)$ as usual, until reaching layer $k$. This provides us with two intermediate minibatches $\\\\left(g\\\\_{k}\\\\left(x\\\\right), y\\\\right)$ and $\\\\left(g\\\\_{k}\\\\left(x'\\\\right), y'\\\\right)$.\\r\\n\\r\\n(3) Perform Input [Mixup](https://paperswithcode.com/method/mixup) on these intermediate minibatches. This produces the mixed minibatch:\\r\\n\\r\\n$$\\r\\n\\\\left(\\\\tilde{g}\\\\_{k}, \\\\tilde{y}\\\\right) = \\\\left(\\\\text{Mix}\\\\_{\\\\lambda}\\\\left(g\\\\_{k}\\\\left(x\\\\right), g\\\\_{k}\\\\left(x'\\\\right)\\\\right), \\\\text{Mix}\\\\_{\\\\lambda}\\\\left(y, y'\\\\right\\r\\n)\\\\right),\\r\\n$$\\r\\n\\r\\nwhere $\\\\text{Mix}\\\\_{\\\\lambda}\\\\left(a, b\\\\right) = \\\\lambda \\\\cdot a + \\\\left(1 − \\\\lambda\\\\right) \\\\cdot b$. Here, $\\\\left(y, y'\\r\\n\\\\right)$ are one-hot labels, and the mixing coefficient\\r\\n$\\\\lambda \\\\sim \\\\text{Beta}\\\\left(\\\\alpha, \\\\alpha\\\\right)$ as in mixup. For instance, $\\\\alpha = 1.0$ is equivalent to sampling $\\\\lambda \\\\sim U\\\\left(0, 1\\\\right)$.\\r\\n\\r\\n(4) Continue the forward pass in the network from layer $k$ until the output using the mixed minibatch $\\\\left(\\\\tilde{g}\\\\_{k}, \\\\tilde{y}\\\\right)$.\\r\\n\\r\\n(5) This output is used to compute the loss value and\\r\\ngradients that update all the parameters of the neural network.\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1806.05236v7',\n",
       "    'source_title': 'Manifold Mixup: Better Representations by Interpolating Hidden States',\n",
       "    'code_snippet_url': 'https://github.com/vikasverma1077/manifold_mixup/blob/118ec709808b79dd336b10f4cf7deeacf541dfc7/supervised/models/resnet.py#L98',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Mixup',\n",
       "    'full_name': 'Mixup',\n",
       "    'description': '**Mixup** is a data augmentation technique that that generates a weighted combinations of random image pairs from the training data. Given two images and their ground truth labels: $\\\\left(x\\\\_{i}, y\\\\_{i}\\\\right), \\\\left(x\\\\_{j}, y\\\\_{j}\\\\right)$, a synthetic training example $\\\\left(\\\\hat{x}, \\\\hat{y}\\\\right)$ is generated as:\\r\\n\\r\\n$$ \\\\hat{x} = \\\\lambda{x\\\\_{i}} + \\\\left(1 − \\\\lambda\\\\right){x\\\\_{j}} $$\\r\\n$$ \\\\hat{y} = \\\\lambda{y\\\\_{i}} + \\\\left(1 − \\\\lambda\\\\right){y\\\\_{j}} $$\\r\\n\\r\\nwhere $\\\\lambda \\\\sim \\\\text{Beta}\\\\left(\\\\alpha = 0.2\\\\right)$ is independently sampled for each augmented example.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1710.09412v2',\n",
       "    'source_title': 'mixup: Beyond Empirical Risk Minimization',\n",
       "    'code_snippet_url': 'https://github.com/facebookresearch/mixup-cifar10',\n",
       "    'main_collection': {'name': 'Image Data Augmentation',\n",
       "     'description': '**Image Data Augmentation** refers to a class of methods that augment an image dataset to increase the effective size of the training set, or as a form of regularization to help the network learn more effective representations.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['cifar-10', 'cifar-100'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'CIFAR-100'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/understanding-the-meaning-of-understanding',\n",
       "  'arxiv_id': '1806.05234',\n",
       "  'title': 'Understanding the Meaning of Understanding',\n",
       "  'abstract': 'Can we train a machine to detect if another machine has understood a concept?\\nIn principle, this is possible by conducting tests on the subject of that\\nconcept. However we want this procedure to be done by avoiding direct\\nquestions. In other words, we would like to isolate the absolute meaning of an\\nabstract idea by putting it into a class of equivalence, hence without adopting\\nstraight definitions or showing how this idea \"works\" in practice. We discuss\\nthe metaphysical implications hidden in the above question, with the aim of\\nproviding a plausible reference framework.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05234v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05234v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Daniele Funaro'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/end-to-end-parkinson-disease-diagnosis-using',\n",
       "  'arxiv_id': '1806.05233',\n",
       "  'title': 'End-to-End Parkinson Disease Diagnosis using Brain MR-Images by 3D-CNN',\n",
       "  'abstract': 'In this work, we use a deep learning framework for simultaneous\\nclassification and regression of Parkinson disease diagnosis based on MR-Images\\nand personal information (i.e. age, gender). We intend to facilitate and\\nincrease the confidence in Parkinson disease diagnosis through our deep\\nlearning framework.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05233v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05233v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Soheil Esmaeilzadeh', 'Yao Yang', 'Ehsan Adeli'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/beyond-bags-of-words-inferring-systemic-nets',\n",
       "  'arxiv_id': '1806.05231',\n",
       "  'title': 'Beyond Bags of Words: Inferring Systemic Nets',\n",
       "  'abstract': 'Textual analytics based on representations of documents as bags of words have\\nbeen reasonably successful. However, analysis that requires deeper insight into\\nlanguage, into author properties, or into the contexts in which documents were\\ncreated requires a richer representation. Systemic nets are one such\\nrepresentation. They have not been extensively used because they required human\\neffort to construct. We show that systemic nets can be algorithmically inferred\\nfrom corpora, that the resulting nets are plausible, and that they can provide\\npractical benefits for knowledge discovery problems. This opens up a new class\\nof practical analysis techniques for textual analytics.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05231v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05231v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['D. B. Skillicorn', 'N. Alsadhan'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/identifying-recurring-patterns-with-deep',\n",
       "  'arxiv_id': '1806.05229',\n",
       "  'title': 'Identifying Recurring Patterns with Deep Neural Networks for Natural Image Denoising',\n",
       "  'abstract': 'Image denoising methods must effectively model, implicitly or explicitly, the vast diversity of patterns and textures that occur in natural images. This is challenging, even for modern methods that leverage deep neural networks trained to regress to clean images from noisy inputs. One recourse is to rely on \"internal\" image statistics, by searching for similar patterns within the input image itself. In this work, we propose a new method for natural image denoising that trains a deep neural network to determine whether patches in a noisy image input share common underlying patterns. Given a pair of noisy patches, our network predicts whether different sub-band coefficients of the original noise-free patches are similar. The denoising algorithm then aggregates matched coefficients to obtain an initial estimate of the clean image. Finally, this estimate is provided as input, along with the original noisy image, to a standard regression-based denoising network. Experiments show that our method achieves state-of-the-art color image denoising performance, including with a blind version that trains a common model for a range of noise levels, and does not require knowledge of level of noise in an input image. Our approach also has a distinct advantage when training with limited amounts of training data.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05229v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05229v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Zhihao Xia', 'Ayan Chakrabarti'],\n",
       "  'tasks': ['Color Image Denoising',\n",
       "   'Denoising',\n",
       "   'Image Denoising',\n",
       "   'Image Restoration'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cbsd68', 'mcmaster'],\n",
       "  'datasets_used_full': ['CBSD68', 'McMaster'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/3d-coded-3d-correspondences-by-deep-1',\n",
       "  'arxiv_id': '1806.05228',\n",
       "  'title': '3D-CODED : 3D Correspondences by Deep Deformation',\n",
       "  'abstract': 'We present a new deep learning approach for matching deformable shapes by\\nintroducing {\\\\it Shape Deformation Networks} which jointly encode 3D shapes and\\ncorrespondences. This is achieved by factoring the surface representation into\\n(i) a template, that parameterizes the surface, and (ii) a learnt global\\nfeature vector that parameterizes the transformation of the template into the\\ninput surface. By predicting this feature for a new shape, we implicitly\\npredict correspondences between this shape and the template. We show that these\\ncorrespondences can be improved by an additional step which improves the shape\\nfeature by minimizing the Chamfer distance between the input and transformed\\ntemplate. We demonstrate that our simple approach improves on state-of-the-art\\nresults on the difficult FAUST-inter challenge, with an average correspondence\\nerror of 2.88cm. We show, on the TOSCA dataset, that our method is robust to\\nmany types of perturbations, and generalizes to non-human shapes. This\\nrobustness allows it to perform well on real unclean, meshes from the the SCAPE\\ndataset.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05228v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05228v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Thibault Groueix',\n",
       "   'Matthew Fisher',\n",
       "   'Vladimir G. Kim',\n",
       "   'Bryan C. Russell',\n",
       "   'Mathieu Aubry'],\n",
       "  'tasks': ['3D Dense Shape Correspondence',\n",
       "   '3D Human Pose Estimation',\n",
       "   '3D Point Cloud Matching',\n",
       "   '3D Surface Generation'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['faust-1', 'surreal-1', 'shrec-19'],\n",
       "  'datasets_used_full': ['FAUST', 'SURREAL', \"SHREC'19\"],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/human-activity-recognition-based-on-wearable',\n",
       "  'arxiv_id': '1806.05226',\n",
       "  'title': 'Human Activity Recognition Based on Wearable Sensor Data: A Standardization of the State-of-the-Art',\n",
       "  'abstract': 'Human activity recognition based on wearable sensor data has been an\\nattractive research topic due to its application in areas such as healthcare\\nand smart environments. In this context, many works have presented remarkable\\nresults using accelerometer, gyroscope and magnetometer data to represent the\\nactivities categories. However, current studies do not consider important\\nissues that lead to skewed results, making it hard to assess the quality of\\nsensor-based human activity recognition and preventing a direct comparison of\\nprevious works. These issues include the samples generation processes and the\\nvalidation protocols used. We emphasize that in other research areas, such as\\nimage classification and object detection, these issues are already\\nwell-defined, which brings more efforts towards the application. Inspired by\\nthis, we conduct an extensive set of experiments that analyze different sample\\ngeneration processes and validation protocols to indicate the vulnerable points\\nin human activity recognition based on wearable sensor data. For this purpose,\\nwe implement and evaluate several top-performance methods, ranging from\\nhandcrafted-based approaches to convolutional neural networks. According to our\\nstudy, most of the experimental evaluations that are currently employed are not\\nadequate to perform the activity recognition in the context of wearable sensor\\ndata, in which the recognition accuracy drops considerably when compared to an\\nappropriate evaluation approach. To the best of our knowledge, this is the\\nfirst study that tackles essential issues that compromise the understanding of\\nthe performance in human activity recognition based on wearable sensor data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05226v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05226v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Artur Jordao',\n",
       "   'Antonio C. Nazare Jr.',\n",
       "   'Jessica Sena',\n",
       "   'William Robson Schwartz'],\n",
       "  'tasks': ['Activity Recognition',\n",
       "   'Image Classification',\n",
       "   'Object Detection'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-spontaneity-to-improve-emotion',\n",
       "  'arxiv_id': '1712.04753',\n",
       "  'title': 'Learning Spontaneity to Improve Emotion Recognition In Speech',\n",
       "  'abstract': 'We investigate the effect and usefulness of spontaneity (i.e. whether a given\\nspeech is spontaneous or not) in speech in the context of emotion recognition.\\nWe hypothesize that emotional content in speech is interrelated with its\\nspontaneity, and use spontaneity classification as an auxiliary task to the\\nproblem of emotion recognition. We propose two supervised learning settings\\nthat utilize spontaneity to improve speech emotion recognition: a hierarchical\\nmodel that performs spontaneity detection before performing emotion\\nrecognition, and a multitask learning model that jointly learns to recognize\\nboth spontaneity and emotion. Through various experiments on the well known\\nIEMOCAP database, we show that by using spontaneity detection as an additional\\ntask, significant improvement can be achieved over emotion recognition systems\\nthat are unaware of spontaneity. We achieve state-of-the-art emotion\\nrecognition accuracy (4-class, 69.1%) on the IEMOCAP database outperforming\\nseveral relevant and competitive baselines.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.04753v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.04753v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Karttikeya Mangalam', 'Tanaya Guha'],\n",
       "  'tasks': ['Emotion Recognition', 'Speech Emotion Recognition'],\n",
       "  'date': '2017-12-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['iemocap'],\n",
       "  'datasets_used_full': ['IEMOCAP'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/bringing-replication-and-reproduction',\n",
       "  'arxiv_id': '1806.05219',\n",
       "  'title': 'Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for Target Dependent Sentiment Analysis',\n",
       "  'abstract': 'Lack of repeatability and generalisability are two significant threats to\\ncontinuing scientific development in Natural Language Processing. Language\\nmodels and learning methods are so complex that scientific conference papers no\\nlonger contain enough space for the technical depth required for replication or\\nreproduction. Taking Target Dependent Sentiment Analysis as a case study, we\\nshow how recent work in the field has not consistently released code, or\\ndescribed settings for learning methods in enough detail, and lacks\\ncomparability and generalisability in train, test or validation data. To\\ninvestigate generalisability and to enable state of the art comparative\\nevaluations, we carry out the first reproduction studies of three groups of\\ncomplementary methods and perform the first large-scale mass evaluation on six\\ndifferent English datasets. Reflecting on our experiences, we recommend that\\nfuture replication or reproduction experiments should always consider a variety\\nof datasets alongside documenting and releasing their methods and published\\ncode in order to minimise the barriers to both repeatability and\\ngeneralisability. We have released our code with a model zoo on GitHub with\\nJupyter Notebooks to aid understanding and full documentation, and we recommend\\nthat others do the same with their papers at submission time through an\\nanonymised GitHub account.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05219v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05219v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Andrew Moore', 'Paul Rayson'],\n",
       "  'tasks': ['Sentiment Analysis'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['youtubean'],\n",
       "  'datasets_used_full': ['Youtubean'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/impostor-networks-for-fast-fine-grained',\n",
       "  'arxiv_id': '1806.05217',\n",
       "  'title': 'Impostor Networks for Fast Fine-Grained Recognition',\n",
       "  'abstract': \"In this work we introduce impostor networks, an architecture that allows to\\nperform fine-grained recognition with high accuracy and using a light-weight\\nconvolutional network, making it particularly suitable for fine-grained\\napplications on low-power and non-GPU enabled platforms. Impostor networks\\ncompensate for the lightness of its `backend' network by combining it with a\\nlightweight non-parametric classifier. The combination of a convolutional\\nnetwork and such non-parametric classifier is trained in an end-to-end fashion.\\nSimilarly to convolutional neural networks, impostor networks can fit\\nlarge-scale training datasets very well, while also being able to generalize to\\nnew data points. At the same time, the bulk of computations within impostor\\nnetworks happen through nearest neighbor search in high-dimensions. Such search\\ncan be performed efficiently on a variety of architectures including standard\\nCPUs, where deep convolutional networks are inefficient. In a series of\\nexperiments with three fine-grained datasets, we show that impostor networks\\nare able to boost the classification accuracy of a moderate-sized convolutional\\nnetwork considerably at a very small computational cost.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05217v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05217v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Vadim Lebedev', 'Artem Babenko', 'Victor Lempitsky'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/an-evaluation-of-neural-machine-translation',\n",
       "  'arxiv_id': '1806.05210',\n",
       "  'title': 'An Evaluation of Neural Machine Translation Models on Historical Spelling Normalization',\n",
       "  'abstract': 'In this paper, we apply different NMT models to the problem of historical\\nspelling normalization for five languages: English, German, Hungarian,\\nIcelandic, and Swedish. The NMT models are at different levels, have different\\nattention mechanisms, and different neural network architectures. Our results\\nshow that NMT models are much better than SMT models in terms of character\\nerror rate. The vanilla RNNs are competitive to GRUs/LSTMs in historical\\nspelling normalization. Transformer models perform better only when provided\\nwith more training data. We also find that subword-level models with a small\\nsubword vocabulary are better than character-level models for low-resource\\nlanguages. In addition, we propose a hybrid method which further improves the\\nperformance of historical spelling normalization.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05210v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05210v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Gongbo Tang', 'Fabienne Cap', 'Eva Pettersson', 'Joakim Nivre'],\n",
       "  'tasks': ['Machine Translation', 'Translation'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'Absolute Position Encodings',\n",
       "    'full_name': 'Absolute Position Encodings',\n",
       "    'description': '**Absolute Position Encodings** are a type of position embeddings for [[Transformer](https://paperswithcode.com/method/transformer)-based models] where positional encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension $d\\\\_{model}$ as the embeddings, so that the two can be summed. In the original implementation, sine and cosine functions of different frequencies are used:\\r\\n\\r\\n$$ \\\\text{PE}\\\\left(pos, 2i\\\\right) = \\\\sin\\\\left(pos/10000^{2i/d\\\\_{model}}\\\\right) $$\\r\\n\\r\\n$$ \\\\text{PE}\\\\left(pos, 2i+1\\\\right) = \\\\cos\\\\left(pos/10000^{2i/d\\\\_{model}}\\\\right) $$\\r\\n\\r\\nwhere $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\\\pi$ to $10000 \\\\dot 2\\\\pi$. This function was chosen because the authors hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$,  $\\\\text{PE}\\\\_{pos+k}$ can be represented as a linear function of $\\\\text{PE}\\\\_{pos}$.\\r\\n\\r\\nImage Source: [D2L.ai](https://d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Position Embeddings',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Position-Wise Feed-Forward Layer',\n",
       "    'full_name': 'Position-Wise Feed-Forward Layer',\n",
       "    'description': '**Position-Wise Feed-Forward Layer** is a type of [feedforward layer](https://www.paperswithcode.com/method/category/feedforwad-networks) consisting of two [dense layers](https://www.paperswithcode.com/method/dense-connections) that applies to the last dimension, which means the same dense layers are used for each position item in the sequence, so called position-wise.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Residual Connection',\n",
       "    'full_name': 'Residual Connection',\n",
       "    'description': '**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'BPE',\n",
       "    'full_name': 'Byte Pair Encoding',\n",
       "    'description': '**Byte Pair Encoding**, or **BPE**, is a subword segmentation algorithm that encodes rare and unknown words as sequences of subword units. The intuition is that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).\\r\\n\\r\\n[Lei Mao](https://leimao.github.io/blog/Byte-Pair-Encoding/) has a detailed blog post that explains how this works.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1508.07909v5',\n",
       "    'source_title': 'Neural Machine Translation of Rare Words with Subword Units',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Subword Segmentation',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Natural Language Processing'}},\n",
       "   {'name': 'Dense Connections',\n",
       "    'full_name': 'Dense Connections',\n",
       "    'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\\\_{\\\\text{inputs}}*n\\\\_{\\\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\\r\\n\\r\\n$$h\\\\_{l} = g\\\\left(\\\\textbf{W}^{T}h\\\\_{l-1}\\\\right)$$\\r\\n\\r\\nwhere $g$ is an activation function.\\r\\n\\r\\nImage Source: Deep Learning by Goodfellow, Bengio and Courville',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Label Smoothing',\n",
       "    'full_name': 'Label Smoothing',\n",
       "    'description': '**Label Smoothing** is a regularization technique that introduces noise for the labels. This accounts for the fact that datasets may have mistakes in them, so maximizing the likelihood of $\\\\log{p}\\\\left(y\\\\mid{x}\\\\right)$ directly can be harmful. Assume for a small constant $\\\\epsilon$, the training set label $y$ is correct with probability $1-\\\\epsilon$ and incorrect otherwise. Label Smoothing regularizes a model based on a [softmax](https://paperswithcode.com/method/softmax) with $k$ output values by replacing the hard $0$ and $1$ classification targets with targets of $\\\\frac{\\\\epsilon}{k-1}$ and $1-\\\\epsilon$ respectively.\\r\\n\\r\\nSource: Deep Learning, Goodfellow et al\\r\\n\\r\\nImage Source: [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629)',\n",
       "    'introduced_year': 1985,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Adam',\n",
       "    'full_name': 'Adam',\n",
       "    'description': '**Adam** is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of [RMSProp](https://paperswithcode.com/method/rmsprop) and [SGD w/th Momentum](https://paperswithcode.com/method/sgd-with-momentum). The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. \\r\\n\\r\\nThe weight updates are performed as:\\r\\n\\r\\n$$ w_{t} = w_{t-1} - \\\\eta\\\\frac{\\\\hat{m}\\\\_{t}}{\\\\sqrt{\\\\hat{v}\\\\_{t}} + \\\\epsilon}  $$\\r\\n\\r\\nwith\\r\\n\\r\\n$$ \\\\hat{m}\\\\_{t} = \\\\frac{m_{t}}{1-\\\\beta^{t}_{1}} $$\\r\\n\\r\\n$$ \\\\hat{v}\\\\_{t} = \\\\frac{v_{t}}{1-\\\\beta^{t}_{2}} $$\\r\\n\\r\\n$$ m_{t} = \\\\beta_{1}m_{t-1} + (1-\\\\beta_{1})g_{t} $$\\r\\n\\r\\n$$ v_{t} = \\\\beta_{2}v_{t-1} + (1-\\\\beta_{2})g_{t}^{2}  $$\\r\\n\\r\\n\\r\\n$ \\\\eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \\\\epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \\\\beta_{1} $ and $ \\\\beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1412.6980v9',\n",
       "    'source_title': 'Adam: A Method for Stochastic Optimization',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Dropout',\n",
       "    'full_name': 'Dropout',\n",
       "    'description': '**Dropout** is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p=0.5$). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).\\r\\n\\r\\nThe idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://jmlr.org/papers/v15/srivastava14a.html',\n",
       "    'source_title': 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/7f3078b70d0ed9bea6228efa420879c56f72ef69/jax/experimental/stax.py#L271-L275',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Multi-Head Attention',\n",
       "    'full_name': 'Multi-Head Attention',\n",
       "    'description': '**Multi-head Attention** is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies). \\r\\n\\r\\n$$ \\\\text{MultiHead}\\\\left(\\\\textbf{Q}, \\\\textbf{K}, \\\\textbf{V}\\\\right) = \\\\left[\\\\text{head}\\\\_{1},\\\\dots,\\\\text{head}\\\\_{h}\\\\right]\\\\textbf{W}_{0}$$\\r\\n\\r\\n$$\\\\text{where} \\\\text{ head}\\\\_{i} = \\\\text{Attention} \\\\left(\\\\textbf{Q}\\\\textbf{W}\\\\_{i}^{Q}, \\\\textbf{K}\\\\textbf{W}\\\\_{i}^{K}, \\\\textbf{V}\\\\textbf{W}\\\\_{i}^{V} \\\\right) $$\\r\\n\\r\\nAbove $\\\\textbf{W}$ are all learnable parameter matrices.\\r\\n\\r\\nNote that [scaled dot-product attention](https://paperswithcode.com/method/scaled) is most commonly used in this module, although in principle it can be swapped out for other types of attention mechanism.\\r\\n\\r\\nSource: [Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': 'https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/fec78a687210851f055f792d45300d27cc60ae41/transformer/SubLayers.py#L9',\n",
       "    'main_collection': {'name': 'Attention Modules',\n",
       "     'description': '**Attention Modules** refer to modules that incorporate attention mechanisms. For example, multi-head attention is a module that incorporates multiple attention heads. Below you can find a continuously updating list of attention modules.',\n",
       "     'parent': 'Attention',\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Layer Normalization',\n",
       "    'full_name': 'Layer Normalization',\n",
       "    'description': 'Unlike [batch normalization](https://paperswithcode.com/method/batch-normalization), **Layer Normalization** directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. It works well for [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) and improves both the training time and the generalization performance of several existing RNN models. More recently, it has been used with [Transformer](https://paperswithcode.com/methods/category/transformers) models.\\r\\n\\r\\nWe compute the layer normalization statistics over all the hidden units in the same layer as follows:\\r\\n\\r\\n$$ \\\\mu^{l} = \\\\frac{1}{H}\\\\sum^{H}\\\\_{i=1}a\\\\_{i}^{l} $$\\r\\n\\r\\n$$ \\\\sigma^{l} = \\\\sqrt{\\\\frac{1}{H}\\\\sum^{H}\\\\_{i=1}\\\\left(a\\\\_{i}^{l}-\\\\mu^{l}\\\\right)^{2}}  $$\\r\\n\\r\\nwhere $H$ denotes the number of hidden units in a layer. Under layer normalization, all the hidden units in a layer share the same normalization terms $\\\\mu$ and $\\\\sigma$, but different training cases have different normalization terms. Unlike batch normalization, layer normalization does not impose any constraint on the size of the mini-batch and it can be used in the pure online regime with batch size 1.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1607.06450v1',\n",
       "    'source_title': 'Layer Normalization',\n",
       "    'code_snippet_url': 'https://github.com/CyberZHG/torch-layer-normalization/blob/89f405b60f53f85da6f03fe685c190ef394ce50c/torch_layer_normalization/layer_normalization.py#L8',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Scaled Dot-Product Attention',\n",
       "    'full_name': 'Scaled Dot-Product Attention',\n",
       "    'description': '**Scaled dot-product attention** is an attention mechanism where the dot products are scaled down by $\\\\sqrt{d_k}$. Formally we have a query $Q$, a key $K$ and a value $V$ and calculate the attention as:\\r\\n\\r\\n$$ {\\\\text{Attention}}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^{T}}{\\\\sqrt{d_k}}\\\\right)V $$\\r\\n\\r\\nIf we assume that $q$ and $k$ are $d_k$-dimensional vectors whose components are independent random variables with mean $0$ and variance $1$, then their dot product, $q \\\\cdot k = \\\\sum_{i=1}^{d_k} u_iv_i$, has mean $0$ and variance $d_k$.  Since we would prefer these values to have variance $1$, we divide by $\\\\sqrt{d_k}$.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': 'https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/5c0264915ab43485adc576f88971fc3d42b10445/transformer/Modules.py#L7',\n",
       "    'main_collection': {'name': 'Attention Mechanisms',\n",
       "     'description': '**Attention Mechanisms** are a component used in neural networks to model long-range interaction, for example across a text in NLP. The key idea is to build shortcuts between a context vector and the input, to allow a model to attend to different parts. Below you can find a continuously updating list of attention mechanisms.',\n",
       "     'parent': 'Attention',\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Transformer',\n",
       "    'full_name': 'Transformer',\n",
       "    'description': 'A **Transformer** is a model architecture that eschews recurrence and instead relies entirely on an [attention mechanism](https://paperswithcode.com/methods/category/attention-mechanisms-1) to draw global dependencies between input and output. Before Transformers, the dominant sequence transduction models were based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The Transformer also employs an encoder and decoder, but removing recurrence in favor of [attention mechanisms](https://paperswithcode.com/methods/category/attention-mechanisms-1) allows for significantly more parallelization than methods like [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) and [CNNs](https://paperswithcode.com/methods/category/convolutional-neural-networks).',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': 'https://github.com/tunz/transformer-pytorch/blob/e7266679f0b32fd99135ea617213f986ceede056/model/transformer.py#L201',\n",
       "    'main_collection': {'name': 'Transformers',\n",
       "     'description': '**Transformers** are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embedidngs.',\n",
       "     'parent': 'Language Models',\n",
       "     'area': 'Natural Language Processing'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/offline-evaluation-of-ranking-policies-with',\n",
       "  'arxiv_id': '1804.10488',\n",
       "  'title': 'Offline Evaluation of Ranking Policies with Click Models',\n",
       "  'abstract': 'Many web systems rank and present a list of items to users, from recommender\\nsystems to search and advertising. An important problem in practice is to\\nevaluate new ranking policies offline and optimize them before they are\\ndeployed. We address this problem by proposing evaluation algorithms for\\nestimating the expected number of clicks on ranked lists from historical logged\\ndata. The existing algorithms are not guaranteed to be statistically efficient\\nin our problem because the number of recommended lists can grow exponentially\\nwith their length. To overcome this challenge, we use models of user\\ninteraction with the list of items, the so-called click models, to construct\\nestimators that learn statistically efficiently. We analyze our estimators and\\nprove that they are more efficient than the estimators that do not use the\\nstructure of the click model, under the assumption that the click model holds.\\nWe evaluate our estimators in a series of experiments on a real-world dataset\\nand show that they consistently outperform prior estimators.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.10488v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.10488v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shuai Li',\n",
       "   'Yasin Abbasi-Yadkori',\n",
       "   'Branislav Kveton',\n",
       "   'S. Muthukrishnan',\n",
       "   'Vishwa Vinay',\n",
       "   'Zheng Wen'],\n",
       "  'tasks': ['Recommendation Systems'],\n",
       "  'date': '2018-04-27',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/skeletracks-automatic-separation-of',\n",
       "  'arxiv_id': '1806.05199',\n",
       "  'title': 'Skeletracks: automatic separation of overlapping fission tracks in apatite and muscovite using image processing',\n",
       "  'abstract': 'One of the major difficulties of automatic track counting using\\nphotomicrographs is separating overlapped tracks. We address this issue\\ncombining image processing algorithms such as skeletonization, and we test our\\nalgorithm with several binarization techniques. The counting algorithm was\\nsuccessfully applied to determine the efficiency factor GQR, necessary for\\nstandardless fission-track dating, involving counting induced tracks in apatite\\nand muscovite with superficial densities of about $6 \\\\times 10^5$\\ntracks/$cm^2$.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05199v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05199v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alexandre Fioravante de Siqueira',\n",
       "   'Wagner Massayuki Nakasuga',\n",
       "   'Sandro Guedes'],\n",
       "  'tasks': ['Binarization'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/interpretable-machine-learning-for-privacy',\n",
       "  'arxiv_id': '1710.08464',\n",
       "  'title': 'Interpretable Machine Learning for Privacy-Preserving Pervasive Systems',\n",
       "  'abstract': 'Our everyday interactions with pervasive systems generate traces that capture various aspects of human behavior and enable machine learning algorithms to extract latent information about users. In this paper, we propose a machine learning interpretability framework that enables users to understand how these generated traces violate their privacy.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1710.08464v6',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1710.08464v6.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Benjamin Baron', 'Mirco Musolesi'],\n",
       "  'tasks': ['Interpretable Machine Learning'],\n",
       "  'date': '2017-10-23',\n",
       "  'methods': [{'name': 'Interpretability',\n",
       "    'full_name': 'Interpretability',\n",
       "    'description': 'Please enter a description about the method here',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1310.1533v2',\n",
       "    'source_title': 'CAM: Causal additive models, high-dimensional order search and penalized regression',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Image Models',\n",
       "     'description': '**Image Models** are methods that build representations of images for downstream tasks such as classification and object detection. The most popular subcategory are convolutional neural networks. Below you can find a continuously updated list of image models.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/overfitting-or-perfect-fitting-risk-bounds',\n",
       "  'arxiv_id': '1806.05161',\n",
       "  'title': 'Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate',\n",
       "  'abstract': 'Many modern machine learning models are trained to achieve zero or near-zero\\ntraining error in order to obtain near-optimal (but non-zero) test error. This\\nphenomenon of strong generalization performance for \"overfitted\" / interpolated\\nclassifiers appears to be ubiquitous in high-dimensional data, having been\\nobserved in deep networks, kernel machines, boosting and random forests. Their\\nperformance is consistently robust even when the data contain large amounts of\\nlabel noise.\\n  Very little theory is available to explain these observations. The vast\\nmajority of theoretical analyses of generalization allows for interpolation\\nonly when there is little or no label noise. This paper takes a step toward a\\ntheoretical foundation for interpolated classifiers by analyzing local\\ninterpolating schemes, including geometric simplicial interpolation algorithm\\nand singularly weighted $k$-nearest neighbor schemes. Consistency or\\nnear-consistency is proved for these schemes in classification and regression\\nproblems. Moreover, the nearest neighbor schemes exhibit optimal rates under\\nsome standard statistical assumptions.\\n  Finally, this paper suggests a way to explain the phenomenon of adversarial\\nexamples, which are seemingly ubiquitous in modern machine learning, and also\\ndiscusses some connections to kernel machines and random forests in the\\ninterpolated regime.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05161v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05161v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Mikhail Belkin', 'Daniel Hsu', 'Partha Mitra'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-tighter-generalization-bound-for-deep',\n",
       "  'arxiv_id': '1806.05159',\n",
       "  'title': 'On Tighter Generalization Bound for Deep Neural Networks: CNNs, ResNets, and Beyond',\n",
       "  'abstract': 'We establish a margin based data dependent generalization error bound for a general family of deep neural networks in terms of the depth and width, as well as the Jacobian of the networks. Through introducing a new characterization of the Lipschitz properties of neural network family, we achieve significantly tighter generalization bounds than existing results. Moreover, we show that the generalization bound can be further improved for bounded losses. Aside from the general feedforward deep neural networks, our results can be applied to derive new bounds for popular architectures, including convolutional neural networks (CNNs) and residual networks (ResNets). When achieving same generalization errors with previous arts, our bounds allow for the choice of larger parameter spaces of weight matrices, inducing potentially stronger expressive ability for neural networks. Numerical evaluation is also provided to support our theory.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05159v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05159v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xingguo Li',\n",
       "   'Junwei Lu',\n",
       "   'Zhaoran Wang',\n",
       "   'Jarvis Haupt',\n",
       "   'Tuo Zhao'],\n",
       "  'tasks': ['Generalization Bounds'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10'],\n",
       "  'datasets_used_full': ['CIFAR-10'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/automated-performance-assessment-in',\n",
       "  'arxiv_id': '1806.05154',\n",
       "  'title': 'Automated Performance Assessment in Transoesophageal Echocardiography with Convolutional Neural Networks',\n",
       "  'abstract': 'Transoesophageal echocardiography (TEE) is a valuable diagnostic and\\nmonitoring imaging modality. Proper image acquisition is essential for\\ndiagnosis, yet current assessment techniques are solely based on manual expert\\nreview. This paper presents a supervised deep learn ing framework for\\nautomatically evaluating and grading the quality of TEE images. To obtain the\\nnecessary dataset, 38 participants of varied experience performed TEE exams\\nwith a high-fidelity virtual reality (VR) platform. Two Convolutional Neural\\nNetwork (CNN) architectures, AlexNet and VGG, structured to perform regression,\\nwere finetuned and validated on manually graded images from three evaluators.\\nTwo different scoring strategies, a criteria-based percentage and an overall\\ngeneral impression, were used. The developed CNN models estimate the average\\nscore with a root mean square accuracy ranging between 84%-93%, indicating the\\nability to replicate expert valuation. Proposed strategies for automated TEE\\nassessment can have a significant impact on the training process of new TEE\\noperators, providing direct feedback and facilitating the development of the\\nnecessary dexterous skills.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05154v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05154v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Evangelos B. Mazomenos',\n",
       "   'Kamakshi Bansal',\n",
       "   'Bruce Martin',\n",
       "   'Andrew Smith',\n",
       "   'Susan Wright',\n",
       "   'Danail Stoyanov'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': '1x1 Convolution',\n",
       "    'full_name': '1x1 Convolution',\n",
       "    'description': 'A **1 x 1 Convolution** is a [convolution](https://paperswithcode.com/method/convolution) with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an [MLP](https://paperswithcode.com/method/feedforward-network) looking at a particular pixel location.\\r\\n\\r\\nImage Credit: [http://deeplearning.ai](http://deeplearning.ai)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://www.healthnutra.org/es/maxup/',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'VGG',\n",
       "    'full_name': 'VGG',\n",
       "    'description': '**VGG** is a classical convolutional neural network architecture. It was based on an analysis of how to increase the depth of such networks. The network utilises small 3 x 3 filters. Otherwise the network is characterized by its simplicity: the only other components being pooling layers and a fully connected layer.\\r\\n\\r\\nImage: [Davi Frossard](https://www.cs.toronto.edu/frossard/post/vgg16/)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1409.1556v6',\n",
       "    'source_title': 'Very Deep Convolutional Networks for Large-Scale Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/vgg.py#L24',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Local Response Normalization',\n",
       "    'full_name': 'Local Response Normalization',\n",
       "    'description': \"**Local Response Normalization** is a normalization layer that implements the idea of lateral inhibition. Lateral inhibition is a concept in neurobiology that refers to the phenomenon of an excited neuron inhibiting its neighbours: this leads to a peak in the form of a local maximum, creating contrast in that area and increasing sensory perception. In practice, we can either normalize within the same channel or normalize across channels when we apply LRN to convolutional neural networks.\\r\\n\\r\\n$$ b_{c} = a_{c}\\\\left(k + \\\\frac{\\\\alpha}{n}\\\\sum_{c'=\\\\max(0, c-n/2)}^{\\\\min(N-1,c+n/2)}a_{c'}^2\\\\right)^{-\\\\beta} $$\\r\\n\\r\\nWhere the size is the number of neighbouring channels used for normalization, $\\\\alpha$ is multiplicative factor, $\\\\beta$ an exponent and $k$ an additive factor\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks',\n",
       "    'source_title': 'ImageNet Classification with Deep Convolutional Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/1c5c289b6218eb1026dcb5fd9738231401cfccea/torch/nn/modules/normalization.py#L13',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Grouped Convolution',\n",
       "    'full_name': 'Grouped Convolution',\n",
       "    'description': 'A **Grouped Convolution** uses a group of convolutions - multiple kernels per layer - resulting in multiple channel outputs per layer. This leads to wider networks helping a network learn a varied set of low level and high level features. The original motivation of using Grouped Convolutions in [AlexNet](https://paperswithcode.com/method/alexnet) was to distribute the model over multiple GPUs as an engineering compromise. But later, with models such as [ResNeXt](https://paperswithcode.com/method/resnext), it was shown this module could be used to improve classification accuracy. Specifically by exposing a new dimension through grouped convolutions, *cardinality* (the size of set of transformations), we can increase accuracy by increasing it.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks',\n",
       "    'source_title': 'ImageNet Classification with Deep Convolutional Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/prlz77/ResNeXt.pytorch/blob/39fb8d03847f26ec02fb9b880ecaaa88db7a7d16/models/model.py#L42',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Dropout',\n",
       "    'full_name': 'Dropout',\n",
       "    'description': '**Dropout** is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p=0.5$). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).\\r\\n\\r\\nThe idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://jmlr.org/papers/v15/srivastava14a.html',\n",
       "    'source_title': 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/7f3078b70d0ed9bea6228efa420879c56f72ef69/jax/experimental/stax.py#L271-L275',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Dense Connections',\n",
       "    'full_name': 'Dense Connections',\n",
       "    'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\\\_{\\\\text{inputs}}*n\\\\_{\\\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\\r\\n\\r\\n$$h\\\\_{l} = g\\\\left(\\\\textbf{W}^{T}h\\\\_{l-1}\\\\right)$$\\r\\n\\r\\nwhere $g$ is an activation function.\\r\\n\\r\\nImage Source: Deep Learning by Goodfellow, Bengio and Courville',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'AlexNet',\n",
       "    'full_name': 'AlexNet',\n",
       "    'description': '**AlexNet** is a classic convolutional neural network architecture. It consists of convolutions, [max pooling](https://paperswithcode.com/method/max-pooling) and dense layers as the basic building blocks. Grouped convolutions are used in order to fit the model across two GPUs.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks',\n",
       "    'source_title': 'ImageNet Classification with Deep Convolutional Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/dansuh17/alexnet-pytorch/blob/d0c1b1c52296ffcbecfbf5b17e1d1685b4ca6744/model.py#L40',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/bandits-with-delayed-aggregated-anonymous',\n",
       "  'arxiv_id': '1709.06853',\n",
       "  'title': 'Bandits with Delayed, Aggregated Anonymous Feedback',\n",
       "  'abstract': 'We study a variant of the stochastic $K$-armed bandit problem, which we call\\n\"bandits with delayed, aggregated anonymous feedback\". In this problem, when\\nthe player pulls an arm, a reward is generated, however it is not immediately\\nobserved. Instead, at the end of each round the player observes only the sum of\\na number of previously generated rewards which happen to arrive in the given\\nround. The rewards are stochastically delayed and due to the aggregated nature\\nof the observations, the information of which arm led to a particular reward is\\nlost. The question is what is the cost of the information loss due to this\\ndelayed, aggregated anonymous feedback? Previous works have studied bandits\\nwith stochastic, non-anonymous delays and found that the regret increases only\\nby an additive factor relating to the expected delay. In this paper, we show\\nthat this additive regret increase can be maintained in the harder delayed,\\naggregated anonymous feedback setting when the expected delay (or a bound on\\nit) is known. We provide an algorithm that matches the worst case regret of the\\nnon-anonymous problem exactly when the delays are bounded, and up to\\nlogarithmic factors or an additive variance term for unbounded delays.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1709.06853v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1709.06853v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Ciara Pike-Burke',\n",
       "   'Shipra Agrawal',\n",
       "   'Csaba Szepesvari',\n",
       "   'Steffen Grunewalder'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-09-20',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-landscape-of-lagrangian-functions-and',\n",
       "  'arxiv_id': '1806.05151',\n",
       "  'title': 'On Landscape of Lagrangian Functions and Stochastic Search for Constrained Nonconvex Optimization',\n",
       "  'abstract': 'We study constrained nonconvex optimization problems in machine learning, signal processing, and stochastic control. It is well-known that these problems can be rewritten to a minimax problem in a Lagrangian form. However, due to the lack of convexity, their landscape is not well understood and how to find the stable equilibria of the Lagrangian function is still unknown. To bridge the gap, we study the landscape of the Lagrangian function. Further, we define a special class of Lagrangian functions. They enjoy two properties: 1.Equilibria are either stable or unstable (Formal definition in Section 2); 2.Stable equilibria correspond to the global optima of the original problem. We show that a generalized eigenvalue (GEV) problem, including canonical correlation analysis and other problems, belongs to the class. Specifically, we characterize its stable and unstable equilibria by leveraging an invariant group and symmetric property (more details in Section 3). Motivated by these neat geometric structures, we propose a simple, efficient, and stochastic primal-dual algorithm solving the online GEV problem. Theoretically, we provide sufficient conditions, based on which we establish an asymptotic convergence rate and obtain the first sample complexity result for the online GEV problem by diffusion approximations, which are widely used in applied probability and stochastic control. Numerical results are provided to support our theory.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05151v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05151v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Zhehui Chen',\n",
       "   'Xingguo Li',\n",
       "   'Lin F. Yang',\n",
       "   'Jarvis Haupt',\n",
       "   'Tuo Zhao'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/3d-convolutional-neural-networks-for',\n",
       "  'arxiv_id': '1806.04209',\n",
       "  'title': '3D Convolutional Neural Networks for Classification of Functional Connectomes',\n",
       "  'abstract': \"Resting-state functional MRI (rs-fMRI) scans hold the potential to serve as a\\ndiagnostic or prognostic tool for a wide variety of conditions, such as autism,\\nAlzheimer's disease, and stroke. While a growing number of studies have\\ndemonstrated the promise of machine learning algorithms for rs-fMRI based\\nclinical or behavioral prediction, most prior models have been limited in their\\ncapacity to exploit the richness of the data. For example, classification\\ntechniques applied to rs-fMRI often rely on region-based summary statistics\\nand/or linear models. In this work, we propose a novel volumetric Convolutional\\nNeural Network (CNN) framework that takes advantage of the full-resolution 3D\\nspatial structure of rs-fMRI data and fits non-linear predictive models. We\\nshowcase our approach on a challenging large-scale dataset (ABIDE, with N >\\n2,000) and report state-of-the-art accuracy results on rs-fMRI-based\\ndiscrimination of autism patients and healthy controls.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04209v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04209v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Meenakshi Khosla',\n",
       "   'Keith Jamison',\n",
       "   'Amy Kuceyeski',\n",
       "   'Mert Sabuncu'],\n",
       "  'tasks': ['Classification', 'General Classification'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/presentation-attack-detection-for-iris',\n",
       "  'arxiv_id': '1804.00194',\n",
       "  'title': 'Presentation Attack Detection for Iris Recognition: An Assessment of the State of the Art',\n",
       "  'abstract': 'Iris recognition is increasingly used in large-scale applications. As a\\nresult, presentation attack detection for iris recognition takes on fundamental\\nimportance. This survey covers the diverse research literature on this topic.\\nDifferent categories of presentation attack are described and placed in an\\napplication-relevant framework, and the state of the art in detecting each\\ncategory of attack is summarized. One conclusion from this is that presentation\\nattack detection for iris recognition is not yet a solved problem. Datasets\\navailable for research are described, research directions for the near- and\\nmedium-term future are outlined, and a short list of recommended readings are\\nsuggested.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.00194v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.00194v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Adam Czajka', 'Kevin W. Bowyer'],\n",
       "  'tasks': ['Iris Recognition'],\n",
       "  'date': '2018-03-31',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/active-learning-with-logged-data',\n",
       "  'arxiv_id': '1802.09069',\n",
       "  'title': 'Active Learning with Logged Data',\n",
       "  'abstract': 'We consider active learning with logged data, where labeled examples are\\ndrawn conditioned on a predetermined logging policy, and the goal is to learn a\\nclassifier on the entire population, not just conditioned on the logging\\npolicy. Prior work addresses this problem either when only logged data is\\navailable, or purely in a controlled random experimentation setting where the\\nlogged data is ignored. In this work, we combine both approaches to provide an\\nalgorithm that uses logged data to bootstrap and inform experimentation, thus\\nachieving the best of both worlds. Our work is inspired by a connection between\\ncontrolled random experimentation and active learning, and modifies existing\\ndisagreement-based active learning algorithms to exploit logged data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.09069v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.09069v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Songbai Yan', 'Kamalika Chaudhuri', 'Tara Javidi'],\n",
       "  'tasks': ['Active Learning'],\n",
       "  'date': '2018-02-25',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/lagrange-coded-computing-optimal-design-for',\n",
       "  'arxiv_id': '1806.00939',\n",
       "  'title': 'Lagrange Coded Computing: Optimal Design for Resiliency, Security and Privacy',\n",
       "  'abstract': 'We consider a scenario involving computations over a massive dataset stored\\ndistributedly across multiple workers, which is at the core of distributed\\nlearning algorithms. We propose Lagrange Coded Computing (LCC), a new framework\\nto simultaneously provide (1) resiliency against stragglers that may prolong\\ncomputations; (2) security against Byzantine (or malicious) workers that\\ndeliberately modify the computation for their benefit; and (3)\\n(information-theoretic) privacy of the dataset amidst possible collusion of\\nworkers. LCC, which leverages the well-known Lagrange polynomial to create\\ncomputation redundancy in a novel coded form across workers, can be applied to\\nany computation scenario in which the function of interest is an arbitrary\\nmultivariate polynomial of the input dataset, hence covering many computations\\nof interest in machine learning. LCC significantly generalizes prior works to\\ngo beyond linear computations. It also enables secure and private computing in\\ndistributed settings, improving the computation and communication efficiency of\\nthe state-of-the-art. Furthermore, we prove the optimality of LCC by showing\\nthat it achieves the optimal tradeoff between resiliency, security, and\\nprivacy, i.e., in terms of tolerating the maximum number of stragglers and\\nadversaries, and providing data privacy against the maximum number of colluding\\nworkers. Finally, we show via experiments on Amazon EC2 that LCC speeds up the\\nconventional uncoded implementation of distributed least-squares linear\\nregression by up to $13.43\\\\times$, and also achieves a\\n$2.36\\\\times$-$12.65\\\\times$ speedup over the state-of-the-art straggler\\nmitigation strategies.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.00939v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.00939v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Qian Yu',\n",
       "   'Songze Li',\n",
       "   'Netanel Raviv',\n",
       "   'Seyed Mohammadreza Mousavi Kalan',\n",
       "   'Mahdi Soltanolkotabi',\n",
       "   'Salman Avestimehr'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-04',\n",
       "  'methods': [{'name': 'LCC',\n",
       "    'full_name': 'Lipschitz Constant Constraint',\n",
       "    'description': 'Please enter a description about the method here',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1804.04368v3',\n",
       "    'source_title': 'Regularisation of Neural Networks by Enforcing Lipschitz Continuity',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/exploiting-inherent-error-resiliency-of',\n",
       "  'arxiv_id': '1806.05141',\n",
       "  'title': 'Exploiting Inherent Error-Resiliency of Neuromorphic Computing to achieve Extreme Energy-Efficiency through Mixed-Signal Neurons',\n",
       "  'abstract': 'Neuromorphic computing, inspired by the brain, promises extreme efficiency\\nfor certain classes of learning tasks, such as classification and pattern\\nrecognition. The performance and power consumption of neuromorphic computing\\ndepends heavily on the choice of the neuron architecture. Digital neurons\\n(Dig-N) are conventionally known to be accurate and efficient at high speed,\\nwhile suffering from high leakage currents from a large number of transistors\\nin a large design. On the other hand, analog/mixed-signal neurons are prone to\\nnoise, variability and mismatch, but can lead to extremely low-power designs.\\nIn this work, we will analyze, compare and contrast existing neuron\\narchitectures with a proposed mixed-signal neuron (MS-N) in terms of\\nperformance, power and noise, thereby demonstrating the applicability of the\\nproposed mixed-signal neuron for achieving extreme energy-efficiency in\\nneuromorphic computing. The proposed MS-N is implemented in 65 nm CMOS\\ntechnology and exhibits > 100X better energy-efficiency across all frequencies\\nover two traditional digital neurons synthesized in the same technology node.\\nWe also demonstrate that the inherent error-resiliency of a fully connected or\\neven convolutional neural network (CNN) can handle the noise as well as the\\nmanufacturing non-idealities of the MS-N up to certain degrees. Notably, a\\nsystem-level implementation on MNIST datasets exhibits a worst-case increase in\\nclassification error by 2.1% when the integrated noise power in the bandwidth\\nis ~ 0.1 uV2, along with +-3{\\\\sigma} amount of variation and mismatch\\nintroduced in the transistor parameters for the proposed neuron with 8-bit\\nprecision.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05141v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05141v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Baibhab Chatterjee',\n",
       "   'Priyadarshini Panda',\n",
       "   'Shovan Maity',\n",
       "   'Ayan Biswas',\n",
       "   'Kaushik Roy',\n",
       "   'Shreyas Sen'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist'],\n",
       "  'datasets_used_full': ['MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/efficient-certifiably-optimal-clustering-with',\n",
       "  'arxiv_id': '1806.00530',\n",
       "  'title': 'Efficient, Certifiably Optimal Clustering with Applications to Latent Variable Graphical Models',\n",
       "  'abstract': 'Motivated by the task of clustering either $d$ variables or $d$ points into\\n$K$ groups, we investigate efficient algorithms to solve the Peng-Wei (P-W)\\n$K$-means semi-definite programming (SDP) relaxation. The P-W SDP has been\\nshown in the literature to have good statistical properties in a variety of\\nsettings, but remains intractable to solve in practice. To this end we propose\\nFORCE, a new algorithm to solve this SDP relaxation. Compared to the naive\\ninterior point method, our method reduces the computational complexity of\\nsolving the SDP from $\\\\tilde{O}(d^7\\\\log\\\\epsilon^{-1})$ to\\n$\\\\tilde{O}(d^{6}K^{-2}\\\\epsilon^{-1})$ arithmetic operations for an\\n$\\\\epsilon$-optimal solution. Our method combines a primal first-order method\\nwith a dual optimality certificate search, which when successful, allows for\\nearly termination of the primal method. We show for certain variable clustering\\nproblems that, with high probability, FORCE is guaranteed to find the optimal\\nsolution to the SDP relaxation and provide a certificate of exact optimality.\\nAs verified by our numerical experiments, this allows FORCE to solve the P-W\\nSDP with dimensions in the hundreds in only tens of seconds. For a variation of\\nthe P-W SDP where $K$ is not known a priori a slight modification of FORCE\\nreduces the computational complexity of solving this problem as well: from\\n$\\\\tilde{O}(d^7\\\\log\\\\epsilon^{-1})$ using a standard SDP solver to\\n$\\\\tilde{O}(d^{4}\\\\epsilon^{-1})$.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.00530v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.00530v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Carson Eisenach', 'Han Liu'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-01',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/high-dimensional-inference-for-cluster-based',\n",
       "  'arxiv_id': '1806.05139',\n",
       "  'title': 'High-Dimensional Inference for Cluster-Based Graphical Models',\n",
       "  'abstract': 'Motivated by modern applications in which one constructs graphical models based on a very large number of features, this paper introduces a new class of cluster-based graphical models, in which variable clustering is applied as an initial step for reducing the dimension of the feature space. We employ model assisted clustering, in which the clusters contain features that are similar to the same unobserved latent variable. Two different cluster-based Gaussian graphical models are considered: the latent variable graph, corresponding to the graphical model associated with the unobserved latent variables, and the cluster-average graph, corresponding to the vector of features averaged over clusters. Our study reveals that likelihood based inference for the latent graph, not analyzed previously, is analytically intractable. Our main contribution is the development and analysis of alternative estimation and inference strategies, for the precision matrix of an unobservable latent vector $Z$. We replace the likelihood of the data by an appropriate class of empirical risk functions, that can be specialized to the latent graphical model and to the simpler, but under-analyzed, cluster-average graphical model. The estimators thus derived can be used for inference on the graph structure, for instance on edge strength or pattern recovery. Inference is based on the asymptotic limits of the entry-wise estimates of the precision matrices associated with the conditional independence graphs under consideration. While taking the uncertainty induced by the clustering step into account, we establish Berry-Esseen central limit theorems for the proposed estimators. It is noteworthy that, although the clusters are estimated adaptively from the data, the central limit theorems regarding the entries of the estimated graphs are proved under the same conditions one would use if the clusters were known....',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05139v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05139v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Carson Eisenach',\n",
       "   'Florentina Bunea',\n",
       "   'Yang Ning',\n",
       "   'Claudiu Dinicu'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fully-convolutional-network-for-automatic',\n",
       "  'arxiv_id': '1806.05182',\n",
       "  'title': 'Fully Convolutional Network for Automatic Road Extraction from Satellite Imagery',\n",
       "  'abstract': 'Analysis of high-resolution satellite images has been an important research\\ntopic for traffic management, city planning, and road monitoring. One of the\\nproblems here is automatic and precise road extraction. From an original image,\\nit is difficult and computationally expensive to extract roads due to presences\\nof other road-like features with straight edges. In this paper, we propose an\\napproach for automatic road extraction based on a fully convolutional neural\\nnetwork of U-net family. This network consists of ResNet-34 pre-trained on\\nImageNet and decoder adapted from vanilla U-Net. Based on validation results,\\nleaderboard and our own experience this network shows superior results for the\\nDEEPGLOBE - CVPR 2018 road extraction sub-challenge. Moreover, this network\\nuses moderate memory that allows using just one GTX 1080 or 1080ti video cards\\nto perform whole training and makes pretty fast predictions.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05182v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05182v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alexander V. Buslaev',\n",
       "   'Selim S. Seferbekov',\n",
       "   'Vladimir I. Iglovikov',\n",
       "   'Alexey A. Shvets'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'Concatenated Skip Connection',\n",
       "    'full_name': 'Concatenated Skip Connection',\n",
       "    'description': 'A **Concatenated Skip Connection** is a type of skip connection that seeks to reuse features by concatenating them to new layers, allowing more information to be retained from previous layers of the network. This contrasts with say, residual connections, where element-wise summation is used instead to incorporate information from previous layers. This type of skip connection is prominently used in DenseNets (and also Inception networks), which the Figure to the right illustrates.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/densenet.py#L113',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'U-Net',\n",
       "    'full_name': 'U-Net',\n",
       "    'description': '**U-Net** is an architecture for semantic segmentation. It consists of a contracting path and an expansive path. The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit ([ReLU](https://paperswithcode.com/method/relu)) and a 2x2 [max pooling](https://paperswithcode.com/method/max-pooling) operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 [convolution](https://paperswithcode.com/method/convolution) (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a [1x1 convolution](https://paperswithcode.com/method/1x1-convolution) is used to map each 64-component feature vector to the desired number of classes. In total the network has 23 convolutional layers.\\r\\n\\r\\n[Original MATLAB Code](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-release-2015-10-02.tar.gz)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1505.04597v1',\n",
       "    'source_title': 'U-Net: Convolutional Networks for Biomedical Image Segmentation',\n",
       "    'code_snippet_url': 'https://github.com/milesial/Pytorch-UNet/blob/67bf11b4db4c5f2891bd7e8e7f58bcde8ee2d2db/unet/unet_model.py#L8',\n",
       "    'main_collection': {'name': 'Semantic Segmentation Models',\n",
       "     'description': '**Semantic Segmentation Models** are a class of methods that address the task of semantically segmenting an image into different object classes. Below you can find a continuously updating list of semantic segmentation models. ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['deepglobe'],\n",
       "  'datasets_used_full': ['DeepGlobe'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/generative-neural-machine-translation',\n",
       "  'arxiv_id': '1806.05138',\n",
       "  'title': 'Generative Neural Machine Translation',\n",
       "  'abstract': 'We introduce Generative Neural Machine Translation (GNMT), a latent variable\\narchitecture which is designed to model the semantics of the source and target\\nsentences. We modify an encoder-decoder translation model by adding a latent\\nvariable as a language agnostic representation which is encouraged to learn the\\nmeaning of the sentence. GNMT achieves competitive BLEU scores on pure\\ntranslation tasks, and is superior when there are missing words in the source\\nsentence. We augment the model to facilitate multilingual translation and\\nsemi-supervised learning without adding parameters. This framework\\nsignificantly reduces overfitting when there is limited paired data available,\\nand is effective for translating between pairs of languages not seen during\\ntraining.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05138v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05138v1.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Harshil Shah', 'David Barber'],\n",
       "  'tasks': ['Machine Translation', 'Translation'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/marginal-policy-gradients-a-unified-family-of',\n",
       "  'arxiv_id': '1806.05134',\n",
       "  'title': 'Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications',\n",
       "  'abstract': 'Many complex domains, such as robotics control and real-time strategy (RTS)\\ngames, require an agent to learn a continuous control. In the former, an agent\\nlearns a policy over $\\\\mathbb{R}^d$ and in the latter, over a discrete set of\\nactions each of which is parametrized by a continuous parameter. Such problems\\nare naturally solved using policy based reinforcement learning (RL) methods,\\nbut unfortunately these often suffer from high variance leading to instability\\nand slow convergence. Unnecessary variance is introduced whenever policies over\\nbounded action spaces are modeled using distributions with unbounded support by\\napplying a transformation $T$ to the sampled action before execution in the\\nenvironment. Recently, the variance reduced clipped action policy gradient\\n(CAPG) was introduced for actions in bounded intervals, but to date no variance\\nreduced methods exist when the action is a direction, something often seen in\\nRTS games. To this end we introduce the angular policy gradient (APG), a\\nstochastic policy gradient method for directional control. With the marginal\\npolicy gradients family of estimators we present a unified analysis of the\\nvariance reduction properties of APG and CAPG; our results provide a stronger\\nguarantee than existing analyses for CAPG. Experimental results on a popular\\nRTS game and a navigation task show that the APG estimator offers a substantial\\nimprovement over the standard policy gradient.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05134v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05134v3.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Carson Eisenach', 'Haichuan Yang', 'Ji Liu', 'Han Liu'],\n",
       "  'tasks': ['Continuous Control'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/quantifying-the-dynamics-of-topical',\n",
       "  'arxiv_id': '1806.00699',\n",
       "  'title': 'Quantifying the dynamics of topical fluctuations in language',\n",
       "  'abstract': 'The availability of large diachronic corpora has provided the impetus for a growing body of quantitative research on language evolution and meaning change. The central quantities in this research are token frequencies of linguistic elements in texts, with changes in frequency taken to reflect the popularity or selective fitness of an element. However, corpus frequencies may change for a wide variety of reasons, including purely random sampling effects, or because corpora are composed of contemporary media and fiction texts within which the underlying topics ebb and flow with cultural and socio-political trends. In this work, we introduce a simple model for controlling for topical fluctuations in corpora - the topical-cultural advection model - and demonstrate how it provides a robust baseline of variability in word frequency changes over time. We validate the model on a diachronic corpus spanning two centuries, and a carefully-controlled artificial language change scenario, and then use it to correct for topical fluctuations in historical time series. Finally, we use the model to show that the emergence of new words typically corresponds with the rise of a trending topic. This suggests that some lexical innovations occur due to growing communicative need in a subspace of the lexicon, and that the topical-cultural advection model can be used to quantify this.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.00699v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.00699v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Andres Karjus',\n",
       "   'Richard A. Blythe',\n",
       "   'Simon Kirby',\n",
       "   'Kenny Smith'],\n",
       "  'tasks': ['Time Series'],\n",
       "  'date': '2018-06-02',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/what-is-it-like-down-there-generating-dense',\n",
       "  'arxiv_id': '1806.05129',\n",
       "  'title': 'What Is It Like Down There? Generating Dense Ground-Level Views and Image Features From Overhead Imagery Using Conditional Generative Adversarial Networks',\n",
       "  'abstract': 'This paper investigates conditional generative adversarial networks (cGANs)\\nto overcome a fundamental limitation of using geotagged media for geographic\\ndiscovery, namely its sparse and uneven spatial distribution. We train a cGAN\\nto generate ground-level views of a location given overhead imagery. We show\\nthe \"fake\" ground-level images are natural looking and are structurally similar\\nto the real images. More significantly, we show the generated images are\\nrepresentative of the locations and that the representations learned by the\\ncGANs are informative. In particular, we show that dense feature maps generated\\nusing our framework are more effective for land-cover classification than\\napproaches which spatially interpolate features extracted from sparse\\nground-level images. To our knowledge, ours is the first work to use cGANs to\\ngenerate ground-level views given overhead imagery and to explore the benefits\\nof the learned representations.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05129v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05129v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xueqing Deng', 'Yi Zhu', 'Shawn Newsam'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/online-multi-object-tracking-with-historical',\n",
       "  'arxiv_id': '1805.10916',\n",
       "  'title': 'Online Multi-Object Tracking with Historical Appearance Matching and Scene Adaptive Detection Filtering',\n",
       "  'abstract': 'In this paper, we propose the methods to handle temporal errors during\\nmulti-object tracking. Temporal error occurs when objects are occluded or noisy\\ndetections appear near the object. In those situations, tracking may fail and\\nvarious errors like drift or ID-switching occur. It is hard to overcome\\ntemporal errors only by using motion and shape information. So, we propose the\\nhistorical appearance matching method and joint-input siamese network which was\\ntrained by 2-step process. It can prevent tracking failures although objects\\nare temporally occluded or last matching information is unreliable. We also\\nprovide useful technique to remove noisy detections effectively according to\\nscene condition. Tracking performance, especially identity consistency, is\\nhighly improved by attaching our methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.10916v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.10916v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Young-chul Yoon',\n",
       "   'Abhijeet Boragule',\n",
       "   'Young-min Song',\n",
       "   'Kwangjin Yoon',\n",
       "   'Moongu Jeon'],\n",
       "  'tasks': ['Multi-Object Tracking',\n",
       "   'Object Tracking',\n",
       "   'Online Multi-Object Tracking'],\n",
       "  'date': '2018-05-28',\n",
       "  'methods': [{'name': 'Siamese Network',\n",
       "    'full_name': 'Siamese Network',\n",
       "    'description': 'A **Siamese Network** consists of twin networks which accept distinct inputs but are joined by an energy function at the top. This function computes a metric between the highest level feature representation on each side. The parameters between the twin networks are tied. [Weight tying](https://paperswithcode.com/method/weight-tying) guarantees that two extremely similar images are not mapped by each network to very different locations in feature space because each network computes the same function. The network is symmetric, so that whenever we present two distinct images to the twin networks, the top conjoining layer will compute the same metric as if we were to we present the same two images but to the opposite twins.\\r\\n\\r\\nIntuitively instead of trying to classify inputs, a siamese network learns to differentiate between inputs, learning their similarity. The loss function used is usually a form of contrastive loss.\\r\\n\\r\\nSource: [Koch et al](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)',\n",
       "    'introduced_year': 1993,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Twin Networks',\n",
       "     'description': '**Twin Networks** are a type of neural network architecture where we use two of the same network architecture to perform a task. For example, Siamese Networks are used to learn representations that differentiate between inputs (learning their similarity). Below you can find a continuously updating list of twin network architectures.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['motchallenge', 'mot17', 'mot16', 'cuhk02'],\n",
       "  'datasets_used_full': ['MOTChallenge', 'MOT17', 'MOT16', 'CUHK02'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-to-shoot-in-first-person-shooter',\n",
       "  'arxiv_id': '1806.05117',\n",
       "  'title': 'Learning to Shoot in First Person Shooter Games by Stabilizing Actions and Clustering Rewards for Reinforcement Learning',\n",
       "  'abstract': 'While reinforcement learning (RL) has been applied to turn-based board games\\nfor many years, more complex games involving decision-making in real-time are\\nbeginning to receive more attention. A challenge in such environments is that\\nthe time that elapses between deciding to take an action and receiving a reward\\nbased on its outcome can be longer than the interval between successive\\ndecisions. We explore this in the context of a non-player character (NPC) in a\\nmodern first-person shooter game. Such games take place in 3D environments\\nwhere players, both human and computer-controlled, compete by engaging in\\ncombat and completing task objectives. We investigate the use of RL to enable\\nNPCs to gather experience from game-play and improve their shooting skill over\\ntime from a reward signal based on the damage caused to opponents. We propose a\\nnew method for RL updates and reward calculations, in which the updates are\\ncarried out periodically, after each shooting encounter has ended, and a new\\nweighted-reward mechanism is used which increases the reward applied to actions\\nthat lead to damaging the opponent in successive hits in what we term \"hit\\nclusters\".',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05117v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05117v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Frank G. Glavin', 'Michael G. Madden'],\n",
       "  'tasks': ['Board Games', 'Decision Making'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-retrospective-analysis-of-the-fake-news-1',\n",
       "  'arxiv_id': '1806.05180',\n",
       "  'title': 'A Retrospective Analysis of the Fake News Challenge Stance Detection Task',\n",
       "  'abstract': \"The 2017 Fake News Challenge Stage 1 (FNC-1) shared task addressed a stance\\nclassification task as a crucial first step towards detecting fake news. To\\ndate, there is no in-depth analysis paper to critically discuss FNC-1's\\nexperimental setup, reproduce the results, and draw conclusions for\\nnext-generation stance classification methods. In this paper, we provide such\\nan in-depth analysis for the three top-performing systems. We first find that\\nFNC-1's proposed evaluation metric favors the majority class, which can be\\neasily classified, and thus overestimates the true discriminative power of the\\nmethods. Therefore, we propose a new F1-based metric yielding a changed system\\nranking. Next, we compare the features and architectures used, which leads to a\\nnovel feature-rich stacked LSTM model that performs on par with the best\\nsystems, but is superior in predicting minority classes. To understand the\\nmethods' ability to generalize, we derive a new dataset and perform both\\nin-domain and cross-domain experiments. Our qualitative and quantitative study\\nhelps interpreting the original FNC-1 scores and understand which features help\\nimproving performance and why. Our new dataset and all source code used during\\nthe reproduction study are publicly available for future research.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05180v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05180v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Andreas Hanselowski',\n",
       "   'Avinesh PVS',\n",
       "   'Benjamin Schiller',\n",
       "   'Felix Caspelherr',\n",
       "   'Debanjan Chaudhuri',\n",
       "   'Christian M. Meyer',\n",
       "   'Iryna Gurevych'],\n",
       "  'tasks': ['General Classification',\n",
       "   'Stance Classification',\n",
       "   'Stance Detection'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': ['fnc-1'],\n",
       "  'datasets_used_full': ['FNC-1'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-physical-model-for-efficient-ranking-in',\n",
       "  'arxiv_id': '1709.09002',\n",
       "  'title': 'A physical model for efficient ranking in networks',\n",
       "  'abstract': 'We present a physically-inspired model and an efficient algorithm to infer\\nhierarchical rankings of nodes in directed networks. It assigns real-valued\\nranks to nodes rather than simply ordinal ranks, and it formalizes the\\nassumption that interactions are more likely to occur between individuals with\\nsimilar ranks. It provides a natural statistical significance test for the\\ninferred hierarchy, and it can be used to perform inference tasks such as\\npredicting the existence or direction of edges. The ranking is obtained by\\nsolving a linear system of equations, which is sparse if the network is; thus\\nthe resulting algorithm is extremely efficient and scalable. We illustrate\\nthese findings by analyzing real and synthetic data, including datasets from\\nanimal behavior, faculty hiring, social support networks, and sports\\ntournaments. We show that our method often outperforms a variety of others, in\\nboth speed and accuracy, in recovering the underlying ranks and predicting edge\\ndirections.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1709.09002v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1709.09002v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Caterina De Bacco', 'Daniel B. Larremore', 'Cristopher Moore'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-09-03',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/comparing-fairness-criteria-based-on-social',\n",
       "  'arxiv_id': '1806.05112',\n",
       "  'title': 'Comparing Fairness Criteria Based on Social Outcome',\n",
       "  'abstract': 'Fairness in algorithmic decision-making processes is attracting increasing\\nconcern. When an algorithm is applied to human-related decision-making an\\nestimator solely optimizing its predictive power can learn biases on the\\nexisting data, which motivates us the notion of fairness in machine learning.\\nwhile several different notions are studied in the literature, little studies\\nare done on how these notions affect the individuals. We demonstrate such a\\ncomparison between several policies induced by well-known fairness criteria,\\nincluding the color-blind (CB), the demographic parity (DP), and the equalized\\nodds (EO). We show that the EO is the only criterion among them that removes\\ngroup-level disparity. Empirical studies on the social welfare and disparity of\\nthese policies are conducted.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05112v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05112v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Junpei Komiyama', 'Hajime Shimao'],\n",
       "  'tasks': ['Decision Making'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/knowledge-amalgam-generating-jokes-and-quotes',\n",
       "  'arxiv_id': '1806.04387',\n",
       "  'title': 'Knowledge Amalgam: Generating Jokes and Quotes Together',\n",
       "  'abstract': \"Generating humor and quotes are very challenging problems in the field of\\ncomputational linguistics and are often tackled separately. In this paper, we\\npresent a controlled Long Short-Term Memory (LSTM) architecture which is\\ntrained with categorical data like jokes and quotes together by passing\\ncategory as an input along with the sequence of words. The idea is that a\\nsingle neural net will learn the structure of both jokes and quotes to generate\\nthem on demand according to input category. Importantly, we believe the neural\\nnet has more knowledge as it's trained on different datasets and hence will\\nenable it to generate more creative jokes or quotes from the mixture of\\ninformation. May the network generate a funny inspirational joke!\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04387v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04387v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Bhargav Chippada', 'Shubajit Saha'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dre-bot-a-hierarchical-first-person-shooter',\n",
       "  'arxiv_id': '1806.05106',\n",
       "  'title': 'DRE-Bot: A Hierarchical First Person Shooter Bot Using Multiple Sarsa(λ) Reinforcement Learners',\n",
       "  'abstract': 'This paper describes an architecture for controlling non-player characters\\n(NPC) in the First Person Shooter (FPS) game Unreal Tournament 2004.\\nSpecifically, the DRE-Bot architecture is made up of three reinforcement\\nlearners, Danger, Replenish and Explore, which use the tabular Sarsa({\\\\lambda})\\nalgorithm. This algorithm enables the NPC to learn through trial and error\\nbuilding up experience over time in an approach inspired by human learning.\\nExperimentation is carried to measure the performance of DRE-Bot when competing\\nagainst fixed strategy bots that ship with the game. The discount parameter,\\n{\\\\gamma}, and the trace parameter, {\\\\lambda}, are also varied to see if their\\nvalues have an effect on the performance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05106v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05106v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Frank G. Glavin', 'Michael G. Madden'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/improving-cytoarchitectonic-segmentation-of',\n",
       "  'arxiv_id': '1806.05104',\n",
       "  'title': 'Improving Cytoarchitectonic Segmentation of Human Brain Areas with Self-supervised Siamese Networks',\n",
       "  'abstract': 'Cytoarchitectonic parcellations of the human brain serve as anatomical\\nreferences in multimodal atlas frameworks. They are based on analysis of\\ncell-body stained histological sections and the identification of borders\\nbetween brain areas. The de-facto standard involves a semi-automatic,\\nreproducible border detection, but does not scale with high-throughput imaging\\nin large series of sections at microscopical resolution. Automatic\\nparcellation, however, is extremely challenging due to high variation in the\\ndata, and the need for a large field of view at microscopic resolution. The\\nperformance of a recently proposed Convolutional Neural Network model that\\naddresses this problem especially suffers from the naturally limited amount of\\nexpert annotations for training. To circumvent this limitation, we propose to\\npre-train neural networks on a self-supervised auxiliary task, predicting the\\n3D distance between two patches sampled from the same brain. Compared to a\\nrandom initialization, fine-tuning from these networks results in significantly\\nbetter segmentations. We show that the self-supervised model has implicitly\\nlearned to distinguish several cortical brain areas -- a strong indicator that\\nthe proposed auxiliary task is appropriate for cytoarchitectonic mapping.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05104v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05104v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hannah Spitzer',\n",
       "   'Kai Kiwitz',\n",
       "   'Katrin Amunts',\n",
       "   'Stefan Harmeling',\n",
       "   'Timo Dickscheid'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/graph-based-decoding-for-event-sequencing-and-1',\n",
       "  'arxiv_id': '1806.05099',\n",
       "  'title': 'Graph-Based Decoding for Event Sequencing and Coreference Resolution',\n",
       "  'abstract': 'Events in text documents are interrelated in complex ways. In this paper, we\\nstudy two types of relation: Event Coreference and Event Sequencing. We show\\nthat the popular tree-like decoding structure for automated Event Coreference\\nis not suitable for Event Sequencing. To this end, we propose a graph-based\\ndecoding algorithm that is applicable to both tasks. The new decoding algorithm\\nsupports flexible feature sets for both tasks. Empirically, our event\\ncoreference system has achieved state-of-the-art performance on the TAC-KBP\\n2015 event coreference task and our event sequencing system beats a strong\\ntemporal-based, oracle-informed baseline. We discuss the challenges of studying\\nthese event relations.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05099v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05099v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Zhengzhong Liu', 'Teruko Mitamura', 'Eduard Hovy'],\n",
       "  'tasks': ['Coreference Resolution'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/spectral-network-embedding-a-fast-and',\n",
       "  'arxiv_id': '1806.02623',\n",
       "  'title': 'Spectral Network Embedding: A Fast and Scalable Method via Sparsity',\n",
       "  'abstract': 'Network embedding aims to learn low-dimensional representations of nodes in a\\nnetwork, while the network structure and inherent properties are preserved. It\\nhas attracted tremendous attention recently due to significant progress in\\ndownstream network learning tasks, such as node classification, link\\nprediction, and visualization. However, most existing network embedding methods\\nsuffer from the expensive computations due to the large volume of networks. In\\nthis paper, we propose a $10\\\\times \\\\sim 100\\\\times$ faster network embedding\\nmethod, called Progle, by elegantly utilizing the sparsity property of online\\nnetworks and spectral analysis. In Progle, we first construct a \\\\textit{sparse}\\nproximity matrix and train the network embedding efficiently via sparse matrix\\ndecomposition. Then we introduce a network propagation pattern via spectral\\nanalysis to incorporate local and global structure information into the\\nembedding. Besides, this model can be generalized to integrate network\\ninformation into other insufficiently trained embeddings at speed. Benefiting\\nfrom sparse spectral network embedding, our experiment on four different\\ndatasets shows that Progle outperforms or is comparable to state-of-the-art\\nunsupervised comparison approaches---DeepWalk, LINE, node2vec, GraRep, and\\nHOPE, regarding accuracy, while is $10\\\\times$ faster than the fastest\\nword2vec-based method. Finally, we validate the scalability of Progle both in\\nreal large-scale networks and multiple scales of synthetic networks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.02623v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.02623v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jie Zhang', 'Yan Wang', 'Jie Tang', 'Ming Ding'],\n",
       "  'tasks': ['Link Prediction', 'Network Embedding', 'Node Classification'],\n",
       "  'date': '2018-06-07',\n",
       "  'methods': [{'name': 'GraRep',\n",
       "    'full_name': 'Graph Representation with Global structure',\n",
       "    'description': '',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://www.researchgate.net/publication/301417811_GraRep',\n",
       "    'source_title': 'GraRep: Learning Graph Representations with Global Structural Information',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Graph Embeddings',\n",
       "     'description': 'graph embeddings, can be homogeneous graph or heterogeneous graph',\n",
       "     'parent': None,\n",
       "     'area': 'Graphs'}},\n",
       "   {'name': 'node2vec',\n",
       "    'full_name': 'node2vec',\n",
       "    'description': '**node2vec** is a framework for learning graph embeddings for nodes in graphs. Node2vec maximizes a likelihood objective over mappings which preserve neighbourhood distances in higher dimensional spaces. From an algorithm design perspective, node2vec exploits the freedom to define neighbourhoods for nodes and provide an explanation for the effect of the choice of neighborhood on the learned representations. \\r\\n\\r\\nFor each node, node2vec simulates biased random walks based on an efficient network-aware search strategy and the nodes appearing in the random walk define neighbourhoods. The search strategy accounts for the relative influence nodes exert in a network. It also generalizes prior work alluding to naive search strategies by providing flexibility in exploring neighborhoods.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1607.00653v1',\n",
       "    'source_title': 'node2vec: Scalable Feature Learning for Networks',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Graph Embeddings',\n",
       "     'description': 'graph embeddings, can be homogeneous graph or heterogeneous graph',\n",
       "     'parent': None,\n",
       "     'area': 'Graphs'}}],\n",
       "  'datasets_used_lower': ['ppi', 'dblp'],\n",
       "  'datasets_used_full': ['PPI', 'DBLP'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/introducing-user-prescribed-constraints-in',\n",
       "  'arxiv_id': '1806.05096',\n",
       "  'title': 'Introducing user-prescribed constraints in Markov chains for nonlinear dimensionality reduction',\n",
       "  'abstract': 'Stochastic kernel based dimensionality reduction approaches have become\\npopular in the last decade. The central component of many of these methods is a\\nsymmetric kernel that quantifies the vicinity between pairs of data points and\\na kernel-induced Markov chain on the data. Typically, the Markov chain is fully\\nspecified by the kernel through row normalization. However, in many cases, it\\nis desirable to impose user-specified stationary-state and dynamical\\nconstraints on the Markov chain. Unfortunately, no systematic framework exists\\nto impose such user-defined constraints. Here, we introduce a path entropy\\nmaximization based approach to derive the transition probabilities of Markov\\nchains using a kernel and additional user-specified constraints. We illustrate\\nthe usefulness of these Markov chains with examples.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05096v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05096v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Purushottam D. Dixit'],\n",
       "  'tasks': ['Dimensionality Reduction'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/tempered-adversarial-networks',\n",
       "  'arxiv_id': '1802.04374',\n",
       "  'title': 'Tempered Adversarial Networks',\n",
       "  'abstract': 'Generative adversarial networks (GANs) have been shown to produce realistic\\nsamples from high-dimensional distributions, but training them is considered\\nhard. A possible explanation for training instabilities is the inherent\\nimbalance between the networks: While the discriminator is trained directly on\\nboth real and fake samples, the generator only has control over the fake\\nsamples it produces since the real data distribution is fixed by the choice of\\na given dataset. We propose a simple modification that gives the generator\\ncontrol over the real samples which leads to a tempered learning process for\\nboth generator and discriminator. The real data distribution passes through a\\nlens before being revealed to the discriminator, balancing the generator and\\ndiscriminator by gradually revealing more detailed features necessary to\\nproduce high-quality results. The proposed module automatically adjusts the\\nlearning process to the current strength of the networks, yet is generic and\\neasy to add to any GAN variant. In a number of experiments, we show that this\\ncan improve quality, stability and/or convergence speed across a range of\\ndifferent GAN architectures (DCGAN, LSGAN, WGAN-GP).',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.04374v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.04374v4.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Mehdi S. M. Sajjadi',\n",
       "   'Giambattista Parascandolo',\n",
       "   'Arash Mehrjou',\n",
       "   'Bernhard Schölkopf'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-12',\n",
       "  'methods': [{'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Batch Normalization',\n",
       "    'full_name': 'Batch Normalization',\n",
       "    'description': '**Batch Normalization** aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for [Dropout](https://paperswithcode.com/method/dropout).\\r\\n\\r\\nWe apply a batch normalization layer as follows for a minibatch $\\\\mathcal{B}$:\\r\\n\\r\\n$$ \\\\mu\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}x\\\\_{i} $$\\r\\n\\r\\n$$ \\\\sigma^{2}\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}\\\\left(x\\\\_{i}-\\\\mu\\\\_{\\\\mathcal{B}}\\\\right)^{2} $$\\r\\n\\r\\n$$ \\\\hat{x}\\\\_{i} = \\\\frac{x\\\\_{i} - \\\\mu\\\\_{\\\\mathcal{B}}}{\\\\sqrt{\\\\sigma^{2}\\\\_{\\\\mathcal{B}}+\\\\epsilon}} $$\\r\\n\\r\\n$$ y\\\\_{i} = \\\\gamma\\\\hat{x}\\\\_{i} + \\\\beta = \\\\text{BN}\\\\_{\\\\gamma, \\\\beta}\\\\left(x\\\\_{i}\\\\right) $$\\r\\n\\r\\nWhere $\\\\gamma$ and $\\\\beta$ are learnable parameters.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.03167v3',\n",
       "    'source_title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/36f91261099b00194922bd93ed1286fe1c199724/jax/experimental/stax.py#L116',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Dense Connections',\n",
       "    'full_name': 'Dense Connections',\n",
       "    'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\\\_{\\\\text{inputs}}*n\\\\_{\\\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\\r\\n\\r\\n$$h\\\\_{l} = g\\\\left(\\\\textbf{W}^{T}h\\\\_{l-1}\\\\right)$$\\r\\n\\r\\nwhere $g$ is an activation function.\\r\\n\\r\\nImage Source: Deep Learning by Goodfellow, Bengio and Courville',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Leaky ReLU',\n",
       "    'full_name': 'Leaky ReLU',\n",
       "    'description': '**Leaky Rectified Linear Unit**, or **Leaky ReLU**, is a type of activation function based on a [ReLU](https://paperswithcode.com/method/relu), but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training. This type of activation function is popular in tasks where we we may suffer from sparse gradients, for example training generative adversarial networks.',\n",
       "    'introduced_year': 2014,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L649',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'GAN Least Squares Loss',\n",
       "    'full_name': 'GAN Least Squares Loss',\n",
       "    'description': '**GAN Least Squares Loss** is a least squares loss function for generative adversarial networks. Minimizing this objective function is equivalent to minimizing the Pearson $\\\\chi^{2}$ divergence. The objective function (here for [LSGAN](https://paperswithcode.com/method/lsgan)) can be defined as:\\r\\n\\r\\n$$ \\\\min\\\\_{D}V\\\\_{LS}\\\\left(D\\\\right) = \\\\frac{1}{2}\\\\mathbb{E}\\\\_{\\\\mathbf{x} \\\\sim p\\\\_{data}\\\\left(\\\\mathbf{x}\\\\right)}\\\\left[\\\\left(D\\\\left(\\\\mathbf{x}\\\\right) - b\\\\right)^{2}\\\\right] + \\\\frac{1}{2}\\\\mathbb{E}\\\\_{\\\\mathbf{z}\\\\sim p\\\\_{data}\\\\left(\\\\mathbf{z}\\\\right)}\\\\left[\\\\left(D\\\\left(G\\\\left(\\\\mathbf{z}\\\\right)\\\\right) - a\\\\right)^{2}\\\\right] $$\\r\\n\\r\\n$$ \\\\min\\\\_{G}V\\\\_{LS}\\\\left(G\\\\right) = \\\\frac{1}{2}\\\\mathbb{E}\\\\_{\\\\mathbf{z} \\\\sim p\\\\_{\\\\mathbf{z}}\\\\left(\\\\mathbf{z}\\\\right)}\\\\left[\\\\left(D\\\\left(G\\\\left(\\\\mathbf{z}\\\\right)\\\\right) - c\\\\right)^{2}\\\\right] $$\\r\\n\\r\\nwhere $a$ and $b$ are the labels for fake data and real data and $c$ denotes the value that $G$ wants $D$ to believe for fake data.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1611.04076v3',\n",
       "    'source_title': 'Least Squares Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/a163b82beff3d01688d8315a3fd39080400e7c01/implementations/lsgan/lsgan.py#L102',\n",
       "    'main_collection': {'name': 'Loss Functions',\n",
       "     'description': '**Loss Functions** are used to frame the problem to be optimized within deep learning. Below you will find a continuously updating list of (specialized) loss functions for neutral networks.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSGAN',\n",
       "    'full_name': 'LSGAN',\n",
       "    'description': '**LSGAN**, or **Least Squares GAN**, is a type of generative adversarial network that adopts the least squares loss function for the discriminator. Minimizing the objective function of LSGAN yields minimizing the Pearson $\\\\chi^{2}$ divergence. The objective function can be defined as:\\r\\n\\r\\n$$ \\\\min\\\\_{D}V\\\\_{LSGAN}\\\\left(D\\\\right) = \\\\frac{1}{2}\\\\mathbb{E}\\\\_{\\\\mathbf{x} \\\\sim p\\\\_{data}\\\\left(\\\\mathbf{x}\\\\right)}\\\\left[\\\\left(D\\\\left(\\\\mathbf{x}\\\\right) - b\\\\right)^{2}\\\\right] + \\\\frac{1}{2}\\\\mathbb{E}\\\\_{\\\\mathbf{z}\\\\sim p\\\\_{data}\\\\left(\\\\mathbf{z}\\\\right)}\\\\left[\\\\left(D\\\\left(G\\\\left(\\\\mathbf{z}\\\\right)\\\\right) - a\\\\right)^{2}\\\\right] $$\\r\\n\\r\\n$$ \\\\min\\\\_{G}V\\\\_{LSGAN}\\\\left(G\\\\right) = \\\\frac{1}{2}\\\\mathbb{E}\\\\_{\\\\mathbf{z} \\\\sim p\\\\_{\\\\mathbf{z}}\\\\left(\\\\mathbf{z}\\\\right)}\\\\left[\\\\left(D\\\\left(G\\\\left(\\\\mathbf{z}\\\\right)\\\\right) - c\\\\right)^{2}\\\\right] $$\\r\\n\\r\\nwhere $a$ and $b$ are the labels for fake data and real data and $c$ denotes the value that $G$ wants $D$ to believe for fake data.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1611.04076v3',\n",
       "    'source_title': 'Least Squares Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/lsgan/lsgan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['celeba'],\n",
       "  'datasets_used_full': ['CelebA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/estimating-achilles-tendon-healing-progress',\n",
       "  'arxiv_id': '1806.05091',\n",
       "  'title': 'Estimating Achilles tendon healing progress with convolutional neural networks',\n",
       "  'abstract': \"Quantitative assessment of a treatment progress in the Achilles tendon\\nhealing process - one of the most common musculoskeletal disorder in modern\\nmedical practice - is typically a long and complex process: multiple MRI\\nprotocols need to be acquired and analysed by radiology experts. In this paper,\\nwe propose to significantly reduce the complexity of this assessment using a\\nnovel method based on a pre-trained convolutional neural network. We first\\ntrain our neural network on over 500,000 2D axial cross-sections from over 3000\\n3D MRI studies to classify MRI images as belonging to a healthy or injured\\nclass, depending on the patient's condition. We then take the outputs of\\nmodified pre-trained network and apply linear regression on the PCA-reduced\\nspace of the features to assess treatment progress. Our method allows to reduce\\nup to 5-fold the amount of data needed to be registered during the MRI scan\\nwithout any information loss. Furthermore, we are able to predict the healing\\nprocess phase with equal accuracy to human experts in 3 out of 6 main criteria.\\nFinally, contrary to the current approaches to regeneration assessment that\\nrely on radiologist subjective opinion, our method allows to objectively\\ncompare different treatments methods which can lead to improved diagnostics and\\npatient's recovery.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05091v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05091v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Norbert Kapinski',\n",
       "   'Jakub Zielinski',\n",
       "   'Bartosz A. Borucki',\n",
       "   'Tomasz Trzcinski',\n",
       "   'Beata Ciszkowska-Lyson',\n",
       "   'Krzysztof S. Nowinski'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'Linear Regression',\n",
       "    'full_name': 'Linear Regression',\n",
       "    'description': '**Linear Regression** is a method for modelling a relationship between a dependent variable and independent variables. These models can be fit with numerous approaches. The most common is *least squares*, where we minimize the mean square error between the predicted values $\\\\hat{y} = \\\\textbf{X}\\\\hat{\\\\beta}$ and actual values $y$: $\\\\left(y-\\\\textbf{X}\\\\beta\\\\right)^{2}$.\\r\\n\\r\\nWe can also define the problem in probabilistic terms as a generalized linear model (GLM) where the pdf is a Gaussian distribution, and then perform maximum likelihood estimation to estimate $\\\\hat{\\\\beta}$.\\r\\n\\r\\nImage Source: [Wikipedia](https://en.wikipedia.org/wiki/Linear_regression)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Generalized Linear Models',\n",
       "     'description': '**Generalized Linear Models (GLMs)** are a class of models that generalize upon linear regression by allowing many more distributions to be modeled for the response variable via a link function. Below you can find a continuously updating list of GLMs.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-distributions-of-shape-trajectories',\n",
       "  'arxiv_id': '1803.10119',\n",
       "  'title': 'Learning distributions of shape trajectories from longitudinal datasets: a hierarchical model on a manifold of diffeomorphisms',\n",
       "  'abstract': \"We propose a method to learn a distribution of shape trajectories from\\nlongitudinal data, i.e. the collection of individual objects repeatedly\\nobserved at multiple time-points. The method allows to compute an average\\nspatiotemporal trajectory of shape changes at the group level, and the\\nindividual variations of this trajectory both in terms of geometry and time\\ndynamics. First, we formulate a non-linear mixed-effects statistical model as\\nthe combination of a generic statistical model for manifold-valued longitudinal\\ndata, a deformation model defining shape trajectories via the action of a\\nfinite-dimensional set of diffeomorphisms with a manifold structure, and an\\nefficient numerical scheme to compute parallel transport on this manifold.\\nSecond, we introduce a MCMC-SAEM algorithm with a specific approach to shape\\nsampling, an adaptive scheme for proposal variances, and a log-likelihood\\ntempering strategy to estimate our model. Third, we validate our algorithm on\\n2D simulated data, and then estimate a scenario of alteration of the shape of\\nthe hippocampus 3D brain structure during the course of Alzheimer's disease.\\nThe method shows for instance that hippocampal atrophy progresses more quickly\\nin female subjects, and occurs earlier in APOE4 mutation carriers. We finally\\nillustrate the potential of our method for classifying pathological\\ntrajectories versus normal ageing.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.10119v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.10119v2.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Alexandre Bône', 'Olivier Colliot', 'Stanley Durrleman'],\n",
       "  'tasks': ['Hippocampus'],\n",
       "  'date': '2018-03-27',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/group-equivariant-capsule-networks',\n",
       "  'arxiv_id': '1806.05086',\n",
       "  'title': 'Group Equivariant Capsule Networks',\n",
       "  'abstract': 'We present group equivariant capsule networks, a framework to introduce\\nguaranteed equivariance and invariance properties to the capsule network idea.\\nOur work can be divided into two contributions. First, we present a generic\\nrouting by agreement algorithm defined on elements of a group and prove that\\nequivariance of output pose vectors, as well as invariance of output\\nactivations, hold under certain conditions. Second, we connect the resulting\\nequivariant capsule networks with work from the field of group convolutional\\nnetworks. Through this connection, we provide intuitions of how both methods\\nrelate and are able to combine the strengths of both approaches in one deep\\nneural network architecture. The resulting framework allows sparse evaluation\\nof the group convolution operator, provides control over specific equivariance\\nand invariance properties, and can use routing by agreement instead of pooling\\noperations. In addition, it is able to provide interpretable and equivariant\\nrepresentation vectors as output capsules, which disentangle evidence of object\\nexistence from its pose.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05086v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05086v2.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Jan Eric Lenssen', 'Matthias Fey', 'Pascal Libuschewski'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['mnist'],\n",
       "  'datasets_used_full': ['MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/your-2-is-my-1-your-3-is-my-9-handling',\n",
       "  'arxiv_id': '1806.05085',\n",
       "  'title': 'Your 2 is My 1, Your 3 is My 9: Handling Arbitrary Miscalibrations in Ratings',\n",
       "  'abstract': \"Cardinal scores (numeric ratings) collected from people are well known to\\nsuffer from miscalibrations. A popular approach to address this issue is to\\nassume simplistic models of miscalibration (such as linear biases) to de-bias\\nthe scores. This approach, however, often fares poorly because people's\\nmiscalibrations are typically far more complex and not well understood. In the\\nabsence of simplifying assumptions on the miscalibration, it is widely believed\\nby the crowdsourcing community that the only useful information in the cardinal\\nscores is the induced ranking. In this paper, inspired by the framework of\\nStein's shrinkage, empirical Bayes, and the classic two-envelope problem, we\\ncontest this widespread belief. Specifically, we consider cardinal scores with\\narbitrary (or even adversarially chosen) miscalibrations which are only\\nrequired to be consistent with the induced ranking. We design estimators which\\ndespite making no assumptions on the miscalibration, strictly and uniformly\\noutperform all possible estimators that rely on only the ranking. Our\\nestimators are flexible in that they can be used as a plug-in for a variety of\\napplications, and we provide a proof-of-concept for A/B testing and ranking.\\nOur results thus provide novel insights in the eternal debate between cardinal\\nand ordinal data.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05085v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05085v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jing-Yan Wang', 'Nihar B. Shah'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multiple-instance-learning-for-heterogeneous',\n",
       "  'arxiv_id': '1806.05083',\n",
       "  'title': 'Multiple Instance Learning for Heterogeneous Images: Training a CNN for Histopathology',\n",
       "  'abstract': 'Multiple instance (MI) learning with a convolutional neural network enables\\nend-to-end training in the presence of weak image-level labels. We propose a\\nnew method for aggregating predictions from smaller regions of the image into\\nan image-level classification by using the quantile function. The quantile\\nfunction provides a more complete description of the heterogeneity within each\\nimage, improving image-level classification. We also adapt image augmentation\\nto the MI framework by randomly selecting cropped regions on which to apply MI\\naggregation during each epoch of training. This provides a mechanism to study\\nthe importance of MI learning. We validate our method on five different\\nclassification tasks for breast tumor histology and provide a visualization\\nmethod for interpreting local image classifications that could lead to future\\ninsights into tumor heterogeneity.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05083v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05083v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Heather D. Couture',\n",
       "   'J. S. Marron',\n",
       "   'Charles M. Perou',\n",
       "   'Melissa A. Troester',\n",
       "   'Marc Niethammer'],\n",
       "  'tasks': ['Classification',\n",
       "   'General Classification',\n",
       "   'Image Augmentation',\n",
       "   'Multiple Instance Learning'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/minimizing-regret-in-bandit-online',\n",
       "  'arxiv_id': '1806.05069',\n",
       "  'title': 'Minimizing Regret of Bandit Online Optimization in Unconstrained Action Spaces',\n",
       "  'abstract': 'We consider online convex optimization with a zero-order oracle feedback. In particular, the decision maker does not know the explicit representation of the time-varying cost functions, or their gradients. At each time step, she observes the value of the corresponding cost function evaluated at her chosen action (zero-order oracle). The objective is to minimize the regret, that is, the difference between the sum of the costs she accumulates and that of a static optimal action had she known the sequence of cost functions a priori. We present a novel algorithm to minimize regret in unconstrained action spaces. Our algorithm hinges on a classical idea of one-point estimation of the gradients of the cost functions based on their observed values. The algorithm is independent of problem parameters. Letting $T$ denote the number of queries of the zero-order oracle and $n$ the problem dimension, the regret rate achieved is $O(n^{2/3}T^{2/3})$. Moreover, we adapt the presented algorithm to the setting with two-point feedback and demonstrate that the adapted procedure achieves the theoretical lower bound on the regret of $(n^{1/2}T^{1/2})$.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05069v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05069v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Tatiana Tatarenko', 'Maryam Kamgarpour'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/error-bounds-for-piecewise-smooth-and',\n",
       "  'arxiv_id': '1707.07938',\n",
       "  'title': 'Error Bounds for Piecewise Smooth and Switching Regression',\n",
       "  'abstract': 'The paper deals with regression problems, in which the nonsmooth target is\\nassumed to switch between different operating modes. Specifically, piecewise\\nsmooth (PWS) regression considers target functions switching deterministically\\nvia a partition of the input space, while switching regression considers\\narbitrary switching laws. The paper derives generalization error bounds in\\nthese two settings by following the approach based on Rademacher complexities.\\nFor PWS regression, our derivation involves a chaining argument and a\\ndecomposition of the covering numbers of PWS classes in terms of the ones of\\ntheir component functions and the capacity of the classifier partitioning the\\ninput space. This yields error bounds with a radical dependency on the number\\nof modes. For switching regression, the decomposition can be performed directly\\nat the level of the Rademacher complexities, which yields bounds with a linear\\ndependency on the number of modes. By using once more chaining and a\\ndecomposition at the level of covering numbers, we show how to recover a\\nradical dependency. Examples of applications are given in particular for PWS\\nand swichting regression with linear and kernel-based component functions.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1707.07938v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1707.07938v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Fabien Lauer'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-07-25',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/map-inference-via-block-coordinate-frank',\n",
       "  'arxiv_id': '1806.05049',\n",
       "  'title': 'MAP inference via Block-Coordinate Frank-Wolfe Algorithm',\n",
       "  'abstract': 'We present a new proximal bundle method for Maximum-A-Posteriori (MAP)\\ninference in structured energy minimization problems. The method optimizes a\\nLagrangean relaxation of the original energy minimization problem using a multi\\nplane block-coordinate Frank-Wolfe method that takes advantage of the specific\\nstructure of the Lagrangean decomposition. We show empirically that our method\\noutperforms state-of-the-art Lagrangean decomposition based algorithms on some\\nchallenging Markov Random Field, multi-label discrete tomography and graph\\nmatching problems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05049v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05049v2.pdf',\n",
       "  'proceeding': 'CVPR 2019 6',\n",
       "  'authors': ['Paul Swoboda', 'Vladimir Kolmogorov'],\n",
       "  'tasks': ['Graph Matching'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/beyond-counting-comparisons-of-density-maps',\n",
       "  'arxiv_id': '1705.10118',\n",
       "  'title': 'Beyond Counting: Comparisons of Density Maps for Crowd Analysis Tasks - Counting, Detection, and Tracking',\n",
       "  'abstract': 'For crowded scenes, the accuracy of object-based computer vision methods\\ndeclines when the images are low-resolution and objects have severe occlusions.\\nTaking counting methods for example, almost all the recent state-of-the-art\\ncounting methods bypass explicit detection and adopt regression-based methods\\nto directly count the objects of interest. Among regression-based methods,\\ndensity map estimation, where the number of objects inside a subregion is the\\nintegral of the density map over that subregion, is especially promising\\nbecause it preserves spatial information, which makes it useful for both\\ncounting and localization (detection and tracking). With the power of deep\\nconvolutional neural networks (CNNs) the counting performance has improved\\nsteadily. The goal of this paper is to evaluate density maps generated by\\ndensity estimation methods on a variety of crowd analysis tasks, including\\ncounting, detection, and tracking. Most existing CNN methods produce density\\nmaps with resolution that is smaller than the original images, due to the\\ndownsample strides in the convolution/pooling operations. To produce an\\noriginal-resolution density map, we also evaluate a classical CNN that uses a\\nsliding window regressor to predict the density for every pixel in the image.\\nWe also consider a fully convolutional (FCNN) adaptation, with skip connections\\nfrom lower convolutional layers to compensate for loss in spatial information\\nduring upsampling. In our experiments, we found that the lower-resolution\\ndensity maps sometimes have better counting performance. In contrast, the\\noriginal-resolution density maps improved localization tasks, such as detection\\nand tracking, compared to bilinear upsampling the lower-resolution density\\nmaps. Finally, we also propose several metrics for measuring the quality of a\\ndensity map, and relate them to experiment results on counting and\\nlocalization.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1705.10118v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1705.10118v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Di Kang', 'Zheng Ma', 'Antoni B. Chan'],\n",
       "  'tasks': ['Density Estimation'],\n",
       "  'date': '2017-05-29',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/extracting-parallel-sentences-with',\n",
       "  'arxiv_id': '1806.05559',\n",
       "  'title': 'Extracting Parallel Sentences with Bidirectional Recurrent Neural Networks to Improve Machine Translation',\n",
       "  'abstract': 'Parallel sentence extraction is a task addressing the data sparsity problem\\nfound in multilingual natural language processing applications. We propose a\\nbidirectional recurrent neural network based approach to extract parallel\\nsentences from collections of multilingual texts. Our experiments with noisy\\nparallel corpora show that we can achieve promising results against a\\ncompetitive baseline by removing the need of specific feature engineering or\\nadditional external resources. To justify the utility of our approach, we\\nextract sentence pairs from Wikipedia articles to train machine translation\\nsystems and show significant improvements in translation performance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05559v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05559v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Francis Grégoire', 'Philippe Langlais'],\n",
       "  'tasks': ['Feature Engineering', 'Machine Translation', 'Translation'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-probabilistic-u-net-for-segmentation-of',\n",
       "  'arxiv_id': '1806.05034',\n",
       "  'title': 'A Probabilistic U-Net for Segmentation of Ambiguous Images',\n",
       "  'abstract': 'Many real-world vision problems suffer from inherent ambiguities. In clinical\\napplications for example, it might not be clear from a CT scan alone which\\nparticular region is cancer tissue. Therefore a group of graders typically\\nproduces a set of diverse but plausible segmentations. We consider the task of\\nlearning a distribution over segmentations given an input. To this end we\\npropose a generative segmentation model based on a combination of a U-Net with\\na conditional variational autoencoder that is capable of efficiently producing\\nan unlimited number of plausible hypotheses. We show on a lung abnormalities\\nsegmentation task and on a Cityscapes segmentation task that our model\\nreproduces the possible segmentation variants as well as the frequencies with\\nwhich they occur, doing so significantly better than published approaches.\\nThese models could have a high impact in real-world applications, such as being\\nused as clinical decision-making algorithms accounting for multiple plausible\\nsemantic segmentation hypotheses to provide possible diagnoses and recommend\\nfurther actions to resolve the present ambiguities.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05034v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05034v4.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Simon A. A. Kohl',\n",
       "   'Bernardino Romera-Paredes',\n",
       "   'Clemens Meyer',\n",
       "   'Jeffrey De Fauw',\n",
       "   'Joseph R. Ledsam',\n",
       "   'Klaus H. Maier-Hein',\n",
       "   'S. M. Ali Eslami',\n",
       "   'Danilo Jimenez Rezende',\n",
       "   'Olaf Ronneberger'],\n",
       "  'tasks': ['Decision Making', 'Semantic Segmentation'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'Concatenated Skip Connection',\n",
       "    'full_name': 'Concatenated Skip Connection',\n",
       "    'description': 'A **Concatenated Skip Connection** is a type of skip connection that seeks to reuse features by concatenating them to new layers, allowing more information to be retained from previous layers of the network. This contrasts with say, residual connections, where element-wise summation is used instead to incorporate information from previous layers. This type of skip connection is prominently used in DenseNets (and also Inception networks), which the Figure to the right illustrates.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/densenet.py#L113',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'U-Net',\n",
       "    'full_name': 'U-Net',\n",
       "    'description': '**U-Net** is an architecture for semantic segmentation. It consists of a contracting path and an expansive path. The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit ([ReLU](https://paperswithcode.com/method/relu)) and a 2x2 [max pooling](https://paperswithcode.com/method/max-pooling) operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 [convolution](https://paperswithcode.com/method/convolution) (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a [1x1 convolution](https://paperswithcode.com/method/1x1-convolution) is used to map each 64-component feature vector to the desired number of classes. In total the network has 23 convolutional layers.\\r\\n\\r\\n[Original MATLAB Code](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-release-2015-10-02.tar.gz)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1505.04597v1',\n",
       "    'source_title': 'U-Net: Convolutional Networks for Biomedical Image Segmentation',\n",
       "    'code_snippet_url': 'https://github.com/milesial/Pytorch-UNet/blob/67bf11b4db4c5f2891bd7e8e7f58bcde8ee2d2db/unet/unet_model.py#L8',\n",
       "    'main_collection': {'name': 'Semantic Segmentation Models',\n",
       "     'description': '**Semantic Segmentation Models** are a class of methods that address the task of semantically segmenting an image into different object classes. Below you can find a continuously updating list of semantic segmentation models. ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'AutoEncoder',\n",
       "    'full_name': 'AutoEncoder',\n",
       "    'description': 'An **Autoencoder** is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then performs a reconstruction of the input with this latent code (the decoder).\\r\\n\\r\\nImage: [Michael Massi](https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://science.sciencemag.org/content/313/5786/504',\n",
       "    'source_title': 'Reducing the Dimensionality of Data with Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/L1aoXingyu/pytorch-beginner/blob/9c86be785c7c318a09cf29112dd1f1a58613239b/08-AutoEncoder/simple_autoencoder.py#L38',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['cityscapes', 'lidc-idri'],\n",
       "  'datasets_used_full': ['Cityscapes', 'LIDC-IDRI'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/informative-gene-selection-for-microarray',\n",
       "  'arxiv_id': '1806.01466',\n",
       "  'title': 'Informative Gene Selection for Microarray Classification via Adaptive Elastic Net with Conditional Mutual Information',\n",
       "  'abstract': 'Due to the advantage of achieving a better performance under weak\\nregularization, elastic net has attracted wide attention in statistics, machine\\nlearning, bioinformatics, and other fields. In particular, a variation of the\\nelastic net, adaptive elastic net (AEN), integrates the adaptive grouping\\neffect. In this paper, we aim to develop a new algorithm: Adaptive Elastic Net\\nwith Conditional Mutual Information (AEN-CMI) that further improves AEN by\\nincorporating conditional mutual information into the gene selection process.\\nWe apply this new algorithm to screen significant genes for two kinds of\\ncancers: colon cancer and leukemia. Compared with other algorithms including\\nSupport Vector Machine, Classic Elastic Net and Adaptive Elastic Net, the\\nproposed algorithm, AEN-CMI, obtains the best classification performance using\\nthe least number of genes.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.01466v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.01466v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xin-Guang Yang', 'Yongjin Lu'],\n",
       "  'tasks': ['General Classification', 'Microarray Classification'],\n",
       "  'date': '2018-06-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/visually-grounded-cross-lingual-keyword',\n",
       "  'arxiv_id': '1806.05030',\n",
       "  'title': 'Visually grounded cross-lingual keyword spotting in speech',\n",
       "  'abstract': 'Recent work considered how images paired with speech can be used as\\nsupervision for building speech systems when transcriptions are not available.\\nWe ask whether visual grounding can be used for cross-lingual keyword spotting:\\ngiven a text keyword in one language, the task is to retrieve spoken utterances\\ncontaining that keyword in another language. This could enable searching\\nthrough speech in a low-resource language using text queries in a high-resource\\nlanguage. As a proof-of-concept, we use English speech with German queries: we\\nuse a German visual tagger to add keyword labels to each training image, and\\nthen train a neural network to map English speech to German keywords. Without\\nseeing parallel speech-transcriptions or translations, the model achieves a\\nprecision at ten of 58%. We show that most erroneous retrievals contain\\nequivalent or semantically relevant keywords; excluding these would improve\\nP@10 to 91%.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05030v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05030v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Herman Kamper', 'Michael Roth'],\n",
       "  'tasks': ['Keyword Spotting', 'Visual Grounding'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['multi30k-1'],\n",
       "  'datasets_used_full': ['Multi30k'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/hyperdrive-a-systolically-scalable-binary',\n",
       "  'arxiv_id': '1804.00623',\n",
       "  'title': 'Hyperdrive: A Multi-Chip Systolically Scalable Binary-Weight CNN Inference Engine',\n",
       "  'abstract': 'Deep neural networks have achieved impressive results in computer vision and\\nmachine learning. Unfortunately, state-of-the-art networks are extremely\\ncompute and memory intensive which makes them unsuitable for mW-devices such as\\nIoT end-nodes. Aggressive quantization of these networks dramatically reduces\\nthe computation and memory footprint. Binary-weight neural networks (BWNs)\\nfollow this trend, pushing weight quantization to the limit. Hardware\\naccelerators for BWNs presented up to now have focused on core efficiency,\\ndisregarding I/O bandwidth and system-level efficiency that are crucial for\\ndeployment of accelerators in ultra-low power devices. We present Hyperdrive: a\\nBWN accelerator dramatically reducing the I/O bandwidth exploiting a novel\\nbinary-weight streaming approach, which can be used for arbitrarily sized\\nconvolutional neural network architecture and input resolution by exploiting\\nthe natural scalability of the compute units both at chip-level and\\nsystem-level by arranging Hyperdrive chips systolically in a 2D mesh while\\nprocessing the entire feature map together in parallel. Hyperdrive achieves 4.3\\nTOp/s/W system-level efficiency (i.e., including I/Os)---3.1x higher than\\nstate-of-the-art BWN accelerators, even if its core uses resource-intensive\\nFP16 arithmetic for increased robustness.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.00623v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.00623v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Renzo Andri', 'Lukas Cavigelli', 'Davide Rossi', 'Luca Benini'],\n",
       "  'tasks': ['Quantization'],\n",
       "  'date': '2018-03-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/self-supervised-feature-learning-by-learning',\n",
       "  'arxiv_id': '1806.05024',\n",
       "  'title': 'Self-Supervised Feature Learning by Learning to Spot Artifacts',\n",
       "  'abstract': 'We introduce a novel self-supervised learning method based on adversarial\\ntraining. Our objective is to train a discriminator network to distinguish real\\nimages from images with synthetic artifacts, and then to extract features from\\nits intermediate layers that can be transferred to other data domains and\\ntasks. To generate images with artifacts, we pre-train a high-capacity\\nautoencoder and then we use a damage and repair strategy: First, we freeze the\\nautoencoder and damage the output of the encoder by randomly dropping its\\nentries. Second, we augment the decoder with a repair network, and train it in\\nan adversarial manner against the discriminator. The repair network helps\\ngenerate more realistic images by inpainting the dropped feature entries. To\\nmake the discriminator focus on the artifacts, we also make it predict what\\nentries in the feature were dropped. We demonstrate experimentally that\\nfeatures learned by creating and spotting artifacts achieve state of the art\\nperformance in several benchmarks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05024v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05024v1.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Simon Jenni', 'Paolo Favaro'],\n",
       "  'tasks': ['Self-Supervised Learning'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet', 'stl-10', 'places205'],\n",
       "  'datasets_used_full': ['ImageNet', 'STL-10', 'Places205'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/brain-computer-interface-with-corrupted-eeg',\n",
       "  'arxiv_id': '1806.05017',\n",
       "  'title': 'Brain-Computer Interface with Corrupted EEG Data: A Tensor Completion Approach',\n",
       "  'abstract': 'One of the current issues in Brain-Computer Interface is how to deal with\\nnoisy Electroencephalography measurements organized as multidimensional\\ndatasets. On the other hand, recently, significant advances have been made in\\nmultidimensional signal completion algorithms that exploit tensor decomposition\\nmodels to capture the intricate relationship among entries in a\\nmultidimensional signal. We propose to use tensor completion applied to EEG\\ndata for improving the classification performance in a motor imagery BCI system\\nwith corrupted measurements. Noisy measurements are considered as unknowns that\\nare inferred from a tensor decomposition model. We evaluate the performance of\\nfour recently proposed tensor completion algorithms plus a simple interpolation\\nstrategy, first with random missing entries and then with missing samples\\nconstrained to have a specific structure (random missing channels), which is a\\nmore realistic assumption in BCI Applications. We measured the ability of these\\nalgorithms to reconstruct the tensor from observed data. Then, we tested the\\nclassification accuracy of imagined movement in a BCI experiment with missing\\nsamples. We show that for random missing entries, all tensor completion\\nalgorithms can recover missing samples increasing the classification\\nperformance compared to a simple interpolation approach. For the random missing\\nchannels case, we show that tensor completion algorithms help to reconstruct\\nmissing channels, significantly improving the accuracy in the classification of\\nmotor imagery, however, not at the same level as clean data. Tensor completion\\nalgorithms are useful in real BCI applications. The proposed strategy could\\nallow using motor imagery BCI systems even when EEG data is highly affected by\\nmissing channels and/or samples, avoiding the need of new acquisitions in the\\ncalibration stage.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05017v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05017v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jordi Sole-Casals',\n",
       "   'Cesar F. Caiafa',\n",
       "   'Qibin Zhao',\n",
       "   'Adrzej Cichocki'],\n",
       "  'tasks': ['Classification',\n",
       "   'EEG',\n",
       "   'General Classification',\n",
       "   'Tensor Decomposition'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/safe-learning-based-optimal-motion-planning',\n",
       "  'arxiv_id': '1805.09994',\n",
       "  'title': 'Safe learning-based optimal motion planning for automated driving',\n",
       "  'abstract': 'This paper presents preliminary work on learning the search heuristic for the\\noptimal motion planning for automated driving in urban traffic. Previous work\\nconsidered search-based optimal motion planning framework (SBOMP) that utilized\\nnumerical or model-based heuristics that did not consider dynamic obstacles.\\nOptimal solution was still guaranteed since dynamic obstacles can only increase\\nthe cost. However, significant variations in the search efficiency are observed\\ndepending whether dynamic obstacles are present or not. This paper introduces\\nmachine learning (ML) based heuristic that takes into account dynamic\\nobstacles, thus adding to the performance consistency for achieving real-time\\nimplementation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.09994v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.09994v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Zlatan Ajanovic',\n",
       "   'Bakir Lacevic',\n",
       "   'Georg Stettinger',\n",
       "   'Daniel Watzenig',\n",
       "   'Martin Horn'],\n",
       "  'tasks': ['Motion Planning', 'Optimal Motion Planning'],\n",
       "  'date': '2018-05-25',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/gradient-based-camera-exposure-control-for',\n",
       "  'arxiv_id': '1708.07338',\n",
       "  'title': 'Gradient-based Camera Exposure Control for Outdoor Mobile Platforms',\n",
       "  'abstract': 'We introduce a novel method to automatically adjust camera exposure for image\\nprocessing and computer vision applications on mobile robot platforms. Because\\nmost image processing algorithms rely heavily on low-level image features that\\nare based mainly on local gradient information, we consider that gradient\\nquantity can determine the proper exposure level, allowing a camera to capture\\nthe important image features in a manner robust to illumination conditions. We\\nthen extend this concept to a multi-camera system and present a new control\\nalgorithm to achieve both brightness consistency between adjacent cameras and a\\nproper exposure level for each camera. We implement our prototype system with\\noff-the-shelf machine-vision cameras and demonstrate the effectiveness of the\\nproposed algorithms on practical applications, including pedestrian detection,\\nvisual odometry, surround-view imaging, panoramic imaging and stereo matching.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1708.07338v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1708.07338v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Inwook Shim',\n",
       "   'Tae-Hyun Oh',\n",
       "   'Joon-Young Lee',\n",
       "   'Jinwook Choi',\n",
       "   'Dong-Geol Choi',\n",
       "   'In So Kweon'],\n",
       "  'tasks': ['Pedestrian Detection',\n",
       "   'Stereo Matching',\n",
       "   'Stereo Matching Hand',\n",
       "   'Visual Odometry'],\n",
       "  'date': '2017-08-24',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fast-model-identification-via-physics-engines',\n",
       "  'arxiv_id': '1710.08893',\n",
       "  'title': 'Fast Model Identification via Physics Engines for Data-Efficient Policy Search',\n",
       "  'abstract': \"This paper presents a method for identifying mechanical parameters of robots\\nor objects, such as their mass and friction coefficients. Key features are the\\nuse of off-the-shelf physics engines and the adaptation of a Bayesian\\noptimization technique towards minimizing the number of real-world experiments\\nneeded for model-based reinforcement learning. The proposed framework\\nreproduces in a physics engine experiments performed on a real robot and\\noptimizes the model's mechanical parameters so as to match real-world\\ntrajectories. The optimized model is then used for learning a policy in\\nsimulation, before real-world deployment. It is well understood, however, that\\nit is hard to exactly reproduce real trajectories in simulation. Moreover, a\\nnear-optimal policy can be frequently found with an imperfect model. Therefore,\\nthis work proposes a strategy for identifying a model that is just good enough\\nto approximate the value of a locally optimal policy with a certain confidence,\\ninstead of wasting effort on identifying the most accurate model. Evaluations,\\nperformed both in simulation and on a real robotic manipulation task, indicate\\nthat the proposed strategy results in an overall time-efficient, integrated\\nmodel identification and learning solution, which significantly improves the\\ndata-efficiency of existing policy search algorithms.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1710.08893v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1710.08893v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shaojun Zhu',\n",
       "   'Andrew Kimmel',\n",
       "   'Kostas E. Bekris',\n",
       "   'Abdeslam Boularias'],\n",
       "  'tasks': ['Model-based Reinforcement Learning'],\n",
       "  'date': '2017-10-24',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['openai-gym'],\n",
       "  'datasets_used_full': ['OpenAI Gym'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/generating-sentences-using-a-dynamic-canvas',\n",
       "  'arxiv_id': '1806.05178',\n",
       "  'title': 'Generating Sentences Using a Dynamic Canvas',\n",
       "  'abstract': 'We introduce the Attentive Unsupervised Text (W)riter (AUTR), which is a word\\nlevel generative model for natural language. It uses a recurrent neural network\\nwith a dynamic attention and canvas memory mechanism to iteratively construct\\nsentences. By viewing the state of the memory at intermediate stages and where\\nthe model is placing its attention, we gain insight into how it constructs\\nsentences. We demonstrate that AUTR learns a meaningful latent representation\\nfor each sentence, and achieves competitive log-likelihood lower bounds whilst\\nbeing computationally efficient. It is effective at generating and\\nreconstructing sentences, as well as imputing missing words.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05178v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05178v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Harshil Shah', 'Bowen Zheng', 'David Barber'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['bookcorpus'],\n",
       "  'datasets_used_full': ['BookCorpus'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/co-evolutionary-multi-task-learning-for',\n",
       "  'arxiv_id': '1703.01887',\n",
       "  'title': 'Co-evolutionary multi-task learning for dynamic time series prediction',\n",
       "  'abstract': 'Time series prediction typically consists of a data reconstruction phase\\nwhere the time series is broken into overlapping windows known as the timespan.\\nThe size of the timespan can be seen as a way of determining the extent of past\\ninformation required for an effective prediction. In certain applications such\\nas the prediction of wind-intensity of storms and cyclones, prediction models\\nneed to be dynamic in accommodating different values of the timespan. These\\napplications require robust prediction as soon as the event takes place. We\\nidentify a new category of problem called dynamic time series prediction that\\nrequires a model to give prediction when presented with varying lengths of the\\ntimespan. In this paper, we propose a co-evolutionary multi-task learning\\nmethod that provides a synergy between multi-task learning and co-evolutionary\\nalgorithms to address dynamic time series prediction. The method features\\neffective use of building blocks of knowledge inspired by dynamic programming\\nand multi-task learning. It enables neural networks to retain modularity during\\ntraining for making a decision in situations even when certain inputs are\\nmissing. The effectiveness of the method is demonstrated using one-step-ahead\\nchaotic time series and tropical cyclone wind-intensity prediction.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1703.01887v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1703.01887v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Rohitash Chandra', 'Yew-Soon Ong', 'Chi-Keong Goh'],\n",
       "  'tasks': ['Multi-Task Learning', 'Time Series', 'Time Series Prediction'],\n",
       "  'date': '2017-02-27',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/plug-and-play-unplugged-optimization-free',\n",
       "  'arxiv_id': '1705.08983',\n",
       "  'title': 'Plug-and-Play Unplugged: Optimization Free Reconstruction using Consensus Equilibrium',\n",
       "  'abstract': \"Regularized inversion methods for image reconstruction are used widely due to\\ntheir tractability and ability to combine complex physical sensor models with\\nuseful regularity criteria. Such methods motivated the recently developed\\nPlug-and-Play prior method, which provides a framework to use advanced\\ndenoising algorithms as regularizers in inversion. However, the need to\\nformulate regularized inversion as the solution to an optimization problem\\nlimits the possible regularity conditions and physical sensor models.\\n  In this paper, we introduce Consensus Equilibrium (CE), which generalizes\\nregularized inversion to include a much wider variety of both forward\\ncomponents and prior components without the need for either to be expressed\\nwith a cost function. CE is based on the solution of a set of equilibrium\\nequations that balance data fit and regularity. In this framework, the problem\\nof MAP estimation in regularized inversion is replaced by the problem of\\nsolving these equilibrium equations, which can be approached in multiple ways.\\n  The key contribution of CE is to provide a novel framework for fusing\\nmultiple heterogeneous models of physical sensors or models learned from data.\\nWe describe the derivation of the CE equations and prove that the solution of\\nthe CE equations generalizes the standard MAP estimate under appropriate\\ncircumstances.\\n  We also discuss algorithms for solving the CE equations, including ADMM with\\na novel form of preconditioning and Newton's method. We give examples to\\nillustrate consensus equilibrium and the convergence properties of these\\nalgorithms and demonstrate this method on some toy problems and on a denoising\\nexample in which we use an array of convolutional neural network denoisers,\\nnone of which is tuned to match the noise level in a noisy image but which in\\nconsensus can achieve a better result than any of them individually.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1705.08983v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1705.08983v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Gregery T. Buzzard',\n",
       "   'Stanley H. Chan',\n",
       "   'Suhas Sreehari',\n",
       "   'Charles A. Bouman'],\n",
       "  'tasks': ['Denoising', 'Image Reconstruction'],\n",
       "  'date': '2017-05-24',\n",
       "  'methods': [{'name': 'ADMM',\n",
       "    'full_name': 'Alternating Direction Method of Multipliers',\n",
       "    'description': 'The **alternating direction method of multipliers** (**ADMM**) is an algorithm that solves convex optimization problems by breaking them into smaller pieces, each of which are then easier to handle. It takes the form of a decomposition-coordination procedure, in which the solutions to small\\r\\nlocal subproblems are coordinated to find a solution to a large global problem. ADMM can be viewed as an attempt to blend the benefits of dual decomposition and augmented Lagrangian methods for constrained optimization. It turns out to be equivalent or closely related to many other algorithms\\r\\nas well, such as Douglas-Rachford splitting from numerical analysis, Spingarn’s method of partial inverses, Dykstra’s alternating projections method, Bregman iterative algorithms for l1 problems in signal processing, proximal methods, and many others.\\r\\n\\r\\nText Source: [https://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf](https://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf)\\r\\n\\r\\nImage Source: [here](https://www.slideshare.net/derekcypang/alternating-direction)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Optimization',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/openedgar-open-source-software-for-sec-edgar',\n",
       "  'arxiv_id': '1806.04973',\n",
       "  'title': 'OpenEDGAR: Open Source Software for SEC EDGAR Analysis',\n",
       "  'abstract': 'OpenEDGAR is an open source Python framework designed to rapidly construct\\nresearch databases based on the Electronic Data Gathering, Analysis, and\\nRetrieval (EDGAR) system operated by the US Securities and Exchange Commission\\n(SEC). OpenEDGAR is built on the Django application framework, supports\\ndistributed compute across one or more servers, and includes functionality to\\n(i) retrieve and parse index and filing data from EDGAR, (ii) build tables for\\nkey metadata like form type and filer, (iii) retrieve, parse, and update CIK to\\nticker and industry mappings, (iv) extract content and metadata from filing\\ndocuments, and (v) search filing document contents. OpenEDGAR is designed for\\nuse in both academic research and industrial applications, and is distributed\\nunder MIT License at https://github.com/LexPredict/openedgar.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04973v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04973v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Michael J Bommarito II',\n",
       "   'Daniel Martin Katz',\n",
       "   'Eric M Detterman'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/unsupervised-detection-of-lesions-in-brain',\n",
       "  'arxiv_id': '1806.04972',\n",
       "  'title': 'Unsupervised Detection of Lesions in Brain MRI using constrained adversarial auto-encoders',\n",
       "  'abstract': 'Lesion detection in brain Magnetic Resonance Images (MRI) remains a\\nchallenging task. State-of-the-art approaches are mostly based on supervised\\nlearning making use of large annotated datasets. Human beings, on the other\\nhand, even non-experts, can detect most abnormal lesions after seeing a handful\\nof healthy brain images. Replicating this capability of using prior information\\non the appearance of healthy brain structure to detect lesions can help\\ncomputers achieve human level abnormality detection, specifically reducing the\\nneed for numerous labeled examples and bettering generalization of previously\\nunseen lesions. To this end, we study detection of lesion regions in an\\nunsupervised manner by learning data distribution of brain MRI of healthy\\nsubjects using auto-encoder based methods. We hypothesize that one of the main\\nlimitations of the current models is the lack of consistency in latent\\nrepresentation. We propose a simple yet effective constraint that helps mapping\\nof an image bearing lesion close to its corresponding healthy image in the\\nlatent space. We use the Human Connectome Project dataset to learn distribution\\nof healthy-appearing brain MRI and report improved detection, in terms of AUC,\\nof the lesions in the BRATS challenge dataset.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04972v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04972v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xiaoran Chen', 'Ender Konukoglu'],\n",
       "  'tasks': ['Anomaly Detection', 'Lesion Detection'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/crowd-powered-data-mining',\n",
       "  'arxiv_id': '1806.04968',\n",
       "  'title': 'Crowd-Powered Data Mining',\n",
       "  'abstract': 'Many data mining tasks cannot be completely addressed by auto- mated\\nprocesses, such as sentiment analysis and image classification. Crowdsourcing\\nis an effective way to harness the human cognitive ability to process these\\nmachine-hard tasks. Thanks to public crowdsourcing platforms, e.g., Amazon\\nMechanical Turk and Crowd- Flower, we can easily involve hundreds of thousands\\nof ordinary workers (i.e., the crowd) to address these machine-hard tasks. In\\nthis tutorial, we will survey and synthesize a wide spectrum of existing\\nstudies on crowd-powered data mining. We first give an overview of\\ncrowdsourcing, and then summarize the fundamental techniques, including quality\\ncontrol, cost control, and latency control, which must be considered in\\ncrowdsourced data mining. Next we review crowd-powered data mining operations,\\nincluding classification, clustering, pattern mining, machine learning using\\nthe crowd (including deep learning, transfer learning and semi-supervised\\nlearning) and knowledge discovery. Finally, we provide the emerging challenges\\nin crowdsourced data mining.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04968v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04968v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Chengliang Chai',\n",
       "   'Ju Fan',\n",
       "   'Guoliang Li',\n",
       "   'Jiannan Wang',\n",
       "   'Yudian Zheng'],\n",
       "  'tasks': ['General Classification',\n",
       "   'Image Classification',\n",
       "   'Sentiment Analysis',\n",
       "   'Transfer Learning'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/one-shot-segmentation-in-clutter',\n",
       "  'arxiv_id': '1803.09597',\n",
       "  'title': 'One-Shot Segmentation in Clutter',\n",
       "  'abstract': 'We tackle the problem of one-shot segmentation: finding and segmenting a\\npreviously unseen object in a cluttered scene based on a single instruction\\nexample. We propose a novel dataset, which we call $\\\\textit{cluttered\\nOmniglot}$. Using a baseline architecture combining a Siamese embedding for\\ndetection with a U-net for segmentation we show that increasing levels of\\nclutter make the task progressively harder. Using oracle models with access to\\nvarious amounts of ground-truth information, we evaluate different aspects of\\nthe problem and show that in this kind of visual search task, detection and\\nsegmentation are two intertwined problems, the solution to each of which helps\\nsolving the other. We therefore introduce $\\\\textit{MaskNet}$, an improved model\\nthat attends to multiple candidate locations, generates segmentation proposals\\nto mask out background clutter and selects among the segmented objects. Our\\nfindings suggest that such image recognition models based on an iterative\\nrefinement of object detection and foreground segmentation may provide a way to\\ndeal with highly cluttered scenes.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.09597v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.09597v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Claudio Michaelis', 'Matthias Bethge', 'Alexander S. Ecker'],\n",
       "  'tasks': ['One-Shot Segmentation'],\n",
       "  'date': '2018-03-26',\n",
       "  'methods': [{'name': 'Concatenated Skip Connection',\n",
       "    'full_name': 'Concatenated Skip Connection',\n",
       "    'description': 'A **Concatenated Skip Connection** is a type of skip connection that seeks to reuse features by concatenating them to new layers, allowing more information to be retained from previous layers of the network. This contrasts with say, residual connections, where element-wise summation is used instead to incorporate information from previous layers. This type of skip connection is prominently used in DenseNets (and also Inception networks), which the Figure to the right illustrates.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/densenet.py#L113',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'U-Net',\n",
       "    'full_name': 'U-Net',\n",
       "    'description': '**U-Net** is an architecture for semantic segmentation. It consists of a contracting path and an expansive path. The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit ([ReLU](https://paperswithcode.com/method/relu)) and a 2x2 [max pooling](https://paperswithcode.com/method/max-pooling) operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 [convolution](https://paperswithcode.com/method/convolution) (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a [1x1 convolution](https://paperswithcode.com/method/1x1-convolution) is used to map each 64-component feature vector to the desired number of classes. In total the network has 23 convolutional layers.\\r\\n\\r\\n[Original MATLAB Code](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-release-2015-10-02.tar.gz)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1505.04597v1',\n",
       "    'source_title': 'U-Net: Convolutional Networks for Biomedical Image Segmentation',\n",
       "    'code_snippet_url': 'https://github.com/milesial/Pytorch-UNet/blob/67bf11b4db4c5f2891bd7e8e7f58bcde8ee2d2db/unet/unet_model.py#L8',\n",
       "    'main_collection': {'name': 'Semantic Segmentation Models',\n",
       "     'description': '**Semantic Segmentation Models** are a class of methods that address the task of semantically segmenting an image into different object classes. Below you can find a continuously updating list of semantic segmentation models. ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['omniglot-1', 'ade20k'],\n",
       "  'datasets_used_full': ['Omniglot', 'ADE20K'],\n",
       "  'datasets_introduced_lower': ['cluttered-omniglot'],\n",
       "  'datasets_introduced_full': ['Cluttered Omniglot']},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-streaming-rollout-of-deep-networks',\n",
       "  'arxiv_id': '1806.04965',\n",
       "  'title': 'The streaming rollout of deep networks - towards fully model-parallel execution',\n",
       "  'abstract': \"Deep neural networks, and in particular recurrent networks, are promising\\ncandidates to control autonomous agents that interact in real-time with the\\nphysical world. However, this requires a seamless integration of temporal\\nfeatures into the network's architecture. For the training of and inference\\nwith recurrent neural networks, they are usually rolled out over time, and\\ndifferent rollouts exist. Conventionally during inference, the layers of a\\nnetwork are computed in a sequential manner resulting in sparse temporal\\nintegration of information and long response times. In this study, we present a\\ntheoretical framework to describe rollouts, the level of model-parallelization\\nthey induce, and demonstrate differences in solving specific tasks. We prove\\nthat certain rollouts, also for networks with only skip and no recurrent\\nconnections, enable earlier and more frequent responses, and show empirically\\nthat these early responses have better performance. The streaming rollout\\nmaximizes these properties and enables a fully parallel execution of the\\nnetwork reducing runtime on massively parallel devices. Finally, we provide an\\nopen-source toolbox to design, train, evaluate, and interact with streaming\\nrollouts.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04965v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04965v2.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Volker Fischer', 'Jan Köhler', 'Thomas Pfeil'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'mnist'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fairness-behind-a-veil-of-ignorance-a-welfare',\n",
       "  'arxiv_id': '1806.04959',\n",
       "  'title': 'Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated Decision Making',\n",
       "  'abstract': \"We draw attention to an important, yet largely overlooked aspect of\\nevaluating fairness for automated decision making systems---namely risk and\\nwelfare considerations. Our proposed family of measures corresponds to the\\nlong-established formulations of cardinal social welfare in economics, and is\\njustified by the Rawlsian conception of fairness behind a veil of ignorance.\\nThe convex formulation of our welfare-based measures of fairness allows us to\\nintegrate them as a constraint into any convex loss minimization pipeline. Our\\nempirical analysis reveals interesting trade-offs between our proposal and (a)\\nprediction accuracy, (b) group discrimination, and (c) Dwork et al.'s notion of\\nindividual fairness. Furthermore and perhaps most importantly, our work\\nprovides both heuristic justification and empirical evidence suggesting that a\\nlower-bound on our measures often leads to bounded inequality in algorithmic\\noutcomes; hence presenting the first computationally feasible mechanism for\\nbounding individual-level inequality.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04959v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04959v4.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Hoda Heidari',\n",
       "   'Claudio Ferrari',\n",
       "   'Krishna P. Gummadi',\n",
       "   'Andreas Krause'],\n",
       "  'tasks': ['Decision Making'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/expression-empowered-residen-network-for',\n",
       "  'arxiv_id': '1806.04957',\n",
       "  'title': 'Expression Empowered ResiDen Network for Facial Action Unit Detection',\n",
       "  'abstract': 'The paper explores the topic of Facial Action Unit (FAU) detection in the\\nwild. In particular, we are interested in answering the following questions:\\n(1) how useful are residual connections across dense blocks for face analysis?\\n(2) how useful is the information from a network trained for categorical Facial\\nExpression Recognition (FER) for the task of FAU detection? The proposed\\nnetwork (ResiDen) exploits dense blocks along with residual connections and\\nuses auxiliary information from a FER network. The experiments are performed on\\nthe EmotionNet and DISFA datasets. The experiments show the usefulness of\\nfacial expression information for AU detection. The proposed network achieves\\nstate-of-art results on the two databases. Analysis of the results for cross\\ndatabase protocol shows the effectiveness of the network.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04957v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04957v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shreyank Jyoti', 'Abhinav Dhall'],\n",
       "  'tasks': ['Action Unit Detection',\n",
       "   'Facial Action Unit Detection',\n",
       "   'Facial Expression Recognition'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['disfa'],\n",
       "  'datasets_used_full': ['DISFA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/higher-order-of-motion-magnification-for',\n",
       "  'arxiv_id': '1806.04955',\n",
       "  'title': 'Higher Order of Motion Magnification for Vessel Localisation in Surgical Video',\n",
       "  'abstract': 'Locating vessels during surgery is critical for avoiding inadvertent damage,\\nyet vasculature can be difficult to identify. Video motion magnification can\\npotentially highlight vessels by exaggerating subtle motion embedded within the\\nvideo to become perceivable to the surgeon. In this paper, we explore a\\nphysiological model of artery distension to extend motion magnification to\\nincorporate higher orders of motion, leveraging the difference in acceleration\\nover time (jerk) in pulsatile motion to highlight the vascular pulse wave. Our\\nmethod is compared to first and second order motion based Eulerian video\\nmagnification algorithms. Using data from a surgical video retrieved during a\\nrobotic prostatectomy, we show that our method can accentuate\\ncardio-physiological features and produce a more succinct and clearer video for\\nmotion magnification, with more similarities in areas without motion to the\\nsource video at large magnifications. We validate the approach with a Structure\\nSimilarity (SSIM) and Peak Signal to Noise Ratio (PSNR) assessment of three\\nvideos at an increasing working distance, using three different levels of\\noptical magnification. Spatio-temporal cross sections are presented to show the\\neffectiveness of our proposal and video samples are provided to demonstrates\\nqualitatively our results.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04955v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04955v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mirek Janatka',\n",
       "   'Ashwin Sridhar',\n",
       "   'John Kelly',\n",
       "   'Danail Stoyanov'],\n",
       "  'tasks': ['SSIM'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/towards-semantically-enhanced-data',\n",
       "  'arxiv_id': '1806.04952',\n",
       "  'title': 'Towards Semantically Enhanced Data Understanding',\n",
       "  'abstract': 'In the field of machine learning, data understanding is the practice of\\ngetting initial insights in unknown datasets. Such knowledge-intensive tasks\\nrequire a lot of documentation, which is necessary for data scientists to grasp\\nthe meaning of the data. Usually, documentation is separate from the data in\\nvarious external documents, diagrams, spreadsheets and tools which causes\\nconsiderable look up overhead. Moreover, other supporting applications are not\\nable to consume and utilize such unstructured data. That is why we propose a\\nmethodology that uses a single semantic model that interlinks data with its\\ndocumentation. Hence, data scientists are able to directly look up the\\nconnected information about the data by simply following links. Equally, they\\ncan browse the documentation which always refers to the data. Furthermore, the\\nmodel can be used by other approaches providing additional support, like\\nsearching, comparing, integrating or visualizing data. To showcase our approach\\nwe also demonstrate an early prototype.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04952v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04952v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Markus Schröder',\n",
       "   'Christian Jilek',\n",
       "   'Jörn Hees',\n",
       "   'Andreas Dengel'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/diachronic-word-embeddings-and-semantic',\n",
       "  'arxiv_id': '1806.03537',\n",
       "  'title': 'Diachronic word embeddings and semantic shifts: a survey',\n",
       "  'abstract': 'Recent years have witnessed a surge of publications aimed at tracing temporal\\nchanges in lexical semantics using distributional methods, particularly\\nprediction-based word embedding models. However, this vein of research lacks\\nthe cohesion, common terminology and shared practices of more established areas\\nof natural language processing. In this paper, we survey the current state of\\nacademic research related to diachronic word embeddings and semantic shifts\\ndetection. We start with discussing the notion of semantic shifts, and then\\ncontinue with an overview of the existing methods for tracing such time-related\\nshifts with word embedding models. We propose several axes along which these\\nmethods can be compared, and outline the main challenges before this emerging\\nsubfield of NLP, as well as prospects and possible applications.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03537v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03537v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Andrey Kutuzov',\n",
       "   'Lilja Øvrelid',\n",
       "   'Terrence Szymanski',\n",
       "   'Erik Velldal'],\n",
       "  'tasks': ['Diachronic Word Embeddings', 'Word Embeddings'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fmri-semantic-category-decoding-using',\n",
       "  'arxiv_id': '1806.05177',\n",
       "  'title': 'fMRI Semantic Category Decoding using Linguistic Encoding of Word Embeddings',\n",
       "  'abstract': \"The dispute of how the human brain represents conceptual knowledge has been\\nargued in many scientific fields. Brain imaging studies have shown that the\\nspatial patterns of neural activation in the brain are correlated with thinking\\nabout different semantic categories of words (for example, tools, animals, and\\nbuildings) or when viewing the related pictures. In this paper, we present a\\ncomputational model that learns to predict the neural activation captured in\\nfunctional magnetic resonance imaging (fMRI) data of test words. Unlike the\\nmodels with hand-crafted features that have been used in the literature, in\\nthis paper we propose a novel approach wherein decoding models are built with\\nfeatures extracted from popular linguistic encodings of Word2Vec, GloVe,\\nMeta-Embeddings in conjunction with the empirical fMRI data associated with\\nviewing several dozen concrete nouns. We compared these models with several\\nother models that use word features extracted from FastText, Randomly-generated\\nfeatures, Mitchell's 25 features [1]. The experimental results show that the\\npredicted fMRI images using Meta-Embeddings meet the state-of-the-art\\nperformance. Although models with features from GloVe and Word2Vec predict fMRI\\nimages similar to the state-of-the-art model, model with features from\\nMeta-Embeddings predicts significantly better. The proposed scheme that uses\\npopular linguistic encoding offers a simple and easy approach for semantic\\ndecoding from fMRI experiments.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05177v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05177v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Subba Reddy Oota', 'Naresh Manwani', 'Bapi Raju S'],\n",
       "  'tasks': ['Word Embeddings'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'GloVe',\n",
       "    'full_name': 'GloVe Embeddings',\n",
       "    'description': '**GloVe Embeddings** are a type of word embedding that encode the co-occurrence probability ratio between two words as vector differences. GloVe uses a weighted least squares objective $J$ that minimizes the difference between the dot product of the vectors of two words and the logarithm of their number of co-occurrences:\\r\\n\\r\\n$$ J=\\\\sum\\\\_{i, j=1}^{V}f\\\\left(𝑋\\\\_{i j}\\\\right)(w^{T}\\\\_{i}\\\\tilde{w}_{j} + b\\\\_{i} + \\\\tilde{b}\\\\_{j} - \\\\log{𝑋}\\\\_{ij})^{2} $$\\r\\n\\r\\nwhere $w\\\\_{i}$ and $b\\\\_{i}$ are the word vector and bias respectively of word $i$, $\\\\tilde{w}_{j}$ and $b\\\\_{j}$ are the context word vector and bias respectively of word $j$, $X\\\\_{ij}$ is the number of times word $i$ occurs in the context of word $j$, and $f$ is a weighting function that assigns lower weights to rare and frequent co-occurrences.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://aclanthology.org/D14-1162',\n",
       "    'source_title': 'GloVe: Global Vectors for Word Representation',\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Word Embeddings',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Natural Language Processing'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/convolutional-sparse-coding-for-high-dynamic',\n",
       "  'arxiv_id': '1806.04942',\n",
       "  'title': 'Convolutional Sparse Coding for High Dynamic Range Imaging',\n",
       "  'abstract': 'Current HDR acquisition techniques are based on either (i) fusing\\nmultibracketed, low dynamic range (LDR) images, (ii) modifying existing\\nhardware and capturing different exposures simultaneously with multiple\\nsensors, or (iii) reconstructing a single image with spatially-varying pixel\\nexposures. In this paper, we propose a novel algorithm to recover high-quality\\nHDRI images from a single, coded exposure. The proposed reconstruction method\\nbuilds on recently-introduced ideas of convolutional sparse coding (CSC); this\\npaper demonstrates how to make CSC practical for HDR imaging. We demonstrate\\nthat the proposed algorithm achieves higher-quality reconstructions than\\nalternative methods, we evaluate optical coding schemes, analyze algorithmic\\nparameters, and build a prototype coded HDR camera that demonstrates the\\nutility of convolutional sparse HDRI coding with a custom hardware platform.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04942v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04942v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ana Serrano',\n",
       "   'Felix Heide',\n",
       "   'Diego Gutierrez',\n",
       "   'Gordon Wetzstein',\n",
       "   'Belen Masia'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/far-ho-a-bilevel-programming-package-for',\n",
       "  'arxiv_id': '1806.04941',\n",
       "  'title': 'Far-HO: A Bilevel Programming Package for Hyperparameter Optimization and Meta-Learning',\n",
       "  'abstract': 'In (Franceschi et al., 2018) we proposed a unified mathematical framework,\\ngrounded on bilevel programming, that encompasses gradient-based hyperparameter\\noptimization and meta-learning. We formulated an approximate version of the\\nproblem where the inner objective is solved iteratively, and gave sufficient\\nconditions ensuring convergence to the exact problem. In this work we show how\\nto optimize learning rates, automatically weight the loss of single examples\\nand learn hyper-representations with Far-HO, a software package based on the\\npopular deep learning framework TensorFlow that allows to seamlessly tackle\\nboth HO and ML problems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04941v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04941v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Luca Franceschi',\n",
       "   'Riccardo Grazzi',\n",
       "   'Massimiliano Pontil',\n",
       "   'Saverio Salzo',\n",
       "   'Paolo Frasconi'],\n",
       "  'tasks': ['Hyperparameter Optimization', 'Meta-Learning'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/convolutional-sparse-coding-for-capturing',\n",
       "  'arxiv_id': '1806.04935',\n",
       "  'title': 'Convolutional sparse coding for capturing high speed video content',\n",
       "  'abstract': 'Video capture is limited by the trade-off between spatial and temporal\\nresolution: when capturing videos of high temporal resolution, the spatial\\nresolution decreases due to bandwidth limitations in the capture system.\\nAchieving both high spatial and temporal resolution is only possible with\\nhighly specialized and very expensive hardware, and even then the same basic\\ntrade-off remains. The recent introduction of compressive sensing and sparse\\nreconstruction techniques allows for the capture of single-shot high-speed\\nvideo, by coding the temporal information in a single frame, and then\\nreconstructing the full video sequence from this single coded image and a\\ntrained dictionary of image patches. In this paper, we first analyze this\\napproach, and find insights that help improve the quality of the reconstructed\\nvideos. We then introduce a novel technique, based on convolutional sparse\\ncoding (CSC), and show how it outperforms the state-of-the-art, patch-based\\napproach in terms of flexibility and efficiency, due to the convolutional\\nnature of its filter banks. The key idea for CSC high-speed video acquisition\\nis extending the basic formulation by imposing an additional constraint in the\\ntemporal dimension, which enforces sparsity of the first-order derivatives over\\ntime.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04935v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04935v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ana Serrano', 'Elena Garces', 'Diego Gutierrez', 'Belen Masia'],\n",
       "  'tasks': ['Compressive Sensing'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/reservoir-computing-hardware-with-cellular',\n",
       "  'arxiv_id': '1806.04932',\n",
       "  'title': 'Reservoir Computing Hardware with Cellular Automata',\n",
       "  'abstract': 'Elementary cellular automata (ECA) is a widely studied one-dimensional processing methodology where the successive iteration of the automaton may lead to the recreation of a rich pattern dynamic. Recently, cellular automata have been proposed as a feasible way to implement Reservoir Computing (RC) systems in which the automata rule is fixed and the training is performed using a linear regression. In this work we perform an exhaustive study of the performance of the different ECA rules when applied to pattern recognition of time-independent input signals using a RC scheme. Once the different ECA rules have been tested, the most accurate one (rule 90) is selected to implement a digital circuit. Rule 90 is easily reproduced using a reduced set of XOR gates and shift-registers, thus representing a high-performance alternative for RC hardware implementation in terms of processing time, circuit area, power dissipation and system accuracy. The model (both in software and its hardware implementation) has been tested using a pattern recognition task of handwritten numbers (the MNIST database) for which we obtained competitive results in terms of accuracy, speed and power dissipation. The proposed model can be considered to be a low-cost method to implement fast pattern recognition digital circuits.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04932v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04932v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alejandro Morán', 'Christiam F. Frasser', 'Josep L. Rosselló'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist'],\n",
       "  'datasets_used_full': ['MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/an-image-representation-based-convolutional',\n",
       "  'arxiv_id': '1806.04931',\n",
       "  'title': 'An image representation based convolutional network for DNA classification',\n",
       "  'abstract': 'The folding structure of the DNA molecule combined with helper molecules,\\nalso referred to as the chromatin, is highly relevant for the functional\\nproperties of DNA. The chromatin structure is largely determined by the\\nunderlying primary DNA sequence, though the interaction is not yet fully\\nunderstood. In this paper we develop a convolutional neural network that takes\\nan image-representation of primary DNA sequence as its input, and predicts key\\ndeterminants of chromatin structure. The method is developed such that it is\\ncapable of detecting interactions between distal elements in the DNA sequence,\\nwhich are known to be highly relevant. Our experiments show that the method\\noutperforms several existing methods both in terms of prediction accuracy and\\ntraining time.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04931v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04931v1.pdf',\n",
       "  'proceeding': 'ICLR 2018 1',\n",
       "  'authors': ['Bojian Yin',\n",
       "   'Marleen Balvert',\n",
       "   'Davide Zambrano',\n",
       "   'Alexander Schönhuth',\n",
       "   'Sander Bohte'],\n",
       "  'tasks': ['Classification', 'General Classification'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/probabilistic-feature-selection-and',\n",
       "  'arxiv_id': '1609.05486',\n",
       "  'title': 'Probabilistic Feature Selection and Classification Vector Machine',\n",
       "  'abstract': 'Sparse Bayesian learning is a state-of-the-art supervised learning algorithm\\nthat can choose a subset of relevant samples from the input data and make\\nreliable probabilistic predictions. However, in the presence of\\nhigh-dimensional data with irrelevant features, traditional sparse Bayesian\\nclassifiers suffer from performance degradation and low efficiency by failing\\nto eliminate irrelevant features. To tackle this problem, we propose a novel\\nsparse Bayesian embedded feature selection method that adopts truncated\\nGaussian distributions as both sample and feature priors. The proposed method,\\ncalled probabilistic feature selection and classification vector machine\\n(PFCVMLP ), is able to simultaneously select relevant features and samples for\\nclassification tasks. In order to derive the analytical solutions, Laplace\\napproximation is applied to compute approximate posteriors and marginal\\nlikelihoods. Finally, parameters and hyperparameters are optimized by the\\ntype-II maximum likelihood method. Experiments on three datasets validate the\\nperformance of PFCVMLP along two dimensions: classification performance and\\neffectiveness for feature selection. Finally, we analyze the generalization\\nperformance and derive a generalization error bound for PFCVMLP . By tightening\\nthe bound, the importance of feature selection is demonstrated.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1609.05486v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1609.05486v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Bingbing Jiang',\n",
       "   'Chang Li',\n",
       "   'Maarten de Rijke',\n",
       "   'Xin Yao',\n",
       "   'Huanhuan Chen'],\n",
       "  'tasks': ['Classification', 'General Classification'],\n",
       "  'date': '2016-09-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['seed-1'],\n",
       "  'datasets_used_full': ['SEED'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-iq-of-artificial-intelligence',\n",
       "  'arxiv_id': '1806.04915',\n",
       "  'title': 'The IQ of Artificial Intelligence',\n",
       "  'abstract': 'All it takes to identify the computer programs which are Artificial\\nIntelligence is to give them a test and award AI to those that pass the test.\\nLet us say that the scores they earn at the test will be called IQ. We cannot\\npinpoint a minimum IQ threshold that a program has to cover in order to be AI,\\nhowever, we will choose a certain value. Thus, our definition for AI will be\\nany program the IQ of which is above the chosen value. While this idea has\\nalready been implemented in [3], here we will revisit this construct in order\\nto introduce certain improvements.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04915v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04915v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Dimiter Dobrev'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/bilevel-programming-for-hyperparameter',\n",
       "  'arxiv_id': '1806.04910',\n",
       "  'title': 'Bilevel Programming for Hyperparameter Optimization and Meta-Learning',\n",
       "  'abstract': 'We introduce a framework based on bilevel programming that unifies\\ngradient-based hyperparameter optimization and meta-learning. We show that an\\napproximate version of the bilevel problem can be solved by taking into\\nexplicit account the optimization dynamics for the inner objective. Depending\\non the specific setting, the outer variables take either the meaning of\\nhyperparameters in a supervised learning problem or parameters of a\\nmeta-learner. We provide sufficient conditions under which solutions of the\\napproximate problem converge to those of the exact problem. We instantiate our\\napproach for meta-learning in the case of deep learning where representation\\nlayers are treated as hyperparameters shared across a set of training episodes.\\nIn experiments, we confirm our theoretical findings, present encouraging\\nresults for few-shot learning and contrast the bilevel approach against\\nclassical approaches for learning-to-learn.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04910v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04910v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Luca Franceschi',\n",
       "   'Paolo Frasconi',\n",
       "   'Saverio Salzo',\n",
       "   'Riccardo Grazzi',\n",
       "   'Massimilano Pontil'],\n",
       "  'tasks': ['Few-Shot Learning',\n",
       "   'Hyperparameter Optimization',\n",
       "   'Meta-Learning'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet', 'miniimagenet-1', 'omniglot-1'],\n",
       "  'datasets_used_full': ['ImageNet', 'miniImageNet', 'Omniglot'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-machine-learning-item-recommendation-system',\n",
       "  'arxiv_id': '1806.04900',\n",
       "  'title': 'A Machine-Learning Item Recommendation System for Video Games',\n",
       "  'abstract': 'Video-game players generate huge amounts of data, as everything they do\\nwithin a game is recorded. In particular, among all the stored actions and\\nbehaviors, there is information on the in-game purchases of virtual products.\\nSuch information is of critical importance in modern free-to-play titles, where\\ngamers can select or buy a profusion of items during the game in order to\\nprogress and fully enjoy their experience.\\n  To try to maximize these kind of purchases, one can use a recommendation\\nsystem so as to present players with items that might be interesting for them.\\nSuch systems can better achieve their goal by employing machine learning\\nalgorithms that are able to predict the rating of an item or product by a\\nparticular user. In this paper we evaluate and compare two of these algorithms,\\nan ensemble-based model (extremely randomized trees) and a deep neural network,\\nboth of which are promising candidates for operational video-game recommender\\nengines.\\n  Item recommenders can help developers improve the game. But, more\\nimportantly, it should be possible to integrate them into the game, so that\\nusers automatically get personalized recommendations while playing. The\\npresented models are not only able to meet this challenge, providing accurate\\npredictions of the items that a particular player will find attractive, but\\nalso sufficiently fast and robust to be used in operational settings.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04900v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04900v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Paul Bertens',\n",
       "   'Anna Guitart',\n",
       "   'Pei Pei Chen',\n",
       "   'África Periáñez'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/look-imagine-and-match-improving-textual',\n",
       "  'arxiv_id': '1711.06420',\n",
       "  'title': 'Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models',\n",
       "  'abstract': 'Textual-visual cross-modal retrieval has been a hot research topic in both\\ncomputer vision and natural language processing communities. Learning\\nappropriate representations for multi-modal data is crucial for the cross-modal\\nretrieval performance. Unlike existing image-text retrieval approaches that\\nembed image-text pairs as single feature vectors in a common representational\\nspace, we propose to incorporate generative processes into the cross-modal\\nfeature embedding, through which we are able to learn not only the global\\nabstract features but also the local grounded features. Extensive experiments\\nshow that our framework can well match images and sentences with complex\\ncontent, and achieve the state-of-the-art cross-modal retrieval results on\\nMSCOCO dataset.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.06420v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.06420v2.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Jiuxiang Gu',\n",
       "   'Jianfei Cai',\n",
       "   'Shafiq Joty',\n",
       "   'Li Niu',\n",
       "   'Gang Wang'],\n",
       "  'tasks': ['Cross-Modal Retrieval'],\n",
       "  'date': '2017-11-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['coco'],\n",
       "  'datasets_used_full': ['COCO'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-longer-term-dependencies-in-rnns',\n",
       "  'arxiv_id': '1803.00144',\n",
       "  'title': 'Learning Longer-term Dependencies in RNNs with Auxiliary Losses',\n",
       "  'abstract': 'Despite recent advances in training recurrent neural networks (RNNs),\\ncapturing long-term dependencies in sequences remains a fundamental challenge.\\nMost approaches use backpropagation through time (BPTT), which is difficult to\\nscale to very long sequences. This paper proposes a simple method that improves\\nthe ability to capture long term dependencies in RNNs by adding an unsupervised\\nauxiliary loss to the original objective. This auxiliary loss forces RNNs to\\neither reconstruct previous events or predict next events in a sequence, making\\ntruncated backpropagation feasible for long sequences and also improving full\\nBPTT. We evaluate our method on a variety of settings, including pixel-by-pixel\\nimage classification with sequence lengths up to 16\\\\,000, and a real document\\nclassification benchmark. Our results highlight good performance and resource\\nefficiency of this approach over competitive baselines, including other\\nrecurrent models and a comparable sized Transformer. Further analyses reveal\\nbeneficial effects of the auxiliary loss on optimization and regularization, as\\nwell as extreme cases where there is little to no backpropagation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.00144v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.00144v3.pdf',\n",
       "  'proceeding': 'ICML 2018',\n",
       "  'authors': ['Trieu H. Trinh',\n",
       "   'Andrew M. Dai',\n",
       "   'Minh-Thang Luong',\n",
       "   'Quoc V. Le'],\n",
       "  'tasks': ['Document Classification',\n",
       "   'General Classification',\n",
       "   'Image Classification'],\n",
       "  'date': '2018-03-01',\n",
       "  'methods': [{'name': 'Absolute Position Encodings',\n",
       "    'full_name': 'Absolute Position Encodings',\n",
       "    'description': '**Absolute Position Encodings** are a type of position embeddings for [[Transformer](https://paperswithcode.com/method/transformer)-based models] where positional encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension $d\\\\_{model}$ as the embeddings, so that the two can be summed. In the original implementation, sine and cosine functions of different frequencies are used:\\r\\n\\r\\n$$ \\\\text{PE}\\\\left(pos, 2i\\\\right) = \\\\sin\\\\left(pos/10000^{2i/d\\\\_{model}}\\\\right) $$\\r\\n\\r\\n$$ \\\\text{PE}\\\\left(pos, 2i+1\\\\right) = \\\\cos\\\\left(pos/10000^{2i/d\\\\_{model}}\\\\right) $$\\r\\n\\r\\nwhere $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\\\pi$ to $10000 \\\\dot 2\\\\pi$. This function was chosen because the authors hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$,  $\\\\text{PE}\\\\_{pos+k}$ can be represented as a linear function of $\\\\text{PE}\\\\_{pos}$.\\r\\n\\r\\nImage Source: [D2L.ai](https://d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Position Embeddings',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Position-Wise Feed-Forward Layer',\n",
       "    'full_name': 'Position-Wise Feed-Forward Layer',\n",
       "    'description': '**Position-Wise Feed-Forward Layer** is a type of [feedforward layer](https://www.paperswithcode.com/method/category/feedforwad-networks) consisting of two [dense layers](https://www.paperswithcode.com/method/dense-connections) that applies to the last dimension, which means the same dense layers are used for each position item in the sequence, so called position-wise.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Residual Connection',\n",
       "    'full_name': 'Residual Connection',\n",
       "    'description': '**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Adam',\n",
       "    'full_name': 'Adam',\n",
       "    'description': '**Adam** is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of [RMSProp](https://paperswithcode.com/method/rmsprop) and [SGD w/th Momentum](https://paperswithcode.com/method/sgd-with-momentum). The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. \\r\\n\\r\\nThe weight updates are performed as:\\r\\n\\r\\n$$ w_{t} = w_{t-1} - \\\\eta\\\\frac{\\\\hat{m}\\\\_{t}}{\\\\sqrt{\\\\hat{v}\\\\_{t}} + \\\\epsilon}  $$\\r\\n\\r\\nwith\\r\\n\\r\\n$$ \\\\hat{m}\\\\_{t} = \\\\frac{m_{t}}{1-\\\\beta^{t}_{1}} $$\\r\\n\\r\\n$$ \\\\hat{v}\\\\_{t} = \\\\frac{v_{t}}{1-\\\\beta^{t}_{2}} $$\\r\\n\\r\\n$$ m_{t} = \\\\beta_{1}m_{t-1} + (1-\\\\beta_{1})g_{t} $$\\r\\n\\r\\n$$ v_{t} = \\\\beta_{2}v_{t-1} + (1-\\\\beta_{2})g_{t}^{2}  $$\\r\\n\\r\\n\\r\\n$ \\\\eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \\\\epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \\\\beta_{1} $ and $ \\\\beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1412.6980v9',\n",
       "    'source_title': 'Adam: A Method for Stochastic Optimization',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Dropout',\n",
       "    'full_name': 'Dropout',\n",
       "    'description': '**Dropout** is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p=0.5$). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).\\r\\n\\r\\nThe idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://jmlr.org/papers/v15/srivastava14a.html',\n",
       "    'source_title': 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/7f3078b70d0ed9bea6228efa420879c56f72ef69/jax/experimental/stax.py#L271-L275',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Multi-Head Attention',\n",
       "    'full_name': 'Multi-Head Attention',\n",
       "    'description': '**Multi-head Attention** is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies). \\r\\n\\r\\n$$ \\\\text{MultiHead}\\\\left(\\\\textbf{Q}, \\\\textbf{K}, \\\\textbf{V}\\\\right) = \\\\left[\\\\text{head}\\\\_{1},\\\\dots,\\\\text{head}\\\\_{h}\\\\right]\\\\textbf{W}_{0}$$\\r\\n\\r\\n$$\\\\text{where} \\\\text{ head}\\\\_{i} = \\\\text{Attention} \\\\left(\\\\textbf{Q}\\\\textbf{W}\\\\_{i}^{Q}, \\\\textbf{K}\\\\textbf{W}\\\\_{i}^{K}, \\\\textbf{V}\\\\textbf{W}\\\\_{i}^{V} \\\\right) $$\\r\\n\\r\\nAbove $\\\\textbf{W}$ are all learnable parameter matrices.\\r\\n\\r\\nNote that [scaled dot-product attention](https://paperswithcode.com/method/scaled) is most commonly used in this module, although in principle it can be swapped out for other types of attention mechanism.\\r\\n\\r\\nSource: [Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': 'https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/fec78a687210851f055f792d45300d27cc60ae41/transformer/SubLayers.py#L9',\n",
       "    'main_collection': {'name': 'Attention Modules',\n",
       "     'description': '**Attention Modules** refer to modules that incorporate attention mechanisms. For example, multi-head attention is a module that incorporates multiple attention heads. Below you can find a continuously updating list of attention modules.',\n",
       "     'parent': 'Attention',\n",
       "     'area': 'General'}},\n",
       "   {'name': 'BPE',\n",
       "    'full_name': 'Byte Pair Encoding',\n",
       "    'description': '**Byte Pair Encoding**, or **BPE**, is a subword segmentation algorithm that encodes rare and unknown words as sequences of subword units. The intuition is that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).\\r\\n\\r\\n[Lei Mao](https://leimao.github.io/blog/Byte-Pair-Encoding/) has a detailed blog post that explains how this works.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1508.07909v5',\n",
       "    'source_title': 'Neural Machine Translation of Rare Words with Subword Units',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Subword Segmentation',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Natural Language Processing'}},\n",
       "   {'name': 'Dense Connections',\n",
       "    'full_name': 'Dense Connections',\n",
       "    'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\\\_{\\\\text{inputs}}*n\\\\_{\\\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\\r\\n\\r\\n$$h\\\\_{l} = g\\\\left(\\\\textbf{W}^{T}h\\\\_{l-1}\\\\right)$$\\r\\n\\r\\nwhere $g$ is an activation function.\\r\\n\\r\\nImage Source: Deep Learning by Goodfellow, Bengio and Courville',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Label Smoothing',\n",
       "    'full_name': 'Label Smoothing',\n",
       "    'description': '**Label Smoothing** is a regularization technique that introduces noise for the labels. This accounts for the fact that datasets may have mistakes in them, so maximizing the likelihood of $\\\\log{p}\\\\left(y\\\\mid{x}\\\\right)$ directly can be harmful. Assume for a small constant $\\\\epsilon$, the training set label $y$ is correct with probability $1-\\\\epsilon$ and incorrect otherwise. Label Smoothing regularizes a model based on a [softmax](https://paperswithcode.com/method/softmax) with $k$ output values by replacing the hard $0$ and $1$ classification targets with targets of $\\\\frac{\\\\epsilon}{k-1}$ and $1-\\\\epsilon$ respectively.\\r\\n\\r\\nSource: Deep Learning, Goodfellow et al\\r\\n\\r\\nImage Source: [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629)',\n",
       "    'introduced_year': 1985,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Layer Normalization',\n",
       "    'full_name': 'Layer Normalization',\n",
       "    'description': 'Unlike [batch normalization](https://paperswithcode.com/method/batch-normalization), **Layer Normalization** directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. It works well for [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) and improves both the training time and the generalization performance of several existing RNN models. More recently, it has been used with [Transformer](https://paperswithcode.com/methods/category/transformers) models.\\r\\n\\r\\nWe compute the layer normalization statistics over all the hidden units in the same layer as follows:\\r\\n\\r\\n$$ \\\\mu^{l} = \\\\frac{1}{H}\\\\sum^{H}\\\\_{i=1}a\\\\_{i}^{l} $$\\r\\n\\r\\n$$ \\\\sigma^{l} = \\\\sqrt{\\\\frac{1}{H}\\\\sum^{H}\\\\_{i=1}\\\\left(a\\\\_{i}^{l}-\\\\mu^{l}\\\\right)^{2}}  $$\\r\\n\\r\\nwhere $H$ denotes the number of hidden units in a layer. Under layer normalization, all the hidden units in a layer share the same normalization terms $\\\\mu$ and $\\\\sigma$, but different training cases have different normalization terms. Unlike batch normalization, layer normalization does not impose any constraint on the size of the mini-batch and it can be used in the pure online regime with batch size 1.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1607.06450v1',\n",
       "    'source_title': 'Layer Normalization',\n",
       "    'code_snippet_url': 'https://github.com/CyberZHG/torch-layer-normalization/blob/89f405b60f53f85da6f03fe685c190ef394ce50c/torch_layer_normalization/layer_normalization.py#L8',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Scaled Dot-Product Attention',\n",
       "    'full_name': 'Scaled Dot-Product Attention',\n",
       "    'description': '**Scaled dot-product attention** is an attention mechanism where the dot products are scaled down by $\\\\sqrt{d_k}$. Formally we have a query $Q$, a key $K$ and a value $V$ and calculate the attention as:\\r\\n\\r\\n$$ {\\\\text{Attention}}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^{T}}{\\\\sqrt{d_k}}\\\\right)V $$\\r\\n\\r\\nIf we assume that $q$ and $k$ are $d_k$-dimensional vectors whose components are independent random variables with mean $0$ and variance $1$, then their dot product, $q \\\\cdot k = \\\\sum_{i=1}^{d_k} u_iv_i$, has mean $0$ and variance $d_k$.  Since we would prefer these values to have variance $1$, we divide by $\\\\sqrt{d_k}$.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': 'https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/5c0264915ab43485adc576f88971fc3d42b10445/transformer/Modules.py#L7',\n",
       "    'main_collection': {'name': 'Attention Mechanisms',\n",
       "     'description': '**Attention Mechanisms** are a component used in neural networks to model long-range interaction, for example across a text in NLP. The key idea is to build shortcuts between a context vector and the input, to allow a model to attend to different parts. Below you can find a continuously updating list of attention mechanisms.',\n",
       "     'parent': 'Attention',\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Transformer',\n",
       "    'full_name': 'Transformer',\n",
       "    'description': 'A **Transformer** is a model architecture that eschews recurrence and instead relies entirely on an [attention mechanism](https://paperswithcode.com/methods/category/attention-mechanisms-1) to draw global dependencies between input and output. Before Transformers, the dominant sequence transduction models were based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The Transformer also employs an encoder and decoder, but removing recurrence in favor of [attention mechanisms](https://paperswithcode.com/methods/category/attention-mechanisms-1) allows for significantly more parallelization than methods like [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) and [CNNs](https://paperswithcode.com/methods/category/convolutional-neural-networks).',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1706.03762v5',\n",
       "    'source_title': 'Attention Is All You Need',\n",
       "    'code_snippet_url': 'https://github.com/tunz/transformer-pytorch/blob/e7266679f0b32fd99135ea617213f986ceede056/model/transformer.py#L201',\n",
       "    'main_collection': {'name': 'Transformers',\n",
       "     'description': '**Transformers** are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embedidngs.',\n",
       "     'parent': 'Language Models',\n",
       "     'area': 'Natural Language Processing'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-multi-task-deep-learning-architecture-for',\n",
       "  'arxiv_id': '1806.03972',\n",
       "  'title': 'A Multi-task Deep Learning Architecture for Maritime Surveillance using AIS Data Streams',\n",
       "  'abstract': 'In a world of global trading, maritime safety, security and efficiency are\\ncrucial issues. We propose a multi-task deep learning framework for vessel\\nmonitoring using Automatic Identification System (AIS) data streams. We combine\\nrecurrent neural networks with latent variable modeling and an embedding of AIS\\nmessages to a new representation space to jointly address key issues to be\\ndealt with when considering AIS data streams: massive amount of streaming data,\\nnoisy data and irregular timesampling. We demonstrate the relevance of the\\nproposed deep learning framework on real AIS datasets for a three-task setting,\\nnamely trajectory reconstruction, anomaly detection and vessel type\\nidentification.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03972v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03972v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Duong Nguyen',\n",
       "   'Rodolphe Vadaine',\n",
       "   'Guillaume Hajduch',\n",
       "   'René Garello',\n",
       "   'Ronan Fablet'],\n",
       "  'tasks': ['Anomaly Detection'],\n",
       "  'date': '2018-06-06',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/weight-initialization-without-local-minima-in',\n",
       "  'arxiv_id': '1806.04884',\n",
       "  'title': 'No Bad Local Minima in Wide and Deep Nonlinear Neural Networks',\n",
       "  'abstract': 'In this paper, we prove that no bad local minimum exists in the deep nonlinear neural networks with sufficiently large widths of the hidden layers if the parameters are initialized by He initialization method. Specifically, in the deep ReLU neural network model with sufficiently large widths of the hidden layers, the following four statements hold true: 1) the loss function is non-convex and non-concave; 2) every local minimum is a global minimum; 3) every critical point that is not a global minimum is a saddle point; and 4) bad saddle points exist.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04884v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04884v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Tohru Nitta'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Kaiming Initialization',\n",
       "    'full_name': 'Kaiming Initialization',\n",
       "    'description': '**Kaiming Initialization**, or **He Initialization**, is an initialization method for neural networks that takes into account the non-linearity of activation functions, such as [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nA proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. Using a derivation they work out that the condition to stop this happening is:\\r\\n\\r\\n$$\\\\frac{1}{2}n\\\\_{l}\\\\text{Var}\\\\left[w\\\\_{l}\\\\right] = 1 $$\\r\\n\\r\\nThis implies an initialization scheme of:\\r\\n\\r\\n$$ w\\\\_{l} \\\\sim \\\\mathcal{N}\\\\left(0,  2/n\\\\_{l}\\\\right)$$\\r\\n\\r\\nThat is, a zero-centered Gaussian with standard deviation of $\\\\sqrt{2/{n}\\\\_{l}}$ (variance shown in equation above). Biases are initialized at $0$.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.01852v1',\n",
       "    'source_title': 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/0adb5843766092fba584791af76383125fd0d01c/torch/nn/init.py#L389',\n",
       "    'main_collection': {'name': 'Initialization',\n",
       "     'description': '**Initialization** methods are used to initialize the weights in a neural network. Below can you find a continuously updating list of initialization methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/visual-speech-enhancement',\n",
       "  'arxiv_id': '1711.08789',\n",
       "  'title': 'Visual Speech Enhancement',\n",
       "  'abstract': 'When video is shot in noisy environment, the voice of a speaker seen in the\\nvideo can be enhanced using the visible mouth movements, reducing background\\nnoise. While most existing methods use audio-only inputs, improved performance\\nis obtained with our visual speech enhancement, based on an audio-visual neural\\nnetwork. We include in the training data videos to which we added the voice of\\nthe target speaker as background noise. Since the audio input is not sufficient\\nto separate the voice of a speaker from his own voice, the trained model better\\nexploits the visual input and generalizes well to different noise types. The\\nproposed model outperforms prior audio visual methods on two public lipreading\\ndatasets. It is also the first to be demonstrated on a dataset not designed for\\nlipreading, such as the weekly addresses of Barack Obama.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.08789v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.08789v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Aviv Gabbay', 'Asaph Shamir', 'Shmuel Peleg'],\n",
       "  'tasks': ['Lipreading', 'Speech Enhancement'],\n",
       "  'date': '2017-11-23',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/unsupervised-adaptation-with-interpretable',\n",
       "  'arxiv_id': '1806.04872',\n",
       "  'title': 'Unsupervised Adaptation with Interpretable Disentangled Representations for Distant Conversational Speech Recognition',\n",
       "  'abstract': 'The current trend in automatic speech recognition is to leverage large\\namounts of labeled data to train supervised neural network models.\\nUnfortunately, obtaining data for a wide range of domains to train robust\\nmodels can be costly. However, it is relatively inexpensive to collect large\\namounts of unlabeled data from domains that we want the models to generalize\\nto. In this paper, we propose a novel unsupervised adaptation method that\\nlearns to synthesize labeled data for the target domain from unlabeled\\nin-domain data and labeled out-of-domain data. We first learn without\\nsupervision an interpretable latent representation of speech that encodes\\nlinguistic and nuisance factors (e.g., speaker and channel) using different\\nlatent variables. To transform a labeled out-of-domain utterance without\\naltering its transcript, we transform the latent nuisance variables while\\nmaintaining the linguistic variables. To demonstrate our approach, we focus on\\na channel mismatch setting, where the domain of interest is distant\\nconversational speech, and labels are only available for close-talking speech.\\nOur proposed method is evaluated on the AMI dataset, outperforming all\\nbaselines and bridging the gap between unadapted and in-domain models by over\\n77% without using any parallel data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04872v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04872v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Wei-Ning Hsu', 'Hao Tang', 'James Glass'],\n",
       "  'tasks': ['Automatic Speech Recognition', 'Speech Recognition'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/when-simple-exploration-is-sample-efficient',\n",
       "  'arxiv_id': '1805.09045',\n",
       "  'title': 'When Simple Exploration is Sample Efficient: Identifying Sufficient Conditions for Random Exploration to Yield PAC RL Algorithms',\n",
       "  'abstract': 'Efficient exploration is one of the key challenges for reinforcement learning\\n(RL) algorithms. Most traditional sample efficiency bounds require strategic\\nexploration. Recently many deep RL algorithms with simple heuristic exploration\\nstrategies that have few formal guarantees, achieve surprising success in many\\ndomains. These results pose an important question about understanding these\\nexploration strategies such as $e$-greedy, as well as understanding what\\ncharacterize the difficulty of exploration in MDPs. In this work we propose\\nproblem specific sample complexity bounds of $Q$ learning with random walk\\nexploration that rely on several structural properties. We also link our\\ntheoretical results to some empirical benchmark domains, to illustrate if our\\nbound gives polynomial sample complexity in these domains and how that is\\nrelated with the empirical performance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.09045v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.09045v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yao Liu', 'Emma Brunskill'],\n",
       "  'tasks': ['Efficient Exploration', 'Q-Learning'],\n",
       "  'date': '2018-05-23',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/mqapviz-a-divide-and-conquer-multi-objective',\n",
       "  'arxiv_id': '1804.00656',\n",
       "  'title': 'mQAPViz: A divide-and-conquer multi-objective optimization algorithm to compute large data visualizations',\n",
       "  'abstract': 'Algorithms for data visualizations are essential tools for transforming data\\ninto useful narratives. Unfortunately, very few visualization algorithms can\\nhandle the large datasets of many real-world scenarios. In this study, we\\naddress the visualization of these datasets as a Multi-Objective Optimization\\nProblem. We propose mQAPViz, a divide-and-conquer multi-objective optimization\\nalgorithm to compute large-scale data visualizations. Our method employs the\\nMulti-Objective Quadratic Assignment Problem (mQAP) as the mathematical\\nfoundation to solve the visualization task at hand. The algorithm applies\\nadvanced sampling techniques originating from the field of machine learning and\\nefficient data structures to scale to millions of data objects. The algorithm\\nallocates objects onto a 2D grid layout. Experimental results on real-world and\\nlarge datasets demonstrate that mQAPViz is a competitive alternative to\\nexisting techniques.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.00656v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.00656v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Claudio Sanhueza',\n",
       "   'Francia Jiménez',\n",
       "   'Regina Berretta',\n",
       "   'Pablo Moscato'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-04-02',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/convolutional-dictionary-learning-a',\n",
       "  'arxiv_id': '1709.02893',\n",
       "  'title': 'Convolutional Dictionary Learning: A Comparative Review and New Algorithms',\n",
       "  'abstract': 'Convolutional sparse representations are a form of sparse representation with\\na dictionary that has a structure that is equivalent to convolution with a set\\nof linear filters. While effective algorithms have recently been developed for\\nthe convolutional sparse coding problem, the corresponding dictionary learning\\nproblem is substantially more challenging. Furthermore, although a number of\\ndifferent approaches have been proposed, the absence of thorough comparisons\\nbetween them makes it difficult to determine which of them represents the\\ncurrent state of the art. The present work both addresses this deficiency and\\nproposes some new approaches that outperform existing ones in certain contexts.\\nA thorough set of performance comparisons indicates a very wide range of\\nperformance differences among the existing and proposed methods, and clearly\\nidentifies those that are the most effective.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1709.02893v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1709.02893v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Cristina Garcia-Cardona', 'Brendt Wohlberg'],\n",
       "  'tasks': ['Dictionary Learning'],\n",
       "  'date': '2017-09-09',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/cell-identity-codes-understanding-cell',\n",
       "  'arxiv_id': '1806.04863',\n",
       "  'title': 'Cell Identity Codes: Understanding Cell Identity from Gene Expression Profiles using Deep Neural Networks',\n",
       "  'abstract': 'Understanding cell identity is an important task in many biomedical areas.\\nExpression patterns of specific marker genes have been used to characterize\\nsome limited cell types, but exclusive markers are not available for many cell\\ntypes. A second approach is to use machine learning to discriminate cell types\\nbased on the whole gene expression profiles (GEPs). The accuracies of simple\\nclassification algorithms such as linear discriminators or support vector\\nmachines are limited due to the complexity of biological systems. We used deep\\nneural networks to analyze 1040 GEPs from 16 different human tissues and cell\\ntypes. After comparing different architectures, we identified a specific\\nstructure of deep autoencoders that can encode a GEP into a vector of 30\\nnumeric values, which we call the cell identity code (CIC). The original GEP\\ncan be reproduced from the CIC with an accuracy comparable to technical\\nreplicates of the same experiment. Although we use an unsupervised approach to\\ntrain the autoencoder, we show different values of the CIC are connected to\\ndifferent biological aspects of the cell, such as different pathways or\\nbiological processes. This network can use CIC to reproduce the GEP of the cell\\ntypes it has never seen during the training. It also can resist some noise in\\nthe measurement of the GEP. Furthermore, we introduce classifier autoencoder,\\nan architecture that can accurately identify cell type based on the GEP or the\\nCIC.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04863v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04863v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Farzad Abdolhosseini',\n",
       "   'Behrooz Azarkhalili',\n",
       "   'Abbas Maazallahi',\n",
       "   'Aryan Kamal',\n",
       "   'Seyed Abolfazl Motahari',\n",
       "   'Ali Sharifi-Zarchi',\n",
       "   'Hamidreza Chitsaz'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-visual-knowledge-memory-networks-for',\n",
       "  'arxiv_id': '1806.04860',\n",
       "  'title': 'Learning Visual Knowledge Memory Networks for Visual Question Answering',\n",
       "  'abstract': \"Visual question answering (VQA) requires joint comprehension of images and\\nnatural language questions, where many questions can't be directly or clearly\\nanswered from visual content but require reasoning from structured human\\nknowledge with confirmation from visual content. This paper proposes visual\\nknowledge memory network (VKMN) to address this issue, which seamlessly\\nincorporates structured human knowledge and deep visual features into memory\\nnetworks in an end-to-end learning framework. Comparing to existing methods for\\nleveraging external knowledge for supporting VQA, this paper stresses more on\\ntwo missing mechanisms. First is the mechanism for integrating visual contents\\nwith knowledge facts. VKMN handles this issue by embedding knowledge triples\\n(subject, relation, target) and deep visual features jointly into the visual\\nknowledge features. Second is the mechanism for handling multiple knowledge\\nfacts expanding from question and answer pairs. VKMN stores joint embedding\\nusing key-value pair structure in the memory networks so that it is easy to\\nhandle multiple facts. Experiments show that the proposed method achieves\\npromising results on both VQA v1.0 and v2.0 benchmarks, while outperforms\\nstate-of-the-art methods on the knowledge-reasoning related questions.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04860v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04860v1.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Zhou Su',\n",
       "   'Chen Zhu',\n",
       "   'Yinpeng Dong',\n",
       "   'Dongqi Cai',\n",
       "   'Yurong Chen',\n",
       "   'Jianguo Li'],\n",
       "  'tasks': ['Question Answering', 'Visual Question Answering'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'Memory Network',\n",
       "    'full_name': 'Memory Network',\n",
       "    'description': 'A **Memory Network** provides a memory component that can be read from and written to with the inference capabilities of a neural network model. The motivation is that many neural networks lack a long-term memory component, and their existing memory component encoded by states and weights is too small and not compartmentalized enough to accurately remember facts from the past (RNNs for example, have difficult memorizing and doing tasks like copying). \\r\\n\\r\\nA memory network consists of a memory $\\\\textbf{m}$ (an array of objects indexed by $\\\\textbf{m}\\\\_{i}$ and four potentially learned components:\\r\\n\\r\\n- Input feature map $I$ - feature representation of the data input.\\r\\n- Generalization $G$ - updates old memories given the new input.\\r\\n- Output feature map $O$ - produces new feature map given $I$ and $G$.\\r\\n- Response $R$ - converts output into the desired response. \\r\\n\\r\\nGiven an input $x$ (e.g., an input character, word or sentence depending on the granularity chosen, an image or an audio signal) the flow of the model is as follows:\\r\\n\\r\\n1. Convert $x$ to an internal feature representation $I\\\\left(x\\\\right)$.\\r\\n2. Update memories $m\\\\_{i}$ given the new input: $m\\\\_{i} = G\\\\left(m\\\\_{i}, I\\\\left(x\\\\right), m\\\\right)$, $\\\\forall{i}$.\\r\\n3. Compute output features $o$ given the new input and the memory: $o = O\\\\left(I\\\\left(x\\\\right), m\\\\right)$.\\r\\n4. Finally, decode output features $o$ to give the final response: $r = R\\\\left(o\\\\right)$.\\r\\n\\r\\nThis process is applied at both train and test time, if there is a distinction between such phases, that\\r\\nis, memories are also stored at test time, but the model parameters of $I$, $G$, $O$ and $R$ are not updated. Memory networks cover a wide class of possible implementations. The components $I$, $G$, $O$ and $R$ can potentially use any existing ideas from the machine learning literature.\\r\\n\\r\\nImage Source: [Adrian Colyer](https://blog.acolyer.org/2016/03/10/memory-networks/)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1410.3916v11',\n",
       "    'source_title': 'Memory Networks',\n",
       "    'code_snippet_url': 'https://github.com/aykutaaykut/Memory-Networks',\n",
       "    'main_collection': {'name': 'Working Memory Models',\n",
       "     'description': '**Working Memory Models** aim to supplement neural networks with a memory module to increase their capability for memorization and allowing them to more easily perform tasks such as retrieving and copying information. Below you can find a continuously updating list of working memory models.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['coco',\n",
       "   'visual-question-answering',\n",
       "   'visual-genome',\n",
       "   'dbpedia'],\n",
       "  'datasets_used_full': ['COCO',\n",
       "   'Visual Question Answering',\n",
       "   'Visual Genome',\n",
       "   'DBpedia'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deepcas-a-deep-reinforcement-learning',\n",
       "  'arxiv_id': '1803.02998',\n",
       "  'title': 'DeepCAS: A Deep Reinforcement Learning Algorithm for Control-Aware Scheduling',\n",
       "  'abstract': 'We consider networked control systems consisting of multiple independent\\ncontrolled subsystems, operating over a shared communication network. Such\\nsystems are ubiquitous in cyber-physical systems, Internet of Things, and\\nlarge-scale industrial systems. In many large-scale settings, the size of the\\ncommunication network is smaller than the size of the system. In consequence,\\nscheduling issues arise. The main contribution of this paper is to develop a\\ndeep reinforcement learning-based \\\\emph{control-aware} scheduling\\n(\\\\textsc{DeepCAS}) algorithm to tackle these issues. We use the following\\n(optimal) design strategy: First, we synthesize an optimal controller for each\\nsubsystem; next, we design a learning algorithm that adapts to the chosen\\nsubsystems (plants) and controllers. As a consequence of this adaptation, our\\nalgorithm finds a schedule that minimizes the \\\\emph{control loss}. We present\\nempirical results to show that \\\\textsc{DeepCAS} finds schedules with better\\nperformance than periodic ones.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.02998v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.02998v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Burak Demirel',\n",
       "   'Arunselvan Ramaswamy',\n",
       "   'Daniel E. Quevedo',\n",
       "   'Holger Karl'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-03-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/curve-reconstruction-via-the-global',\n",
       "  'arxiv_id': '1711.03172',\n",
       "  'title': 'Curve Reconstruction via the Global Statistics of Natural Curves',\n",
       "  'abstract': 'Reconstructing the missing parts of a curve has been the subject of much\\ncomputational research, with applications in image inpainting, object\\nsynthesis, etc. Different approaches for solving that problem are typically\\nbased on processes that seek visually pleasing or perceptually plausible\\ncompletions. In this work we focus on reconstructing the underlying physically\\nlikely shape by utilizing the global statistics of natural curves. More\\nspecifically, we develop a reconstruction model that seeks the mean physical\\ncurve for a given inducer configuration. This simple model is both\\nstraightforward to compute and it is receptive to diverse additional\\ninformation, but it requires enough samples for all curve configurations, a\\npractical requirement that limits its effective utilization. To address this\\npractical issue we explore and exploit statistical geometrical properties of\\nnatural curves, and in particular, we show that in many cases the mean curve is\\nscale invariant and oftentimes it is extensible. This, in turn, allows to boost\\nthe number of examples and thus the robustness of the statistics and its\\napplicability. The reconstruction results are not only more physically\\nplausible but they also lead to important insights on the reconstruction\\nproblem, including an elegant explanation why certain inducer configurations\\nare more likely to yield consistent perceptual completions than others.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.03172v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.03172v3.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Ehud Barnea', 'Ohad Ben-Shahar'],\n",
       "  'tasks': ['Image Inpainting'],\n",
       "  'date': '2017-11-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/double-path-networks-for-sequence-to-sequence',\n",
       "  'arxiv_id': '1806.04856',\n",
       "  'title': 'Double Path Networks for Sequence to Sequence Learning',\n",
       "  'abstract': 'Encoder-decoder based Sequence to Sequence learning (S2S) has made remarkable\\nprogress in recent years. Different network architectures have been used in the\\nencoder/decoder. Among them, Convolutional Neural Networks (CNN) and Self\\nAttention Networks (SAN) are the prominent ones. The two architectures achieve\\nsimilar performances but use very different ways to encode and decode context:\\nCNN use convolutional layers to focus on the local connectivity of the\\nsequence, while SAN uses self-attention layers to focus on global semantics. In\\nthis work we propose Double Path Networks for Sequence to Sequence learning\\n(DPN-S2S), which leverage the advantages of both models by using double path\\ninformation fusion. During the encoding step, we develop a double path\\narchitecture to maintain the information coming from different paths with\\nconvolutional layers and self-attention layers separately. To effectively use\\nthe encoded context, we develop a cross attention module with gating and use it\\nto automatically pick up the information needed during the decoding step. By\\ndeeply integrating the two paths with cross attention, both types of\\ninformation are combined and well exploited. Experiments show that our proposed\\nmethod can significantly improve the performance of sequence to sequence\\nlearning over state-of-the-art systems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04856v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04856v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Kaitao Song',\n",
       "   'Xu Tan',\n",
       "   'Di He',\n",
       "   'Jianfeng Lu',\n",
       "   'Tao Qin',\n",
       "   'Tie-Yan Liu'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fast-and-scalable-bayesian-deep-learning-by',\n",
       "  'arxiv_id': '1806.04854',\n",
       "  'title': 'Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam',\n",
       "  'abstract': 'Uncertainty computation in deep learning is essential to design robust and\\nreliable systems. Variational inference (VI) is a promising approach for such\\ncomputation, but requires more effort to implement and execute compared to\\nmaximum-likelihood methods. In this paper, we propose new natural-gradient\\nalgorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms\\ncan be implemented within the Adam optimizer by perturbing the network weights\\nduring gradient evaluations, and uncertainty estimates can be cheaply obtained\\nby using the vector that adapts the learning rate. This requires lower memory,\\ncomputation, and implementation effort than existing VI methods, while\\nobtaining uncertainty estimates of comparable quality. Our empirical results\\nconfirm this and further suggest that the weight-perturbation in our algorithm\\ncould be useful for exploration in reinforcement learning and stochastic\\noptimization.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04854v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04854v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Mohammad Emtiyaz Khan',\n",
       "   'Didrik Nielsen',\n",
       "   'Voot Tangkaratt',\n",
       "   'Wu Lin',\n",
       "   'Yarin Gal',\n",
       "   'Akash Srivastava'],\n",
       "  'tasks': ['Stochastic Optimization', 'Variational Inference'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [{'name': 'Adam',\n",
       "    'full_name': 'Adam',\n",
       "    'description': '**Adam** is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of [RMSProp](https://paperswithcode.com/method/rmsprop) and [SGD w/th Momentum](https://paperswithcode.com/method/sgd-with-momentum). The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. \\r\\n\\r\\nThe weight updates are performed as:\\r\\n\\r\\n$$ w_{t} = w_{t-1} - \\\\eta\\\\frac{\\\\hat{m}\\\\_{t}}{\\\\sqrt{\\\\hat{v}\\\\_{t}} + \\\\epsilon}  $$\\r\\n\\r\\nwith\\r\\n\\r\\n$$ \\\\hat{m}\\\\_{t} = \\\\frac{m_{t}}{1-\\\\beta^{t}_{1}} $$\\r\\n\\r\\n$$ \\\\hat{v}\\\\_{t} = \\\\frac{v_{t}}{1-\\\\beta^{t}_{2}} $$\\r\\n\\r\\n$$ m_{t} = \\\\beta_{1}m_{t-1} + (1-\\\\beta_{1})g_{t} $$\\r\\n\\r\\n$$ v_{t} = \\\\beta_{2}v_{t-1} + (1-\\\\beta_{2})g_{t}^{2}  $$\\r\\n\\r\\n\\r\\n$ \\\\eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \\\\epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \\\\beta_{1} $ and $ \\\\beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1412.6980v9',\n",
       "    'source_title': 'Adam: A Method for Stochastic Optimization',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-structural-node-embeddings-via',\n",
       "  'arxiv_id': '1710.10321',\n",
       "  'title': 'Learning Structural Node Embeddings Via Diffusion Wavelets',\n",
       "  'abstract': \"Nodes residing in different parts of a graph can have similar structural\\nroles within their local network topology. The identification of such roles\\nprovides key insight into the organization of networks and can be used for a\\nvariety of machine learning tasks. However, learning structural representations\\nof nodes is a challenging problem, and it has typically involved manually\\nspecifying and tailoring topological features for each node. In this paper, we\\ndevelop GraphWave, a method that represents each node's network neighborhood\\nvia a low-dimensional embedding by leveraging heat wavelet diffusion patterns.\\nInstead of training on hand-selected features, GraphWave learns these\\nembeddings in an unsupervised way. We mathematically prove that nodes with\\nsimilar network neighborhoods will have similar GraphWave embeddings even\\nthough these nodes may reside in very different parts of the network, and our\\nmethod scales linearly with the number of edges. Experiments in a variety of\\ndifferent settings demonstrate GraphWave's real-world potential for capturing\\nstructural roles in networks, and our approach outperforms existing\\nstate-of-the-art baselines in every experiment, by as much as 137%.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1710.10321v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1710.10321v4.pdf',\n",
       "  'proceeding': 'KDD 2018 6',\n",
       "  'authors': ['Claire Donnat',\n",
       "   'Marinka Zitnik',\n",
       "   'David Hallac',\n",
       "   'Jure Leskovec'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-10-27',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/analyzing-the-robustness-of-nearest-neighbors',\n",
       "  'arxiv_id': '1706.03922',\n",
       "  'title': 'Analyzing the Robustness of Nearest Neighbors to Adversarial Examples',\n",
       "  'abstract': 'Motivated by safety-critical applications, test-time attacks on classifiers via adversarial examples has recently received a great deal of attention. However, there is a general lack of understanding on why adversarial examples arise; whether they originate due to inherent properties of data or due to lack of training samples remains ill-understood. In this work, we introduce a theoretical framework analogous to bias-variance theory for understanding these effects. We use our framework to analyze the robustness of a canonical non-parametric classifier - the k-nearest neighbors. Our analysis shows that its robustness properties depend critically on the value of k - the classifier may be inherently non-robust for small k, but its robustness approaches that of the Bayes Optimal classifier for fast-growing k. We propose a novel modified 1-nearest neighbor classifier, and guarantee its robustness in the large sample limit. Our experiments suggest that this classifier may have good robustness properties even for reasonable data set sizes.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1706.03922v6',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1706.03922v6.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Yizhen Wang', 'Somesh Jha', 'Kamalika Chaudhuri'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/polynomial-regression-as-an-alternative-to',\n",
       "  'arxiv_id': '1806.06850',\n",
       "  'title': 'Polynomial Regression As an Alternative to Neural Nets',\n",
       "  'abstract': 'Despite the success of neural networks (NNs), there is still a concern among\\nmany over their \"black box\" nature. Why do they work? Here we present a simple\\nanalytic argument that NNs are in fact essentially polynomial regression\\nmodels. This view will have various implications for NNs, e.g. providing an\\nexplanation for why convergence problems arise in NNs, and it gives rough\\nguidance on avoiding overfitting. In addition, we use this phenomenon to\\npredict and confirm a multicollinearity property of NNs not previously reported\\nin the literature. Most importantly, given this loose correspondence, one may\\nchoose to routinely use polynomial models instead of NNs, thus avoiding some\\nmajor problems of the latter, such as having to set many tuning parameters and\\ndealing with convergence issues. We present a number of empirical results; in\\neach case, the accuracy of the polynomial approach matches or exceeds that of\\nNN approaches. A many-featured, open-source software package, polyreg, is\\navailable.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06850v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06850v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xi Cheng',\n",
       "   'Bohdan Khomtchouk',\n",
       "   'Norman Matloff',\n",
       "   'Pete Mohanty'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-from-mutants-using-code-mutation-to',\n",
       "  'arxiv_id': '1801.00903',\n",
       "  'title': 'Learning from Mutants: Using Code Mutation to Learn and Monitor Invariants of a Cyber-Physical System',\n",
       "  'abstract': 'Cyber-physical systems (CPS) consist of sensors, actuators, and controllers\\nall communicating over a network; if any subset becomes compromised, an\\nattacker could cause significant damage. With access to data logs and a model\\nof the CPS, the physical effects of an attack could potentially be detected\\nbefore any damage is done. Manually building a model that is accurate enough in\\npractice, however, is extremely difficult. In this paper, we propose a novel\\napproach for constructing models of CPS automatically, by applying supervised\\nmachine learning to data traces obtained after systematically seeding their\\nsoftware components with faults (\"mutants\"). We demonstrate the efficacy of\\nthis approach on the simulator of a real-world water purification plant,\\npresenting a framework that automatically generates mutants, collects data\\ntraces, and learns an SVM-based model. Using cross-validation and statistical\\nmodel checking, we show that the learnt model characterises an invariant\\nphysical property of the system. Furthermore, we demonstrate the usefulness of\\nthe invariant by subjecting the system to 55 network and code-modification\\nattacks, and showing that it can detect 85% of them from the data logs\\ngenerated at runtime.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1801.00903v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1801.00903v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yuqi Chen', 'Christopher M. Poskitt', 'Jun Sun'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-01-03',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-study-of-enhancement-augmentation-and',\n",
       "  'arxiv_id': '1806.04841',\n",
       "  'title': 'A Study of Enhancement, Augmentation, and Autoencoder Methods for Domain Adaptation in Distant Speech Recognition',\n",
       "  'abstract': 'Speech recognizers trained on close-talking speech do not generalize to\\ndistant speech and the word error rate degradation can be as large as 40%\\nabsolute. Most studies focus on tackling distant speech recognition as a\\nseparate problem, leaving little effort to adapting close-talking speech\\nrecognizers to distant speech. In this work, we review several approaches from\\na domain adaptation perspective. These approaches, including speech\\nenhancement, multi-condition training, data augmentation, and autoencoders, all\\ninvolve a transformation of the data between domains. We conduct experiments on\\nthe AMI data set, where these approaches can be realized under the same\\ncontrolled setting. These approaches lead to different amounts of improvement\\nunder their respective assumptions. The purpose of this paper is to quantify\\nand characterize the performance gap between the two domains, setting up the\\nbasis for studying adaptation of speech recognizers from close-talking speech\\nto distant speech. Our results also have implications for improving distant\\nspeech recognition.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04841v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04841v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hao Tang', 'Wei-Ning Hsu', 'Francois Grondin', 'James Glass'],\n",
       "  'tasks': ['Data Augmentation',\n",
       "   'Distant Speech Recognition',\n",
       "   'Domain Adaptation',\n",
       "   'Speech Enhancement',\n",
       "   'Speech Recognition'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/sequence-prediction-with-neural-segmental',\n",
       "  'arxiv_id': '1709.01572',\n",
       "  'title': 'Sequence Prediction with Neural Segmental Models',\n",
       "  'abstract': 'Segments that span contiguous parts of inputs, such as phonemes in speech,\\nnamed entities in sentences, actions in videos, occur frequently in sequence\\nprediction problems. Segmental models, a class of models that explicitly\\nhypothesizes segments, have allowed the exploration of rich segment features\\nfor sequence prediction. However, segmental models suffer from slow decoding,\\nhampering the use of computationally expensive features.\\n  In this thesis, we introduce discriminative segmental cascades, a multi-pass\\ninference framework that allows us to improve accuracy by adding higher-order\\nfeatures and neural segmental features while maintaining efficiency. We also\\nshow that instead of including more features to obtain better accuracy,\\nsegmental cascades can be used to speed up training and decoding.\\n  Segmental models, similarly to conventional speech recognizers, are typically\\ntrained in multiple stages. In the first stage, a frame classifier is trained\\nwith manual alignments, and then in the second stage, segmental models are\\ntrained with manual alignments and the out- puts of the frame classifier.\\nHowever, obtaining manual alignments are time-consuming and expensive. We\\nexplore end-to-end training for segmental models with various loss functions,\\nand show how end-to-end training with marginal log loss can eliminate the need\\nfor detailed manual alignments.\\n  We draw the connections between the marginal log loss and a popular\\nend-to-end training approach called connectionist temporal classification. We\\npresent a unifying framework for various end-to-end graph search-based models,\\nsuch as hidden Markov models, connectionist temporal classification, and\\nsegmental models. Finally, we discuss possible extensions of segmental models\\nto large-vocabulary sequence prediction tasks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1709.01572v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1709.01572v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hao Tang'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2017-09-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/partial-auc-maximization-via-nonlinear',\n",
       "  'arxiv_id': '1806.04838',\n",
       "  'title': 'Partial AUC Maximization via Nonlinear Scoring Functions',\n",
       "  'abstract': 'We propose a method for maximizing a partial area under a receiver operating\\ncharacteristic (ROC) curve (pAUC) for binary classification tasks. In binary\\nclassification tasks, accuracy is the most commonly used as a measure of\\nclassifier performance. In some applications such as anomaly detection and\\ndiagnostic testing, accuracy is not an appropriate measure since prior\\nprobabilties are often greatly biased. Although in such cases the pAUC has been\\nutilized as a performance measure, few methods have been proposed for directly\\nmaximizing the pAUC. This optimization is achieved by using a scoring function.\\nThe conventional approach utilizes a linear function as the scoring function.\\nIn contrast we newly introduce nonlinear scoring functions for this purpose.\\nSpecifically, we present two types of nonlinear scoring functions based on\\ngenerative models and deep neural networks. We show experimentally that\\nnonlinear scoring fucntions improve the conventional methods through the\\napplication of a binary classification of real and bogus objects obtained with\\nthe Hyper Suprime-Cam on the Subaru telescope.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04838v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04838v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Naonori Ueda', 'Akinori Fujino'],\n",
       "  'tasks': ['Anomaly Detection', 'Classification', 'General Classification'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/3d-pose-estimation-for-fine-grained-object',\n",
       "  'arxiv_id': '1806.04314',\n",
       "  'title': '3D Pose Estimation for Fine-Grained Object Categories',\n",
       "  'abstract': 'Existing object pose estimation datasets are related to generic object types\\nand there is so far no dataset for fine-grained object categories. In this\\nwork, we introduce a new large dataset to benchmark pose estimation for\\nfine-grained objects, thanks to the availability of both 2D and 3D fine-grained\\ndata recently. Specifically, we augment two popular fine-grained recognition\\ndatasets (StanfordCars and CompCars) by finding a fine-grained 3D CAD model for\\neach sub-category and manually annotating each object in images with 3D pose.\\nWe show that, with enough training data, a full perspective model with\\ncontinuous parameters can be estimated using 2D appearance information alone.\\nWe achieve this via a framework based on Faster/Mask R-CNN. This goes beyond\\nprevious works on category-level pose estimation, which only estimate\\ndiscrete/continuous viewpoint angles or recover rotation matrices often with\\nthe help of key points. Furthermore, with fine-grained 3D models available, we\\nincorporate a dense 3D representation named as location field into the\\nCNN-based pose estimation framework to further improve the performance. The new\\ndataset is available at www.umiacs.umd.edu/~wym/3dpose.html',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04314v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04314v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yaming Wang',\n",
       "   'Xiao Tan',\n",
       "   'Yi Yang',\n",
       "   'Xiao Liu',\n",
       "   'Errui Ding',\n",
       "   'Feng Zhou',\n",
       "   'Larry S. Davis'],\n",
       "  'tasks': ['3D Pose Estimation', 'Pose Estimation'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['shapenet', 'pascal3d-2'],\n",
       "  'datasets_used_full': ['ShapeNet', 'PASCAL3D+'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/state-space-representations-of-deep-neural',\n",
       "  'arxiv_id': '1806.03751',\n",
       "  'title': 'State Space Representations of Deep Neural Networks',\n",
       "  'abstract': 'This paper deals with neural networks as dynamical systems governed by\\ndifferential or difference equations. It shows that the introduction of skip\\nconnections into network architectures, such as residual networks and dense\\nnetworks, turns a system of static equations into a system of dynamical\\nequations with varying levels of smoothness on the layer-wise transformations.\\nClosed form solutions for the state space representations of general dense\\nnetworks, as well as $k^{th}$ order smooth networks, are found in general\\nsettings. Furthermore, it is shown that imposing $k^{th}$ order smoothness on a\\nnetwork architecture with $d$-many nodes per layer increases the state space\\ndimension by a multiple of $k$, and so the effective embedding dimension of the\\ndata manifold is $k \\\\cdot d$-many dimensions. It follows that network\\narchitectures of these types reduce the number of parameters needed to maintain\\nthe same embedding dimension by a factor of $k^2$ when compared to an\\nequivalent first-order, residual network, significantly motivating the\\ndevelopment of network architectures of these types. Numerical simulations were\\nrun to validate parts of the developed theory.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03751v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03751v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Michael Hauser', 'Sean Gunn', 'Samer Saab Jr', 'Asok Ray'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-multiscale-model-learning',\n",
       "  'arxiv_id': '1806.04830',\n",
       "  'title': 'Deep Multiscale Model Learning',\n",
       "  'abstract': 'The objective of this paper is to design novel multi-layer neural network\\narchitectures for multiscale simulations of flows taking into account the\\nobserved data and physical modeling concepts. Our approaches use deep learning\\nconcepts combined with local multiscale model reduction methodologies to\\npredict flow dynamics. Using reduced-order model concepts is important for\\nconstructing robust deep learning architectures since the reduced-order models\\nprovide fewer degrees of freedom. Flow dynamics can be thought of as\\nmulti-layer networks. More precisely, the solution (e.g., pressures and\\nsaturations) at the time instant $n+1$ depends on the solution at the time\\ninstant $n$ and input parameters, such as permeability fields, forcing terms,\\nand initial conditions. One can regard the solution as a multi-layer network,\\nwhere each layer, in general, is a nonlinear forward map and the number of\\nlayers relates to the internal time steps. We will rely on rigorous model\\nreduction concepts to define unknowns and connections for each layer. In each\\nlayer, our reduced-order models will provide a forward map, which will be\\nmodified (\"trained\") using available data. It is critical to use reduced-order\\nmodels for this purpose, which will identify the regions of influence and the\\nappropriate number of variables. Because of the lack of available data, the\\ntraining will be supplemented with computational data as needed and the\\ninterpolation between data-rich and data-deficient models. We will also use\\ndeep learning algorithms to train the elements of the reduced model discrete\\nsystem. We will present main ingredients of our approach and numerical results.\\nNumerical results show that using deep learning and multiscale models, we can\\nimprove the forward models, which are conditioned to the available data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04830v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04830v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yating Wang',\n",
       "   'Siu Wun Cheung',\n",
       "   'Eric T. Chung',\n",
       "   'Yalchin Efendiev',\n",
       "   'Min Wang'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/position-detection-and-direction-prediction',\n",
       "  'arxiv_id': '1806.04828',\n",
       "  'title': 'Position Detection and Direction Prediction for Arbitrary-Oriented Ships via Multitask Rotation Region Convolutional Neural Network',\n",
       "  'abstract': 'Ship detection is of great importance and full of challenges in the field of\\nremote sensing. The complexity of application scenarios, the redundancy of\\ndetection region, and the difficulty of dense ship detection are all the main\\nobstacles that limit the successful operation of traditional methods in ship\\ndetection. In this paper, we propose a brand new detection model based on\\nmultitask rotational region convolutional neural network to solve the problems\\nabove. This model is mainly consist of five consecutive parts: Dense Feature\\nPyramid Network (DFPN), adaptive region of interest (ROI) Align, rotational\\nbounding box regression, prow direction prediction and rotational nonmaximum\\nsuppression (R-NMS). First of all, the low-level location information and\\nhigh-level semantic information are fully utilized through multiscale feature\\nnetworks. Then, we design Adaptive ROI Align to obtain high quality proposals\\nwhich remain complete spatial and semantic information. Unlike most previous\\napproaches, the prediction obtained by our method is the minimum bounding\\nrectangle of the object with less redundant regions. Therefore, rotational\\nregion detection framework is more suitable to detect the dense object than\\ntraditional detection model. Additionally, we can find the berthing and sailing\\ndirection of ship through prediction. A detailed evaluation based on SRSS for\\nrotation detection shows that our detection method has a competitive\\nperformance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04828v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04828v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xue Yang',\n",
       "   'Hao Sun',\n",
       "   'Xian Sun',\n",
       "   'Menglong Yan',\n",
       "   'Zhi Guo',\n",
       "   'Kun fu'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['dota'],\n",
       "  'datasets_used_full': ['DOTA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/detection-of-premature-ventricular',\n",
       "  'arxiv_id': '1806.04564',\n",
       "  'title': 'Detection of Premature Ventricular Contractions Using Densely Connected Deep Convolutional Neural Network with Spatial Pyramid Pooling Layer',\n",
       "  'abstract': 'Premature ventricular contraction(PVC) is a type of premature ectopic beat originating from the ventricles. Automatic method for accurate and robust detection of PVC is highly clinically desired.Currently, most of these methods are developed and tested using the same database divided into training and testing set and their generalization performance across databases has not been fully validated. In this paper, a method based on densely connected convolutional neural network and spatial pyramid pooling is proposed for PVC detection which can take arbitrarily-sized QRS complexes as input both in training and testing. With a much less complicated and more straightforward architecture,the proposed network achieves comparable results to current state-of-the-art deep learning based method with regard to accuracy,sensitivity and specificity by training and testing using the MIT-BIH arrhythmia database as benchmark.Besides the benchmark database,QRS complexes are extracted from four more open databases namely the St-Petersburg Institute of Cardiological Technics 12-lead Arrhythmia Database,The MIT-BIH Normal Sinus Rhythm Database,The MIT-BIH Long Term Database and European ST-T Database. The extracted QRS complexes are different in length and sampling rate among the five databases.Cross-database training and testing is also experimented.The performance of the network shows an improvement on the benchmark database according to the result demonstrating the advantage of using multiple databases for training over using only a single database.The network also achieves satisfactory scores on the other four databases showing good generalization capability.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04564v7',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04564v7.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jianning Li'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [{'name': 'Spatial Pyramid Pooling',\n",
       "    'full_name': 'Spatial Pyramid Pooling',\n",
       "    'description': '** Spatial Pyramid Pooling (SPP)** is a pooling layer that removes the fixed-size constraint of the network, i.e. a CNN does not require a fixed-size input image. Specifically, we add an SPP layer on top of the last convolutional layer. The SPP layer pools the features and generates fixed-length outputs, which are then fed into the fully-connected layers (or other classifiers). In other words, we perform some information aggregation at a deeper stage of the network hierarchy (between convolutional layers and fully-connected layers) to avoid the need for cropping or warping at the beginning.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1406.4729v4',\n",
       "    'source_title': 'Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition',\n",
       "    'code_snippet_url': 'https://github.com/yueruchen/sppnet-pytorch/blob/270529337baa5211538bf553bda222b9140838b3/spp_layer.py#L2',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/plug-in-regularized-estimation-of-high',\n",
       "  'arxiv_id': '1806.04823',\n",
       "  'title': 'Regularized Orthogonal Machine Learning for Nonlinear Semiparametric Models',\n",
       "  'abstract': \"This paper proposes a Lasso-type estimator for a high-dimensional sparse parameter identified by a single index conditional moment restriction (CMR). In addition to this parameter, the moment function can also depend on a nuisance function, such as the propensity score or the conditional choice probability, which we estimate by modern machine learning tools. We first adjust the moment function so that the gradient of the future loss function is insensitive (formally, Neyman-orthogonal) with respect to the first-stage regularization bias, preserving the single index property. We then take the loss function to be an indefinite integral of the adjusted moment function with respect to the single index. The proposed Lasso estimator converges at the oracle rate, where the oracle knows the nuisance function and solves only the parametric problem. We demonstrate our method by estimating the short-term heterogeneous impact of Connecticut's Jobs First welfare reform experiment on women's welfare participation decision.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04823v8',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04823v8.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Denis Nekipelov', 'Vira Semenova', 'Vasilis Syrgkanis'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/does-higher-order-lstm-have-better-accuracy',\n",
       "  'arxiv_id': '1711.08231',\n",
       "  'title': 'Does Higher Order LSTM Have Better Accuracy for Segmenting and Labeling Sequence Data?',\n",
       "  'abstract': 'Existing neural models usually predict the tag of the current token\\nindependent of the neighboring tags. The popular LSTM-CRF model considers the\\ntag dependencies between every two consecutive tags. However, it is hard for\\nexisting neural models to take longer distance dependencies of tags into\\nconsideration. The scalability is mainly limited by the complex model\\nstructures and the cost of dynamic programming during training. In our work, we\\nfirst design a new model called \"high order LSTM\" to predict multiple tags for\\nthe current token which contains not only the current tag but also the previous\\nseveral tags. We call the number of tags in one prediction as \"order\". Then we\\npropose a new method called Multi-Order BiLSTM (MO-BiLSTM) which combines low\\norder and high order LSTMs together. MO-BiLSTM keeps the scalability to high\\norder models with a pruning technique. We evaluate MO-BiLSTM on all-phrase\\nchunking and NER datasets. Experiment results show that MO-BiLSTM achieves the\\nstate-of-the-art result in chunking and highly competitive results in two NER\\ndatasets.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.08231v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.08231v3.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Yi Zhang',\n",
       "   'Xu sun',\n",
       "   'Shuming Ma',\n",
       "   'Yang Yang',\n",
       "   'Xuancheng Ren'],\n",
       "  'tasks': ['Chunking', 'NER', 'TAG'],\n",
       "  'date': '2017-11-22',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}},\n",
       "   {'name': 'BiLSTM',\n",
       "    'full_name': 'Bidirectional LSTM',\n",
       "    'description': 'A **Bidirectional LSTM**, or **biLSTM**, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction. BiLSTMs effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow *and* precede a word in a sentence).\\r\\n\\r\\nImage Source: Modelling Radiological Language with Bidirectional Long Short-Term Memory Networks, Cornegruta et al',\n",
       "    'introduced_year': 2001,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Bidirectional Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': 'Recurrent Neural Networks',\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/integral-privacy-for-sampling-from-mollifier',\n",
       "  'arxiv_id': '1806.04819',\n",
       "  'title': 'Integral Privacy for Sampling',\n",
       "  'abstract': 'Differential privacy is a leading protection setting, focused by design on individual privacy. Many applications, in medical / pharmaceutical domains or social networks, rather posit privacy at a group level, a setting we call integral privacy. We aim for the strongest form of privacy: the group size is in particular not known in advance. We study a problem with related applications in domains cited above that have recently met with substantial recent press: sampling. Keeping correct utility levels in such a strong model of statistical indistinguishability looks difficult to be achieved with the usual differential privacy toolbox because it would typically scale in the worst case the sensitivity by the sample size and so the noise variance by up to its square. We introduce a trick specific to sampling that bypasses the sensitivity analysis. Privacy enforces an information theoretic barrier on approximation, and we show how to reach this barrier with guarantees on the approximation of the target non private density. We do so using a recent approach to non private density estimation relying on the original boosting theory, learning the sufficient statistics of an exponential family with classifiers. Approximation guarantees cover the mode capture problem. In the context of learning, the sampling problem is particularly important: because integral privacy enjoys the same closure under post-processing as differential privacy does, any algorithm using integrally privacy sampled data would result in an output equally integrally private. We also show that this brings fairness guarantees on post-processing that would eventually elude classical differential privacy: any decision process has bounded data-dependent bias when the data is integrally privately sampled. Experimental results against private kernel density estimation and private GANs displays the quality of our results.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04819v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04819v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hisham Husain', 'Zac Cranko', 'Richard Nock'],\n",
       "  'tasks': ['Density Estimation'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/disintegration-and-bayesian-inversion-via',\n",
       "  'arxiv_id': '1709.00322',\n",
       "  'title': 'Disintegration and Bayesian Inversion via String Diagrams',\n",
       "  'abstract': 'The notions of disintegration and Bayesian inversion are fundamental in\\nconditional probability theory. They produce channels, as conditional\\nprobabilities, from a joint state, or from an already given channel (in\\nopposite direction). These notions exist in the literature, in concrete\\nsituations, but are presented here in abstract graphical formulations. The\\nresulting abstract descriptions are used for proving basic results in\\nconditional probability theory. The existence of disintegration and Bayesian\\ninversion is discussed for discrete probability, and also for measure-theoretic\\nprobability --- via standard Borel spaces and via likelihoods. Finally, the\\nusefulness of disintegration and Bayesian inversion is illustrated in several\\nexamples.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1709.00322v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1709.00322v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kenta Cho', 'Bart Jacobs'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-08-29',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-unified-framework-for-generalizable-style',\n",
       "  'arxiv_id': '1806.05173',\n",
       "  'title': 'A Unified Framework for Generalizable Style Transfer: Style and Content Separation',\n",
       "  'abstract': \"Image style transfer has drawn broad attention in recent years. However, most\\nexisting methods aim to explicitly model the transformation between different\\nstyles, and the learned model is thus not generalizable to new styles. We here\\npropose a unified style transfer framework for both character typeface transfer\\nand neural style transfer tasks leveraging style and content separation. A key\\nmerit of such framework is its generalizability to new styles and contents. The\\noverall framework consists of style encoder, content encoder, mixer and\\ndecoder. The style encoder and content encoder are used to extract the style\\nand content representations from the corresponding reference images. The mixer\\nintegrates the above two representations and feeds it into the decoder to\\ngenerate images with the target style and content. During training, the encoder\\nnetworks learn to extract styles and contents from limited size of\\nstyle/content reference images. This learning framework allows simultaneous\\nstyle transfer among multiple styles and can be deemed as a special\\n`multi-task' learning scenario. The encoders are expected to capture the\\nunderlying features for different styles and contents which is generalizable to\\nnew styles and contents. Under this framework, we design two individual\\nnetworks for character typeface transfer and neural style transfer,\\nrespectively. For character typeface transfer, to separate the style features\\nand content features, we leverage the conditional dependence of styles and\\ncontents given an image. For neural style transfer, we leverage the statistical\\ninformation of feature maps in certain layers to represent style. Extensive\\nexperimental results have demonstrated the effectiveness and robustness of the\\nproposed methods.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05173v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05173v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yexun Zhang', 'Ya zhang', 'Wenbin Cai'],\n",
       "  'tasks': ['Multi-Task Learning', 'Style Transfer'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-learning-for-forecasting-stock-returns',\n",
       "  'arxiv_id': '1801.01777',\n",
       "  'title': 'Deep Learning for Forecasting Stock Returns in the Cross-Section',\n",
       "  'abstract': 'Many studies have been undertaken by using machine learning techniques,\\nincluding neural networks, to predict stock returns. Recently, a method known\\nas deep learning, which achieves high performance mainly in image recognition\\nand speech recognition, has attracted attention in the machine learning field.\\nThis paper implements deep learning to predict one-month-ahead stock returns in\\nthe cross-section in the Japanese stock market and investigates the performance\\nof the method. Our results show that deep neural networks generally outperform\\nshallow neural networks, and the best networks also outperform representative\\nmachine learning models. These results indicate that deep learning shows\\npromise as a skillful machine learning method to predict stock returns in the\\ncross-section.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1801.01777v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1801.01777v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Masaya Abe', 'Hideki Nakayama'],\n",
       "  'tasks': ['Speech Recognition'],\n",
       "  'date': '2018-01-03',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-representations-of-ultrahigh',\n",
       "  'arxiv_id': '1806.04808',\n",
       "  'title': 'Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection',\n",
       "  'abstract': 'Learning expressive low-dimensional representations of ultrahigh-dimensional\\ndata, e.g., data with thousands/millions of features, has been a major way to\\nenable learning methods to address the curse of dimensionality. However,\\nexisting unsupervised representation learning methods mainly focus on\\npreserving the data regularity information and learning the representations\\nindependently of subsequent outlier detection methods, which can result in\\nsuboptimal and unstable performance of detecting irregularities (i.e.,\\noutliers).\\n  This paper introduces a ranking model-based framework, called RAMODO, to\\naddress this issue. RAMODO unifies representation learning and outlier\\ndetection to learn low-dimensional representations that are tailored for a\\nstate-of-the-art outlier detection approach - the random distance-based\\napproach. This customized learning yields more optimal and stable\\nrepresentations for the targeted outlier detectors. Additionally, RAMODO can\\nleverage little labeled data as prior knowledge to learn more expressive and\\napplication-relevant representations. We instantiate RAMODO to an efficient\\nmethod called REPEN to demonstrate the performance of RAMODO.\\n  Extensive empirical results on eight real-world ultrahigh dimensional data\\nsets show that REPEN (i) enables a random distance-based detector to obtain\\nsignificantly better AUC performance and two orders of magnitude speedup; (ii)\\nperforms substantially better and more stably than four state-of-the-art\\nrepresentation learning methods; and (iii) leverages less than 1% labeled data\\nto achieve up to 32% AUC improvement.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04808v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04808v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Guansong Pang', 'Longbing Cao', 'Ling Chen', 'Huan Liu'],\n",
       "  'tasks': ['Anomaly Detection',\n",
       "   'Disease Prediction',\n",
       "   'Network Intrusion Detection',\n",
       "   'Outlier Detection',\n",
       "   'Representation Learning'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/ba-net-dense-bundle-adjustment-network',\n",
       "  'arxiv_id': '1806.04807',\n",
       "  'title': 'BA-Net: Dense Bundle Adjustment Network',\n",
       "  'abstract': 'This paper introduces a network architecture to solve the structure-from-motion (SfM) problem via feature-metric bundle adjustment (BA), which explicitly enforces multi-view geometry constraints in the form of feature-metric error. The whole pipeline is differentiable so that the network can learn suitable features that make the BA problem more tractable. Furthermore, this work introduces a novel depth parameterization to recover dense per-pixel depth. The network first generates several basis depth maps according to the input image and optimizes the final depth as a linear combination of these basis depth maps via feature-metric BA. The basis depth maps generator is also learned via end-to-end training. The whole system nicely combines domain knowledge (i.e. hard-coded multi-view geometry constraints) and deep learning (i.e. feature learning and basis depth maps learning) to address the challenging dense SfM problem. Experiments on large scale real data prove the success of the proposed method.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04807v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04807v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Chengzhou Tang', 'Ping Tan'],\n",
       "  'tasks': ['Depth And Camera Motion'],\n",
       "  'date': '2018-06-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['kitti'],\n",
       "  'datasets_used_full': ['KITTI'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/meta-learning-transferable-active-learning',\n",
       "  'arxiv_id': '1806.04798',\n",
       "  'title': 'Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning',\n",
       "  'abstract': 'Active learning (AL) aims to enable training high performance classifiers\\nwith low annotation cost by predicting which subset of unlabelled instances\\nwould be most beneficial to label. The importance of AL has motivated extensive\\nresearch, proposing a wide variety of manually designed AL algorithms with\\ndiverse theoretical and intuitive motivations. In contrast to this body of\\nresearch, we propose to treat active learning algorithm design as a\\nmeta-learning problem and learn the best criterion from data. We model an\\nactive learning algorithm as a deep neural network that inputs the base learner\\nstate and the unlabelled point set and predicts the best point to annotate\\nnext. Training this active query policy network with reinforcement learning,\\nproduces the best non-myopic policy for a given dataset. The key challenge in\\nachieving a general solution to AL then becomes that of learner generalisation,\\nparticularly across heterogeneous datasets. We propose a multi-task\\ndataset-embedding approach that allows dataset-agnostic active learners to be\\ntrained. Our evaluation shows that AL algorithms trained in this way can\\ndirectly generalise across diverse problems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04798v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04798v1.pdf',\n",
       "  'proceeding': 'ICLR 2018 1',\n",
       "  'authors': ['Kunkun Pang', 'Mingzhi Dong', 'Yang Wu', 'Timothy Hospedales'],\n",
       "  'tasks': ['Active Learning', 'Meta-Learning'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/drive2vec-multiscale-state-space-embedding-of',\n",
       "  'arxiv_id': '1806.04795',\n",
       "  'title': 'Drive2Vec: Multiscale State-Space Embedding of Vehicular Sensor Data',\n",
       "  'abstract': 'With automobiles becoming increasingly reliant on sensors to perform various\\ndriving tasks, it is important to encode the relevant CAN bus sensor data in a\\nway that captures the general state of the vehicle in a compact form. In this\\npaper, we develop a deep learning-based method, called Drive2Vec, for embedding\\nsuch sensor data in a low-dimensional yet actionable form. Our method is based\\non stacked gated recurrent units (GRUs). It accepts a short interval of\\nautomobile sensor data as input and computes a low-dimensional representation\\nof that data, which can then be used to accurately solve a range of tasks. With\\nthis representation, we (1) predict the exact values of the sensors in the\\nshort term (up to three seconds in the future), (2) forecast the long-term\\naverage values of these same sensors, (3) infer additional contextual\\ninformation that is not encoded in the data, including the identity of the\\ndriver behind the wheel, and (4) build a knowledge base that can be used to\\nauto-label data and identify risky states. We evaluate our approach on a\\ndataset collected by Audi, which equipped a fleet of test vehicles with data\\nloggers to store all sensor readings on 2,098 hours of driving on real roads.\\nWe show in several experiments that our method outperforms other baselines by\\nup to 90%, and we further demonstrate how these embeddings of sensor data can\\nbe used to solve a variety of real-world automotive applications.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04795v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04795v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['David Hallac',\n",
       "   'Suvrat Bhooshan',\n",
       "   'Michael Chen',\n",
       "   'Kacem Abida',\n",
       "   'Rok Sosic',\n",
       "   'Jure Leskovec'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-connectome-based-hexagonal-lattice',\n",
       "  'arxiv_id': '1806.04793',\n",
       "  'title': 'A Connectome Based Hexagonal Lattice Convolutional Network Model of the Drosophila Visual System',\n",
       "  'abstract': 'What can we learn from a connectome? We constructed a simplified model of the\\nfirst two stages of the fly visual system, the lamina and medulla. The\\nresulting hexagonal lattice convolutional network was trained using\\nbackpropagation through time to perform object tracking in natural scene\\nvideos. Networks initialized with weights from connectome reconstructions\\nautomatically discovered well-known orientation and direction selectivity\\nproperties in T4 neurons and their inputs, while networks initialized at random\\ndid not. Our work is the first demonstration, that knowledge of the connectome\\ncan enable in silico predictions of the functional properties of individual\\nneurons in a circuit, leading to an understanding of circuit function from\\nstructure alone.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04793v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04793v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Fabian David Tschopp',\n",
       "   'Michael B. Reiser',\n",
       "   'Srinivas C. Turaga'],\n",
       "  'tasks': ['Object Tracking'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['davis'],\n",
       "  'datasets_used_full': ['DAVIS'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/aspa-fast-adversarial-attack-example',\n",
       "  'arxiv_id': '1802.05763',\n",
       "  'title': 'ASP:A Fast Adversarial Attack Example Generation Framework based on Adversarial Saliency Prediction',\n",
       "  'abstract': 'With the excellent accuracy and feasibility, the Neural Networks have been\\nwidely applied into the novel intelligent applications and systems. However,\\nwith the appearance of the Adversarial Attack, the NN based system performance\\nbecomes extremely vulnerable:the image classification results can be\\narbitrarily misled by the adversarial examples, which are crafted images with\\nhuman unperceivable pixel-level perturbation. As this raised a significant\\nsystem security issue, we implemented a series of investigations on the\\nadversarial attack in this work: We first identify an image\\'s pixel\\nvulnerability to the adversarial attack based on the adversarial saliency\\nanalysis. By comparing the analyzed saliency map and the adversarial\\nperturbation distribution, we proposed a new evaluation scheme to\\ncomprehensively assess the adversarial attack precision and efficiency. Then,\\nwith a novel adversarial saliency prediction method, a fast adversarial example\\ngeneration framework, namely \"ASP\", is proposed with significant attack\\nefficiency improvement and dramatic computation cost reduction. Compared to the\\nprevious methods, experiments show that ASP has at most 12 times speed-up for\\nadversarial example generation, 2 times lower perturbation rate, and high\\nattack success rate of 87% on both MNIST and Cifar10. ASP can be also well\\nutilized to support the data-hungry NN adversarial training. By reducing the\\nattack success rate as much as 90%, ASP can quickly and effectively enhance the\\ndefense capability of NN based system to the adversarial attacks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.05763v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.05763v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Fuxun Yu', 'Qide Dong', 'Xiang Chen'],\n",
       "  'tasks': ['Adversarial Attack',\n",
       "   'Image Classification',\n",
       "   'Saliency Prediction'],\n",
       "  'date': '2018-02-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist'],\n",
       "  'datasets_used_full': ['MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/global-locally-self-attentive-dialogue-state',\n",
       "  'arxiv_id': '1805.09655',\n",
       "  'title': 'Global-Locally Self-Attentive Dialogue State Tracker',\n",
       "  'abstract': 'Dialogue state tracking, which estimates user goals and requests given the\\ndialogue context, is an essential part of task-oriented dialogue systems. In\\nthis paper, we propose the Global-Locally Self-Attentive Dialogue State Tracker\\n(GLAD), which learns representations of the user utterance and previous system\\nactions with global-local modules. Our model uses global modules to share\\nparameters between estimators for different types (called slots) of dialogue\\nstates, and uses local modules to learn slot-specific features. We show that\\nthis significantly improves tracking of rare states and achieves\\nstate-of-the-art performance on the WoZ and DSTC2 state tracking tasks. GLAD\\nobtains 88.1% joint goal accuracy and 97.1% request accuracy on WoZ,\\noutperforming prior work by 3.7% and 5.5%. On DSTC2, our model obtains 74.5%\\njoint goal accuracy and 97.5% request accuracy, outperforming prior work by\\n1.1% and 1.0%.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.09655v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.09655v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Victor Zhong', 'Caiming Xiong', 'Richard Socher'],\n",
       "  'tasks': ['Dialogue State Tracking',\n",
       "   'Multi-domain Dialogue State Tracking',\n",
       "   'Task-Oriented Dialogue Systems'],\n",
       "  'date': '2018-05-19',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['multiwoz',\n",
       "   'dialogue-state-tracking-challenge',\n",
       "   'wizard-of-oz'],\n",
       "  'datasets_used_full': ['MultiWOZ',\n",
       "   'Dialogue State Tracking Challenge',\n",
       "   'Wizard-of-Oz'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/distribution-matching-in-variational',\n",
       "  'arxiv_id': '1802.06847',\n",
       "  'title': 'Distribution Matching in Variational Inference',\n",
       "  'abstract': 'With the increasingly widespread deployment of generative models, there is a mounting need for a deeper understanding of their behaviors and limitations. In this paper, we expose the limitations of Variational Autoencoders (VAEs), which consistently fail to learn marginal distributions in both latent and visible spaces. We show this to be a consequence of learning by matching conditional distributions, and the limitations of explicit model and posterior distributions. It is popular to consider Generative Adversarial Networks (GANs) as a means of overcoming these limitations, leading to hybrids of VAEs and GANs. We perform a large-scale evaluation of several VAE-GAN hybrids and analyze the implications of class probability estimation for learning distributions. While promising, we conclude that at present, VAE-GAN hybrids have limited applicability: they are harder to scale, evaluate, and use for inference compared to VAEs; and they do not improve over the generation quality of GANs.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1802.06847v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1802.06847v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mihaela Rosca', 'Balaji Lakshminarayanan', 'Shakir Mohamed'],\n",
       "  'tasks': ['Variational Inference'],\n",
       "  'date': '2018-02-19',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'celeba'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'CelebA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-fast-algorithm-with-minimax-optimal',\n",
       "  'arxiv_id': '1805.06837',\n",
       "  'title': 'A fast algorithm with minimax optimal guarantees for topic models with an unknown number of topics',\n",
       "  'abstract': 'We propose a new method of estimation in topic models, that is not a variation on the existing simplex finding algorithms, and that estimates the number of topics K from the observed data. We derive new finite sample minimax lower bounds for the estimation of A, as well as new upper bounds for our proposed estimator. We describe the scenarios where our estimator is minimax adaptive. Our finite sample analysis is valid for any number of documents (n), individual document length (N_i), dictionary size (p) and number of topics (K), and both p and K are allowed to increase with n, a situation not handled well by previous analyses. We complement our theoretical results with a detailed simulation study. We illustrate that the new algorithm is faster and more accurate than the current ones, although we start out with a computational and theoretical disadvantage of not knowing the correct number of topics K, while we provide the competing methods with the correct value in our simulations.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1805.06837v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1805.06837v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xin Bing', 'Florentina Bunea', 'Marten Wegkamp'],\n",
       "  'tasks': ['Topic Models'],\n",
       "  'date': '2018-05-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/viton-an-image-based-virtual-try-on-network',\n",
       "  'arxiv_id': '1711.08447',\n",
       "  'title': 'VITON: An Image-based Virtual Try-on Network',\n",
       "  'abstract': 'We present an image-based VIirtual Try-On Network (VITON) without using 3D\\ninformation in any form, which seamlessly transfers a desired clothing item\\nonto the corresponding region of a person using a coarse-to-fine strategy.\\nConditioned upon a new clothing-agnostic yet descriptive person representation,\\nour framework first generates a coarse synthesized image with the target\\nclothing item overlaid on that same person in the same pose. We further enhance\\nthe initial blurry clothing area with a refinement network. The network is\\ntrained to learn how much detail to utilize from the target clothing item, and\\nwhere to apply to the person in order to synthesize a photo-realistic image in\\nwhich the target item deforms naturally with clear visual patterns. Experiments\\non our newly collected Zalando dataset demonstrate its promise in the\\nimage-based virtual try-on task over state-of-the-art generative models.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.08447v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.08447v4.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Xintong Han',\n",
       "   'Zuxuan Wu',\n",
       "   'Zhe Wu',\n",
       "   'Ruichi Yu',\n",
       "   'Larry S. Davis'],\n",
       "  'tasks': ['Virtual Try-on'],\n",
       "  'date': '2017-11-22',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': ['viton'],\n",
       "  'datasets_introduced_full': ['VITON']},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/adaptive-three-operator-splitting',\n",
       "  'arxiv_id': '1804.02339',\n",
       "  'title': 'Adaptive Three Operator Splitting',\n",
       "  'abstract': 'We propose and analyze an adaptive step-size variant of the Davis-Yin three\\noperator splitting. This method can solve optimization problems composed by a\\nsum of a smooth term for which we have access to its gradient and an arbitrary\\nnumber of potentially non-smooth terms for which we have access to their\\nproximal operator. The proposed method sets the step-size based on local\\ninformation of the objective --hence allowing for larger step-sizes--, only\\nrequires two extra function evaluations per iteration and does not depend on\\nany step-size hyperparameter besides an initial estimate. We provide an\\niteration complexity analysis that matches the best known results for the\\nnon-adaptive variant: sublinear convergence for general convex functions and\\nlinear convergence under strong convexity of the smooth term and smoothness of\\none of the proximal terms. Finally, an empirical comparison with related\\nmethods on 6 different problems illustrates the computational advantage of the\\nproposed method.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.02339v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.02339v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Fabian Pedregosa', 'Gauthier Gidel'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-04-06',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['rcv1'],\n",
       "  'datasets_used_full': ['RCV1'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/convolutional-neural-networks-for-aircraft',\n",
       "  'arxiv_id': '1806.04779',\n",
       "  'title': 'Convolutional Neural Networks for Aircraft Noise Monitoring',\n",
       "  'abstract': 'Air travel is one of the fastest growing modes of transportation, however,\\nthe effects of aircraft noise on populations surrounding airports is hindering\\nits growth. In an effort to study and ultimately mitigate the impact that this\\nnoise has, many airports continuously monitor the aircraft noise in their\\nsurrounding communities. Noise monitoring and analysis is complicated by the\\nfact that aircraft are not the only source of noise. In this work, we show that\\na Convolutional Neural Network is well-suited for the task of identifying noise\\nevents which are not caused by aircraft. Our system achieves an accuracy of\\n0.970 when trained on 900 manually labeled noise events. Our training data and\\na TensorFlow implementation of our model are available at\\nhttps://github.com/neheller/aircraftnoise.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04779v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04779v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Nicholas Heller',\n",
       "   'Derek Anderson',\n",
       "   'Matt Baker',\n",
       "   'Brad Juffer',\n",
       "   'Nikolaos Papanikolopoulos'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-to-ask-good-questions-ranking',\n",
       "  'arxiv_id': '1805.04655',\n",
       "  'title': 'Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information',\n",
       "  'abstract': 'Inquiry is fundamental to communication, and machines cannot effectively\\ncollaborate with humans unless they can ask questions. In this work, we build a\\nneural network model for the task of ranking clarification questions. Our model\\nis inspired by the idea of expected value of perfect information: a good\\nquestion is one whose expected answer will be useful. We study this problem\\nusing data from StackExchange, a plentiful online resource in which people\\nroutinely ask clarifying questions to posts so that they can better offer\\nassistance to the original poster. We create a dataset of clarification\\nquestions consisting of ~77K posts paired with a clarification question (and\\nanswer) from three domains of StackExchange: askubuntu, unix and superuser. We\\nevaluate our model on 500 samples of this dataset against expert human\\njudgments and demonstrate significant improvements over controlled baselines.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.04655v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.04655v2.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Sudha Rao', 'Hal Daumé III'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/early-stopping-for-nonparametric-testing',\n",
       "  'arxiv_id': '1805.09950',\n",
       "  'title': 'Early Stopping for Nonparametric Testing',\n",
       "  'abstract': 'Early stopping of iterative algorithms is an algorithmic regularization\\nmethod to avoid over-fitting in estimation and classification. In this paper,\\nwe show that early stopping can also be applied to obtain the minimax optimal\\ntesting in a general non-parametric setup. Specifically, a Wald-type test\\nstatistic is obtained based on an iterated estimate produced by functional\\ngradient descent algorithms in a reproducing kernel Hilbert space. A notable\\ncontribution is to establish a \"sharp\" stopping rule: when the number of\\niterations achieves an optimal order, testing optimality is achievable;\\notherwise, testing optimality becomes impossible. As a by-product, a similar\\nsharpness result is also derived for minimax optimal estimation under early\\nstopping studied in [11] and [19]. All obtained results hold for various kernel\\nclasses, including Sobolev smoothness classes and Gaussian kernel classes.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.09950v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.09950v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Meimei Liu', 'Guang Cheng'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-05-25',\n",
       "  'methods': [{'name': 'Early Stopping',\n",
       "    'full_name': 'Early Stopping',\n",
       "    'description': '**Early Stopping** is a regularization technique for deep neural networks that stops training when parameter updates no longer begin to yield improves on a validation set. In essence, we store and update the current best parameters during training, and when parameter updates no longer yield an improvement (after a set number of iterations) we stop training and use the last best parameters. It works as a regularizer by restricting the optimization procedure to a smaller volume of parameter space.\\r\\n\\r\\nImage Source: [Ramazan Gençay](https://www.researchgate.net/figure/Early-stopping-based-on-cross-validation_fig1_3302948)',\n",
       "    'introduced_year': 1995,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/static-malware-detection-subterfuge',\n",
       "  'arxiv_id': '1806.04773',\n",
       "  'title': 'Static Malware Detection & Subterfuge: Quantifying the Robustness of Machine Learning and Current Anti-Virus',\n",
       "  'abstract': \"As machine-learning (ML) based systems for malware detection become more\\nprevalent, it becomes necessary to quantify the benefits compared to the more\\ntraditional anti-virus (AV) systems widely used today. It is not practical to\\nbuild an agreed upon test set to benchmark malware detection systems on pure\\nclassification performance. Instead we tackle the problem by creating a new\\ntesting methodology, where we evaluate the change in performance on a set of\\nknown benign & malicious files as adversarial modifications are performed. The\\nchange in performance combined with the evasion techniques then quantifies a\\nsystem's robustness against that approach. Through these experiments we are\\nable to show in a quantifiable way how purely ML based systems can be more\\nrobust than AV products at detecting malware that attempts evasion through\\nmodification, but may be slower to adapt in the face of significantly novel\\nattacks.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04773v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04773v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['William Fleshman',\n",
       "   'Edward Raff',\n",
       "   'Richard Zak',\n",
       "   'Mark McLean',\n",
       "   'Charles Nicholas'],\n",
       "  'tasks': ['Malware Detection'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-sparse-coding-for-invariant-multimodal',\n",
       "  'arxiv_id': '1711.07998',\n",
       "  'title': 'Deep Sparse Coding for Invariant Multimodal Halle Berry Neurons',\n",
       "  'abstract': 'Deep feed-forward convolutional neural networks (CNNs) have become ubiquitous\\nin virtually all machine learning and computer vision challenges; however,\\nadvancements in CNNs have arguably reached an engineering saturation point\\nwhere incremental novelty results in minor performance gains. Although there is\\nevidence that object classification has reached human levels on narrowly\\ndefined tasks, for general applications, the biological visual system is far\\nsuperior to that of any computer. Research reveals there are numerous missing\\ncomponents in feed-forward deep neural networks that are critical in mammalian\\nvision. The brain does not work solely in a feed-forward fashion, but rather\\nall of the neurons are in competition with each other; neurons are integrating\\ninformation in a bottom up and top down fashion and incorporating expectation\\nand feedback in the modeling process. Furthermore, our visual cortex is working\\nin tandem with our parietal lobe, integrating sensory information from various\\nmodalities.\\n  In our work, we sought to improve upon the standard feed-forward deep\\nlearning model by augmenting them with biologically inspired concepts of\\nsparsity, top-down feedback, and lateral inhibition. We define our model as a\\nsparse coding problem using hierarchical layers. We solve the sparse coding\\nproblem with an additional top-down feedback error driving the dynamics of the\\nneural network. While building and observing the behavior of our model, we were\\nfascinated that multimodal, invariant neurons naturally emerged that mimicked,\\n\"Halle Berry neurons\" found in the human brain. Furthermore, our sparse\\nrepresentation of multimodal signals demonstrates qualitative and quantitative\\nsuperiority to the standard feed-forward joint embedding in common vision and\\nmachine learning tasks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.07998v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.07998v2.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Edward Kim', 'Darryl Hannan', 'Garrett Kenyon'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-11-21',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/improving-optimization-for-models-with',\n",
       "  'arxiv_id': '1803.03234',\n",
       "  'title': 'Improving Optimization for Models With Continuous Symmetry Breaking',\n",
       "  'abstract': 'Many loss functions in representation learning are invariant under a\\ncontinuous symmetry transformation. For example, the loss function of word\\nembeddings (Mikolov et al., 2013) remains unchanged if we simultaneously rotate\\nall word and context embedding vectors. We show that representation learning\\nmodels for time series possess an approximate continuous symmetry that leads to\\nslow convergence of gradient descent. We propose a new optimization algorithm\\nthat speeds up convergence using ideas from gauge theory in physics. Our\\nalgorithm leads to orders of magnitude faster convergence and to more\\ninterpretable representations, as we show for dynamic extensions of matrix\\nfactorization and word embedding models. We further present an example\\napplication of our proposed algorithm that translates modern words into their\\nhistoric equivalents.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.03234v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.03234v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Robert Bamler', 'Stephan Mandt'],\n",
       "  'tasks': ['Representation Learning', 'Time Series', 'Word Embeddings'],\n",
       "  'date': '2018-03-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['movielens'],\n",
       "  'datasets_used_full': ['MovieLens'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/hierarchical-long-term-video-prediction',\n",
       "  'arxiv_id': '1806.04768',\n",
       "  'title': 'Hierarchical Long-term Video Prediction without Supervision',\n",
       "  'abstract': 'Much of recent research has been devoted to video prediction and generation,\\nyet most of the previous works have demonstrated only limited success in\\ngenerating videos on short-term horizons. The hierarchical video prediction\\nmethod by Villegas et al. (2017) is an example of a state-of-the-art method for\\nlong-term video prediction, but their method is limited because it requires\\nground truth annotation of high-level structures (e.g., human joint landmarks)\\nat training time. Our network encodes the input frame, predicts a high-level\\nencoding into the future, and then a decoder with access to the first frame\\nproduces the predicted image from the predicted encoding. The decoder also\\nproduces a mask that outlines the predicted foreground object (e.g., person) as\\na by-product. Unlike Villegas et al. (2017), we develop a novel training method\\nthat jointly trains the encoder, the predictor, and the decoder together\\nwithout highlevel supervision; we further improve upon this by using an\\nadversarial loss in the feature space to train the predictor. Our method can\\npredict about 20 seconds into the future and provides better results compared\\nto Denton and Fergus (2018) and Finn et al. (2016) on the Human 3.6M dataset.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04768v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04768v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Nevan Wichers',\n",
       "   'Ruben Villegas',\n",
       "   'Dumitru Erhan',\n",
       "   'Honglak Lee'],\n",
       "  'tasks': ['Video Prediction'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['coco', 'human3-6m'],\n",
       "  'datasets_used_full': ['COCO', 'Human3.6M'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-mixtures-of-linear-regressions-with',\n",
       "  'arxiv_id': '1802.07895',\n",
       "  'title': 'Learning Mixtures of Linear Regressions with Nearly Optimal Complexity',\n",
       "  'abstract': 'Mixtures of Linear Regressions (MLR) is an important mixture model with many applications. In this model, each observation is generated from one of the several unknown linear regression components, where the identity of the generated component is also unknown. Previous works either assume strong assumptions on the data distribution or have high complexity. This paper proposes a fixed parameter tractable algorithm for the problem under general conditions, which achieves global convergence and the sample complexity scales nearly linearly in the dimension. In particular, different from previous works that require the data to be from the standard Gaussian, the algorithm allows the data from Gaussians with different covariances. When the conditional number of the covariances and the number of components are fixed, the algorithm has nearly optimal sample complexity $N = \\\\tilde{O}(d)$ as well as nearly optimal computational complexity $\\\\tilde{O}(Nd)$, where $d$ is the dimension of the data space. To the best of our knowledge, this approach provides the first such recovery guarantee for this general setting.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1802.07895v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1802.07895v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yuanzhi Li', 'YIngyu Liang'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-22',\n",
       "  'methods': [{'name': 'Linear Regression',\n",
       "    'full_name': 'Linear Regression',\n",
       "    'description': '**Linear Regression** is a method for modelling a relationship between a dependent variable and independent variables. These models can be fit with numerous approaches. The most common is *least squares*, where we minimize the mean square error between the predicted values $\\\\hat{y} = \\\\textbf{X}\\\\hat{\\\\beta}$ and actual values $y$: $\\\\left(y-\\\\textbf{X}\\\\beta\\\\right)^{2}$.\\r\\n\\r\\nWe can also define the problem in probabilistic terms as a generalized linear model (GLM) where the pdf is a Gaussian distribution, and then perform maximum likelihood estimation to estimate $\\\\hat{\\\\beta}$.\\r\\n\\r\\nImage Source: [Wikipedia](https://en.wikipedia.org/wiki/Linear_regression)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Generalized Linear Models',\n",
       "     'description': '**Generalized Linear Models (GLMs)** are a class of models that generalize upon linear regression by allowing many more distributions to be modeled for the response variable via a link function. Below you can find a continuously updating list of GLMs.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fully-convolutional-network-for-melanoma',\n",
       "  'arxiv_id': '1806.04765',\n",
       "  'title': 'Fully Convolutional Network for Melanoma Diagnostics',\n",
       "  'abstract': \"This work seeks to determine how modern machine learning techniques may be\\napplied to the previously unexplored topic of melanoma diagnostics using\\ndigital pathology. We curated a new dataset of 50 patient cases of cutaneous\\nmelanoma using digital pathology. We provide gold standard annotations for\\nthree tissue types (tumour, epidermis, and dermis) which are important for the\\nprognostic measurements known as Breslow thickness and Clark level. Then, we\\ndevised a novel multi-stride fully convolutional network (FCN) architecture\\nthat outperformed other networks trained and evaluated using the same data\\naccording to standard metrics. Finally, we trained a model to detect and\\nlocalize the target tissue types. When processing previously unseen cases, our\\nmodel's output is qualitatively very similar to the gold standard. In addition\\nto the standard metrics computed as a baseline for our approach, we asked three\\nadditional pathologists to measure the Breslow thickness on the network's\\noutput. Their responses were diagnostically equivalent to the ground truth\\nmeasurements, and when removing cases where a measurement was not appropriate,\\ninter-rater reliability (IRR) between the four pathologists was 75.0%. Given\\nthe qualitative and quantitative results, it is possible to overcome the\\ndiscriminative challenges of the skin and tumour anatomy for segmentation using\\nmodern machine learning techniques, though more work is required to improve the\\nnetwork's performance on dermis segmentation. Further, we show that it is\\npossible to achieve a level of accuracy required to manually perform the\\nBreslow thickness measurement.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04765v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04765v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Adon Phillips', 'Iris Teo', 'Jochen Lang'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/inferno-inference-aware-neural-optimisation',\n",
       "  'arxiv_id': '1806.04743',\n",
       "  'title': 'INFERNO: Inference-Aware Neural Optimisation',\n",
       "  'abstract': 'Complex computer simulations are commonly required for accurate data\\nmodelling in many scientific disciplines, making statistical inference\\nchallenging due to the intractability of the likelihood evaluation for the\\nobserved data. Furthermore, sometimes one is interested on inference drawn over\\na subset of the generative model parameters while taking into account model\\nuncertainty or misspecification on the remaining nuisance parameters. In this\\nwork, we show how non-linear summary statistics can be constructed by\\nminimising inference-motivated losses via stochastic gradient descent such they\\nprovided the smallest uncertainty for the parameters of interest. As a use\\ncase, the problem of confidence interval estimation for the mixture coefficient\\nin a multi-dimensional two-component mixture model (i.e. signal vs background)\\nis considered, where the proposed technique clearly outperforms summary\\nstatistics based on probabilistic classification, which are a commonly used\\nalternative but do not account for the presence of nuisance parameters.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04743v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04743v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Pablo de Castro', 'Tommaso Dorigo'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/pricing-engine-estimating-causal-impacts-in',\n",
       "  'arxiv_id': '1806.03285',\n",
       "  'title': 'Pricing Engine: Estimating Causal Impacts in Real World Business Settings',\n",
       "  'abstract': 'We introduce the Pricing Engine package to enable the use of Double ML\\nestimation techniques in general panel data settings. Customization allows the\\nuser to specify first-stage models, first-stage featurization, second stage\\ntreatment selection and second stage causal-modeling. We also introduce a\\nDynamicDML class that allows the user to generate dynamic treatment-aware\\nforecasts at a range of leads and to understand how the forecasts will vary as\\na function of causally estimated treatment parameters. The Pricing Engine is\\nbuilt on Python 3.5 and can be run on an Azure ML Workbench environment with\\nthe addition of only a few Python packages. This note provides high-level\\ndiscussion of the Double ML method, describes the packages intended use and\\nincludes an example Jupyter notebook demonstrating application to some publicly\\navailable data. Installation of the package and additional technical\\ndocumentation is available at\\n$\\\\href{https://github.com/bquistorff/pricingengine}{github.com/bquistorff/pricingengine}$.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03285v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03285v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Matt Goldman', 'Brian Quistorff'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/delta-encoder-an-effective-sample-synthesis',\n",
       "  'arxiv_id': '1806.04734',\n",
       "  'title': 'Delta-encoder: an effective sample synthesis method for few-shot object recognition',\n",
       "  'abstract': 'Learning to classify new categories based on just one or a few examples is a\\nlong-standing challenge in modern computer vision. In this work, we proposes a\\nsimple yet effective method for few-shot (and one-shot) object recognition. Our\\napproach is based on a modified auto-encoder, denoted Delta-encoder, that\\nlearns to synthesize new samples for an unseen category just by seeing few\\nexamples from it. The synthesized samples are then used to train a classifier.\\nThe proposed approach learns to both extract transferable intra-class\\ndeformations, or \"deltas\", between same-class pairs of training examples, and\\nto apply those deltas to the few provided examples of a novel class (unseen\\nduring training) in order to efficiently synthesize samples from that new\\nclass. The proposed method improves over the state-of-the-art in one-shot\\nobject-recognition and compares favorably in the few-shot case. Upon acceptance\\ncode will be made available.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04734v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04734v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Eli Schwartz',\n",
       "   'Leonid Karlinsky',\n",
       "   'Joseph Shtok',\n",
       "   'Sivan Harary',\n",
       "   'Mattias Marder',\n",
       "   'Rogerio Feris',\n",
       "   'Abhishek Kumar',\n",
       "   'Raja Giryes',\n",
       "   'Alex M. Bronstein'],\n",
       "  'tasks': ['Few-Shot Image Classification',\n",
       "   'Few-Shot Learning',\n",
       "   'Object Recognition'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet',\n",
       "   'cifar-100',\n",
       "   'cub-200-2011',\n",
       "   'miniimagenet-1',\n",
       "   'caltech-256',\n",
       "   'awa2-1',\n",
       "   'apy'],\n",
       "  'datasets_used_full': ['ImageNet',\n",
       "   'CIFAR-100',\n",
       "   'CUB-200-2011',\n",
       "   'miniImageNet',\n",
       "   'Caltech-256',\n",
       "   'AwA2',\n",
       "   'aPY'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/some-variations-on-ensembled-random-survival',\n",
       "  'arxiv_id': '1709.05515',\n",
       "  'title': 'Some variations on Ensembled Random Survival Forest with application to Cancer Research',\n",
       "  'abstract': 'In this paper we describe a novel implementation of adaboost for prediction\\nof survival function. We take different variations of the algorithm and compare\\nthe algorithms based on system run time and root mean square error. Our\\nconstruction includes right censoring data and competing risk data too. We take\\ndifferent data set to illustrate the performance of the algorithms.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1709.05515v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1709.05515v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Arabin Kumar Dey',\n",
       "   'Suhas N.',\n",
       "   'Talasila Sai Teja',\n",
       "   'Anshul Juneja'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-09-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/accurate-detection-of-inner-ears-in-head-cts',\n",
       "  'arxiv_id': '1806.04725',\n",
       "  'title': 'Accurate Detection of Inner Ears in Head CTs Using a Deep Volume-to-Volume Regression Network with False Positive Suppression and a Shape-Based Constraint',\n",
       "  'abstract': 'Cochlear implants (CIs) are neural prosthetics which are used to treat\\npatients with hearing loss. CIs use an array of electrodes which are surgically\\ninserted into the cochlea to stimulate the auditory nerve endings. After\\nsurgery, CIs need to be programmed. Studies have shown that the spatial\\nrelationship between the intra-cochlear anatomy and electrodes derived from\\nmedical images can guide CI programming and lead to significant improvement in\\nhearing outcomes. However, clinical head CT images are usually obtained from\\nscanners of different brands with different protocols. The field of view thus\\nvaries greatly and visual inspection is needed to document their content prior\\nto applying algorithms for electrode localization and intra-cochlear anatomy\\nsegmentation. In this work, to determine the presence/absence of inner ears and\\nto accurately localize them in head CTs, we use a volume-to-volume\\nconvolutional neural network which can be trained end-to-end to map a raw CT\\nvolume to probability maps which indicate inner ear positions. We incorporate a\\nfalse positive suppression strategy in training and apply a shape-based\\nconstraint. We achieve a labeling accuracy of 98.59% and a localization error\\nof 2.45mm. The localization error is significantly smaller than a random\\nforest-based approach that has been proposed recently to perform the same task.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04725v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04725v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Dongqing Zhang',\n",
       "   'Jianing Wang',\n",
       "   'Jack H. Noble',\n",
       "   'Benoit M. Dawant'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/improving-exploration-in-evolution-strategies',\n",
       "  'arxiv_id': '1712.06560',\n",
       "  'title': 'Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents',\n",
       "  'abstract': 'Evolution strategies (ES) are a family of black-box optimization algorithms\\nable to train deep neural networks roughly as well as Q-learning and policy\\ngradient methods on challenging deep reinforcement learning (RL) problems, but\\nare much faster (e.g. hours vs. days) because they parallelize better. However,\\nmany RL problems require directed exploration because they have reward\\nfunctions that are sparse or deceptive (i.e. contain local optima), and it is\\nunknown how to encourage such exploration with ES. Here we show that algorithms\\nthat have been invented to promote directed exploration in small-scale evolved\\nneural networks via populations of exploring agents, specifically novelty\\nsearch (NS) and quality diversity (QD) algorithms, can be hybridized with ES to\\nimprove its performance on sparse or deceptive deep RL tasks, while retaining\\nscalability. Our experiments confirm that the resultant new algorithms, NS-ES\\nand two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES\\nto achieve higher performance on Atari and simulated robots learning to walk\\naround a deceptive trap. This paper thus introduces a family of fast, scalable\\nalgorithms for reinforcement learning that are capable of directed exploration.\\nIt also adds this new family of exploration algorithms to the RL toolbox and\\nraises the interesting possibility that analogous algorithms with multiple\\nsimultaneous paths of exploration might also combine well with existing RL\\nalgorithms outside ES.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.06560v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.06560v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Edoardo Conti',\n",
       "   'Vashisht Madhavan',\n",
       "   'Felipe Petroski Such',\n",
       "   'Joel Lehman',\n",
       "   'Kenneth O. Stanley',\n",
       "   'Jeff Clune'],\n",
       "  'tasks': ['Policy Gradient Methods', 'Q-Learning'],\n",
       "  'date': '2017-12-18',\n",
       "  'methods': [{'name': 'Q-Learning',\n",
       "    'full_name': 'Q-Learning',\n",
       "    'description': '**Q-Learning** is an off-policy temporal difference control algorithm:\\r\\n\\r\\n$$Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) \\\\leftarrow Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) + \\\\alpha\\\\left[R_{t+1} + \\\\gamma\\\\max\\\\_{a}Q\\\\left(S\\\\_{t+1}, a\\\\right) - Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right)\\\\right] $$\\r\\n\\r\\nThe learned action-value function $Q$ directly approximates $q\\\\_{*}$, the optimal action-value function, independent of the policy being followed.\\r\\n\\r\\nSource: Sutton and Barto, Reinforcement Learning, 2nd Edition',\n",
       "    'introduced_year': 1984,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Off-Policy TD Control',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Reinforcement Learning'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/weakly-supervised-semantic-segmentation-by',\n",
       "  'arxiv_id': '1806.04659',\n",
       "  'title': 'Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features',\n",
       "  'abstract': 'Weakly-supervised semantic segmentation under image tags supervision is a\\nchallenging task as it directly associates high-level semantic to low-level\\nappearance. To bridge this gap, in this paper, we propose an iterative\\nbottom-up and top-down framework which alternatively expands object regions and\\noptimizes segmentation network. We start from initial localization produced by\\nclassification networks. While classification networks are only responsive to\\nsmall and coarse discriminative object regions, we argue that, these regions\\ncontain significant common features about objects. So in the bottom-up step, we\\nmine common object features from the initial localization and expand object\\nregions with the mined features. To supplement non-discriminative regions,\\nsaliency maps are then considered under Bayesian framework to refine the object\\nregions. Then in the top-down step, the refined object regions are used as\\nsupervision to train the segmentation network and to predict object masks.\\nThese object masks provide more accurate localization and contain more regions\\nof object. Further, we take these object masks as initial localization and mine\\ncommon object features from them. These processes are conducted iteratively to\\nprogressively produce fine object masks and optimize segmentation networks.\\nExperimental results on Pascal VOC 2012 dataset demonstrate that the proposed\\nmethod outperforms previous state-of-the-art methods by a large margin.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04659v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04659v1.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Xiang Wang', 'ShaoDi You', 'Xi Li', 'Huimin Ma'],\n",
       "  'tasks': ['General Classification',\n",
       "   'Semantic Segmentation',\n",
       "   'Weakly-Supervised Semantic Segmentation'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-question-answering-framework-for-plots',\n",
       "  'arxiv_id': '1806.04655',\n",
       "  'title': 'FigureNet: A Deep Learning model for Question-Answering on Scientific Plots',\n",
       "  'abstract': 'Deep Learning has managed to push boundaries in a wide variety of tasks. One\\narea of interest is to tackle problems in reasoning and understanding, with an\\naim to emulate human intelligence. In this work, we describe a deep learning\\nmodel that addresses the reasoning task of question-answering on categorical\\nplots. We introduce a novel architecture FigureNet, that learns to identify\\nvarious plot elements, quantify the represented values and determine a relative\\nordering of these statistical values. We test our model on the FigureQA dataset\\nwhich provides images and accompanying questions for scientific plots like bar\\ngraphs and pie charts, augmented with rich annotations. Our approach\\noutperforms the state-of-the-art Relation Networks baseline by approximately\\n$7\\\\%$ on this dataset, with a training time that is over an order of magnitude\\nlesser.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04655v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04655v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Revanth Reddy',\n",
       "   'Rahul Ramesh',\n",
       "   'Ameet Deshpande',\n",
       "   'Mitesh M. Khapra'],\n",
       "  'tasks': ['Question Answering'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['figureqa'],\n",
       "  'datasets_used_full': ['FigureQA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/disentangled-sequential-autoencoder',\n",
       "  'arxiv_id': '1803.02991',\n",
       "  'title': 'Disentangled Sequential Autoencoder',\n",
       "  'abstract': 'We present a VAE architecture for encoding and generating high dimensional\\nsequential data, such as video or audio. Our deep generative model learns a\\nlatent representation of the data which is split into a static and dynamic\\npart, allowing us to approximately disentangle latent time-dependent features\\n(dynamics) from features which are preserved over time (content). This\\narchitecture gives us partial control over generating content and dynamics by\\nconditioning on either one of these sets of features. In our experiments on\\nartificially generated cartoon video clips and voice recordings, we show that\\nwe can convert the content of a given sequence into another one by such content\\nswapping. For audio, this allows us to convert a male speaker into a female\\nspeaker and vice versa, while for video we can separately manipulate shapes and\\ndynamics. Furthermore, we give empirical evidence for the hypothesis that\\nstochastic RNNs as latent state models are more efficient at compressing and\\ngenerating long sequences than deterministic ones, which may be relevant for\\napplications in video compression.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.02991v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.02991v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Yingzhen Li', 'Stephan Mandt'],\n",
       "  'tasks': ['Video Compression'],\n",
       "  'date': '2018-03-08',\n",
       "  'methods': [{'name': 'VAE',\n",
       "    'full_name': 'Variational Autoencoder',\n",
       "    'description': 'A **Variational Autoencoder** is a type of likelihood-based generative model. It consists of an encoder, that takes in data $x$ as input and transforms this into a latent representation $z$,  and a decoder, that takes a latent representation $z$ and returns a reconstruction $\\\\hat{x}$. Inference is performed via variational inference to approximate the posterior of the model.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.6114v10',\n",
       "    'source_title': 'Auto-Encoding Variational Bayes',\n",
       "    'code_snippet_url': 'https://github.com/AntixK/PyTorch-VAE/blob/8700d245a9735640dda458db4cf40708caf2e77f/models/vanilla_vae.py#L8',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['sprites'],\n",
       "  'datasets_used_full': ['Sprites'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-to-estimate-indoor-lighting-from-3d',\n",
       "  'arxiv_id': '1806.03994',\n",
       "  'title': 'Learning to Estimate Indoor Lighting from 3D Objects',\n",
       "  'abstract': 'In this work, we propose a step towards a more accurate prediction of the\\nenvironment light given a single picture of a known object. To achieve this, we\\ndeveloped a deep learning method that is able to encode the latent space of\\nindoor lighting using few parameters and that is trained on a database of\\nenvironment maps. This latent space is then used to generate predictions of the\\nlight that are both more realistic and accurate than previous methods. To\\nachieve this, our first contribution is a deep autoencoder which is capable of\\nlearning the feature space that compactly models lighting. Our second\\ncontribution is a convolutional neural network that predicts the light from a\\nsingle image of a known object. To train these networks, our third contribution\\nis a novel dataset that contains 21,000 HDR indoor environment maps. The\\nresults indicate that the predictor can generate plausible lighting estimations\\neven from diffuse objects.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03994v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03994v3.pdf',\n",
       "  'proceeding': '3DV 2018 - International Conference on 3D Vision 2018 9',\n",
       "  'authors': ['Henrique Weber', 'Donald Prévost', 'Jean-François Lalonde'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [{'name': 'AutoEncoder',\n",
       "    'full_name': 'AutoEncoder',\n",
       "    'description': 'An **Autoencoder** is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then performs a reconstruction of the input with this latent code (the decoder).\\r\\n\\r\\nImage: [Michael Massi](https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://science.sciencemag.org/content/313/5786/504',\n",
       "    'source_title': 'Reducing the Dimensionality of Data with Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/L1aoXingyu/pytorch-beginner/blob/9c86be785c7c318a09cf29112dd1f1a58613239b/08-AutoEncoder/simple_autoencoder.py#L38',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/boosting-black-box-variational-inference',\n",
       "  'arxiv_id': '1806.02185',\n",
       "  'title': 'Boosting Black Box Variational Inference',\n",
       "  'abstract': 'Approximating a probability density in a tractable manner is a central task\\nin Bayesian statistics. Variational Inference (VI) is a popular technique that\\nachieves tractability by choosing a relatively simple variational family.\\nBorrowing ideas from the classic boosting framework, recent approaches attempt\\nto \\\\emph{boost} VI by replacing the selection of a single density with a\\ngreedily constructed mixture of densities. In order to guarantee convergence,\\nprevious works impose stringent assumptions that require significant effort for\\npractitioners. Specifically, they require a custom implementation of the greedy\\nstep (called the LMO) for every probabilistic model with respect to an\\nunnatural variational family of truncated distributions. Our work fixes these\\nissues with novel theoretical and algorithmic insights. On the theoretical\\nside, we show that boosting VI satisfies a relaxed smoothness assumption which\\nis sufficient for the convergence of the functional Frank-Wolfe (FW) algorithm.\\nFurthermore, we rephrase the LMO problem and propose to maximize the Residual\\nELBO (RELBO) which replaces the standard ELBO optimization in VI. These\\ntheoretical enhancements allow for black box implementation of the boosting\\nsubroutine. Finally, we present a stopping criterion drawn from the duality gap\\nin the classic FW analyses and exhaustive experiments to illustrate the\\nusefulness of our theoretical and algorithmic contributions.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.02185v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.02185v5.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Francesco Locatello',\n",
       "   'Gideon Dresdner',\n",
       "   'Rajiv Khanna',\n",
       "   'Isabel Valera',\n",
       "   'Gunnar Rätsch'],\n",
       "  'tasks': ['Variational Inference'],\n",
       "  'date': '2018-06-06',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/adversarial-attacks-on-variational',\n",
       "  'arxiv_id': '1806.04646',\n",
       "  'title': 'Adversarial Attacks on Variational Autoencoders',\n",
       "  'abstract': \"Adversarial attacks are malicious inputs that derail machine-learning models.\\nWe propose a scheme to attack autoencoders, as well as a quantitative\\nevaluation framework that correlates well with the qualitative assessment of\\nthe attacks. We assess --- with statistically validated experiments --- the\\nresistance to attacks of three variational autoencoders (simple, convolutional,\\nand DRAW) in three datasets (MNIST, SVHN, CelebA), showing that both DRAW's\\nrecurrence and attention mechanism lead to better resistance. As autoencoders\\nare proposed for compressing data --- a scenario in which their safety is\\nparamount --- we expect more attention will be given to adversarial attacks on\\nthem.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04646v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04646v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['George Gondim-Ribeiro', 'Pedro Tabacof', 'Eduardo Valle'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['svhn', 'celeba'],\n",
       "  'datasets_used_full': ['SVHN', 'CelebA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/accelerating-imitation-learning-with',\n",
       "  'arxiv_id': '1806.04642',\n",
       "  'title': 'Accelerating Imitation Learning with Predictive Models',\n",
       "  'abstract': 'Sample efficiency is critical in solving real-world reinforcement learning\\nproblems, where agent-environment interactions can be costly. Imitation\\nlearning from expert advice has proved to be an effective strategy for reducing\\nthe number of interactions required to train a policy. Online imitation\\nlearning, which interleaves policy evaluation and policy optimization, is a\\nparticularly effective technique with provable performance guarantees. In this\\nwork, we seek to further accelerate the convergence rate of online imitation\\nlearning, thereby making it more sample efficient. We propose two model-based\\nalgorithms inspired by Follow-the-Leader (FTL) with prediction: MoBIL-VI based\\non solving variational inequalities and MoBIL-Prox based on stochastic\\nfirst-order updates. These two methods leverage a model to predict future\\ngradients to speed up policy learning. When the model oracle is learned online,\\nthese algorithms can provably accelerate the best known convergence rate up to\\nan order. Our algorithms can be viewed as a generalization of stochastic\\nMirror-Prox (Juditsky et al., 2011), and admit a simple constructive FTL-style\\nanalysis of performance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04642v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04642v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ching-An Cheng',\n",
       "   'Xinyan Yan',\n",
       "   'Evangelos A. Theodorou',\n",
       "   'Byron Boots'],\n",
       "  'tasks': ['Imitation Learning'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/unsupervised-meta-learning-for-reinforcement',\n",
       "  'arxiv_id': '1806.04640',\n",
       "  'title': 'Unsupervised Meta-Learning for Reinforcement Learning',\n",
       "  'abstract': 'Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated. In this work, we take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approach. Our conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learners. Our experimental results indicate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design and these procedures exceed the performance of learning from scratch.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04640v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04640v3.pdf',\n",
       "  'proceeding': 'ICLR 2020 1',\n",
       "  'authors': ['Abhishek Gupta',\n",
       "   'Benjamin Eysenbach',\n",
       "   'Chelsea Finn',\n",
       "   'Sergey Levine'],\n",
       "  'tasks': ['Meta-Learning',\n",
       "   'Meta Reinforcement Learning',\n",
       "   'Multi-Task Learning'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/measures-of-tractography-convergence',\n",
       "  'arxiv_id': '1806.04634',\n",
       "  'title': 'Measures of Tractography Convergence',\n",
       "  'abstract': 'In the present work, we use information theory to understand the empirical\\nconvergence rate of tractography, a widely-used approach to reconstruct\\nanatomical fiber pathways in the living brain. Based on diffusion MRI data,\\ntractography is the starting point for many methods to study brain\\nconnectivity. Of the available methods to perform tractography, most\\nreconstruct a finite set of streamlines, or 3D curves, representing probable\\nconnections between anatomical regions, yet relatively little is known about\\nhow the sampling of this set of streamlines affects downstream results, and how\\nexhaustive the sampling should be. Here we provide a method to measure the\\ninformation theoretic surprise (self-cross entropy) for tract sampling schema.\\nWe then empirically assess four streamline methods. We demonstrate that the\\nrelative information gain is very low after a moderate number of streamlines\\nhave been generated for each tested method. The results give rise to several\\nguidelines for optimal sampling in brain connectivity analyses.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04634v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04634v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Daniel Moyer', 'Paul M. Thompson', 'Greg Ver Steeg'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/organizing-experience-a-deeper-look-at-replay',\n",
       "  'arxiv_id': '1806.04624',\n",
       "  'title': 'Organizing Experience: A Deeper Look at Replay Mechanisms for Sample-based Planning in Continuous State Domains',\n",
       "  'abstract': 'Model-based strategies for control are critical to obtain sample efficient\\nlearning. Dyna is a planning paradigm that naturally interleaves learning and\\nplanning, by simulating one-step experience to update the action-value\\nfunction. This elegant planning strategy has been mostly explored in the\\ntabular setting. The aim of this paper is to revisit sample-based planning, in\\nstochastic and continuous domains with learned models. We first highlight the\\nflexibility afforded by a model over Experience Replay (ER). Replay-based\\nmethods can be seen as stochastic planning methods that repeatedly sample from\\na buffer of recent agent-environment interactions and perform updates to\\nimprove data efficiency. We show that a model, as opposed to a replay buffer,\\nis particularly useful for specifying which states to sample from during\\nplanning, such as predecessor states that propagate information in reverse from\\na state more quickly. We introduce a semi-parametric model learning approach,\\ncalled Reweighted Experience Models (REMs), that makes it simple to sample next\\nstates or predecessors. We demonstrate that REM-Dyna exhibits similar\\nadvantages over replay-based methods in learning in continuous state problems,\\nand that the performance gap grows when moving to stochastic domains, of\\nincreasing size.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04624v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04624v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yangchen Pan',\n",
       "   'Muhammad Zaheer',\n",
       "   'Adam White',\n",
       "   'Andrew Patterson',\n",
       "   'Martha White'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [{'name': 'Experience Replay',\n",
       "    'full_name': 'Experience Replay',\n",
       "    'description': '**Experience Replay** is a replay memory technique used in reinforcement learning where we store the agent’s experiences at each time-step, $e\\\\_{t} = \\\\left(s\\\\_{t}, a\\\\_{t}, r\\\\_{t}, s\\\\_{t+1}\\\\right)$ in a data-set $D = e\\\\_{1}, \\\\cdots, e\\\\_{N}$ , pooled over many episodes into a replay memory. We then usually sample the memory randomly for a minibatch of experience, and use this to learn off-policy, as with Deep Q-Networks. This tackles the problem of autocorrelation leading to unstable training, by making the problem more like a supervised learning problem.\\r\\n\\r\\nImage Credit: [Hands-On Reinforcement Learning with Python, Sudharsan Ravichandiran](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781788836524)',\n",
       "    'introduced_year': 1993,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Replay Memory',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Reinforcement Learning'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fast-forwarding-egocentric-videos-by',\n",
       "  'arxiv_id': '1806.04620',\n",
       "  'title': 'Fast forwarding Egocentric Videos by Listening and Watching',\n",
       "  'abstract': 'The remarkable technological advance in well-equipped wearable devices is\\npushing an increasing production of long first-person videos. However, since\\nmost of these videos have long and tedious parts, they are forgotten or never\\nseen. Despite a large number of techniques proposed to fast-forward these\\nvideos by highlighting relevant moments, most of them are image based only.\\nMost of these techniques disregard other relevant sensors present in the\\ncurrent devices such as high-definition microphones. In this work, we propose a\\nnew approach to fast-forward videos using psychoacoustic metrics extracted from\\nthe soundtrack. These metrics can be used to estimate the annoyance of a\\nsegment allowing our method to emphasize moments of sound pleasantness. The\\nefficiency of our method is demonstrated through qualitative results and\\nquantitative results as far as of speed-up and instability are concerned.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04620v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04620v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Vinicius S. Furlan', 'Ruzena Bajcsy', 'Erickson R. Nascimento'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['domsev'],\n",
       "  'datasets_used_full': ['DoMSEV'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/imperfect-segmentation-labels-how-much-do',\n",
       "  'arxiv_id': '1806.04618',\n",
       "  'title': 'Imperfect Segmentation Labels: How Much Do They Matter?',\n",
       "  'abstract': 'Labeled datasets for semantic segmentation are imperfect, especially in\\nmedical imaging where borders are often subtle or ill-defined. Little work has\\nbeen done to analyze the effect that label errors have on the performance of\\nsegmentation methodologies. Here we present a large-scale study of model\\nperformance in the presence of varying types and degrees of error in training\\ndata. We trained U-Net, SegNet, and FCN32 several times for liver segmentation\\nwith 10 different modes of ground-truth perturbation. Our results show that for\\neach architecture, performance steadily declines with boundary-localized\\nerrors, however, U-Net was significantly more robust to jagged boundary errors\\nthan the other architectures. We also found that each architecture was very\\nrobust to non-boundary-localized errors, suggesting that boundary-localized\\nerrors are fundamentally different and more challenging problem than random\\nlabel errors in a classification setting.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04618v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04618v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Nicholas Heller', 'Joshua Dean', 'Nikolaos Papanikolopoulos'],\n",
       "  'tasks': ['Liver Segmentation', 'Semantic Segmentation'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [{'name': 'Concatenated Skip Connection',\n",
       "    'full_name': 'Concatenated Skip Connection',\n",
       "    'description': 'A **Concatenated Skip Connection** is a type of skip connection that seeks to reuse features by concatenating them to new layers, allowing more information to be retained from previous layers of the network. This contrasts with say, residual connections, where element-wise summation is used instead to incorporate information from previous layers. This type of skip connection is prominently used in DenseNets (and also Inception networks), which the Figure to the right illustrates.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/densenet.py#L113',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'U-Net',\n",
       "    'full_name': 'U-Net',\n",
       "    'description': '**U-Net** is an architecture for semantic segmentation. It consists of a contracting path and an expansive path. The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit ([ReLU](https://paperswithcode.com/method/relu)) and a 2x2 [max pooling](https://paperswithcode.com/method/max-pooling) operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 [convolution](https://paperswithcode.com/method/convolution) (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a [1x1 convolution](https://paperswithcode.com/method/1x1-convolution) is used to map each 64-component feature vector to the desired number of classes. In total the network has 23 convolutional layers.\\r\\n\\r\\n[Original MATLAB Code](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-release-2015-10-02.tar.gz)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1505.04597v1',\n",
       "    'source_title': 'U-Net: Convolutional Networks for Biomedical Image Segmentation',\n",
       "    'code_snippet_url': 'https://github.com/milesial/Pytorch-UNet/blob/67bf11b4db4c5f2891bd7e8e7f58bcde8ee2d2db/unet/unet_model.py#L8',\n",
       "    'main_collection': {'name': 'Semantic Segmentation Models',\n",
       "     'description': '**Semantic Segmentation Models** are a class of methods that address the task of semantically segmenting an image into different object classes. Below you can find a continuously updating list of semantic segmentation models. ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Kaiming Initialization',\n",
       "    'full_name': 'Kaiming Initialization',\n",
       "    'description': '**Kaiming Initialization**, or **He Initialization**, is an initialization method for neural networks that takes into account the non-linearity of activation functions, such as [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nA proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. Using a derivation they work out that the condition to stop this happening is:\\r\\n\\r\\n$$\\\\frac{1}{2}n\\\\_{l}\\\\text{Var}\\\\left[w\\\\_{l}\\\\right] = 1 $$\\r\\n\\r\\nThis implies an initialization scheme of:\\r\\n\\r\\n$$ w\\\\_{l} \\\\sim \\\\mathcal{N}\\\\left(0,  2/n\\\\_{l}\\\\right)$$\\r\\n\\r\\nThat is, a zero-centered Gaussian with standard deviation of $\\\\sqrt{2/{n}\\\\_{l}}$ (variance shown in equation above). Biases are initialized at $0$.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.01852v1',\n",
       "    'source_title': 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/0adb5843766092fba584791af76383125fd0d01c/torch/nn/init.py#L389',\n",
       "    'main_collection': {'name': 'Initialization',\n",
       "     'description': '**Initialization** methods are used to initialize the weights in a neural network. Below can you find a continuously updating list of initialization methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Batch Normalization',\n",
       "    'full_name': 'Batch Normalization',\n",
       "    'description': '**Batch Normalization** aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for [Dropout](https://paperswithcode.com/method/dropout).\\r\\n\\r\\nWe apply a batch normalization layer as follows for a minibatch $\\\\mathcal{B}$:\\r\\n\\r\\n$$ \\\\mu\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}x\\\\_{i} $$\\r\\n\\r\\n$$ \\\\sigma^{2}\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}\\\\left(x\\\\_{i}-\\\\mu\\\\_{\\\\mathcal{B}}\\\\right)^{2} $$\\r\\n\\r\\n$$ \\\\hat{x}\\\\_{i} = \\\\frac{x\\\\_{i} - \\\\mu\\\\_{\\\\mathcal{B}}}{\\\\sqrt{\\\\sigma^{2}\\\\_{\\\\mathcal{B}}+\\\\epsilon}} $$\\r\\n\\r\\n$$ y\\\\_{i} = \\\\gamma\\\\hat{x}\\\\_{i} + \\\\beta = \\\\text{BN}\\\\_{\\\\gamma, \\\\beta}\\\\left(x\\\\_{i}\\\\right) $$\\r\\n\\r\\nWhere $\\\\gamma$ and $\\\\beta$ are learnable parameters.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.03167v3',\n",
       "    'source_title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/36f91261099b00194922bd93ed1286fe1c199724/jax/experimental/stax.py#L116',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'SegNet',\n",
       "    'full_name': 'SegNet',\n",
       "    'description': '**SegNet** is a semantic segmentation model. This core trainable segmentation architecture consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the\\r\\nVGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature maps. Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to\\r\\nperform non-linear upsampling.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1511.00561v3',\n",
       "    'source_title': 'SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation',\n",
       "    'code_snippet_url': 'https://github.com/yassouali/pytorch_segmentation/blob/8b8e3ee20a3aa733cb19fc158ad5d7773ed6da7f/models/segnet.py#L9',\n",
       "    'main_collection': {'name': 'Semantic Segmentation Models',\n",
       "     'description': '**Semantic Segmentation Models** are a class of methods that address the task of semantically segmenting an image into different object classes. Below you can find a continuously updating list of semantic segmentation models. ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-learning-to-detect-redundant-method',\n",
       "  'arxiv_id': '1806.04616',\n",
       "  'title': 'Deep Learning to Detect Redundant Method Comments',\n",
       "  'abstract': \"Comments in software are critical for maintenance and reuse. But apart from\\nprescriptive advice, there is little practical support or quantitative\\nunderstanding of what makes a comment useful. In this paper, we introduce the\\ntask of identifying comments which are uninformative about the code they are\\nmeant to document. To address this problem, we introduce the notion of comment\\nentailment from code, high entailment indicating that a comment's natural\\nlanguage semantics can be inferred directly from the code. Although not all\\nentailed comments are low quality, comments that are too easily inferred, for\\nexample, comments that restate the code, are widely discouraged by authorities\\non software style. Based on this, we develop a tool called CRAIC which scores\\nmethod-level comments for redundancy. Highly redundant comments can then be\\nexpanded or alternately removed by the developer. CRAIC uses deep language\\nmodels to exploit large software corpora without requiring expensive manual\\nannotations of entailment. We show that CRAIC can perform the comment\\nentailment task with good agreement with human judgements. Our findings also\\nhave implications for documentation tools. For example, we find that common\\ntags in Javadoc are at least two times more predictable from code than\\nnon-Javadoc sentences, suggesting that Javadoc tags are less informative than\\nmore free-form comments\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04616v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04616v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Annie Louis',\n",
       "   'Santanu Kumar Dash',\n",
       "   'Earl T. Barr',\n",
       "   'Charles Sutton'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/improving-regression-performance-with',\n",
       "  'arxiv_id': '1806.04613',\n",
       "  'title': 'Improving Regression Performance with Distributional Losses',\n",
       "  'abstract': 'There is growing evidence that converting targets to soft targets in\\nsupervised learning can provide considerable gains in performance. Much of this\\nwork has considered classification, converting hard zero-one values to soft\\nlabels---such as by adding label noise, incorporating label ambiguity or using\\ndistillation. In parallel, there is some evidence from a regression setting in\\nreinforcement learning that learning distributions can improve performance. In\\nthis work, we investigate the reasons for this improvement, in a regression\\nsetting. We introduce a novel distributional regression loss, and similarly\\nfind it significantly improves prediction accuracy. We investigate several\\ncommon hypotheses, around reducing overfitting and improved representations. We\\ninstead find evidence for an alternative hypothesis: this loss is easier to\\noptimize, with better behaved gradients, resulting in improved generalization.\\nWe provide theoretical support for this alternative hypothesis, by\\ncharacterizing the norm of the gradients of this loss.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04613v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04613v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Ehsan Imani', 'Martha White'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-novel-bayesian-approach-for-latent-variable',\n",
       "  'arxiv_id': '1806.04610',\n",
       "  'title': 'A Novel Bayesian Approach for Latent Variable Modeling from Mixed Data with Missing Values',\n",
       "  'abstract': \"We consider the problem of learning parameters of latent variable models from\\nmixed (continuous and ordinal) data with missing values. We propose a novel\\nBayesian Gaussian copula factor (BGCF) approach that is consistent under\\ncertain conditions and that is quite robust to the violations of these\\nconditions. In simulations, BGCF substantially outperforms two state-of-the-art\\nalternative approaches. An illustration on the `Holzinger & Swineford 1939'\\ndataset indicates that BGCF is favorable over the so-called robust maximum\\nlikelihood (MLR) even if the data match the assumptions of MLR.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04610v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04610v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ruifei Cui', 'Ioan Gabriel Bucur', 'Perry Groot', 'Tom Heskes'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/streaming-pca-and-subspace-tracking-the',\n",
       "  'arxiv_id': '1806.04609',\n",
       "  'title': 'Streaming PCA and Subspace Tracking: The Missing Data Case',\n",
       "  'abstract': 'For many modern applications in science and engineering, data are collected\\nin a streaming fashion carrying time-varying information, and practitioners\\nneed to process them with a limited amount of memory and computational\\nresources in a timely manner for decision making. This often is coupled with\\nthe missing data problem, such that only a small fraction of data attributes\\nare observed. These complications impose significant, and unconventional,\\nconstraints on the problem of streaming Principal Component Analysis (PCA) and\\nsubspace tracking, which is an essential building block for many inference\\ntasks in signal processing and machine learning. This survey article reviews a\\nvariety of classical and recent algorithms for solving this problem with low\\ncomputational and memory complexities, particularly those applicable in the big\\ndata regime with missing data. We illustrate that streaming PCA and subspace\\ntracking algorithms can be understood through algebraic and geometric\\nperspectives, and they need to be adjusted carefully to handle missing data.\\nBoth asymptotic and non-asymptotic convergence guarantees are reviewed.\\nFinally, we benchmark the performance of several competitive algorithms in the\\npresence of missing data for both well-conditioned and ill-conditioned systems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04609v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04609v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Laura Balzano', 'Yuejie Chi', 'Yue M. Lu'],\n",
       "  'tasks': ['Decision Making'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [{'name': 'PCA',\n",
       "    'full_name': 'Principal Components Analysis',\n",
       "    'description': '**Principle Components Analysis (PCA)** is an unsupervised method primary used for dimensionality reduction within machine learning.  PCA is calculated via a singular value decomposition (SVD) of the design matrix, or alternatively, by calculating the covariance matrix of the data and performing eigenvalue decomposition on the covariance matrix. The results of PCA provide a low-dimensional picture of the structure of the data and the leading (uncorrelated) latent factors determining variation in the data.\\r\\n\\r\\nImage Source: [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#/media/File:GaussianScatterPCA.svg)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Dimensionality Reduction',\n",
       "     'description': '**Dimensionality Reduction** methods transform data from a high-dimensional space into a low-dimensional space so that the low-dimensional space retains the most important properties of the original data. Below you can find a continuously updating list of dimensionality reduction methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/knowledge-distillation-by-on-the-fly-native',\n",
       "  'arxiv_id': '1806.04606',\n",
       "  'title': 'Knowledge Distillation by On-the-Fly Native Ensemble',\n",
       "  'abstract': 'Knowledge distillation is effective to train small and generalisable network\\nmodels for meeting the low-memory and fast running requirements. Existing\\noffline distillation methods rely on a strong pre-trained teacher, which\\nenables favourable knowledge discovery and transfer but requires a complex\\ntwo-phase training procedure. Online counterparts address this limitation at\\nthe price of lacking a highcapacity teacher. In this work, we present an\\nOn-the-fly Native Ensemble (ONE) strategy for one-stage online distillation.\\nSpecifically, ONE trains only a single multi-branch network while\\nsimultaneously establishing a strong teacher on-the- fly to enhance the\\nlearning of target network. Extensive evaluations show that ONE improves the\\ngeneralisation performance a variety of deep neural networks more significantly\\nthan alternative methods on four image classification dataset: CIFAR10,\\nCIFAR100, SVHN, and ImageNet, whilst having the computational efficiency\\nadvantages.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04606v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04606v2.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Xu Lan', 'Xiatian Zhu', 'Shaogang Gong'],\n",
       "  'tasks': ['Image Classification', 'Knowledge Distillation'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'cifar-100'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'CIFAR-100'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multiview-two-task-recursive-attention-model',\n",
       "  'arxiv_id': '1806.04597',\n",
       "  'title': 'Multiview Two-Task Recursive Attention Model for Left Atrium and Atrial Scars Segmentation',\n",
       "  'abstract': 'Late Gadolinium Enhanced Cardiac MRI (LGE-CMRI) for detecting atrial scars in\\natrial fibrillation (AF) patients has recently emerged as a promising technique\\nto stratify patients, guide ablation therapy and predict treatment success.\\nVisualisation and quantification of scar tissues require a segmentation of both\\nthe left atrium (LA) and the high intensity scar regions from LGE-CMRI images.\\nThese two segmentation tasks are challenging due to the cancelling of healthy\\ntissue signal, low signal-to-noise ratio and often limited image quality in\\nthese patients. Most approaches require manual supervision and/or a second\\nbright-blood MRI acquisition for anatomical segmentation. Segmenting both the\\nLA anatomy and the scar tissues automatically from a single LGE-CMRI\\nacquisition is highly in demand. In this study, we proposed a novel fully\\nautomated multiview two-task (MVTT) recursive attention model working directly\\non LGE-CMRI images that combines a sequential learning and a dilated residual\\nlearning to segment the LA (including attached pulmonary veins) and delineate\\nthe atrial scars simultaneously via an innovative attention model. Compared to\\nother state-of-the-art methods, the proposed MVTT achieves compelling\\nimprovement, enabling to generate a patient-specific anatomical and atrial scar\\nassessment model.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04597v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04597v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jun Chen',\n",
       "   'Guang Yang',\n",
       "   'Zhifan Gao',\n",
       "   'Hao Ni',\n",
       "   'Elsa Angelini',\n",
       "   'Raad Mohiaddin',\n",
       "   'Tom Wong',\n",
       "   'Yanping Zhang',\n",
       "   'Xiuquan Du',\n",
       "   'Heye Zhang',\n",
       "   'Jennifer Keegan',\n",
       "   'David Firmin'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/exponential-weights-on-the-hypercube-in',\n",
       "  'arxiv_id': '1806.04594',\n",
       "  'title': 'Exponential Weights on the Hypercube in Polynomial Time',\n",
       "  'abstract': \"We study a general online linear optimization problem(OLO). At each round, a subset of objects from a fixed universe of $n$ objects is chosen, and a linear cost associated with the chosen subset is incurred. To measure the performance of our algorithms, we use the notion of regret which is the difference between the total cost incurred over all iterations and the cost of the best fixed subset in hindsight. We consider Full Information and Bandit feedback for this problem. This problem is equivalent to OLO on the $\\\\{0,1\\\\}^n$ hypercube. The Exp2 algorithm and its bandit variant are commonly used strategies for this problem. It was previously unknown if it is possible to run Exp2 on the hypercube in polynomial time. In this paper, we present a polynomial time algorithm called PolyExp for OLO on the hypercube. We show that our algorithm is equivalent Exp2 on $\\\\{0,1\\\\}^n$, Online Mirror Descent(OMD), Follow The Regularized Leader(FTRL) and Follow The Perturbed Leader(FTPL) algorithms. We show PolyExp achieves expected regret bound that is a factor of $\\\\sqrt{n}$ better than Exp2 in the full information setting under $L_\\\\infty$ adversarial losses. Because of the equivalence of these algorithms, this implies an improvement on Exp2's regret bound in full information. We also show matching regret lower bounds. Finally, we show how to use PolyExp on the $\\\\{-1,+1\\\\}^n$ hypercube, solving an open problem in Bubeck et al (COLT 2012).\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04594v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04594v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sudeep Raja Putta', 'Abhishek Shetty'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/using-inherent-structures-to-design-lean-2',\n",
       "  'arxiv_id': '1806.04577',\n",
       "  'title': 'Using Inherent Structures to design Lean 2-layer RBMs',\n",
       "  'abstract': 'Understanding the representational power of Restricted Boltzmann Machines\\n(RBMs) with multiple layers is an ill-understood problem and is an area of\\nactive research. Motivated from the approach of \\\\emph{Inherent Structure\\nformalism} (Stillinger & Weber, 1982), extensively used in analysing Spin\\nGlasses, we propose a novel measure called \\\\emph{Inherent Structure Capacity}\\n(ISC), which characterizes the representation capacity of a fixed architecture\\nRBM by the expected number of modes of distributions emanating from the RBM\\nwith parameters drawn from a prior distribution. Though ISC is intractable, we\\nshow that for a single layer RBM architecture ISC approaches a finite constant\\nas number of hidden units are increased and to further improve the ISC, one\\nneeds to add a second layer. Furthermore, we introduce \\\\emph{Lean} RBMs, which\\nare multi-layer RBMs where each layer can have at-most $O(n)$ units with the\\nnumber of visible units being n. We show that for every single layer RBM with\\n$\\\\Omega(n^{2+r}), r \\\\ge 0$, hidden units there exists a two-layered \\\\emph{lean}\\nRBM with $\\\\Theta(n^2)$ parameters with the same ISC, establishing that 2 layer\\nRBMs can achieve the same representational power as single-layer RBMs but using\\nfar fewer number of parameters. To the best of our knowledge, this is the first\\nresult which quantitatively establishes the need for layering.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04577v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04577v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Abhishek Bansal', 'Abhinav Anand', 'Chiranjib Bhattacharyya'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/acting-thoughts-towards-a-mobile-robotic',\n",
       "  'arxiv_id': '1707.06633',\n",
       "  'title': 'Acting Thoughts: Towards a Mobile Robotic Service Assistant for Users with Limited Communication Skills',\n",
       "  'abstract': 'As autonomous service robots become more affordable and thus available also\\nfor the general public, there is a growing need for user friendly interfaces to\\ncontrol the robotic system. Currently available control modalities typically\\nexpect users to be able to express their desire through either touch, speech or\\ngesture commands. While this requirement is fulfilled for the majority of\\nusers, paralyzed users may not be able to use such systems. In this paper, we\\npresent a novel framework, that allows these users to interact with a robotic\\nservice assistant in a closed-loop fashion, using only thoughts. The\\nbrain-computer interface (BCI) system is composed of several interacting\\ncomponents, i.e., non-invasive neuronal signal recording and decoding,\\nhigh-level task planning, motion and manipulation planning as well as\\nenvironment perception. In various experiments, we demonstrate its\\napplicability and robustness in real world scenarios, considering\\nfetch-and-carry tasks and tasks involving human-robot interaction. As our\\nresults demonstrate, our system is capable of adapting to frequent changes in\\nthe environment and reliably completing given tasks within a reasonable amount\\nof time. Combined with high-level planning and autonomous robotic systems,\\ninteresting new perspectives open up for non-invasive BCI-based human-robot\\ninteractions.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1707.06633v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1707.06633v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Felix Burget',\n",
       "   'Lukas Dominique Josef Fiederer',\n",
       "   'Daniel Kuhner',\n",
       "   'Martin Völker',\n",
       "   'Johannes Aldinger',\n",
       "   'Robin Tibor Schirrmeister',\n",
       "   'Chau Do',\n",
       "   'Joschka Boedecker',\n",
       "   'Bernhard Nebel',\n",
       "   'Tonio Ball',\n",
       "   'Wolfram Burgard'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-07-20',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/structured-evolution-with-compact',\n",
       "  'arxiv_id': '1804.02395',\n",
       "  'title': 'Structured Evolution with Compact Architectures for Scalable Policy Optimization',\n",
       "  'abstract': 'We present a new method of blackbox optimization via gradient approximation\\nwith the use of structured random orthogonal matrices, providing more accurate\\nestimators than baselines and with provable theoretical guarantees. We show\\nthat this algorithm can be successfully applied to learn better quality compact\\npolicies than those using standard gradient estimation techniques. The compact\\npolicies we learn have several advantages over unstructured ones, including\\nfaster training algorithms and faster inference. These benefits are important\\nwhen the policy is deployed on real hardware with limited resources. Further,\\ncompact policies provide more scalable architectures for derivative-free\\noptimization (DFO) in high-dimensional spaces. We show that most robotics tasks\\nfrom the OpenAI Gym can be solved using neural networks with less than 300\\nparameters, with almost linear time complexity of the inference phase, with up\\nto 13x fewer parameters relative to the Evolution Strategies (ES) algorithm\\nintroduced by Salimans et al. (2017). We do not need heuristics such as fitness\\nshaping to learn good quality policies, resulting in a simple and theoretically\\nmotivated training mechanism.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.02395v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.02395v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Krzysztof Choromanski',\n",
       "   'Mark Rowland',\n",
       "   'Vikas Sindhwani',\n",
       "   'Richard E. Turner',\n",
       "   'Adrian Weller'],\n",
       "  'tasks': ['OpenAI Gym', 'Text-to-Image Generation'],\n",
       "  'date': '2018-04-06',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['openai-gym'],\n",
       "  'datasets_used_full': ['OpenAI Gym'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/benchmarking-evolutionary-algorithms-for',\n",
       "  'arxiv_id': '1806.04563',\n",
       "  'title': 'Benchmarking Evolutionary Algorithms For Single Objective Real-valued Constrained Optimization - A Critical Review',\n",
       "  'abstract': 'Benchmarking plays an important role in the development of novel search\\nalgorithms as well as for the assessment and comparison of contemporary\\nalgorithmic ideas. This paper presents common principles that need to be taken\\ninto account when considering benchmarking problems for constrained\\noptimization. Current benchmark environments for testing Evolutionary\\nAlgorithms are reviewed in the light of these principles. Along with this line,\\nthe reader is provided with an overview of the available problem domains in the\\nfield of constrained benchmarking. Hence, the review supports algorithms\\ndevelopers with information about the merits and demerits of the available\\nframeworks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04563v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04563v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Michael Hellwig', 'Hans-Georg Beyer'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multi-agent-deep-reinforcement-learning-with-1',\n",
       "  'arxiv_id': '1806.04562',\n",
       "  'title': 'Multi-Agent Deep Reinforcement Learning with Human Strategies',\n",
       "  'abstract': 'Deep learning has enabled traditional reinforcement learning methods to deal with high-dimensional problems. However, one of the disadvantages of deep reinforcement learning methods is the limited exploration capacity of learning agents. In this paper, we introduce an approach that integrates human strategies to increase the exploration capacity of multiple deep reinforcement learning agents. We also report the development of our own multi-agent environment called Multiple Tank Defence to simulate the proposed approach. The results show the significant performance improvement of multiple agents that have learned cooperatively with human strategies. This implies that there is a critical need for human intellect teamed with machines to solve complex problems. In addition, the success of this simulation indicates that our multi-agent environment can be used as a testbed platform to develop and validate other multi-agent control algorithms.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04562v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04562v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Thanh Nguyen', 'Ngoc Duy Nguyen', 'Saeid Nahavandi'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['arcade-learning-environment'],\n",
       "  'datasets_used_full': ['Arcade Learning Environment'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/an-extension-of-averaged-operator-based',\n",
       "  'arxiv_id': '1806.04561',\n",
       "  'title': 'An Extension of Averaged-Operator-Based Algorithms',\n",
       "  'abstract': \"Many of the algorithms used to solve minimization problems with\\nsparsity-inducing regularizers are generic in the sense that they do not take\\ninto account the sparsity of the solution in any particular way. However,\\nalgorithms known as semismooth Newton are able to take advantage of this\\nsparsity to accelerate their convergence. We show how to extend these\\nalgorithms in different directions, and study the convergence of the resulting\\nalgorithms by showing that they are a particular case of an extension of the\\nwell-known Krasnosel'ski\\\\u{\\\\i}--Mann scheme.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04561v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04561v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Miguel Simões', 'José Bioucas-Dias', 'Luis B. Almeida'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/transfer-learning-from-speaker-verification',\n",
       "  'arxiv_id': '1806.04558',\n",
       "  'title': 'Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis',\n",
       "  'abstract': 'Clone a voice in 5 seconds to generate arbitrary speech in real-time',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04558v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04558v4.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Ye Jia',\n",
       "   'Yu Zhang',\n",
       "   'Ron J. Weiss',\n",
       "   'Quan Wang',\n",
       "   'Jonathan Shen',\n",
       "   'Fei Ren',\n",
       "   'Zhifeng Chen',\n",
       "   'Patrick Nguyen',\n",
       "   'Ruoming Pang',\n",
       "   'Ignacio Lopez Moreno',\n",
       "   'Yonghui Wu'],\n",
       "  'tasks': ['Speaker Verification',\n",
       "   'Speech Synthesis',\n",
       "   'Text-To-Speech Synthesis',\n",
       "   'Transfer Learning'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['librispeech', 'voxceleb1', 'vctk'],\n",
       "  'datasets_used_full': ['LibriSpeech', 'VoxCeleb1', 'VCTK'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/logistic-ensemble-models',\n",
       "  'arxiv_id': '1806.04555',\n",
       "  'title': 'Logistic Ensemble Models',\n",
       "  'abstract': 'Predictive models that are developed in a regulated industry or a regulated\\napplication, like determination of credit worthiness, must be interpretable and\\nrational (e.g., meaningful improvements in basic credit behavior must result in\\nimproved credit worthiness scores). Machine Learning technologies provide very\\ngood performance with minimal analyst intervention, making them well suited to\\na high volume analytic environment, but the majority are black box tools that\\nprovide very limited insight or interpretability into key drivers of model\\nperformance or predicted model output values. This paper presents a methodology\\nthat blends one of the most popular predictive statistical modeling methods for\\nbinary classification with a core model enhancement strategy found in machine\\nlearning. The resulting prediction methodology provides solid performance, from\\nminimal analyst effort, while providing the interpretability and rationality\\nrequired in regulated industries, as well as in other environments where\\ninterpretation of model parameters is required (e.g. businesses that require\\ninterpretation of models, to take action on them).',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04555v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04555v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Bob Vanderheyden', 'Jennifer Priestley'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [{'name': 'Interpretability',\n",
       "    'full_name': 'Interpretability',\n",
       "    'description': 'Please enter a description about the method here',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1310.1533v2',\n",
       "    'source_title': 'CAM: Causal additive models, high-dimensional order search and penalized regression',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Image Models',\n",
       "     'description': '**Image Models** are methods that build representations of images for downstream tasks such as classification and object detection. The most popular subcategory are convolutional neural networks. Below you can find a continuously updated list of image models.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/bitcoin-volatility-forecasting-with-a-glimpse',\n",
       "  'arxiv_id': '1802.04065',\n",
       "  'title': 'Bitcoin Volatility Forecasting with a Glimpse into Buy and Sell Orders',\n",
       "  'abstract': 'In this paper, we study the ability to make the short-term prediction of the\\nexchange price fluctuations towards the United States dollar for the Bitcoin\\nmarket. We use the data of realized volatility collected from one of the\\nlargest Bitcoin digital trading offices in 2016 and 2017 as well as order\\ninformation. Experiments are performed to evaluate a variety of statistical and\\nmachine learning approaches.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.04065v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.04065v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Tian Guo', 'Albert Bifet', 'Nino Antulov-Fantulin'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/combining-model-free-q-ensembles-and-model',\n",
       "  'arxiv_id': '1806.04552',\n",
       "  'title': 'Combining Model-Free Q-Ensembles and Model-Based Approaches for Informed Exploration',\n",
       "  'abstract': 'Q-Ensembles are a model-free approach where input images are fed into\\ndifferent Q-networks and exploration is driven by the assumption that\\nuncertainty is proportional to the variance of the output Q-values obtained.\\nThey have been shown to perform relatively well compared to other exploration\\nstrategies. Further, model-based approaches, such as encoder-decoder models\\nhave been used successfully for next frame prediction given previous frames.\\nThis paper proposes to integrate the model-free Q-ensembles and model-based\\napproaches with the hope of compounding the benefits of both and achieving\\nsuperior exploration as a result. Results show that a model-based trajectory\\nmemory approach when combined with Q-ensembles produces superior performance\\nwhen compared to only using Q-ensembles.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04552v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04552v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Sreecharan Sankaranarayanan',\n",
       "   'Raghuram Mandyam Annasamy',\n",
       "   'Katia Sycara',\n",
       "   'Carolyn Penstein Rosé'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/adversarial-risk-and-the-dangers-of',\n",
       "  'arxiv_id': '1802.05666',\n",
       "  'title': 'Adversarial Risk and the Dangers of Evaluating Against Weak Attacks',\n",
       "  'abstract': \"This paper investigates recently proposed approaches for defending against\\nadversarial examples and evaluating adversarial robustness. We motivate\\n'adversarial risk' as an objective for achieving models robust to worst-case\\ninputs. We then frame commonly used attacks and evaluation metrics as defining\\na tractable surrogate objective to the true adversarial risk. This suggests\\nthat models may optimize this surrogate rather than the true adversarial risk.\\nWe formalize this notion as 'obscurity to an adversary,' and develop tools and\\nheuristics for identifying obscured models and designing transparent models. We\\ndemonstrate that this is a significant problem in practice by repurposing\\ngradient-free optimization techniques into adversarial attacks, which we use to\\ndecrease the accuracy of several recently proposed defenses to near zero. Our\\nhope is that our formulations and results will help researchers to develop more\\npowerful defenses.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.05666v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.05666v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Jonathan Uesato',\n",
       "   \"Brendan O'Donoghue\",\n",
       "   'Aaron van den Oord',\n",
       "   'Pushmeet Kohli'],\n",
       "  'tasks': ['Adversarial Robustness'],\n",
       "  'date': '2018-02-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'imagenet'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'ImageNet'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-state-space-models-for-unconditional',\n",
       "  'arxiv_id': '1806.04550',\n",
       "  'title': 'Deep State Space Models for Unconditional Word Generation',\n",
       "  'abstract': 'Autoregressive feedback is considered a necessity for successful\\nunconditional text generation using stochastic sequence models. However, such\\nfeedback is known to introduce systematic biases into the training process and\\nit obscures a principle of generation: committing to global information and\\nforgetting local nuances. We show that a non-autoregressive deep state space\\nmodel with a clear separation of global and local uncertainty can be built from\\nonly two ingredients: An independent noise source and a deterministic\\ntransition function. Recent advances on flow-based variational inference can be\\nused to train an evidence lower-bound without resorting to annealing, auxiliary\\nlosses or similar measures. The result is a highly interpretable generative\\nmodel on par with comparable auto-regressive models on the task of word\\ngeneration.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04550v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04550v2.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Florian Schmidt', 'Thomas Hofmann'],\n",
       "  'tasks': ['Text Generation', 'Variational Inference'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/early-seizure-detection-with-an-energy',\n",
       "  'arxiv_id': '1806.04549',\n",
       "  'title': 'Early Seizure Detection with an Energy-Efficient Convolutional Neural Network on an Implantable Microcontroller',\n",
       "  'abstract': 'Implantable, closed-loop devices for automated early detection and\\nstimulation of epileptic seizures are promising treatment options for patients\\nwith severe epilepsy that cannot be treated with traditional means. Most\\napproaches for early seizure detection in the literature are, however, not\\noptimized for implementation on ultra-low power microcontrollers required for\\nlong-term implantation. In this paper we present a convolutional neural network\\nfor the early detection of seizures from intracranial EEG signals, designed\\nspecifically for this purpose. In addition, we investigate approximations to\\ncomply with hardware limits while preserving accuracy. We compare our approach\\nto three previously proposed convolutional neural networks and a feature-based\\nSVM classifier with respect to detection accuracy, latency and computational\\nneeds. Evaluation is based on a comprehensive database with long-term EEG\\nrecordings. The proposed method outperforms the other detectors with a median\\nsensitivity of 0.96, false detection rate of 10.1 per hour and median detection\\ndelay of 3.7 seconds, while being the only approach suited to be realized on a\\nlow power microcontroller due to its parsimonious use of computational and\\nmemory resources.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04549v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04549v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Maria Hügle',\n",
       "   'Simon Heller',\n",
       "   'Manuel Watter',\n",
       "   'Manuel Blum',\n",
       "   'Farrokh Manzouri',\n",
       "   'Matthias Dümpelmann',\n",
       "   'Andreas Schulze-Bonhage',\n",
       "   'Peter Woias',\n",
       "   'Joschka Boedecker'],\n",
       "  'tasks': ['EEG', 'Seizure Detection'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-deep-similarity-metric-for-3d-mr',\n",
       "  'arxiv_id': '1806.04548',\n",
       "  'title': 'Learning Deep Similarity Metric for 3D MR-TRUS Registration',\n",
       "  'abstract': 'Purpose: The fusion of transrectal ultrasound (TRUS) and magnetic resonance\\n(MR) images for guiding targeted prostate biopsy has significantly improved the\\nbiopsy yield of aggressive cancers. A key component of MR-TRUS fusion is image\\nregistration. However, it is very challenging to obtain a robust automatic\\nMR-TRUS registration due to the large appearance difference between the two\\nimaging modalities. The work presented in this paper aims to tackle this\\nproblem by addressing two challenges: (i) the definition of a suitable\\nsimilarity metric and (ii) the determination of a suitable optimization\\nstrategy.\\n  Methods: This work proposes the use of a deep convolutional neural network to\\nlearn a similarity metric for MR-TRUS registration. We also use a composite\\noptimization strategy that explores the solution space in order to search for a\\nsuitable initialization for the second-order optimization of the learned\\nmetric. Further, a multi-pass approach is used in order to smooth the metric\\nfor optimization.\\n  Results: The learned similarity metric outperforms the classical mutual\\ninformation and also the state-of-the-art MIND feature based methods. The\\nresults indicate that the overall registration framework has a large capture\\nrange. The proposed deep similarity metric based approach obtained a mean TRE\\nof 3.86mm (with an initial TRE of 16mm) for this challenging problem.\\n  Conclusion: A similarity metric that is learned using a deep neural network\\ncan be used to assess the quality of any given image registration and can be\\nused in conjunction with the aforementioned optimization framework to perform\\nautomatic registration that is robust to poor initialization.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04548v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04548v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Grant Haskins',\n",
       "   'Jochen Kruecker',\n",
       "   'Uwe Kruger',\n",
       "   'Sheng Xu',\n",
       "   'Peter A. Pinto',\n",
       "   'Brad J. Wood',\n",
       "   'Pingkun Yan'],\n",
       "  'tasks': ['Image Registration'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/approximate-inference-with-wasserstein',\n",
       "  'arxiv_id': '1806.04542',\n",
       "  'title': 'Approximate inference with Wasserstein gradient flows',\n",
       "  'abstract': 'We present a novel approximate inference method for diffusion processes,\\nbased on the Wasserstein gradient flow formulation of the diffusion. In this\\nformulation, the time-dependent density of the diffusion is derived as the\\nlimit of implicit Euler steps that follow the gradients of a particular free\\nenergy functional. Existing methods for computing Wasserstein gradient flows\\nrely on discretization of the domain of the diffusion, prohibiting their\\napplication to domains in more than several dimensions. We propose instead a\\ndiscretization-free inference method that computes the Wasserstein gradient\\nflow directly in a space of continuous functions. We characterize approximation\\nproperties of the proposed method and evaluate it on a nonlinear filtering\\ntask, finding performance comparable to the state-of-the-art for filtering\\ndiffusions.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04542v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04542v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Charlie Frogner', 'Tomaso Poggio'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/adversarial-attacks-on-neural-networks-for',\n",
       "  'arxiv_id': '1805.07984',\n",
       "  'title': 'Adversarial Attacks on Neural Networks for Graph Data',\n",
       "  'abstract': \"Deep learning models for graphs have achieved strong performance for the task of node classification. Despite their proliferation, currently there is no study of their robustness to adversarial attacks. Yet, in domains where they are likely to be used, e.g. the web, adversaries are common. Can deep learning models for graphs be easily fooled? In this work, we introduce the first study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model. We generate adversarial perturbations targeting the node's features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. To cope with the underlying discrete domain we propose an efficient algorithm Nettack exploiting incremental computations. Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1805.07984v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1805.07984v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Daniel Zügner', 'Amir Akbarnejad', 'Stephan Günnemann'],\n",
       "  'tasks': ['General Classification', 'Node Classification'],\n",
       "  'date': '2018-05-21',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cora'],\n",
       "  'datasets_used_full': ['Cora'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/term-definitions-help-hypernymy-detection',\n",
       "  'arxiv_id': '1806.04532',\n",
       "  'title': 'Term Definitions Help Hypernymy Detection',\n",
       "  'abstract': 'Existing methods of hypernymy detection mainly rely on statistics over a big\\ncorpus, either mining some co-occurring patterns like \"animals such as cats\" or\\nembedding words of interest into context-aware vectors. These approaches are\\ntherefore limited by the availability of a large enough corpus that can cover\\nall terms of interest and provide sufficient contextual information to\\nrepresent their meaning. In this work, we propose a new paradigm, HyperDef, for\\nhypernymy detection -- expressing word meaning by encoding word definitions,\\nalong with context driven representation. This has two main benefits: (i)\\nDefinitional sentences express (sense-specific) corpus-independent meanings of\\nwords, hence definition-driven approaches enable strong generalization -- once\\ntrained, the model is expected to work well in open-domain testbeds; (ii)\\nGlobal context from a large corpus and definitions provide complementary\\ninformation for words. Consequently, our model, HyperDef, once trained on\\ntask-agnostic data, gets state-of-the-art results in multiple benchmarks',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04532v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04532v1.pdf',\n",
       "  'proceeding': 'SEMEVAL 2018 6',\n",
       "  'authors': ['Wenpeng Yin', 'Dan Roth'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['dbpedia', 'yago', 'evalution'],\n",
       "  'datasets_used_full': ['DBpedia', 'YAGO', 'EVALution'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/online-parallel-portfolio-selection-with',\n",
       "  'arxiv_id': '1806.04528',\n",
       "  'title': 'Online Parallel Portfolio Selection with Heterogeneous Island Model',\n",
       "  'abstract': 'We present an online parallel portfolio selection algorithm based on the\\nisland model commonly used for parallelization of evolutionary algorithms. In\\nour case each of the islands runs a different optimization algorithm. The\\ndistributed computation is managed by a central planner which periodically\\nchanges the running methods during the execution of the algorithm -- less\\nsuccessful methods are removed while new instances of more successful methods\\nare added.\\n  We compare different types of planners in the heterogeneous island model\\namong themselves and also to the traditional homogeneous model on a wide set of\\nproblems. The tests include experiments with different representations of the\\nindividuals and different duration of fitness function evaluations. The results\\nshow that heterogeneous models are a more general and universal computational\\ntool compared to homogeneous models.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04528v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04528v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Štěpán Balcar', 'Martin Pilát'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-to-automatically-generate-fill-in',\n",
       "  'arxiv_id': '1806.04524',\n",
       "  'title': 'Learning to Automatically Generate Fill-In-The-Blank Quizzes',\n",
       "  'abstract': 'In this paper we formalize the problem automatic fill-in-the-blank question\\ngeneration using two standard NLP machine learning schemes, proposing concrete\\ndeep learning models for each. We present an empirical study based on data\\nobtained from a language learning platform showing that both of our proposed\\nsettings offer promising results.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04524v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04524v1.pdf',\n",
       "  'proceeding': 'WS 2018 7',\n",
       "  'authors': ['Edison Marrese-Taylor',\n",
       "   'Ai Nakajima',\n",
       "   'Yutaka Matsuo',\n",
       "   'Ono Yuichi'],\n",
       "  'tasks': ['Question Generation'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/recurrent-one-hop-predictions-for-reasoning',\n",
       "  'arxiv_id': '1806.04523',\n",
       "  'title': 'Recurrent One-Hop Predictions for Reasoning over Knowledge Graphs',\n",
       "  'abstract': 'Large scale knowledge graphs (KGs) such as Freebase are generally incomplete.\\nReasoning over multi-hop (mh) KG paths is thus an important capability that is\\nneeded for question answering or other NLP tasks that require knowledge about\\nthe world. mh-KG reasoning includes diverse scenarios, e.g., given a head\\nentity and a relation path, predict the tail entity; or given two entities\\nconnected by some relation paths, predict the unknown relation between them. We\\npresent ROPs, recurrent one-hop predictors, that predict entities at each step\\nof mh-KB paths by using recurrent neural networks and vector representations of\\nentities and relations, with two benefits: (i) modeling mh-paths of arbitrary\\nlengths while updating the entity and relation representations by the training\\nsignal at each step; (ii) handling different types of mh-KG reasoning in a\\nunified framework. Our models show state-of-the-art for two important multi-hop\\nKG reasoning tasks: Knowledge Base Completion and Path Query Answering.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04523v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04523v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Wenpeng Yin', 'Yadollah Yaghoobzadeh', 'Hinrich Schütze'],\n",
       "  'tasks': ['Knowledge Base Completion',\n",
       "   'Knowledge Graphs',\n",
       "   'Question Answering'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/meta-learning-for-stochastic-gradient-mcmc',\n",
       "  'arxiv_id': '1806.04522',\n",
       "  'title': 'Meta-Learning for Stochastic Gradient MCMC',\n",
       "  'abstract': 'Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has become\\nincreasingly popular for simulating posterior samples in large-scale Bayesian\\nmodeling. However, existing SG-MCMC schemes are not tailored to any specific\\nprobabilistic model, even a simple modification of the underlying dynamical\\nsystem requires significant physical intuition. This paper presents the first\\nmeta-learning algorithm that allows automated design for the underlying\\ncontinuous dynamics of an SG-MCMC sampler. The learned sampler generalizes\\nHamiltonian dynamics with state-dependent drift and diffusion, enabling fast\\ntraversal and efficient exploration of neural network energy landscapes.\\nExperiments validate the proposed approach on both Bayesian fully connected\\nneural network and Bayesian recurrent neural network tasks, showing that the\\nlearned sampler out-performs generic, hand-designed SG-MCMC algorithms, and\\ngeneralizes to different datasets and larger architectures.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04522v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04522v1.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Wenbo Gong', 'Yingzhen Li', 'José Miguel Hernández-Lobato'],\n",
       "  'tasks': ['Efficient Exploration', 'Meta-Learning'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/efficient-first-order-algorithms-for-adaptive',\n",
       "  'arxiv_id': '1803.11262',\n",
       "  'title': 'Efficient First-Order Algorithms for Adaptive Signal Denoising',\n",
       "  'abstract': 'We consider the problem of discrete-time signal denoising, focusing on a\\nspecific family of non-linear convolution-type estimators. Each such estimator\\nis associated with a time-invariant filter which is obtained adaptively, by\\nsolving a certain convex optimization problem. Adaptive convolution-type\\nestimators were demonstrated to have favorable statistical properties. However,\\nthe question of their computational complexity remains largely unexplored, and\\nin fact we are not aware of any publicly available implementation of these\\nestimators. Our first contribution is an efficient implementation of these\\nestimators via some known first-order proximal algorithms. Our second\\ncontribution is a computational complexity analysis of the proposed procedures,\\nwhich takes into account their statistical nature and the related notion of\\nstatistical accuracy. The proposed procedures and their analysis are\\nillustrated on a simulated data benchmark.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.11262v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.11262v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Dmitrii Ostrovskii', 'Zaid Harchaoui'],\n",
       "  'tasks': ['Denoising'],\n",
       "  'date': '2018-03-29',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-review-on-distance-based-time-series',\n",
       "  'arxiv_id': '1806.04509',\n",
       "  'title': 'A review on distance based time series classification',\n",
       "  'abstract': 'Time series classification is an increasing research topic due to the vast\\namount of time series data that are being created over a wide variety of\\nfields. The particularity of the data makes it a challenging task and different\\napproaches have been taken, including the distance based approach. 1-NN has\\nbeen a widely used method within distance based time series classification due\\nto it simplicity but still good performance. However, its supremacy may be\\nattributed to being able to use specific distances for time series within the\\nclassification process and not to the classifier itself. With the aim of\\nexploiting these distances within more complex classifiers, new approaches have\\narisen in the past few years that are competitive or which outperform the 1-NN\\nbased approaches. In some cases, these new methods use the distance measure to\\ntransform the series into feature vectors, bridging the gap between time series\\nand traditional classifiers. In other cases, the distances are employed to\\nobtain a time series kernel and enable the use of kernel methods for time\\nseries classification. One of the main challenges is that a kernel function\\nmust be positive semi-definite, a matter that is also addressed within this\\nreview. The presented review includes a taxonomy of all those methods that aim\\nto classify time series using a distance based approach, as well as a\\ndiscussion of the strengths and weaknesses of each method.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04509v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04509v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Amaia Abanda', 'Usue Mori', 'Jose A. Lozano'],\n",
       "  'tasks': ['Classification',\n",
       "   'General Classification',\n",
       "   'Time Series',\n",
       "   'Time Series Classification'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-unusual-effectiveness-of-averaging-in-gan',\n",
       "  'arxiv_id': '1806.04498',\n",
       "  'title': 'The Unusual Effectiveness of Averaging in GAN Training',\n",
       "  'abstract': 'We examine two different techniques for parameter averaging in GAN training.\\nMoving Average (MA) computes the time-average of parameters, whereas\\nExponential Moving Average (EMA) computes an exponentially discounted sum.\\nWhilst MA is known to lead to convergence in bilinear settings, we provide the\\n-- to our knowledge -- first theoretical arguments in support of EMA. We show\\nthat EMA converges to limit cycles around the equilibrium with vanishing\\namplitude as the discount parameter approaches one for simple bilinear games\\nand also enhances the stability of general GAN training. We establish\\nexperimentally that both techniques are strikingly effective in the\\nnon-convex-concave GAN setting as well. Both improve inception and FID scores\\non different architectures and for different GAN objectives. We provide\\ncomprehensive experimental results across a range of datasets -- mixture of\\nGaussians, CIFAR-10, STL-10, CelebA and ImageNet -- to demonstrate its\\neffectiveness. We achieve state-of-the-art results on CIFAR-10 and produce\\nclean CelebA face images.\\\\footnote{~The code is available at\\n\\\\url{https://github.com/yasinyazici/EMA_GAN}}',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04498v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04498v2.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Yasin Yazici',\n",
       "   'Chuan-Sheng Foo',\n",
       "   'Stefan Winkler',\n",
       "   'Kim-Hui Yap',\n",
       "   'Georgios Piliouras',\n",
       "   'Vijay Chandrasekhar'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['celeba', 'stl-10'],\n",
       "  'datasets_used_full': ['CelebA', 'STL-10'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-virtual-environment-with-multi-robot',\n",
       "  'arxiv_id': '1806.04497',\n",
       "  'title': 'A Virtual Environment with Multi-Robot Navigation, Analytics, and Decision Support for Critical Incident Investigation',\n",
       "  'abstract': \"Accidents and attacks that involve chemical, biological, radiological/nuclear\\nor explosive (CBRNE) substances are rare, but can be of high consequence. Since\\nthe investigation of such events is not anybody's routine work, a range of AI\\ntechniques can reduce investigators' cognitive load and support\\ndecision-making, including: planning the assessment of the scene; ongoing\\nevaluation and updating of risks; control of autonomous vehicles for collecting\\nimages and sensor data; reviewing images/videos for items of interest;\\nidentification of anomalies; and retrieval of relevant documentation. Because\\nof the rare and high-risk nature of these events, realistic simulations can\\nsupport the development and evaluation of AI-based tools. We have developed\\nrealistic models of CBRNE scenarios and implemented an initial set of tools.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04497v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04497v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['David L. Smyth',\n",
       "   'James Fennell',\n",
       "   'Sai Abinesh',\n",
       "   'Nazli B. Karimi',\n",
       "   'Frank G. Glavin',\n",
       "   'Ihsan Ullah',\n",
       "   'Brett Drury',\n",
       "   'Michael G. Madden'],\n",
       "  'tasks': ['Autonomous Vehicles', 'Decision Making', 'Robot Navigation'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['3d-chairs'],\n",
       "  'datasets_used_full': ['3D Chairs'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/approximate-kernel-pca-using-random-features',\n",
       "  'arxiv_id': '1706.06296',\n",
       "  'title': 'Approximate Kernel PCA Using Random Features: Computational vs. Statistical Trade-off',\n",
       "  'abstract': 'Kernel methods are powerful learning methodologies that provide a simple way to construct nonlinear algorithms from linear ones. Despite their popularity, they suffer from poor scalability in big data scenarios. Various approximation methods, including random feature approximation, have been proposed to alleviate the problem. However, the statistical consistency of most of these approximate kernel methods is not well understood except for kernel ridge regression wherein it has been shown that the random feature approximation is not only computationally efficient but also statistically consistent with a minimax optimal rate of convergence. In this paper, we investigate the efficacy of random feature approximation in the context of kernel principal component analysis (KPCA) by studying the trade-off between computational and statistical behaviors of approximate KPCA. We show that the approximate KPCA is both computationally and statistically efficient compared to KPCA in terms of the error associated with reconstructing a kernel function based on its projection onto the corresponding eigenspaces. The analysis hinges on Bernstein-type inequalities for the operator and Hilbert-Schmidt norms of a self-adjoint Hilbert-Schmidt operator-valued U-statistics, which is of independent interest.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1706.06296v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1706.06296v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Bharath Sriperumbudur', 'Nicholas Sterge'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-06-20',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/examining-the-use-of-neural-networks-for',\n",
       "  'arxiv_id': '1805.02294',\n",
       "  'title': 'Examining the Use of Neural Networks for Feature Extraction: A Comparative Analysis using Deep Learning, Support Vector Machines, and K-Nearest Neighbor Classifiers',\n",
       "  'abstract': 'Neural networks in many varieties are touted as very powerful machine\\nlearning tools because of their ability to distill large amounts of information\\nfrom different forms of data, extracting complex features and enabling powerful\\nclassification abilities. In this study, we use neural networks to extract\\nfeatures from both images and numeric data and use these extracted features as\\ninputs for other machine learning models, namely support vector machines (SVMs)\\nand k-nearest neighbor classifiers (KNNs), in order to see if\\nneural-network-extracted features enhance the capabilities of these models. We\\ntested 7 different neural network architectures in this manner, 4 for images\\nand 3 for numeric data, training each for varying lengths of time and then\\ncomparing the results of the neural network independently to those of an SVM\\nand KNN on the data, and finally comparing these results to models of SVM and\\nKNN trained using features extracted via the neural network architecture. This\\nprocess was repeated on 3 different image datasets and 2 different numeric\\ndatasets. The results show that, in many cases, the features extracted using\\nthe neural network significantly improve the capabilities of SVMs and KNNs\\ncompared to running these algorithms on the raw features, and in some cases\\nalso surpass the performance of the neural network alone. This in turn suggests\\nthat it may be a reasonable practice to use neural networks as a means to\\nextract features for classification by other machine learning models for some\\ndatasets.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.02294v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.02294v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Stephen Notley', 'Malik Magdon-Ismail'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-05-06',\n",
       "  'methods': [{'name': 'SVM',\n",
       "    'full_name': 'Support Vector Machine',\n",
       "    'description': 'A **Support Vector Machine**, or **SVM**, is a non-parametric supervised learning model. For non-linear classification and regression, they utilise the kernel trick to map inputs to high-dimensional feature spaces. SVMs construct a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure to the right shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called “support vectors”. \\r\\n\\r\\nSource: [scikit-learn](https://scikit-learn.org/stable/modules/svm.html)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Non-Parametric Classification',\n",
       "     'description': '**Non-Parametric Classification** methods perform classification where we use non-parametric methods to approximate the functional form of the relationship. Below you can find a continuously updating list of non-parametric classification methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/localized-multiple-kernel-learning-for',\n",
       "  'arxiv_id': '1805.07892',\n",
       "  'title': 'Localized Multiple Kernel Learning for Anomaly Detection: One-class Classification',\n",
       "  'abstract': 'Multi-kernel learning has been well explored in the recent past and has\\nexhibited promising outcomes for multi-class classification and regression\\ntasks. In this paper, we present a multiple kernel learning approach for the\\nOne-class Classification (OCC) task and employ it for anomaly detection.\\nRecently, the basic multi-kernel approach has been proposed to solve the OCC\\nproblem, which is simply a convex combination of different kernels with equal\\nweights. This paper proposes a Localized Multiple Kernel learning approach for\\nAnomaly Detection (LMKAD) using OCC, where the weight for each kernel is\\nassigned locally. Proposed LMKAD approach adapts the weight for each kernel\\nusing a gating function. The parameters of the gating function and one-class\\nclassifier are optimized simultaneously through a two-step optimization\\nprocess. We present the empirical results of the performance of LMKAD on 25\\nbenchmark datasets from various disciplines. This performance is evaluated\\nagainst existing Multi Kernel Anomaly Detection (MKAD) algorithm, and four\\nother existing kernel-based one-class classifiers to showcase the credibility\\nof our approach. Our algorithm achieves significantly better Gmean scores while\\nusing a lesser number of support vectors compared to MKAD. Friedman test is\\nalso performed to verify the statistical significance of the results claimed in\\nthis paper.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.07892v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.07892v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Chandan Gautam',\n",
       "   'Ramesh Balaji',\n",
       "   'K Sudharsan',\n",
       "   'Aruna Tiwari',\n",
       "   'Kapil Ahuja'],\n",
       "  'tasks': ['Anomaly Detection',\n",
       "   'Classification',\n",
       "   'General Classification',\n",
       "   'Multi-class Classification',\n",
       "   'One-class classifier'],\n",
       "  'date': '2018-05-21',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/improving-latent-variable-descriptiveness',\n",
       "  'arxiv_id': '1806.04480',\n",
       "  'title': 'Improving latent variable descriptiveness with AutoGen',\n",
       "  'abstract': 'Powerful generative models, particularly in Natural Language Modelling, are\\ncommonly trained by maximizing a variational lower bound on the data log\\nlikelihood. These models often suffer from poor use of their latent variable,\\nwith ad-hoc annealing factors used to encourage retention of information in the\\nlatent variable. We discuss an alternative and general approach to latent\\nvariable modelling, based on an objective that combines the data log likelihood\\nas well as the likelihood of a perfect reconstruction through an autoencoder.\\nTying these together ensures by design that the latent variable captures\\ninformation about the observations, whilst retaining the ability to generate\\nwell. Interestingly, though this approach is a priori unrelated to VAEs, the\\nlower bound attained is identical to the standard VAE bound but with the\\naddition of a simple pre-factor; thus, providing a formal interpretation of the\\ncommonly used, ad-hoc pre-factors in training VAEs.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04480v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04480v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alex Mansbridge',\n",
       "   'Roberto Fierimonte',\n",
       "   'Ilya Feige',\n",
       "   'David Barber'],\n",
       "  'tasks': ['Language Modelling'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [{'name': 'VAE',\n",
       "    'full_name': 'Variational Autoencoder',\n",
       "    'description': 'A **Variational Autoencoder** is a type of likelihood-based generative model. It consists of an encoder, that takes in data $x$ as input and transforms this into a latent representation $z$,  and a decoder, that takes a latent representation $z$ and returns a reconstruction $\\\\hat{x}$. Inference is performed via variational inference to approximate the posterior of the model.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.6114v10',\n",
       "    'source_title': 'Auto-Encoding Variational Bayes',\n",
       "    'code_snippet_url': 'https://github.com/AntixK/PyTorch-VAE/blob/8700d245a9735640dda458db4cf40708caf2e77f/models/vanilla_vae.py#L8',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/two-use-cases-of-machine-learning-for-sdn',\n",
       "  'arxiv_id': '1804.07433',\n",
       "  'title': 'Two Use Cases of Machine Learning for SDN-Enabled IP/Optical Networks: Traffic Matrix Prediction and Optical Path Performance Prediction',\n",
       "  'abstract': 'We describe two applications of machine learning in the context of IP/Optical\\nnetworks. The first one allows agile management of resources at a core\\nIP/Optical network by using machine learning for short-term and long-term\\nprediction of traffic flows and joint global optimization of IP and optical\\nlayers using colorless/directionless (CD) flexible ROADMs. Multilayer\\ncoordination allows for significant cost savings, flexible new services to meet\\ndynamic capacity needs, and improved robustness by being able to proactively\\nadapt to new traffic patterns and network conditions. The second application is\\nimportant as we migrate our metro networks to Open ROADM networks, to allow\\nphysical routing without the need for detailed knowledge of optical parameters.\\nWe discuss a proof-of-concept study, where detailed performance data for\\nwavelengths on a current flexible ROADM network is used for machine learning to\\npredict the optical performance of each wavelength. Both applications can be\\nefficiently implemented by using a SDN (Software Defined Network) controller.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.07433v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.07433v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Gagan Choudhury', 'David Lynch', 'Gaurav Thakur', 'Simon Tse'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-04-20',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/slice-as-an-evolutionary-service-genetic',\n",
       "  'arxiv_id': '1802.04491',\n",
       "  'title': 'Slice as an Evolutionary Service: Genetic Optimization for Inter-Slice Resource Management in 5G Networks',\n",
       "  'abstract': 'In the context of Fifth Generation (5G) mobile networks, the concept of\\n\"Slice as a Service\" (SlaaS) promotes mobile network operators to flexibly\\nshare infrastructures with mobile service providers and stakeholders. However,\\nit also challenges with an emerging demand for efficient online algorithms to\\noptimize the request-and-decision-based inter-slice resource management\\nstrategy. Based on genetic algorithms, this paper presents a novel online\\noptimizer that efficiently approaches towards the ideal slicing strategy with\\nmaximized long-term network utility. The proposed method encodes slicing\\nstrategies into binary sequences to cope with the request-and-decision\\nmechanism. It requires no a priori knowledge about the traffic/utility models,\\nand therefore supports heterogeneous slices, while providing solid\\neffectiveness, good robustness against non-stationary service scenarios, and\\nhigh scalability.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.04491v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.04491v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Bin Han', 'Lianghai Ji', 'Hans D. Schotten'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/trading-algorithms-with-learning-in-latent',\n",
       "  'arxiv_id': '1806.04472',\n",
       "  'title': 'Trading algorithms with learning in latent alpha models',\n",
       "  'abstract': \"Alpha signals for statistical arbitrage strategies are often driven by latent\\nfactors. This paper analyses how to optimally trade with latent factors that\\ncause prices to jump and diffuse. Moreover, we account for the effect of the\\ntrader's actions on quoted prices and the prices they receive from trading.\\nUnder fairly general assumptions, we demonstrate how the trader can learn the\\nposterior distribution over the latent states, and explicitly solve the latent\\noptimal trading problem. We provide a verification theorem, and a methodology\\nfor calibrating the model by deriving a variation of the\\nexpectation-maximization algorithm. To illustrate the efficacy of the optimal\\nstrategy, we demonstrate its performance through simulations and compare it to\\nstrategies which ignore learning in the latent factors. We also provide\\ncalibration results for a particular model using Intel Corporation stock as an\\nexample.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04472v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04472v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Philippe Casgrain', 'Sebastian Jaimungal'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/autoregressive-convolutional-neural-networks',\n",
       "  'arxiv_id': '1703.04122',\n",
       "  'title': 'Autoregressive Convolutional Neural Networks for Asynchronous Time Series',\n",
       "  'abstract': 'We propose Significance-Offset Convolutional Neural Network, a deep\\nconvolutional network architecture for regression of multivariate asynchronous\\ntime series. The model is inspired by standard autoregressive (AR) models and\\ngating mechanisms used in recurrent neural networks. It involves an AR-like\\nweighting system, where the final predictor is obtained as a weighted sum of\\nadjusted regressors, while the weights are datadependent functions learnt\\nthrough a convolutional network. The architecture was designed for applications\\non asynchronous time series and is evaluated on such datasets: a hedge fund\\nproprietary dataset of over 2 million quotes for a credit derivative index, an\\nartificially generated noisy autoregressive series and UCI household\\nelectricity consumption dataset. The proposed architecture achieves promising\\nresults as compared to convolutional and recurrent neural networks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1703.04122v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1703.04122v4.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Mikołaj Bińkowski', 'Gautier Marti', 'Philippe Donnat'],\n",
       "  'tasks': ['Time Series'],\n",
       "  'date': '2017-03-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/colwells-castle-defence-a-custom-game-using',\n",
       "  'arxiv_id': '1806.04471',\n",
       "  'title': \"Colwell's Castle Defence: A Custom Game Using Dynamic Difficulty Adjustment to Increase Player Enjoyment\",\n",
       "  'abstract': \"Dynamic Difficulty Adjustment (DDA) is a mechanism used in video games that\\nautomatically tailors the individual gaming experience to match an appropriate\\ndifficulty setting. This is generally achieved by removing pre-defined\\ndifficulty tiers such as Easy, Medium and Hard; and instead concentrates on\\nbalancing the gameplay to match the challenge to the individual's abilities.\\nThe work presented in this paper examines the implementation of DDA in a custom\\nsurvival game developed by the author, namely Colwell's Castle Defence. The\\npremise of this arcade-style game is to defend a castle from hordes of oncoming\\nenemies. The AI system that we developed adjusts the enemy spawn rate based on\\nthe current performance of the player. Specifically, we read the Player Health\\nand Gate Health at the end of each level and then assign the player with an\\nappropriate difficulty tier for the proceeding level. We tested the impact of\\nour technique on thirty human players and concluded, based on questionnaire\\nfeedback, that enabling the technique led to more enjoyable gameplay.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04471v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04471v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Anthony M. Colwell', 'Frank G. Glavin'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/design-challenges-and-misconceptions-in',\n",
       "  'arxiv_id': '1806.04470',\n",
       "  'title': 'Design Challenges and Misconceptions in Neural Sequence Labeling',\n",
       "  'abstract': 'We investigate the design challenges of constructing effective and efficient\\nneural sequence labeling systems, by reproducing twelve neural sequence\\nlabeling models, which include most of the state-of-the-art structures, and\\nconduct a systematic model comparison on three benchmarks (i.e. NER, Chunking,\\nand POS tagging). Misconceptions and inconsistent conclusions in existing\\nliterature are examined and clarified under statistical experiments. In the\\ncomparison and analysis process, we reach several practical conclusions which\\ncan be useful to practitioners.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04470v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04470v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Jie Yang', 'Shuailong Liang', 'Yue Zhang'],\n",
       "  'tasks': ['Chunking', 'NER', 'POS'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['penn-treebank'],\n",
       "  'datasets_used_full': ['Penn Treebank'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/training-medical-image-analysis-systems-like',\n",
       "  'arxiv_id': '1805.10884',\n",
       "  'title': 'Training Medical Image Analysis Systems like Radiologists',\n",
       "  'abstract': 'The training of medical image analysis systems using machine learning\\napproaches follows a common script: collect and annotate a large dataset, train\\nthe classifier on the training set, and test it on a hold-out test set. This\\nprocess bears no direct resemblance with radiologist training, which is based\\non solving a series of tasks of increasing difficulty, where each task involves\\nthe use of significantly smaller datasets than those used in machine learning.\\nIn this paper, we propose a novel training approach inspired by how\\nradiologists are trained. In particular, we explore the use of meta-training\\nthat models a classifier based on a series of tasks. Tasks are selected using\\nteacher-student curriculum learning, where each task consists of simple\\nclassification problems containing small training sets. We hypothesize that our\\nproposed meta-training approach can be used to pre-train medical image analysis\\nmodels. This hypothesis is tested on the automatic breast screening\\nclassification from DCE-MRI trained with weakly labeled datasets. The\\nclassification performance achieved by our approach is shown to be the best in\\nthe field for that application, compared to state of art baseline approaches:\\nDenseNet, multiple instance learning and multi-task learning.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.10884v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.10884v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Gabriel Maicas',\n",
       "   'Andrew P. Bradley',\n",
       "   'Jacinto C. Nascimento',\n",
       "   'Ian Reid',\n",
       "   'Gustavo Carneiro'],\n",
       "  'tasks': ['Classification',\n",
       "   'General Classification',\n",
       "   'Multiple Instance Learning',\n",
       "   'Multi-Task Learning'],\n",
       "  'date': '2018-05-28',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fusing-recency-into-neural-machine',\n",
       "  'arxiv_id': '1806.04466',\n",
       "  'title': 'Fusing Recency into Neural Machine Translation with an Inter-Sentence Gate Model',\n",
       "  'abstract': 'Neural machine translation (NMT) systems are usually trained on a large\\namount of bilingual sentence pairs and translate one sentence at a time,\\nignoring inter-sentence information. This may make the translation of a\\nsentence ambiguous or even inconsistent with the translations of neighboring\\nsentences. In order to handle this issue, we propose an inter-sentence gate\\nmodel that uses the same encoder to encode two adjacent sentences and controls\\nthe amount of information flowing from the preceding sentence to the\\ntranslation of the current sentence with an inter-sentence gate. In this way,\\nour proposed model can capture the connection between sentences and fuse\\nrecency from neighboring sentences into neural machine translation. On several\\nNIST Chinese-English translation tasks, our experiments demonstrate that the\\nproposed inter-sentence gate model achieves substantial improvements over the\\nbaseline.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04466v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04466v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Shaohui Kuang', 'Deyi Xiong'],\n",
       "  'tasks': ['Machine Translation', 'Translation'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/gaussian-mixture-models-with-wasserstein',\n",
       "  'arxiv_id': '1806.04465',\n",
       "  'title': 'Gaussian mixture models with Wasserstein distance',\n",
       "  'abstract': 'Generative models with both discrete and continuous latent variables are\\nhighly motivated by the structure of many real-world data sets. They present,\\nhowever, subtleties in training often manifesting in the discrete latent being\\nunder leveraged. In this paper, we show that such models are more amenable to\\ntraining when using the Optimal Transport framework of Wasserstein\\nAutoencoders. We find our discrete latent variable to be fully leveraged by the\\nmodel when trained, without any modifications to the objective function or\\nsignificant fine tuning. Our model generates comparable samples to other\\napproaches while using relatively simple neural networks, since the discrete\\nlatent variable carries much of the descriptive burden. Furthermore, the\\ndiscrete latent provides significant control over generation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04465v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04465v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Benoit Gaujac', 'Ilya Feige', 'David Barber'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/discovery-and-recognition-of-motion',\n",
       "  'arxiv_id': '1709.10494',\n",
       "  'title': 'Discovery and recognition of motion primitives in human activities',\n",
       "  'abstract': \"We present a novel framework for the automatic discovery and recognition of\\nmotion primitives in videos of human activities. Given the 3D pose of a human\\nin a video, human motion primitives are discovered by optimizing the `motion\\nflux', a quantity which captures the motion variation of a group of skeletal\\njoints. A normalization of the primitives is proposed in order to make them\\ninvariant with respect to a subject anatomical variations and data sampling\\nrate. The discovered primitives are unknown and unlabeled and are\\nunsupervisedly collected into classes via a hierarchical non-parametric Bayes\\nmixture model. Once classes are determined and labeled they are further\\nanalyzed for establishing models for recognizing discovered primitives. Each\\nprimitive model is defined by a set of learned parameters.\\n  Given new video data and given the estimated pose of the subject appearing on\\nthe video, the motion is segmented into primitives, which are recognized with a\\nprobability given according to the parameters of the learned models.\\n  Using our framework we build a publicly available dataset of human motion\\nprimitives, using sequences taken from well-known motion capture datasets. We\\nexpect that our framework, by providing an objective way for discovering and\\ncategorizing human motion, will be a useful tool in numerous research fields\\nincluding video analysis, human inspired motion generation, learning by\\ndemonstration, intuitive human-robot interaction, and human behavior analysis.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1709.10494v7',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1709.10494v7.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Marta Sanzari', 'Valsamis Ntouskos', 'Fiora Pirri'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-09-29',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['ucf-crime'],\n",
       "  'datasets_used_full': ['UCF-Crime'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/sparse-stochastic-zeroth-order-optimization',\n",
       "  'arxiv_id': '1806.04458',\n",
       "  'title': 'Sparse Stochastic Zeroth-Order Optimization with an Application to Bandit Structured Prediction',\n",
       "  'abstract': 'Stochastic zeroth-order (SZO), or gradient-free, optimization allows to optimize arbitrary functions by relying only on function evaluations under parameter perturbations, however, the iteration complexity of SZO methods suffers a factor proportional to the dimensionality of the perturbed function. We show that in scenarios with natural sparsity patterns as in structured prediction applications, this factor can be reduced to the expected number of active features over input-output pairs. We give a general proof that applies sparse SZO optimization to Lipschitz-continuous, nonconvex, stochastic objectives, and present an experimental evaluation on linear bandit structured prediction tasks with sparse word-based feature representations that confirm our theoretical results.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04458v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04458v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Artem Sokolov',\n",
       "   'Julian Hitschler',\n",
       "   'Mayumi Ohta',\n",
       "   'Stefan Riezler'],\n",
       "  'tasks': ['Structured Prediction'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/impersonation-modeling-persona-in-smart',\n",
       "  'arxiv_id': '1806.04456',\n",
       "  'title': 'Impersonation: Modeling Persona in Smart Responses to Email',\n",
       "  'abstract': 'In this paper, we present design, implementation, and effectiveness of\\ngenerating personalized suggestions for email replies. To personalize email\\nresponses based on users style and personality, we model the users persona\\nbased on her past responses to emails. This model is added to the\\nlanguage-based model created across users using past responses of the all user\\nemails.\\n  A users model captures the typical responses of the user given a particular\\ncontext. The context includes the email received, recipient of the email, and\\nother external signals such as calendar activities, preferences, etc. The\\ncontext along with users personality (e.g., extrovert, formal, reserved, etc.)\\nis used to suggest responses. These responses can be a mixture of multiple\\nmodes: email replies (textual), audio clips, etc. This helps in making\\nresponses mimic the user as much as possible and helps the user to be more\\nproductive while retaining her mark in the responses.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04456v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04456v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Rajeev Gupta', 'Ranganath Kondapally', 'Chakrapani Ravi Kiran'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/convergence-of-gradient-descent-on-separable',\n",
       "  'arxiv_id': '1803.01905',\n",
       "  'title': 'Convergence of Gradient Descent on Separable Data',\n",
       "  'abstract': 'We provide a detailed study on the implicit bias of gradient descent when\\noptimizing loss functions with strictly monotone tails, such as the logistic\\nloss, over separable datasets. We look at two basic questions: (a) what are the\\nconditions on the tail of the loss function under which gradient descent\\nconverges in the direction of the $L_2$ maximum-margin separator? (b) how does\\nthe rate of margin convergence depend on the tail of the loss function and the\\nchoice of the step size? We show that for a large family of super-polynomial\\ntailed losses, gradient descent iterates on linear networks of any depth\\nconverge in the direction of $L_2$ maximum-margin solution, while this does not\\nhold for losses with heavier tails. Within this family, for simple linear\\nmodels we show that the optimal rates with fixed step size is indeed obtained\\nfor the commonly used exponentially tailed losses such as logistic loss.\\nHowever, with a fixed step size the optimal convergence rate is extremely slow\\nas $1/\\\\log(t)$, as also proved in Soudry et al. (2018). For linear models with\\nexponential loss, we further prove that the convergence rate could be improved\\nto $\\\\log (t) /\\\\sqrt{t}$ by using aggressive step sizes that compensates for the\\nrapidly vanishing gradients. Numerical results suggest this method might be\\nuseful for deep networks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.01905v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.01905v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mor Shpigel Nacson',\n",
       "   'Jason D. Lee',\n",
       "   'Suriya Gunasekar',\n",
       "   'Pedro H. P. Savarese',\n",
       "   'Nathan Srebro',\n",
       "   'Daniel Soudry'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-03-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10'],\n",
       "  'datasets_used_full': ['CIFAR-10'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/an-ensemble-model-for-sentiment-analysis-of',\n",
       "  'arxiv_id': '1806.04450',\n",
       "  'title': 'An Ensemble Model for Sentiment Analysis of Hindi-English Code-Mixed Data',\n",
       "  'abstract': 'In multilingual societies like India, code-mixed social media texts comprise\\nthe majority of the Internet. Detecting the sentiment of the code-mixed user\\nopinions plays a crucial role in understanding social, economic and political\\ntrends. In this paper, we propose an ensemble of character-trigrams based LSTM\\nmodel and word-ngrams based Multinomial Naive Bayes (MNB) model to identify the\\nsentiments of Hindi-English (Hi-En) code-mixed data. The ensemble model\\ncombines the strengths of rich sequential patterns from the LSTM model and\\npolarity of keywords from the probabilistic ngram model to identify sentiments\\nin sparse and inconsistent code-mixed data. Experiments on reallife user\\ncode-mixed data reveals that our approach yields state-of-the-art results as\\ncompared to several baselines and other deep learning based proposed methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04450v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04450v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Madan Gopal Jhanwar', 'Arpita Das'],\n",
       "  'tasks': ['Sentiment Analysis'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/toxicblend-virtual-screening-of-toxic',\n",
       "  'arxiv_id': '1806.04449',\n",
       "  'title': 'ToxicBlend: Virtual Screening of Toxic Compounds with Ensemble Predictors',\n",
       "  'abstract': 'Timely assessment of compound toxicity is one of the biggest challenges\\nfacing the pharmaceutical industry today. A significant proportion of compounds\\nidentified as potential leads are ultimately discarded due to the toxicity they\\ninduce. In this paper, we propose a novel machine learning approach for the\\nprediction of molecular activity on ToxCast targets. We combine extreme\\ngradient boosting with fully-connected and graph-convolutional neural network\\narchitectures trained on QSAR physical molecular property descriptors, PubChem\\nmolecular fingerprints, and SMILES sequences. Our ensemble predictor leverages\\nthe strengths of each individual technique, significantly outperforming\\nexisting state-of-the art models on the ToxCast and Tox21 toxicity-prediction\\ndatasets. We provide free access to molecule toxicity prediction using our\\nmodel at http://www.owkin.com/toxicblend.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04449v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04449v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mikhail Zaslavskiy',\n",
       "   'Simon Jégou',\n",
       "   'Eric W. Tramel',\n",
       "   'Gilles Wainrib'],\n",
       "  'tasks': ['Drug Discovery'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['tox21-1'],\n",
       "  'datasets_used_full': ['Tox21'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/direct-estimation-of-pharmacokinetic',\n",
       "  'arxiv_id': '1804.02745',\n",
       "  'title': 'Direct Estimation of Pharmacokinetic Parameters from DCE-MRI using Deep CNN with Forward Physical Model Loss',\n",
       "  'abstract': 'Dynamic contrast-enhanced (DCE) MRI is an evolving imaging technique that\\nprovides a quantitative measure of pharmacokinetic (PK) parameters in body\\ntissues, in which series of T1-weighted images are collected following the\\nadministration of a paramagnetic contrast agent. Unfortunately, in many\\napplications, conventional clinical DCE-MRI suffers from low spatiotemporal\\nresolution and insufficient volume coverage. In this paper, we propose a novel\\ndeep learning based approach to directly estimate the PK parameters from\\nundersampled DCE-MRI data. Specifically, we design a custom loss function where\\nwe incorporate a forward physical model that relates the PK parameters to\\ncorrupted image-time series obtained due to subsampling in k-space. This allows\\nthe network to directly exploit the knowledge of true contrast agent kinetics\\nin the training phase, and hence provide more accurate restoration of PK\\nparameters. Experiments on clinical brain DCE datasets demonstrate the efficacy\\nof our approach in terms of fidelity of PK parameter reconstruction and\\nsignificantly faster parameter inference compared to a model-based iterative\\nreconstruction method.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.02745v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.02745v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Cagdas Ulas',\n",
       "   'Giles Tetteh',\n",
       "   'Michael J. Thrippleton',\n",
       "   'Paul A. Armitage',\n",
       "   'Stephen D. Makin',\n",
       "   'Joanna M. Wardlaw',\n",
       "   'Mike E. Davies',\n",
       "   'Bjoern H. Menze'],\n",
       "  'tasks': ['Time Series'],\n",
       "  'date': '2018-04-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deepasl-kinetic-model-incorporated-loss-for',\n",
       "  'arxiv_id': '1804.02755',\n",
       "  'title': 'DeepASL: Kinetic Model Incorporated Loss for Denoising Arterial Spin Labeled MRI via Deep Residual Learning',\n",
       "  'abstract': 'Arterial spin labeling (ASL) allows to quantify the cerebral blood flow (CBF)\\nby magnetic labeling of the arterial blood water. ASL is increasingly used in\\nclinical studies due to its noninvasiveness, repeatability and benefits in\\nquantification. However, ASL suffers from an inherently low-signal-to-noise\\nratio (SNR) requiring repeated measurements of control/spin-labeled (C/L) pairs\\nto achieve a reasonable image quality, which in return increases motion\\nsensitivity. This leads to clinically prolonged scanning times increasing the\\nrisk of motion artifacts. Thus, there is an immense need of advanced imaging\\nand processing techniques in ASL. In this paper, we propose a novel deep\\nlearning based approach to improve the perfusion-weighted image quality\\nobtained from a subset of all available pairwise C/L subtractions.\\nSpecifically, we train a deep fully convolutional network (FCN) to learn a\\nmapping from noisy perfusion-weighted image and its subtraction (residual) from\\nthe clean image. Additionally, we incorporate the CBF estimation model in the\\nloss function during training, which enables the network to produce high\\nquality images while simultaneously enforcing the CBF estimates to be as close\\nas reference CBF values. Extensive experiments on synthetic and clinical ASL\\ndatasets demonstrate the effectiveness of our method in terms of improved ASL\\nimage quality, accurate CBF parameter estimation and considerably small\\ncomputation time during testing.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.02755v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.02755v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Cagdas Ulas',\n",
       "   'Giles Tetteh',\n",
       "   'Stephan Kaczmarz',\n",
       "   'Christine Preibisch',\n",
       "   'Bjoern H. Menze'],\n",
       "  'tasks': ['Denoising'],\n",
       "  'date': '2018-04-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/sequence-to-sequence-learning-for-task',\n",
       "  'arxiv_id': '1806.04441',\n",
       "  'title': 'Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State Representation',\n",
       "  'abstract': 'Classic pipeline models for task-oriented dialogue system require explicit\\nmodeling the dialogue states and hand-crafted action spaces to query a\\ndomain-specific knowledge base. Conversely, sequence-to-sequence models learn\\nto map dialogue history to the response in current turn without explicit\\nknowledge base querying. In this work, we propose a novel framework that\\nleverages the advantages of classic pipeline and sequence-to-sequence models.\\nOur framework models a dialogue state as a fixed-size distributed\\nrepresentation and use this representation to query a knowledge base via an\\nattention mechanism. Experiment on Stanford Multi-turn Multi-domain\\nTask-oriented Dialogue Dataset shows that our framework significantly\\noutperforms other sequence-to-sequence based baseline models on both automatic\\nand human evaluation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04441v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04441v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Haoyang Wen',\n",
       "   'Yijia Liu',\n",
       "   'Wanxiang Che',\n",
       "   'Libo Qin',\n",
       "   'Ting Liu'],\n",
       "  'tasks': ['Task-Oriented Dialogue Systems'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['kvret-1'],\n",
       "  'datasets_used_full': ['KVRET'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/ml-fv-heartsuit-a-survey-on-the-application',\n",
       "  'arxiv_id': '1806.03600',\n",
       "  'title': 'ML + FV = $\\\\heartsuit$? A Survey on the Application of Machine Learning to Formal Verification',\n",
       "  'abstract': 'Formal Verification (FV) and Machine Learning (ML) can seem incompatible due\\nto their opposite mathematical foundations and their use in real-life problems:\\nFV mostly relies on discrete mathematics and aims at ensuring correctness; ML\\noften relies on probabilistic models and consists of learning patterns from\\ntraining data. In this paper, we postulate that they are complementary in\\npractice, and explore how ML helps FV in its classical approaches: static\\nanalysis, model-checking, theorem-proving, and SAT solving. We draw a landscape\\nof the current practice and catalog some of the most prominent uses of ML\\ninside FV tools, thus offering a new perspective on FV techniques that can help\\nresearchers and practitioners to better locate the possible synergies. We\\ndiscuss lessons learned from our work, point to possible improvements and offer\\nvisions for the future of the domain in the light of the science of software\\nand systems modeling.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03600v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03600v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Moussa Amrani', 'Levi Lúcio', 'Adrien Bibal'],\n",
       "  'tasks': ['Automated Theorem Proving'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/analyzing-uncertainty-in-neural-machine',\n",
       "  'arxiv_id': '1803.00047',\n",
       "  'title': 'Analyzing Uncertainty in Neural Machine Translation',\n",
       "  'abstract': 'Machine translation is a popular test bed for research in neural\\nsequence-to-sequence models but despite much recent research, there is still a\\nlack of understanding of these models. Practitioners report performance\\ndegradation with large beams, the under-estimation of rare words and a lack of\\ndiversity in the final translations. Our study relates some of these issues to\\nthe inherent uncertainty of the task, due to the existence of multiple valid\\ntranslations for a single source sentence, and to the extrinsic uncertainty\\ncaused by noisy training data. We propose tools and metrics to assess how\\nuncertainty in the data is captured by the model distribution and how it\\naffects search strategies that generate translations. Our results show that\\nsearch works remarkably well but that models tend to spread too much\\nprobability mass over the hypothesis space. Next, we propose tools to assess\\nmodel calibration and show how to easily fix some shortcomings of current\\nmodels. As part of this study, we release multiple human reference translations\\nfor two popular benchmarks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.00047v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.00047v4.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Myle Ott',\n",
       "   'Michael Auli',\n",
       "   'David Grangier',\n",
       "   \"Marc'Aurelio Ranzato\"],\n",
       "  'tasks': ['Machine Translation', 'Translation'],\n",
       "  'date': '2018-02-28',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/ask-no-more-deciding-when-to-guess-in',\n",
       "  'arxiv_id': '1805.06960',\n",
       "  'title': 'Ask No More: Deciding when to guess in referential visual dialogue',\n",
       "  'abstract': 'Our goal is to explore how the abilities brought in by a dialogue manager can\\nbe included in end-to-end visually grounded conversational agents. We make\\ninitial steps towards this general goal by augmenting a task-oriented visual\\ndialogue model with a decision-making component that decides whether to ask a\\nfollow-up question to identify a target referent in an image, or to stop the\\nconversation to make a guess. Our analyses show that adding a decision making\\ncomponent produces dialogues that are less repetitive and that include fewer\\nunnecessary questions, thus potentially leading to more efficient and less\\nunnatural interactions.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.06960v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.06960v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Ravi Shekhar',\n",
       "   'Tim Baumgartner',\n",
       "   'Aashish Venkatesh',\n",
       "   'Elia Bruni',\n",
       "   'Raffaella Bernardi',\n",
       "   'Raquel Fernandez'],\n",
       "  'tasks': ['Decision Making', 'Visual Dialog'],\n",
       "  'date': '2018-05-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['guesswhat'],\n",
       "  'datasets_used_full': ['GuessWhat?!'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/u-segnet-fully-convolutional-neural-network',\n",
       "  'arxiv_id': '1806.04429',\n",
       "  'title': 'U-SegNet: Fully Convolutional Neural Network based Automated Brain tissue segmentation Tool',\n",
       "  'abstract': \"Automated brain tissue segmentation into white matter (WM), gray matter (GM),\\nand cerebro-spinal fluid (CSF) from magnetic resonance images (MRI) is helpful\\nin the diagnosis of neuro-disorders such as epilepsy, Alzheimer's, multiple\\nsclerosis, etc. However, thin GM structures at the periphery of cortex and\\nsmooth transitions on tissue boundaries such as between GM and WM, or WM and\\nCSF pose difficulty in building a reliable segmentation tool. This paper\\nproposes a Fully Convolutional Neural Network (FCN) tool, that is a hybrid of\\ntwo widely used deep learning segmentation architectures SegNet and U-Net, for\\nimproved brain tissue segmentation. We propose a skip connection inspired from\\nU-Net, in the SegNet architetcure, to incorporate fine multiscale information\\nfor better tissue boundary identification. We show that the proposed U-SegNet\\narchitecture, improves segmentation performance, as measured by average dice\\nratio, to 89.74% on the widely used IBSR dataset consisting of T-1 weighted MRI\\nvolumes of 18 subjects.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04429v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04429v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Pulkit Kumar', 'Pravin Nagar', 'Chetan Arora', 'Anubha Gupta'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [{'name': 'Concatenated Skip Connection',\n",
       "    'full_name': 'Concatenated Skip Connection',\n",
       "    'description': 'A **Concatenated Skip Connection** is a type of skip connection that seeks to reuse features by concatenating them to new layers, allowing more information to be retained from previous layers of the network. This contrasts with say, residual connections, where element-wise summation is used instead to incorporate information from previous layers. This type of skip connection is prominently used in DenseNets (and also Inception networks), which the Figure to the right illustrates.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/densenet.py#L113',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'U-Net',\n",
       "    'full_name': 'U-Net',\n",
       "    'description': '**U-Net** is an architecture for semantic segmentation. It consists of a contracting path and an expansive path. The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit ([ReLU](https://paperswithcode.com/method/relu)) and a 2x2 [max pooling](https://paperswithcode.com/method/max-pooling) operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 [convolution](https://paperswithcode.com/method/convolution) (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a [1x1 convolution](https://paperswithcode.com/method/1x1-convolution) is used to map each 64-component feature vector to the desired number of classes. In total the network has 23 convolutional layers.\\r\\n\\r\\n[Original MATLAB Code](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-release-2015-10-02.tar.gz)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1505.04597v1',\n",
       "    'source_title': 'U-Net: Convolutional Networks for Biomedical Image Segmentation',\n",
       "    'code_snippet_url': 'https://github.com/milesial/Pytorch-UNet/blob/67bf11b4db4c5f2891bd7e8e7f58bcde8ee2d2db/unet/unet_model.py#L8',\n",
       "    'main_collection': {'name': 'Semantic Segmentation Models',\n",
       "     'description': '**Semantic Segmentation Models** are a class of methods that address the task of semantically segmenting an image into different object classes. Below you can find a continuously updating list of semantic segmentation models. ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Kaiming Initialization',\n",
       "    'full_name': 'Kaiming Initialization',\n",
       "    'description': '**Kaiming Initialization**, or **He Initialization**, is an initialization method for neural networks that takes into account the non-linearity of activation functions, such as [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nA proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. Using a derivation they work out that the condition to stop this happening is:\\r\\n\\r\\n$$\\\\frac{1}{2}n\\\\_{l}\\\\text{Var}\\\\left[w\\\\_{l}\\\\right] = 1 $$\\r\\n\\r\\nThis implies an initialization scheme of:\\r\\n\\r\\n$$ w\\\\_{l} \\\\sim \\\\mathcal{N}\\\\left(0,  2/n\\\\_{l}\\\\right)$$\\r\\n\\r\\nThat is, a zero-centered Gaussian with standard deviation of $\\\\sqrt{2/{n}\\\\_{l}}$ (variance shown in equation above). Biases are initialized at $0$.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.01852v1',\n",
       "    'source_title': 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/0adb5843766092fba584791af76383125fd0d01c/torch/nn/init.py#L389',\n",
       "    'main_collection': {'name': 'Initialization',\n",
       "     'description': '**Initialization** methods are used to initialize the weights in a neural network. Below can you find a continuously updating list of initialization methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Batch Normalization',\n",
       "    'full_name': 'Batch Normalization',\n",
       "    'description': '**Batch Normalization** aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for [Dropout](https://paperswithcode.com/method/dropout).\\r\\n\\r\\nWe apply a batch normalization layer as follows for a minibatch $\\\\mathcal{B}$:\\r\\n\\r\\n$$ \\\\mu\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}x\\\\_{i} $$\\r\\n\\r\\n$$ \\\\sigma^{2}\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}\\\\left(x\\\\_{i}-\\\\mu\\\\_{\\\\mathcal{B}}\\\\right)^{2} $$\\r\\n\\r\\n$$ \\\\hat{x}\\\\_{i} = \\\\frac{x\\\\_{i} - \\\\mu\\\\_{\\\\mathcal{B}}}{\\\\sqrt{\\\\sigma^{2}\\\\_{\\\\mathcal{B}}+\\\\epsilon}} $$\\r\\n\\r\\n$$ y\\\\_{i} = \\\\gamma\\\\hat{x}\\\\_{i} + \\\\beta = \\\\text{BN}\\\\_{\\\\gamma, \\\\beta}\\\\left(x\\\\_{i}\\\\right) $$\\r\\n\\r\\nWhere $\\\\gamma$ and $\\\\beta$ are learnable parameters.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.03167v3',\n",
       "    'source_title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/36f91261099b00194922bd93ed1286fe1c199724/jax/experimental/stax.py#L116',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'SegNet',\n",
       "    'full_name': 'SegNet',\n",
       "    'description': '**SegNet** is a semantic segmentation model. This core trainable segmentation architecture consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the\\r\\nVGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature maps. Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to\\r\\nperform non-linear upsampling.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1511.00561v3',\n",
       "    'source_title': 'SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation',\n",
       "    'code_snippet_url': 'https://github.com/yassouali/pytorch_segmentation/blob/8b8e3ee20a3aa733cb19fc158ad5d7773ed6da7f/models/segnet.py#L9',\n",
       "    'main_collection': {'name': 'Semantic Segmentation Models',\n",
       "     'description': '**Semantic Segmentation Models** are a class of methods that address the task of semantically segmenting an image into different object classes. Below you can find a continuously updating list of semantic segmentation models. ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/sample-dropout-for-audio-scene-classification',\n",
       "  'arxiv_id': '1806.04422',\n",
       "  'title': 'Sample Dropout for Audio Scene Classification Using Multi-Scale Dense Connected Convolutional Neural Network',\n",
       "  'abstract': 'Acoustic scene classification is an intricate problem for a machine. As an\\nemerging field of research, deep Convolutional Neural Networks (CNN) achieve\\nconvincing results. In this paper, we explore the use of multi-scale Dense\\nconnected convolutional neural network (DenseNet) for the classification task,\\nwith the goal to improve the classification performance as multi-scale features\\ncan be extracted from the time-frequency representation of the audio signal. On\\nthe other hand, most of previous CNN-based audio scene classification\\napproaches aim to improve the classification accuracy, by employing different\\nregularization techniques, such as the dropout of hidden units and data\\naugmentation, to reduce overfitting. It is widely known that outliers in the\\ntraining set have a high negative influence on the trained model, and culling\\nthe outliers may improve the classification performance, while it is often\\nunder-explored in previous studies. In this paper, inspired by the silence\\nremoval in the speech signal processing, a novel sample dropout approach is\\nproposed, which aims to remove outliers in the training dataset. Using the\\nDCASE 2017 audio scene classification datasets, the experimental results\\ndemonstrates the proposed multi-scale DenseNet providing a superior performance\\nthan the traditional single-scale DenseNet, while the sample dropout method can\\nfurther improve the classification robustness of multi-scale DenseNet.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04422v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04422v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Dawei Feng', 'Kele Xu', 'Haibo Mi', 'Feifan Liao', 'Yan Zhou'],\n",
       "  'tasks': ['Acoustic Scene Classification',\n",
       "   'Classification',\n",
       "   'Data Augmentation',\n",
       "   'General Classification',\n",
       "   'Scene Classification'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [{'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Batch Normalization',\n",
       "    'full_name': 'Batch Normalization',\n",
       "    'description': '**Batch Normalization** aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for [Dropout](https://paperswithcode.com/method/dropout).\\r\\n\\r\\nWe apply a batch normalization layer as follows for a minibatch $\\\\mathcal{B}$:\\r\\n\\r\\n$$ \\\\mu\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}x\\\\_{i} $$\\r\\n\\r\\n$$ \\\\sigma^{2}\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}\\\\left(x\\\\_{i}-\\\\mu\\\\_{\\\\mathcal{B}}\\\\right)^{2} $$\\r\\n\\r\\n$$ \\\\hat{x}\\\\_{i} = \\\\frac{x\\\\_{i} - \\\\mu\\\\_{\\\\mathcal{B}}}{\\\\sqrt{\\\\sigma^{2}\\\\_{\\\\mathcal{B}}+\\\\epsilon}} $$\\r\\n\\r\\n$$ y\\\\_{i} = \\\\gamma\\\\hat{x}\\\\_{i} + \\\\beta = \\\\text{BN}\\\\_{\\\\gamma, \\\\beta}\\\\left(x\\\\_{i}\\\\right) $$\\r\\n\\r\\nWhere $\\\\gamma$ and $\\\\beta$ are learnable parameters.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.03167v3',\n",
       "    'source_title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/36f91261099b00194922bd93ed1286fe1c199724/jax/experimental/stax.py#L116',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Average Pooling',\n",
       "    'full_name': 'Average Pooling',\n",
       "    'description': '**Average Pooling** is a pooling operation that calculates the average value for patches of a feature map, and uses it to create a downsampled (pooled) feature map. It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs. It extracts features more smoothly than [Max Pooling](https://paperswithcode.com/method/max-pooling), whereas max pooling extracts more pronounced features like edges.\\r\\n\\r\\nImage Source: [here](https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Concatenated Skip Connection',\n",
       "    'full_name': 'Concatenated Skip Connection',\n",
       "    'description': 'A **Concatenated Skip Connection** is a type of skip connection that seeks to reuse features by concatenating them to new layers, allowing more information to be retained from previous layers of the network. This contrasts with say, residual connections, where element-wise summation is used instead to incorporate information from previous layers. This type of skip connection is prominently used in DenseNets (and also Inception networks), which the Figure to the right illustrates.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/densenet.py#L113',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Global Average Pooling',\n",
       "    'full_name': 'Global Average Pooling',\n",
       "    'description': '**Global Average Pooling** is a pooling operation designed to replace fully connected layers in classical CNNs. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the [softmax](https://paperswithcode.com/method/softmax) layer. \\r\\n\\r\\nOne advantage of global [average pooling](https://paperswithcode.com/method/average-pooling) over the fully connected layers is that it is more native to the [convolution](https://paperswithcode.com/method/convolution) structure by enforcing correspondences between feature maps and categories. Thus the feature maps can be easily interpreted as categories confidence maps. Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. Furthermore, global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/baa592b215804927e28638f6a7f3318cbc411d49/torchvision/models/resnet.py#L157',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Dense Block',\n",
       "    'full_name': 'Dense Block',\n",
       "    'description': 'A **Dense Block** is a module used in convolutional neural networks that connects *all layers* (with matching feature-map sizes) directly with each other. It was originally proposed as part of the [DenseNet](https://paperswithcode.com/method/densenet) architecture. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. In contrast to [ResNets](https://paperswithcode.com/method/resnet), we never combine features through summation before they are passed into a layer; instead, we combine features by concatenating them. Hence, the $\\\\ell^{th}$ layer has $\\\\ell$ inputs, consisting of the feature-maps of all preceding convolutional blocks. Its own feature-maps are passed on to all $L-\\\\ell$ subsequent layers. This introduces $\\\\frac{L(L+1)}{2}$  connections in an $L$-layer network, instead of just $L$, as in traditional architectures: \"dense connectivity\".',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1608.06993v5',\n",
       "    'source_title': 'Densely Connected Convolutional Networks',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/densenet.py#L93',\n",
       "    'main_collection': {'name': 'Image Model Blocks',\n",
       "     'description': '**Image Model Blocks** are building blocks used in image models such as convolutional neural networks. Below you can find a continuously updating list of image model blocks.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Kaiming Initialization',\n",
       "    'full_name': 'Kaiming Initialization',\n",
       "    'description': '**Kaiming Initialization**, or **He Initialization**, is an initialization method for neural networks that takes into account the non-linearity of activation functions, such as [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nA proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. Using a derivation they work out that the condition to stop this happening is:\\r\\n\\r\\n$$\\\\frac{1}{2}n\\\\_{l}\\\\text{Var}\\\\left[w\\\\_{l}\\\\right] = 1 $$\\r\\n\\r\\nThis implies an initialization scheme of:\\r\\n\\r\\n$$ w\\\\_{l} \\\\sim \\\\mathcal{N}\\\\left(0,  2/n\\\\_{l}\\\\right)$$\\r\\n\\r\\nThat is, a zero-centered Gaussian with standard deviation of $\\\\sqrt{2/{n}\\\\_{l}}$ (variance shown in equation above). Biases are initialized at $0$.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.01852v1',\n",
       "    'source_title': 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/0adb5843766092fba584791af76383125fd0d01c/torch/nn/init.py#L389',\n",
       "    'main_collection': {'name': 'Initialization',\n",
       "     'description': '**Initialization** methods are used to initialize the weights in a neural network. Below can you find a continuously updating list of initialization methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': '1x1 Convolution',\n",
       "    'full_name': '1x1 Convolution',\n",
       "    'description': 'A **1 x 1 Convolution** is a [convolution](https://paperswithcode.com/method/convolution) with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an [MLP](https://paperswithcode.com/method/feedforward-network) looking at a particular pixel location.\\r\\n\\r\\nImage Credit: [http://deeplearning.ai](http://deeplearning.ai)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://www.healthnutra.org/es/maxup/',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Dense Connections',\n",
       "    'full_name': 'Dense Connections',\n",
       "    'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\\\_{\\\\text{inputs}}*n\\\\_{\\\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\\r\\n\\r\\n$$h\\\\_{l} = g\\\\left(\\\\textbf{W}^{T}h\\\\_{l-1}\\\\right)$$\\r\\n\\r\\nwhere $g$ is an activation function.\\r\\n\\r\\nImage Source: Deep Learning by Goodfellow, Bengio and Courville',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'DenseNet',\n",
       "    'full_name': 'DenseNet',\n",
       "    'description': 'A **DenseNet** is a type of convolutional neural network that utilises [dense connections](https://paperswithcode.com/method/dense-connections) between layers, through [Dense Blocks](http://www.paperswithcode.com/method/dense-block), where we connect *all layers* (with matching feature-map sizes) directly with each other. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1608.06993v5',\n",
       "    'source_title': 'Densely Connected Convolutional Networks',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/densenet.py#L126',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Dropout',\n",
       "    'full_name': 'Dropout',\n",
       "    'description': '**Dropout** is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p=0.5$). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).\\r\\n\\r\\nThe idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://jmlr.org/papers/v15/srivastava14a.html',\n",
       "    'source_title': 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/7f3078b70d0ed9bea6228efa420879c56f72ef69/jax/experimental/stax.py#L271-L275',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['dcase-2013'],\n",
       "  'datasets_used_full': ['DCASE 2013'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/using-chaos-in-grey-wolf-optimizer-and',\n",
       "  'arxiv_id': '1806.04419',\n",
       "  'title': 'Using Chaos in Grey Wolf Optimizer and Application to Prime Factorization',\n",
       "  'abstract': 'The Grey Wolf Optimizer (GWO) is a swarm intelligence meta-heuristic\\nalgorithm inspired by the hunting behaviour and social hierarchy of grey wolves\\nin nature. This paper analyses the use of chaos theory in this algorithm to\\nimprove its ability to escape local optima by replacing the key parameters by\\nchaotic variables. The optimal choice of chaotic maps is then used to apply the\\nChaotic Grey Wolf Optimizer (CGWO) to the problem of factoring a large semi\\nprime into its prime factors. Assuming the number of digits of the factors to\\nbe equal, this is a computationally difficult task upon which the\\nRSA-cryptosystem relies. This work proposes the use of a new objective function\\nto solve the problem and uses the CGWO to optimize it and compute the factors.\\nIt is shown that this function performs better than its predecessor for large\\nsemi primes and CGWO is an efficient algorithm to optimize it.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04419v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04419v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Harshit Mehrotra', 'Dr. Saibal K. Pal'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/quaternion-recurrent-neural-networks',\n",
       "  'arxiv_id': '1806.04418',\n",
       "  'title': 'Quaternion Recurrent Neural Networks',\n",
       "  'abstract': 'Recurrent neural networks (RNNs) are powerful architectures to model\\nsequential data, due to their capability to learn short and long-term\\ndependencies between the basic elements of a sequence. Nonetheless, popular\\ntasks such as speech or images recognition, involve multi-dimensional input\\nfeatures that are characterized by strong internal dependencies between the\\ndimensions of the input vector. We propose a novel quaternion recurrent neural\\nnetwork (QRNN), alongside with a quaternion long-short term memory neural\\nnetwork (QLSTM), that take into account both the external relations and these\\ninternal structural dependencies with the quaternion algebra. Similarly to\\ncapsules, quaternions allow the QRNN to code internal dependencies by composing\\nand processing multidimensional features as single entities, while the\\nrecurrent operation reveals correlations between the elements composing the\\nsequence. We show that both QRNN and QLSTM achieve better performances than RNN\\nand LSTM in a realistic application of automatic speech recognition. Finally,\\nwe show that QRNN and QLSTM reduce by a maximum factor of 3.3x the number of\\nfree parameters needed, compared to real-valued RNNs and LSTMs to reach better\\nresults, leading to a more compact representation of the relevant information.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04418v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04418v3.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Titouan Parcollet',\n",
       "   'Mirco Ravanelli',\n",
       "   'Mohamed Morchid',\n",
       "   'Georges Linarès',\n",
       "   'Chiheb Trabelsi',\n",
       "   'Renato de Mori',\n",
       "   'Yoshua Bengio'],\n",
       "  'tasks': ['Automatic Speech Recognition', 'Speech Recognition'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/markerless-inside-out-tracking-for',\n",
       "  'arxiv_id': '1804.01708',\n",
       "  'title': 'Markerless Inside-Out Tracking for Interventional Applications',\n",
       "  'abstract': 'Tracking of rotation and translation of medical instruments plays a\\nsubstantial role in many modern interventions. Traditional external optical\\ntracking systems are often subject to line-of-sight issues, in particular when\\nthe region of interest is difficult to access or the procedure allows only for\\nlimited rigid body markers. The introduction of inside-out tracking systems\\naims to overcome these issues. We propose a marker-less tracking system based\\non visual SLAM to enable tracking of instruments in an interventional scenario.\\nTo achieve this goal, we mount a miniature multi-modal (monocular, stereo,\\nactive depth) vision system on the object of interest and relocalize its pose\\nwithin an adaptive map of the operating room. We compare state-of-the-art\\nalgorithmic pipelines and apply the idea to transrectal 3D Ultrasound (TRUS)\\ncompounding of the prostate. Obtained volumes are compared to reconstruction\\nusing a commercial optical tracking system as well as a robotic manipulator.\\nFeature-based binocular SLAM is identified as the most promising method and is\\ntested extensively in challenging clinical environment under severe occlusion\\nand for the use case of prostate US biopsies.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.01708v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.01708v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Benjamin Busam',\n",
       "   'Patrick Ruhkamp',\n",
       "   'Salvatore Virga',\n",
       "   'Beatrice Lentes',\n",
       "   'Julia Rackerseder',\n",
       "   'Nassir Navab',\n",
       "   'Christoph Hennersperger'],\n",
       "  'tasks': ['Translation'],\n",
       "  'date': '2018-04-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/enhancing-clinical-mri-perfusion-maps-with',\n",
       "  'arxiv_id': '1806.04413',\n",
       "  'title': 'Enhancing clinical MRI Perfusion maps with data-driven maps of complementary nature for lesion outcome prediction',\n",
       "  'abstract': \"Stroke is the second most common cause of death in developed countries, where\\nrapid clinical intervention can have a major impact on a patient's life. To\\nperform the revascularization procedure, the decision making of physicians\\nconsiders its risks and benefits based on multi-modal MRI and clinical\\nexperience. Therefore, automatic prediction of the ischemic stroke lesion\\noutcome has the potential to assist the physician towards a better stroke\\nassessment and information about tissue outcome. Typically, automatic methods\\nconsider the information of the standard kinetic models of diffusion and\\nperfusion MRI (e.g. Tmax, TTP, MTT, rCBF, rCBV) to perform lesion outcome\\nprediction. In this work, we propose a deep learning method to fuse this\\ninformation with an automated data selection of the raw 4D PWI image\\ninformation, followed by a data-driven deep-learning modeling of the underlying\\nblood flow hemodynamics. We demonstrate the ability of the proposed approach to\\nimprove prediction of tissue at risk before therapy, as compared to only using\\nthe standard clinical perfusion maps, hence suggesting on the potential\\nbenefits of the proposed data-driven raw perfusion data modelling approach.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04413v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04413v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Adriano Pinto',\n",
       "   'Sergio Pereira',\n",
       "   'Raphael Meier',\n",
       "   'Victor Alves',\n",
       "   'Roland Wiest',\n",
       "   'Carlos A. Silva',\n",
       "   'Mauricio Reyes'],\n",
       "  'tasks': ['Decision Making'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-learning-for-electromyographic-hand',\n",
       "  'arxiv_id': '1801.07756',\n",
       "  'title': 'Deep Learning for Electromyographic Hand Gesture Signal Classification Using Transfer Learning',\n",
       "  'abstract': \"In recent years, deep learning algorithms have become increasingly more\\nprominent for their unparalleled ability to automatically learn discriminant\\nfeatures from large amounts of data. However, within the field of\\nelectromyography-based gesture recognition, deep learning algorithms are seldom\\nemployed as they require an unreasonable amount of effort from a single person,\\nto generate tens of thousands of examples.\\n  This work's hypothesis is that general, informative features can be learned\\nfrom the large amounts of data generated by aggregating the signals of multiple\\nusers, thus reducing the recording burden while enhancing gesture recognition.\\nConsequently, this paper proposes applying transfer learning on aggregated data\\nfrom multiple users, while leveraging the capacity of deep learning algorithms\\nto learn discriminant features from large datasets. Two datasets comprised of\\n19 and 17 able-bodied participants respectively (the first one is employed for\\npre-training) were recorded for this work, using the Myo Armband. A third Myo\\nArmband dataset was taken from the NinaPro database and is comprised of 10\\nable-bodied participants. Three different deep learning networks employing\\nthree different modalities as input (raw EMG, Spectrograms and Continuous\\nWavelet Transform (CWT)) are tested on the second and third dataset. The\\nproposed transfer learning scheme is shown to systematically and significantly\\nenhance the performance for all three networks on the two datasets, achieving\\nan offline accuracy of 98.31% for 7 gestures over 17 participants for the\\nCWT-based ConvNet and 68.98% for 18 gestures over 10 participants for the raw\\nEMG-based ConvNet. Finally, a use-case study employing eight able-bodied\\nparticipants suggests that real-time feedback allows users to adapt their\\nmuscle activation strategy which reduces the degradation in accuracy normally\\nexperienced over time.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1801.07756v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1801.07756v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ulysse Côté-Allard',\n",
       "   'Cheikh Latyr Fall',\n",
       "   'Alexandre Drouin',\n",
       "   'Alexandre Campeau-Lecours',\n",
       "   'Clément Gosselin',\n",
       "   'Kyrre Glette',\n",
       "   'François Laviolette',\n",
       "   'Benoit Gosselin'],\n",
       "  'tasks': ['EMG Gesture Recognition',\n",
       "   'General Classification',\n",
       "   'Gesture Recognition',\n",
       "   'Transfer Learning'],\n",
       "  'date': '2018-01-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/explaining-and-generalizing-back-translation',\n",
       "  'arxiv_id': '1806.04402',\n",
       "  'title': 'Explaining and Generalizing Back-Translation through Wake-Sleep',\n",
       "  'abstract': 'Back-translation has become a commonly employed heuristic for semi-supervised\\nneural machine translation. The technique is both straightforward to apply and\\nhas led to state-of-the-art results. In this work, we offer a principled\\ninterpretation of back-translation as approximate inference in a generative\\nmodel of bitext and show how the standard implementation of back-translation\\ncorresponds to a single iteration of the wake-sleep algorithm in our proposed\\nmodel. Moreover, this interpretation suggests a natural iterative\\ngeneralization, which we demonstrate leads to further improvement of up to 1.6\\nBLEU.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04402v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04402v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ryan Cotterell', 'Julia Kreutzer'],\n",
       "  'tasks': ['Machine Translation', 'Translation'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/quantum-classification-of-the-mnist-dataset',\n",
       "  'arxiv_id': '1805.08837',\n",
       "  'title': 'Quantum classification of the MNIST dataset with Slow Feature Analysis',\n",
       "  'abstract': 'Quantum machine learning carries the promise to revolutionize information and communication technologies. While a number of quantum algorithms with potential exponential speedups have been proposed already, it is quite difficult to provide convincing evidence that quantum computers with quantum memories will be in fact useful to solve real-world problems. Our work makes considerable progress towards this goal. We design quantum techniques for Dimensionality Reduction and for Classification, and combine them to provide an efficient and high accuracy quantum classifier that we test on the MNIST dataset. More precisely, we propose a quantum version of Slow Feature Analysis (QSFA), a dimensionality reduction technique that maps the dataset in a lower dimensional space where we can apply a novel quantum classification procedure, the Quantum Frobenius Distance (QFD). We simulate the quantum classifier (including errors) and show that it can provide classification of the MNIST handwritten digit dataset, a widely used dataset for benchmarking classification algorithms, with $98.5\\\\%$ accuracy, similar to the classical case. The running time of the quantum classifier is polylogarithmic in the dimension and number of data points. We also provide evidence that the other parameters on which the running time depends (condition number, Frobenius norm, error threshold, etc.) scale favorably in practice, thus ascertaining the efficiency of our algorithm.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1805.08837v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1805.08837v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Iordanis Kerenidis', 'Alessandro Luongo'],\n",
       "  'tasks': ['Classification',\n",
       "   'Dimensionality Reduction',\n",
       "   'General Classification'],\n",
       "  'date': '2018-05-22',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist'],\n",
       "  'datasets_used_full': ['MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/attentive-cross-modal-paratope-prediction',\n",
       "  'arxiv_id': '1806.04398',\n",
       "  'title': 'Attentive cross-modal paratope prediction',\n",
       "  'abstract': 'Antibodies are a critical part of the immune system, having the function of\\ndirectly neutralising or tagging undesirable objects (the antigens) for future\\ndestruction. Being able to predict which amino acids belong to the paratope,\\nthe region on the antibody which binds to the antigen, can facilitate antibody\\ndesign and contribute to the development of personalised medicine. The\\nsuitability of deep neural networks has recently been confirmed for this task,\\nwith Parapred outperforming all prior physical models. Our contribution is\\ntwofold: first, we significantly outperform the computational efficiency of\\nParapred by leveraging \\\\`a trous convolutions and self-attention. Secondly, we\\nimplement cross-modal attention by allowing the antibody residues to attend\\nover antigen residues. This leads to new state-of-the-art results on this task,\\nalong with insightful interpretations.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04398v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04398v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Andreea Deac', 'Petar Veličković', 'Pietro Sormanni'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/community-detection-in-partially-observable',\n",
       "  'arxiv_id': '1801.00132',\n",
       "  'title': 'Community Detection in Partially Observable Social Networks',\n",
       "  'abstract': \"The discovery of community structures in social networks has gained significant attention since it is a fundamental problem in understanding the networks' topology and functions. However, most social network data are collected from partially observable networks with both missing nodes and edges. In this paper, we address a new problem of detecting overlapping community structures in the context of such an incomplete network, where communities in the network are allowed to overlap since nodes belong to multiple communities at once. To solve this problem, we introduce KroMFac, a new framework that conducts community detection via regularized nonnegative matrix factorization (NMF) based on the Kronecker graph model. Specifically, from an inferred Kronecker generative parameter matrix, we first estimate the missing part of the network. As our major contribution to the proposed framework, to improve community detection accuracy, we then characterize and select influential nodes (which tend to have high degrees) by ranking, and add them to the existing graph. Finally, we uncover the community structures by solving the regularized NMF-aided optimization problem in terms of maximizing the likelihood of the underlying graph. Furthermore, adopting normalized mutual information (NMI), we empirically show superiority of our KroMFac approach over two baseline schemes by using both synthetic and real-world networks.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1801.00132v8',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1801.00132v8.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Cong Tran', 'Won-Yong Shin', 'Andreas Spitz'],\n",
       "  'tasks': ['Community Detection'],\n",
       "  'date': '2017-12-30',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['orkut'],\n",
       "  'datasets_used_full': ['Orkut'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/qiniu-submission-to-activitynet-challenge',\n",
       "  'arxiv_id': '1806.04391',\n",
       "  'title': 'Qiniu Submission to ActivityNet Challenge 2018',\n",
       "  'abstract': 'In this paper, we introduce our submissions for the tasks of trimmed activity\\nrecognition (Kinetics) and trimmed event recognition (Moments in Time) for\\nActivitynet Challenge 2018. In the two tasks, non-local neural networks and\\ntemporal segment networks are implemented as our base models. Multi-modal cues\\nsuch as RGB image, optical flow and acoustic signal have also been used in our\\nmethod. We also propose new non-local-based models for further improvement on\\nthe recognition accuracy. The final submissions after ensembling the models\\nachieve 83.5% top-1 accuracy and 96.8% top-5 accuracy on the Kinetics\\nvalidation set, 35.81% top-1 accuracy and 62.59% top-5 accuracy on the MIT\\nvalidation set.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04391v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04391v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xiaoteng Zhang',\n",
       "   'Yixin Bao',\n",
       "   'Feiyun Zhang',\n",
       "   'Kai Hu',\n",
       "   'Yicheng Wang',\n",
       "   'Liang Zhu',\n",
       "   'Qinzhu He',\n",
       "   'Yining Lin',\n",
       "   'Jie Shao',\n",
       "   'Yao Peng'],\n",
       "  'tasks': ['Activity Recognition', 'Optical Flow Estimation'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['kinetics', 'moments-in-time'],\n",
       "  'datasets_used_full': ['Kinetics', 'Moments in Time'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-weighted-kendall-and-high-order-kernels',\n",
       "  'arxiv_id': '1802.08526',\n",
       "  'title': 'The Weighted Kendall and High-order Kernels for Permutations',\n",
       "  'abstract': 'We propose new positive definite kernels for permutations. First we introduce\\na weighted version of the Kendall kernel, which allows to weight unequally the\\ncontributions of different item pairs in the permutations depending on their\\nranks. Like the Kendall kernel, we show that the weighted version is invariant\\nto relabeling of items and can be computed efficiently in $O(n \\\\ln(n))$\\noperations, where $n$ is the number of items in the permutation. Second, we\\npropose a supervised approach to learn the weights by jointly optimizing them\\nwith the function estimated by a kernel machine. Third, while the Kendall\\nkernel considers pairwise comparison between items, we extend it by considering\\nhigher-order comparisons among tuples of items and show that the supervised\\napproach of learning the weights can be systematically generalized to\\nhigher-order permutation kernels.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.08526v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.08526v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Yunlong Jiao', 'Jean-Philippe Vert'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-23',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/how-to-make-the-gradients-small',\n",
       "  'arxiv_id': '1801.02982',\n",
       "  'title': 'How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD',\n",
       "  'abstract': \"Stochastic gradient descent (SGD) gives an optimal convergence rate when minimizing convex stochastic objectives $f(x)$. However, in terms of making the gradients small, the original SGD does not give an optimal rate, even when $f(x)$ is convex. If $f(x)$ is convex, to find a point with gradient norm $\\\\varepsilon$, we design an algorithm SGD3 with a near-optimal rate $\\\\tilde{O}(\\\\varepsilon^{-2})$, improving the best known rate $O(\\\\varepsilon^{-8/3})$ of [18]. If $f(x)$ is nonconvex, to find its $\\\\varepsilon$-approximate local minimum, we design an algorithm SGD5 with rate $\\\\tilde{O}(\\\\varepsilon^{-3.5})$, where previously SGD variants only achieve $\\\\tilde{O}(\\\\varepsilon^{-4})$ [6, 15, 33]. This is no slower than the best known stochastic version of Newton's method in all parameter regimes [30].\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1801.02982v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1801.02982v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Zeyuan Allen-Zhu'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-01-08',\n",
       "  'methods': [{'name': 'SGD',\n",
       "    'full_name': 'Stochastic Gradient Descent',\n",
       "    'description': '**Stochastic Gradient Descent** is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:\\r\\n\\r\\n$$ w\\\\_{t+1} = w\\\\_{t} - \\\\eta\\\\hat{\\\\nabla}\\\\_{w}{L(w\\\\_{t})} $$\\r\\n\\r\\nWhere $\\\\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.\\r\\n\\r\\n(Image Source: [here](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/))',\n",
       "    'introduced_year': 1951,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/4e0ac120e9a8b096069c2f892488d630a5c8f358/torch/optim/sgd.py#L97-L112',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fast-rotational-sparse-coding',\n",
       "  'arxiv_id': '1806.04374',\n",
       "  'title': 'Fast Rotational Sparse Coding',\n",
       "  'abstract': 'We propose an algorithm for rotational sparse coding along with an efficient implementation using steerability. Sparse coding (also called dictionary learning) is an important technique in image processing, useful in inverse problems, compression, and analysis; however, the usual formulation fails to capture an important aspect of the structure of images: images are formed from building blocks, e.g., edges, lines, or points, that appear at different locations, orientations, and scales. The sparse coding problem can be reformulated to explicitly account for these transforms, at the cost of increased computation. In this work, we propose an algorithm for a rotational version of sparse coding that is based on K-SVD with additional rotation operations. We then propose a method to accelerate these rotations by learning the dictionary in a steerable basis. Our experiments on patch coding and texture classification demonstrate that the proposed algorithm is fast enough for practical use and compares favorably to standard sparse coding.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04374v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04374v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Michael T. McCann',\n",
       "   'Vincent Andrearczyk',\n",
       "   'Michael Unser',\n",
       "   'Adrien Depeursinge'],\n",
       "  'tasks': ['Dictionary Learning', 'Texture Classification'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet'],\n",
       "  'datasets_used_full': ['ImageNet'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/kernel-recursive-abc-point-estimation-with',\n",
       "  'arxiv_id': '1802.08404',\n",
       "  'title': 'Kernel Recursive ABC: Point Estimation with Intractable Likelihood',\n",
       "  'abstract': 'We propose a novel approach to parameter estimation for simulator-based\\nstatistical models with intractable likelihood. Our proposed method involves\\nrecursive application of kernel ABC and kernel herding to the same observed\\ndata. We provide a theoretical explanation regarding why the approach works,\\nshowing (for the population setting) that, under a certain assumption, point\\nestimates obtained with this method converge to the true parameter, as\\nrecursion proceeds. We have conducted a variety of numerical experiments,\\nincluding parameter estimation for a real-world pedestrian flow simulator, and\\nshow that in most cases our method outperforms existing approaches.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.08404v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.08404v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Takafumi Kajihara',\n",
       "   'Motonobu Kanagawa',\n",
       "   'Keisuke Yamazaki',\n",
       "   'Kenji Fukumizu'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-23',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/initialize-globally-before-acting-locally',\n",
       "  'arxiv_id': '1806.04368',\n",
       "  'title': 'Initialize globally before acting locally: Enabling Landmark-free 3D US to MRI Registration',\n",
       "  'abstract': \"Registration of partial-view 3D US volumes with MRI data is influenced by\\ninitialization. The standard of practice is using extrinsic or intrinsic\\nlandmarks, which can be very tedious to obtain. To overcome the limitations of\\nregistration initialization, we present a novel approach that is based on\\nEuclidean distance maps derived from easily obtainable coarse segmentations. We\\nevaluate our approach quantitatively on the publicly available RESECT dataset\\nand show that it is robust regarding overlap of target area and initial\\nposition. Furthermore, our method provides initializations that are suitable\\nfor state-of-the-art nonlinear, deformable image registration algorithm's\\ncapture ranges.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04368v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04368v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Julia Rackerseder',\n",
       "   'Maximilian Baust',\n",
       "   'Rüdiger Göbl',\n",
       "   'Nassir Navab',\n",
       "   'Christoph Hennersperger'],\n",
       "  'tasks': ['Image Registration'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/mean-field-multi-agent-reinforcement-learning',\n",
       "  'arxiv_id': '1802.05438',\n",
       "  'title': 'Mean Field Multi-Agent Reinforcement Learning',\n",
       "  'abstract': \"Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of agent interactions. In this paper, we present \\\\emph{Mean Field Reinforcement Learning} where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent's optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution to Nash equilibrium. Experiments on Gaussian squeeze, Ising model, and battle games justify the learning effectiveness of our mean field approaches. In addition, we report the first result to solve the Ising model via model-free reinforcement learning methods.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1802.05438v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1802.05438v5.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Yaodong Yang',\n",
       "   'Rui Luo',\n",
       "   'Minne Li',\n",
       "   'Ming Zhou',\n",
       "   'Wei-Nan Zhang',\n",
       "   'Jun Wang'],\n",
       "  'tasks': ['Multi-agent Reinforcement Learning', 'Q-Learning'],\n",
       "  'date': '2018-02-15',\n",
       "  'methods': [{'name': 'Q-Learning',\n",
       "    'full_name': 'Q-Learning',\n",
       "    'description': '**Q-Learning** is an off-policy temporal difference control algorithm:\\r\\n\\r\\n$$Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) \\\\leftarrow Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) + \\\\alpha\\\\left[R_{t+1} + \\\\gamma\\\\max\\\\_{a}Q\\\\left(S\\\\_{t+1}, a\\\\right) - Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right)\\\\right] $$\\r\\n\\r\\nThe learned action-value function $Q$ directly approximates $q\\\\_{*}$, the optimal action-value function, independent of the policy being followed.\\r\\n\\r\\nSource: Sutton and Barto, Reinforcement Learning, 2nd Edition',\n",
       "    'introduced_year': 1984,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Off-Policy TD Control',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Reinforcement Learning'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/drift-analysis',\n",
       "  'arxiv_id': '1712.00964',\n",
       "  'title': 'Drift Analysis',\n",
       "  'abstract': 'Drift analysis is one of the major tools for analysing evolutionary\\nalgorithms and nature-inspired search heuristics. In this chapter we give an\\nintroduction to drift analysis and give some examples of how to use it for the\\nanalysis of evolutionary algorithms.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.00964v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.00964v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Johannes Lengler'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-12-04',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/msplit-lbi-realizing-feature-selection-and',\n",
       "  'arxiv_id': '1806.04360',\n",
       "  'title': 'MSplit LBI: Realizing Feature Selection and Dense Estimation Simultaneously in Few-shot and Zero-shot Learning',\n",
       "  'abstract': 'It is one typical and general topic of learning a good embedding model to\\nefficiently learn the representation coefficients between two spaces/subspaces.\\nTo solve this task, $L_{1}$ regularization is widely used for the pursuit of\\nfeature selection and avoiding overfitting, and yet the sparse estimation of\\nfeatures in $L_{1}$ regularization may cause the underfitting of training data.\\n$L_{2}$ regularization is also frequently used, but it is a biased estimator.\\nIn this paper, we propose the idea that the features consist of three\\northogonal parts, \\\\emph{namely} sparse strong signals, dense weak signals and\\nrandom noise, in which both strong and weak signals contribute to the fitting\\nof data. To facilitate such novel decomposition, \\\\emph{MSplit} LBI is for the\\nfirst time proposed to realize feature selection and dense estimation\\nsimultaneously. We provide theoretical and simulational verification that our\\nmethod exceeds $L_{1}$ and $L_{2}$ regularization, and extensive experimental\\nresults show that our method achieves state-of-the-art performance in the\\nfew-shot and zero-shot learning.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04360v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04360v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Bo Zhao', 'Xinwei Sun', 'Yanwei Fu', 'Yuan YAO', 'Yizhou Wang'],\n",
       "  'tasks': ['Zero-Shot Learning'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet', 'cub-200-2011'],\n",
       "  'datasets_used_full': ['ImageNet', 'CUB-200-2011'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multi-task-neural-models-for-translating',\n",
       "  'arxiv_id': '1806.04357',\n",
       "  'title': 'Multi-Task Neural Models for Translating Between Styles Within and Across Languages',\n",
       "  'abstract': 'Generating natural language requires conveying content in an appropriate\\nstyle. We explore two related tasks on generating text of varying formality:\\nmonolingual formality transfer and formality-sensitive machine translation. We\\npropose to solve these tasks jointly using multi-task learning, and show that\\nour models achieve state-of-the-art performance for formality transfer and are\\nable to perform formality-sensitive translation without being explicitly\\ntrained on style-annotated translation examples.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04357v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04357v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Xing Niu', 'Sudha Rao', 'Marine Carpuat'],\n",
       "  'tasks': ['Machine Translation', 'Multi-Task Learning', 'Translation'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['gyafc'],\n",
       "  'datasets_used_full': ['GYAFC'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/gradnorm-gradient-normalization-for-adaptive',\n",
       "  'arxiv_id': '1711.02257',\n",
       "  'title': 'GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks',\n",
       "  'abstract': 'Deep multitask networks, in which one neural network produces multiple\\npredictive outputs, can offer better speed and performance than their\\nsingle-task counterparts but are challenging to train properly. We present a\\ngradient normalization (GradNorm) algorithm that automatically balances\\ntraining in deep multitask models by dynamically tuning gradient magnitudes. We\\nshow that for various network architectures, for both regression and\\nclassification tasks, and on both synthetic and real datasets, GradNorm\\nimproves accuracy and reduces overfitting across multiple tasks when compared\\nto single-task networks, static baselines, and other adaptive multitask loss\\nbalancing techniques. GradNorm also matches or surpasses the performance of\\nexhaustive grid search methods, despite only involving a single asymmetry\\nhyperparameter $\\\\alpha$. Thus, what was once a tedious search process that\\nincurred exponentially more compute for each task added can now be accomplished\\nwithin a few training runs, irrespective of the number of tasks. Ultimately, we\\nwill demonstrate that gradient manipulation affords us great control over the\\ntraining dynamics of multitask networks and may be one of the keys to unlocking\\nthe potential of multitask learning.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.02257v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.02257v4.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Zhao Chen',\n",
       "   'Vijay Badrinarayanan',\n",
       "   'Chen-Yu Lee',\n",
       "   'Andrew Rabinovich'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-11-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['nyuv2'],\n",
       "  'datasets_used_full': ['NYUv2'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/exploiting-document-knowledge-for-aspect',\n",
       "  'arxiv_id': '1806.04346',\n",
       "  'title': 'Exploiting Document Knowledge for Aspect-level Sentiment Classification',\n",
       "  'abstract': 'Attention-based long short-term memory (LSTM) networks have proven to be\\nuseful in aspect-level sentiment classification. However, due to the\\ndifficulties in annotating aspect-level data, existing public datasets for this\\ntask are all relatively small, which largely limits the effectiveness of those\\nneural models. In this paper, we explore two approaches that transfer knowledge\\nfrom document- level data, which is much less expensive to obtain, to improve\\nthe performance of aspect-level sentiment classification. We demonstrate the\\neffectiveness of our approaches on 4 public datasets from SemEval 2014, 2015,\\nand 2016, and we show that attention-based LSTM benefits from document-level\\nknowledge in multiple ways.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04346v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04346v1.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Ruidan He', 'Wee Sun Lee', 'Hwee Tou Ng', 'Daniel Dahlmeier'],\n",
       "  'tasks': ['Aspect-Based Sentiment Analysis',\n",
       "   'Classification',\n",
       "   'General Classification',\n",
       "   'Sentiment Analysis'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': ['semeval-2014-task-4-sub-task-2'],\n",
       "  'datasets_used_full': ['SemEval 2014 Task 4 Sub Task 2'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/optimizing-variational-quantum-circuits-using',\n",
       "  'arxiv_id': '1806.04344',\n",
       "  'title': 'Optimizing Variational Quantum Circuits using Evolution Strategies',\n",
       "  'abstract': 'This version withdrawn by arXiv administrators because the submitter did not\\nhave the right to agree to our license at the time of submission.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04344v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04344v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Johannes S. Otterbach'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/focused-hierarchical-rnns-for-conditional',\n",
       "  'arxiv_id': '1806.04342',\n",
       "  'title': 'Focused Hierarchical RNNs for Conditional Sequence Processing',\n",
       "  'abstract': 'Recurrent Neural Networks (RNNs) with attention mechanisms have obtained\\nstate-of-the-art results for many sequence processing tasks. Most of these\\nmodels use a simple form of encoder with attention that looks over the entire\\nsequence and assigns a weight to each token independently. We present a\\nmechanism for focusing RNN encoders for sequence modelling tasks which allows\\nthem to attend to key parts of the input as needed. We formulate this using a\\nmulti-layer conditional sequence encoder that reads in one token at a time and\\nmakes a discrete decision on whether the token is relevant to the context or\\nquestion being asked. The discrete gating mechanism takes in the context\\nembedding and the current hidden state as inputs and controls information flow\\ninto the layer above. We train it using policy gradient methods. We evaluate\\nthis method on several types of tasks with different attributes. First, we\\nevaluate the method on synthetic tasks which allow us to evaluate the model for\\nits generalization ability and probe the behavior of the gates in more\\ncontrolled settings. We then evaluate this approach on large scale Question\\nAnswering tasks including the challenging MS MARCO and SearchQA tasks. Our\\nmodels shows consistent improvements for both tasks over prior work and our\\nbaselines. It has also shown to generalize significantly better on synthetic\\ntasks as compared to the baselines.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04342v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04342v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Nan Rosemary Ke',\n",
       "   'Konrad Zolna',\n",
       "   'Alessandro Sordoni',\n",
       "   'Zhouhan Lin',\n",
       "   'Adam Trischler',\n",
       "   'Yoshua Bengio',\n",
       "   'Joelle Pineau',\n",
       "   'Laurent Charlin',\n",
       "   'Chris Pal'],\n",
       "  'tasks': ['Open-Domain Question Answering',\n",
       "   'Policy Gradient Methods',\n",
       "   'Question Answering'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['ms-marco', 'searchqa'],\n",
       "  'datasets_used_full': ['MS MARCO', 'SearchQA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/when-will-gradient-methods-converge-to-max',\n",
       "  'arxiv_id': '1806.04339',\n",
       "  'title': 'When Will Gradient Methods Converge to Max-margin Classifier under ReLU Models?',\n",
       "  'abstract': 'We study the implicit bias of gradient descent methods in solving a binary\\nclassification problem over a linearly separable dataset. The classifier is\\ndescribed by a nonlinear ReLU model and the objective function adopts the\\nexponential loss function. We first characterize the landscape of the loss\\nfunction and show that there can exist spurious asymptotic local minima besides\\nasymptotic global minima. We then show that gradient descent (GD) can converge\\nto either a global or a local max-margin direction, or may diverge from the\\ndesired max-margin direction in a general context. For stochastic gradient\\ndescent (SGD), we show that it converges in expectation to either the global or\\nthe local max-margin direction if SGD converges. We further explore the\\nimplicit bias of these algorithms in learning a multi-neuron network under\\ncertain stationary conditions, and show that the learned classifier maximizes\\nthe margins of each sample pattern partition under the ReLU activation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04339v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04339v2.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Tengyu Xu', 'Yi Zhou', 'Kaiyi Ji', 'Yingbin Liang'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [{'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'SGD',\n",
       "    'full_name': 'Stochastic Gradient Descent',\n",
       "    'description': '**Stochastic Gradient Descent** is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:\\r\\n\\r\\n$$ w\\\\_{t+1} = w\\\\_{t} - \\\\eta\\\\hat{\\\\nabla}\\\\_{w}{L(w\\\\_{t})} $$\\r\\n\\r\\nWhere $\\\\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.\\r\\n\\r\\n(Image Source: [here](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/))',\n",
       "    'introduced_year': 1951,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/4e0ac120e9a8b096069c2f892488d630a5c8f358/torch/optim/sgd.py#L97-L112',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-compromise-principle-in-deep-monocular',\n",
       "  'arxiv_id': '1708.08267',\n",
       "  'title': 'A Compromise Principle in Deep Monocular Depth Estimation',\n",
       "  'abstract': 'Monocular depth estimation, which plays a key role in understanding 3D scene\\ngeometry, is fundamentally an ill-posed problem. Existing methods based on deep\\nconvolutional neural networks (DCNNs) have examined this problem by learning\\nconvolutional networks to estimate continuous depth maps from monocular images.\\nHowever, we find that training a network to predict a high spatial resolution\\ncontinuous depth map often suffers from poor local solutions. In this paper, we\\nhypothesize that achieving a compromise between spatial and depth resolutions\\ncan improve network training. Based on this \"compromise principle\", we propose\\na regression-classification cascaded network (RCCN), which consists of a\\nregression branch predicting a low spatial resolution continuous depth map and\\na classification branch predicting a high spatial resolution discrete depth\\nmap. The two branches form a cascaded structure allowing the classification and\\nregression branches to benefit from each other. By leveraging large-scale raw\\ntraining datasets and some data augmentation strategies, our network achieves\\ntop or state-of-the-art results on the NYU Depth V2, KITTI, and Make3D\\nbenchmarks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1708.08267v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1708.08267v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Huan Fu', 'Mingming Gong', 'Chaohui Wang', 'DaCheng Tao'],\n",
       "  'tasks': ['Classification',\n",
       "   'Data Augmentation',\n",
       "   'Depth Estimation',\n",
       "   'General Classification',\n",
       "   'Monocular Depth Estimation'],\n",
       "  'date': '2017-08-28',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cityscapes', 'nyuv2', 'make3d'],\n",
       "  'datasets_used_full': ['Cityscapes', 'NYUv2', 'Make3D'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fast-and-accurate-tensor-completion-with',\n",
       "  'arxiv_id': '1804.06128',\n",
       "  'title': 'Fast and Accurate Tensor Completion with Total Variation Regularized Tensor Trains',\n",
       "  'abstract': 'We propose a new tensor completion method based on tensor trains. The\\nto-be-completed tensor is modeled as a low-rank tensor train, where we use the\\nknown tensor entries and their coordinates to update the tensor train. A novel\\ntensor train initialization procedure is proposed specifically for image and\\nvideo completion, which is demonstrated to ensure fast convergence of the\\ncompletion algorithm. The tensor train framework is also shown to easily\\naccommodate Total Variation and Tikhonov regularization due to their low-rank\\ntensor train representations. Image and video inpainting experiments verify the\\nsuperiority of the proposed scheme in terms of both speed and scalability,\\nwhere a speedup of up to 155X is observed compared to state-of-the-art tensor\\ncompletion methods at a similar accuracy. Moreover, we demonstrate the proposed\\nscheme is especially advantageous over existing algorithms when only tiny\\nportions (say, 1%) of the to-be-completed images/videos are known.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.06128v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.06128v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ching-Yun Ko', 'Kim Batselier', 'Wenjian Yu', 'Ngai Wong'],\n",
       "  'tasks': ['Video Inpainting'],\n",
       "  'date': '2018-04-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/small-loss-bounds-for-online-learning-with',\n",
       "  'arxiv_id': '1711.03639',\n",
       "  'title': 'Small-loss bounds for online learning with partial information',\n",
       "  'abstract': 'We consider the problem of adversarial (non-stochastic) online learning with partial information feedback, where at each round, a decision maker selects an action from a finite set of alternatives. We develop a black-box approach for such problems where the learner observes as feedback only losses of a subset of the actions that includes the selected action. When losses of actions are non-negative, under the graph-based feedback model introduced by Mannor and Shamir, we offer algorithms that attain the so called \"small-loss\" $o(\\\\alpha L^{\\\\star})$ regret bounds with high probability, where $\\\\alpha$ is the independence number of the graph, and $L^{\\\\star}$ is the loss of the best action. Prior to our work, there was no data-dependent guarantee for general feedback graphs even for pseudo-regret (without dependence on the number of actions, i.e. utilizing the increased information feedback). Taking advantage of the black-box nature of our technique, we extend our results to many other applications such as semi-bandits (including routing in networks), contextual bandits (even with an infinite comparator class), as well as learning with slowly changing (shifting) comparators. In the special case of classical bandit and semi-bandit problems, we provide optimal small-loss, high-probability guarantees of $\\\\tilde{O}(\\\\sqrt{dL^{\\\\star}})$ for actual regret, where $d$ is the number of actions, answering open questions of Neu. Previous bounds for bandits and semi-bandits were known only for pseudo-regret and only in expectation. We also offer an optimal $\\\\tilde{O}(\\\\sqrt{\\\\kappa L^{\\\\star}})$ regret guarantee for fixed feedback graphs with clique-partition number at most $\\\\kappa$.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1711.03639v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1711.03639v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Thodoris Lykouris', 'Karthik Sridharan', 'Eva Tardos'],\n",
       "  'tasks': ['Multi-Armed Bandits', 'online learning'],\n",
       "  'date': '2017-11-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/automatic-ship-detection-of-remote-sensing',\n",
       "  'arxiv_id': '1806.04331',\n",
       "  'title': 'Automatic Ship Detection of Remote Sensing Images from Google Earth in Complex Scenes Based on Multi-Scale Rotation Dense Feature Pyramid Networks',\n",
       "  'abstract': 'Ship detection has been playing a significant role in the field of remote\\nsensing for a long time but it is still full of challenges. The main\\nlimitations of traditional ship detection methods usually lie in the complexity\\nof application scenarios, the difficulty of intensive object detection and the\\nredundancy of detection region. In order to solve such problems above, we\\npropose a framework called Rotation Dense Feature Pyramid Networks (R-DFPN)\\nwhich can effectively detect ship in different scenes including ocean and port.\\nSpecifically, we put forward the Dense Feature Pyramid Network (DFPN), which is\\naimed at solving the problem resulted from the narrow width of the ship.\\nCompared with previous multi-scale detectors such as Feature Pyramid Network\\n(FPN), DFPN builds the high-level semantic feature-maps for all scales by means\\nof dense connections, through which enhances the feature propagation and\\nencourages the feature reuse. Additionally, in the case of ship rotation and\\ndense arrangement, we design a rotation anchor strategy to predict the minimum\\ncircumscribed rectangle of the object so as to reduce the redundant detection\\nregion and improve the recall. Furthermore, we also propose multi-scale ROI\\nAlign for the purpose of maintaining the completeness of semantic and spatial\\ninformation. Experiments based on remote sensing images from Google Earth for\\nship detection show that our detection method based on R-DFPN representation\\nhas a state-of-the-art performance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04331v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04331v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xue Yang',\n",
       "   'Hao Sun',\n",
       "   'Kun fu',\n",
       "   'Jirui Yang',\n",
       "   'Xian Sun',\n",
       "   'Menglong Yan',\n",
       "   'Zhi Guo'],\n",
       "  'tasks': ['Object Detection'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['100doh'],\n",
       "  'datasets_used_full': ['100DOH'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/neural-network-models-for-paraphrase',\n",
       "  'arxiv_id': '1806.04330',\n",
       "  'title': 'Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering',\n",
       "  'abstract': 'In this paper, we analyze several neural network designs (and their\\nvariations) for sentence pair modeling and compare their performance\\nextensively across eight datasets, including paraphrase identification,\\nsemantic textual similarity, natural language inference, and question answering\\ntasks. Although most of these models have claimed state-of-the-art performance,\\nthe original papers often reported on only one or two selected datasets. We\\nprovide a systematic study and show that (i) encoding contextual information by\\nLSTM and inter-sentence interactions are critical, (ii) Tree-LSTM does not help\\nas much as previously claimed but surprisingly improves performance on Twitter\\ndatasets, (iii) the Enhanced Sequential Inference Model is the best so far for\\nlarger datasets, while the Pairwise Word Interaction Model achieves the best\\nperformance when less data is available. We release our implementations as an\\nopen-source toolkit.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04330v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04330v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Wuwei Lan', 'Wei Xu'],\n",
       "  'tasks': ['Natural Language Inference',\n",
       "   'Paraphrase Identification',\n",
       "   'Question Answering',\n",
       "   'Semantic Textual Similarity',\n",
       "   'Sentence Pair Modeling'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['snli',\n",
       "   'multinli',\n",
       "   'wikiqa',\n",
       "   'quora-1',\n",
       "   'trecqa',\n",
       "   'sts-2014',\n",
       "   'pit'],\n",
       "  'datasets_used_full': ['SNLI',\n",
       "   'MultiNLI',\n",
       "   'WikiQA',\n",
       "   'Quora',\n",
       "   'TrecQA',\n",
       "   'STS 2014',\n",
       "   'PIT'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/sparse-collaborative-or-nonnegative',\n",
       "  'arxiv_id': '1806.04329',\n",
       "  'title': 'Sparse, Collaborative, or Nonnegative Representation: Which Helps Pattern Classification?',\n",
       "  'abstract': 'The use of sparse representation (SR) and collaborative representation (CR)\\nfor pattern classification has been widely studied in tasks such as face\\nrecognition and object categorization. Despite the success of SR/CR based\\nclassifiers, it is still arguable whether it is the $\\\\ell_{1}$-norm sparsity or\\nthe $\\\\ell_{2}$-norm collaborative property that brings the success of SR/CR\\nbased classification. In this paper, we investigate the use of nonnegative\\nrepresentation (NR) for pattern classification, which is largely ignored by\\nprevious work. Our analyses reveal that NR can boost the representation power\\nof homogeneous samples while limiting the representation power of heterogeneous\\nsamples, making the representation sparse and discriminative simultaneously and\\nthus providing a more effective solution to representation based classification\\nthan SR/CR. Our experiments demonstrate that the proposed NR based classifier\\n(NRC) outperforms previous representation based classifiers. With deep features\\nas inputs, it also achieves state-of-the-art performance on various visual\\nclassification tasks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04329v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04329v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jun Xu', 'Wangpeng An', 'Lei Zhang', 'David Zhang'],\n",
       "  'tasks': ['Classification', 'Face Recognition', 'General Classification'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist',\n",
       "   'cub-200-2011',\n",
       "   'oxford-102-flower',\n",
       "   'caltech-256',\n",
       "   'usps',\n",
       "   'extended-yale-b-1'],\n",
       "  'datasets_used_full': ['MNIST',\n",
       "   'CUB-200-2011',\n",
       "   'Oxford 102 Flower',\n",
       "   'Caltech-256',\n",
       "   'USPS',\n",
       "   'Extended Yale B'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-representations-and-generative',\n",
       "  'arxiv_id': '1707.02392',\n",
       "  'title': 'Learning Representations and Generative Models for 3D Point Clouds',\n",
       "  'abstract': 'Three-dimensional geometric data offer an excellent domain for studying\\nrepresentation learning and generative modeling. In this paper, we look at\\ngeometric data represented as point clouds. We introduce a deep AutoEncoder\\n(AE) network with state-of-the-art reconstruction quality and generalization\\nability. The learned representations outperform existing methods on 3D\\nrecognition tasks and enable shape editing via simple algebraic manipulations,\\nsuch as semantic part editing, shape analogies and shape interpolation, as well\\nas shape completion. We perform a thorough study of different generative models\\nincluding GANs operating on the raw point clouds, significantly improved GANs\\ntrained in the fixed latent space of our AEs, and Gaussian Mixture Models\\n(GMMs). To quantitatively evaluate generative models we introduce measures of\\nsample fidelity and diversity based on matchings between sets of point clouds.\\nInterestingly, our evaluation of generalization, fidelity and diversity reveals\\nthat GMMs trained in the latent space of our AEs yield the best results\\noverall.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1707.02392v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1707.02392v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Panos Achlioptas',\n",
       "   'Olga Diamanti',\n",
       "   'Ioannis Mitliagkas',\n",
       "   'Leonidas Guibas'],\n",
       "  'tasks': ['Representation Learning'],\n",
       "  'date': '2017-07-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['modelnet'],\n",
       "  'datasets_used_full': ['ModelNet'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/iso-standard-domain-independent-dialogue-act',\n",
       "  'arxiv_id': '1806.04327',\n",
       "  'title': 'ISO-Standard Domain-Independent Dialogue Act Tagging for Conversational Agents',\n",
       "  'abstract': \"Dialogue Act (DA) tagging is crucial for spoken language understanding\\nsystems, as it provides a general representation of speakers' intents, not\\nbound to a particular dialogue system. Unfortunately, publicly available data\\nsets with DA annotation are all based on different annotation schemes and thus\\nincompatible with each other. Moreover, their schemes often do not cover all\\naspects necessary for open-domain human-machine interaction. In this paper, we\\npropose a methodology to map several publicly available corpora to a subset of\\nthe ISO standard, in order to create a large task-independent training corpus\\nfor DA classification. We show the feasibility of using this corpus to train a\\ndomain-independent DA tagger testing it on out-of-domain conversational data,\\nand argue the importance of training on multiple corpora to achieve robustness\\nacross different DA categories.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04327v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04327v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Stefano Mezza',\n",
       "   'Alessandra Cervone',\n",
       "   'Giuliano Tortoreto',\n",
       "   'Evgeny A. Stepanov',\n",
       "   'Giuseppe Riccardi'],\n",
       "  'tasks': ['General Classification', 'Spoken Language Understanding'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/differentiable-compositional-kernel-learning',\n",
       "  'arxiv_id': '1806.04326',\n",
       "  'title': 'Differentiable Compositional Kernel Learning for Gaussian Processes',\n",
       "  'abstract': 'The generalization properties of Gaussian processes depend heavily on the\\nchoice of kernel, and this choice remains a dark art. We present the Neural\\nKernel Network (NKN), a flexible family of kernels represented by a neural\\nnetwork. The NKN architecture is based on the composition rules for kernels, so\\nthat each unit of the network corresponds to a valid kernel. It can compactly\\napproximate compositional kernel structures such as those used by the Automatic\\nStatistician (Lloyd et al., 2014), but because the architecture is\\ndifferentiable, it is end-to-end trainable with gradient-based optimization. We\\nshow that the NKN is universal for the class of stationary kernels. Empirically\\nwe demonstrate pattern discovery and extrapolation abilities of NKN on several\\ntasks that depend crucially on identifying the underlying structure, including\\ntime series and texture extrapolation, as well as Bayesian optimization.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04326v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04326v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Shengyang Sun',\n",
       "   'Guodong Zhang',\n",
       "   'Chaoqi Wang',\n",
       "   'Wenyuan Zeng',\n",
       "   'Jiaman Li',\n",
       "   'Roger Grosse'],\n",
       "  'tasks': ['Gaussian Processes', 'Time Series'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/augmenting-stream-constraint-programming-with',\n",
       "  'arxiv_id': '1806.04325',\n",
       "  'title': 'Augmenting Stream Constraint Programming with Eventuality Conditions',\n",
       "  'abstract': 'Stream constraint programming is a recent addition to the family of\\nconstraint programming frameworks, where variable domains are sets of infinite\\nstreams over finite alphabets. Previous works showed promising results for its\\napplicability to real-world planning and control problems. In this paper,\\nmotivated by the modelling of planning applications, we improve the\\nexpressiveness of the framework by introducing 1) the \"until\" constraint, a new\\nconstruct that is adapted from Linear Temporal Logic and 2) the @ operator on\\nstreams, a syntactic sugar for which we provide a more efficient solving\\nalgorithm over simple desugaring. For both constructs, we propose corresponding\\nnovel solving algorithms and prove their correctness. We present competitive\\nexperimental results on the Missionaries and Cannibals logic puzzle and a\\nstandard path planning application on the grid, by comparing with Apt and\\nBrand\\'s method for verifying eventuality conditions using a CP approach.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04325v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04325v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jasper C. H. Lee', 'Jimmy H. M. Lee', 'Allen Z. Zhong'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/end-to-end-learning-of-energy-constrained',\n",
       "  'arxiv_id': '1806.04321',\n",
       "  'title': 'Energy-Constrained Compression for Deep Neural Networks via Weighted Sparse Projection and Layer Input Masking',\n",
       "  'abstract': 'Deep Neural Networks (DNNs) are increasingly deployed in highly energy-constrained environments such as autonomous drones and wearable devices while at the same time must operate in real-time. Therefore, reducing the energy consumption has become a major design consideration in DNN training. This paper proposes the first end-to-end DNN training framework that provides quantitative energy consumption guarantees via weighted sparse projection and input masking. The key idea is to formulate the DNN training as an optimization problem in which the energy budget imposes a previously unconsidered optimization constraint. We integrate the quantitative DNN energy estimation into the DNN training process to assist the constrained optimization. We prove that an approximate algorithm can be used to efficiently solve the optimization problem. Compared to the best prior energy-saving methods, our framework trains DNNs that provide higher accuracies under same or lower energy budgets. Code is publicly available.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04321v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04321v3.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Haichuan Yang', 'Yuhao Zhu', 'Ji Liu'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet', 'mnist', 'ms-celeb-1m'],\n",
       "  'datasets_used_full': ['ImageNet', 'MNIST', 'MS-Celeb-1M'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/support-vector-machine-application-for',\n",
       "  'arxiv_id': '1806.05054',\n",
       "  'title': 'Support Vector Machine Application for Multiphase Flow Pattern Prediction',\n",
       "  'abstract': 'In this paper a data analytical approach featuring support vector machines\\n(SVM) is employed to train a predictive model over an experimentaldataset,\\nwhich consists of the most relevant studies for two-phase flow pattern\\nprediction. The database for this study consists of flow patterns or flow\\nregimes in gas-liquid two-phase flow. The term flow pattern refers to the\\ngeometrical configuration of the gas and liquid phases in the pipe. When gas\\nand liquid flow simultaneously in a pipe, the two phases can distribute\\nthemselves in a variety of flow configurations. Gas-liquid two-phase flow\\noccurs ubiquitously in various major industrial fields: petroleum, chemical,\\nnuclear, and geothermal industries. The flow configurations differ from each\\nother in the spatial distribution of the interface, resulting in different flow\\ncharacteristics. Experimental results obtained by applying the presented\\nmethodology to different combinations of flow patterns demonstrate that the\\nproposed approach is state-of-the-art alternatives by achieving 97% correct\\nclassification. The results suggest machine learning could be used as an\\neffective tool for automatic detection and classification of gas-liquid flow\\npatterns.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05054v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05054v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Pablo Guillen-Rondon',\n",
       "   'Melvin D. Robinson',\n",
       "   'Carlos Torres',\n",
       "   'Eduardo Pereya'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/embedding-text-in-hyperbolic-spaces',\n",
       "  'arxiv_id': '1806.04313',\n",
       "  'title': 'Embedding Text in Hyperbolic Spaces',\n",
       "  'abstract': \"Natural language text exhibits hierarchical structure in a variety of\\nrespects. Ideally, we could incorporate our prior knowledge of this\\nhierarchical structure into unsupervised learning algorithms that work on text\\ndata. Recent work by Nickel & Kiela (2017) proposed using hyperbolic instead of\\nEuclidean embedding spaces to represent hierarchical data and demonstrated\\nencouraging results when embedding graphs. In this work, we extend their method\\nwith a re-parameterization technique that allows us to learn hyperbolic\\nembeddings of arbitrarily parameterized objects. We apply this framework to\\nlearn word and sentence embeddings in hyperbolic space in an unsupervised\\nmanner from text corpora. The resulting embeddings seem to encode certain\\nintuitive notions of hierarchy, such as word-context frequency and phrase\\nconstituency. However, the implicit continuous hierarchy in the learned\\nhyperbolic space makes interrogating the model's learned hierarchies more\\ndifficult than for models that learn explicit edges between items. The learned\\nhyperbolic embeddings show improvements over Euclidean embeddings in some --\\nbut not all -- downstream tasks, suggesting that hierarchical organization is\\nmore useful for some tasks than others.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04313v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04313v1.pdf',\n",
       "  'proceeding': 'WS 2018 6',\n",
       "  'authors': ['Bhuwan Dhingra',\n",
       "   'Christopher J. Shallue',\n",
       "   'Mohammad Norouzi',\n",
       "   'Andrew M. Dai',\n",
       "   'George E. Dahl'],\n",
       "  'tasks': ['Sentence Embeddings'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['penn-treebank', 'snli', 'multinli', 'hyperlex'],\n",
       "  'datasets_used_full': ['Penn Treebank', 'SNLI', 'MultiNLI', 'HyperLex'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/pixels-voxels-and-views-a-study-of-shape',\n",
       "  'arxiv_id': '1804.06032',\n",
       "  'title': 'Pixels, voxels, and views: A study of shape representations for single view 3D object shape prediction',\n",
       "  'abstract': 'The goal of this paper is to compare surface-based and volumetric 3D object\\nshape representations, as well as viewer-centered and object-centered reference\\nframes for single-view 3D shape prediction. We propose a new algorithm for\\npredicting depth maps from multiple viewpoints, with a single depth or RGB\\nimage as input. By modifying the network and the way models are evaluated, we\\ncan directly compare the merits of voxels vs. surfaces and viewer-centered vs.\\nobject-centered for familiar vs. unfamiliar objects, as predicted from RGB or\\ndepth images. Among our findings, we show that surface-based methods outperform\\nvoxel representations for objects from novel classes and produce higher\\nresolution outputs. We also find that using viewer-centered coordinates is\\nadvantageous for novel objects, while object-centered representations are\\nbetter for more familiar objects. Interestingly, the coordinate frame\\nsignificantly affects the shape representation learned, with object-centered\\nplacing more importance on implicitly recognizing the object category and\\nviewer-centered producing shape representations with less dependence on\\ncategory recognition.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.06032v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.06032v2.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Daeyun Shin', 'Charless C. Fowlkes', 'Derek Hoiem'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-04-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['shapenet', 'pascal3d-2', 'shapenetcore'],\n",
       "  'datasets_used_full': ['ShapeNet', 'PASCAL3D+', 'ShapeNetCore'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/efficient-end-to-end-learning-for-quantizable',\n",
       "  'arxiv_id': '1805.05809',\n",
       "  'title': 'Efficient end-to-end learning for quantizable representations',\n",
       "  'abstract': 'Embedding representation learning via neural networks is at the core\\nfoundation of modern similarity based search. While much effort has been put in\\ndeveloping algorithms for learning binary hamming code representations for\\nsearch efficiency, this still requires a linear scan of the entire dataset per\\neach query and trades off the search accuracy through binarization. To this\\nend, we consider the problem of directly learning a quantizable embedding\\nrepresentation and the sparse binary hash code end-to-end which can be used to\\nconstruct an efficient hash table not only providing significant search\\nreduction in the number of data but also achieving the state of the art search\\naccuracy outperforming previous state of the art deep metric learning methods.\\nWe also show that finding the optimal sparse binary hash code in a mini-batch\\ncan be computed exactly in polynomial time by solving a minimum cost flow\\nproblem. Our results on Cifar-100 and on ImageNet datasets show the state of\\nthe art search accuracy in precision@k and NMI metrics while providing up to\\n98X and 478X search speedup respectively over exhaustive linear search. The\\nsource code is available at\\nhttps://github.com/maestrojeong/Deep-Hash-Table-ICML18',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.05809v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.05809v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Yeonwoo Jeong', 'Hyun Oh Song'],\n",
       "  'tasks': ['Binarization', 'Metric Learning', 'Representation Learning'],\n",
       "  'date': '2018-05-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/mission-ultra-large-scale-feature-selection',\n",
       "  'arxiv_id': '1806.04310',\n",
       "  'title': 'MISSION: Ultra Large-Scale Feature Selection using Count-Sketches',\n",
       "  'abstract': 'Feature selection is an important challenge in machine learning. It plays a\\ncrucial role in the explainability of machine-driven decisions that are rapidly\\npermeating throughout modern society. Unfortunately, the explosion in the size\\nand dimensionality of real-world datasets poses a severe challenge to standard\\nfeature selection algorithms. Today, it is not uncommon for datasets to have\\nbillions of dimensions. At such scale, even storing the feature vector is\\nimpossible, causing most existing feature selection methods to fail.\\nWorkarounds like feature hashing, a standard approach to large-scale machine\\nlearning, helps with the computational feasibility, but at the cost of losing\\nthe interpretability of features. In this paper, we present MISSION, a novel\\nframework for ultra large-scale feature selection that performs stochastic\\ngradient descent while maintaining an efficient representation of the features\\nin memory using a Count-Sketch data structure. MISSION retains the simplicity\\nof feature hashing without sacrificing the interpretability of the features\\nwhile using only O(log^2(p)) working memory. We demonstrate that MISSION\\naccurately and efficiently performs feature selection on real-world,\\nlarge-scale datasets with billions of dimensions.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04310v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04310v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Amirali Aghazadeh',\n",
       "   'Ryan Spring',\n",
       "   'Daniel Lejeune',\n",
       "   'Gautam Dasarathy',\n",
       "   'Anshumali Shrivastava',\n",
       "   'Richard G. Baraniuk'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [{'name': 'Interpretability',\n",
       "    'full_name': 'Interpretability',\n",
       "    'description': 'Please enter a description about the method here',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1310.1533v2',\n",
       "    'source_title': 'CAM: Causal additive models, high-dimensional order search and penalized regression',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Image Models',\n",
       "     'description': '**Image Models** are methods that build representations of images for downstream tasks such as classification and object detection. The most popular subcategory are convolutional neural networks. Below you can find a continuously updated list of image models.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/differentially-private-matrix-completion',\n",
       "  'arxiv_id': '1712.09765',\n",
       "  'title': 'Differentially Private Matrix Completion Revisited',\n",
       "  'abstract': \"We provide the first provably joint differentially private algorithm with\\nformal utility guarantees for the problem of user-level privacy-preserving\\ncollaborative filtering. Our algorithm is based on the Frank-Wolfe method, and\\nit consistently estimates the underlying preference matrix as long as the\\nnumber of users $m$ is $\\\\omega(n^{5/4})$, where $n$ is the number of items, and\\neach user provides her preference for at least $\\\\sqrt{n}$ randomly selected\\nitems. Along the way, we provide an optimal differentially private algorithm\\nfor singular vector computation, based on the celebrated Oja's method, that\\nprovides significant savings in terms of space and time while operating on\\nsparse matrices. We also empirically evaluate our algorithm on a suite of\\ndatasets, and show that it consistently outperforms the state-of-the-art\\nprivate algorithms.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.09765v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.09765v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Prateek Jain', 'Om Thakkar', 'Abhradeep Thakurta'],\n",
       "  'tasks': ['Collaborative Filtering', 'Matrix Completion'],\n",
       "  'date': '2017-12-28',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['movielens', 'netflix-prize', 'jester'],\n",
       "  'datasets_used_full': ['MovieLens', 'Netflix Prize', 'Jester'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/diverse-online-feature-selection',\n",
       "  'arxiv_id': '1806.04308',\n",
       "  'title': 'Diverse Online Feature Selection',\n",
       "  'abstract': 'Online feature selection has been an active research area in recent years. We\\npropose a novel diverse online feature selection method based on Determinantal\\nPoint Processes (DPP). Our model aims to provide diverse features which can be\\ncomposed in either a supervised or unsupervised framework. The framework aims\\nto promote diversity based on the kernel produced on a feature level, through\\nat most three stages: feature sampling, local criteria and global criteria for\\nfeature selection. In the feature sampling, we sample incoming stream of\\nfeatures using conditional DPP. The local criteria is used to assess and select\\nstreamed features (i.e. only when they arrive), we use unsupervised scale\\ninvariant methods to remove redundant features and optionally supervised\\nmethods to introduce label information to assess relevant features. Lastly, the\\nglobal criteria uses regularization methods to select a global optimal subset\\nof features. This three stage procedure continues until there are no more\\nfeatures arriving or some predefined stopping condition is met. We demonstrate\\nbased on experiments conducted on that this approach yields better compactness,\\nis comparable and in some instances outperforms other state-of-the-art online\\nfeature selection methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04308v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04308v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Chapman Siu', 'Richard Yi Da Xu'],\n",
       "  'tasks': ['Point Processes'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-blur-mapping-exploiting-high-level',\n",
       "  'arxiv_id': '1612.01227',\n",
       "  'title': 'Deep Blur Mapping: Exploiting High-Level Semantics by Deep Neural Networks',\n",
       "  'abstract': 'The human visual system excels at detecting local blur of visual images, but\\nthe underlying mechanism is not well understood. Traditional views of blur such\\nas reduction in energy at high frequencies and loss of phase coherence at\\nlocalized features have fundamental limitations. For example, they cannot well\\ndiscriminate flat regions from blurred ones. Here we propose that high-level\\nsemantic information is critical in successfully identifying local blur.\\nTherefore, we resort to deep neural networks that are proficient at learning\\nhigh-level features and propose the first end-to-end local blur mapping\\nalgorithm based on a fully convolutional network. By analyzing various\\narchitectures with different depths and design philosophies, we empirically\\nshow that high-level features of deeper layers play a more important role than\\nlow-level features of shallower layers in resolving challenging ambiguities for\\nthis task. We test the proposed method on a standard blur detection benchmark\\nand demonstrate that it significantly advances the state-of-the-art (ODS\\nF-score of 0.853). Furthermore, we explore the use of the generated blur maps\\nin three applications, including blur region segmentation, blur degree\\nestimation, and blur magnification.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1612.01227v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1612.01227v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kede Ma',\n",
       "   'Huan Fu',\n",
       "   'Tongliang Liu',\n",
       "   'Zhou Wang',\n",
       "   'DaCheng Tao'],\n",
       "  'tasks': [],\n",
       "  'date': '2016-12-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/model-free-information-extraction-in-enriched',\n",
       "  'arxiv_id': '1804.05170',\n",
       "  'title': 'Model-Free Information Extraction in Enriched Nonlinear Phase-Space',\n",
       "  'abstract': 'Detecting anomalies and discovering driving signals is an essential component\\nof scientific research and industrial practice. Often the underlying mechanism\\nis highly complex, involving hidden evolving nonlinear dynamics and noise\\ncontamination. When representative physical models and large labeled data sets\\nare unavailable, as is the case with most real-world applications,\\nmodel-dependent Bayesian approaches would yield misleading results, and most\\nsupervised learning machines would also fail to reliably resolve the\\nintricately evolving systems. Here, we propose an unsupervised machine-learning\\napproach that operates in a well-constructed function space, whereby the\\nevolving nonlinear dynamics are captured through a linear functional\\nrepresentation determined by the Koopman operator. This breakthrough leverages\\non the time-feature embedding and the ensuing reconstruction of a phase-space\\nrepresentation of the dynamics, thereby permitting the reliable identification\\nof critical global signatures from the whole trajectory. This dramatically\\nimproves over commonly used static local features, which are vulnerable to\\nunknown transitions or noise. Thanks to its data-driven nature, our method\\nexcludes any prior models and training corpus. We benchmark the astonishing\\naccuracy of our method on three diverse and challenging problems in: biology,\\nmedicine, and engineering. In all cases, it outperforms existing\\nstate-of-the-art methods. As a new unsupervised information processing\\nparadigm, it is suitable for ubiquitous nonlinear dynamical systems or\\nend-users with little expertise, which permits an unbiased excavation of\\nunderlying working principles or intrinsic correlations submerged in unlabeled\\ndata flows.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.05170v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.05170v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Bin Li', 'Yueheng Lan', 'Weisi Guo', 'Chenglin Zhao'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-04-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-a-discriminative-filter-bank-within',\n",
       "  'arxiv_id': '1611.09932',\n",
       "  'title': 'Learning a Discriminative Filter Bank within a CNN for Fine-grained Recognition',\n",
       "  'abstract': 'Compared to earlier multistage frameworks using CNN features, recent\\nend-to-end deep approaches for fine-grained recognition essentially enhance the\\nmid-level learning capability of CNNs. Previous approaches achieve this by\\nintroducing an auxiliary network to infuse localization information into the\\nmain classification network, or a sophisticated feature encoding method to\\ncapture higher order feature statistics. We show that mid-level representation\\nlearning can be enhanced within the CNN framework, by learning a bank of\\nconvolutional filters that capture class-specific discriminative patches\\nwithout extra part or bounding box annotations. Such a filter bank is well\\nstructured, properly initialized and discriminatively learned through a novel\\nasymmetric multi-stream architecture with convolutional filter supervision and\\na non-random layer initialization. Experimental results show that our approach\\nachieves state-of-the-art on three publicly available fine-grained recognition\\ndatasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and\\nvisualizations are provided to understand our approach.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1611.09932v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1611.09932v3.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Yaming Wang', 'Vlad I. Morariu', 'Larry S. Davis'],\n",
       "  'tasks': ['Representation Learning'],\n",
       "  'date': '2016-11-29',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cub-200-2011', 'stanford-cars', 'fgvc-aircraft-1'],\n",
       "  'datasets_used_full': ['CUB-200-2011', 'Stanford Cars', 'FGVC-Aircraft'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/findings-of-the-second-workshop-on-neural',\n",
       "  'arxiv_id': '1806.02940',\n",
       "  'title': 'Findings of the Second Workshop on Neural Machine Translation and Generation',\n",
       "  'abstract': \"This document describes the findings of the Second Workshop on Neural Machine\\nTranslation and Generation, held in concert with the annual conference of the\\nAssociation for Computational Linguistics (ACL 2018). First, we summarize the\\nresearch trends of papers presented in the proceedings, and note that there is\\nparticular interest in linguistic structure, domain adaptation, data\\naugmentation, handling inadequate resources, and analysis of models. Second, we\\ndescribe the results of the workshop's shared task on efficient neural machine\\ntranslation, where participants were tasked with creating MT systems that are\\nboth accurate and efficient.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.02940v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.02940v3.pdf',\n",
       "  'proceeding': 'WS 2018 7',\n",
       "  'authors': ['Alexandra Birch',\n",
       "   'Andrew Finch',\n",
       "   'Minh-Thang Luong',\n",
       "   'Graham Neubig',\n",
       "   'Yusuke Oda'],\n",
       "  'tasks': ['Data Augmentation',\n",
       "   'Domain Adaptation',\n",
       "   'Machine Translation',\n",
       "   'Translation'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/object-detection-and-tracking-benchmark-in',\n",
       "  'arxiv_id': '1806.03853',\n",
       "  'title': 'Object detection and tracking benchmark in industry based on improved correlation filter',\n",
       "  'abstract': 'Real-time object detection and tracking have shown to be the basis of\\nintelligent production for industrial 4.0 applications. It is a challenging\\ntask because of various distorted data in complex industrial setting. The\\ncorrelation filter (CF) has been used to trade off the low-cost computation and\\nhigh performance. However, traditional CF training strategy can not get\\nsatisfied performance for the various industrial data; because the simple\\nsampling(bagging) during training process will not find the exact solutions in\\na data space with a large diversity. In this paper, we propose\\nDijkstra-distance based correlation filters (DBCF), which establishes a new\\nlearning framework that embeds distribution-related constraints into the\\nmulti-channel correlation filters (MCCF). DBCF is able to handle the huge\\nvariations existing in the industrial data by improving those constraints based\\non the shortest path among all solutions. To evaluate DBCF, we build a new\\ndataset as the benchmark for industrial 4.0 application. Extensive experiments\\ndemonstrate that DBCF produces high performance and exceeds the\\nstate-of-the-art methods. The dataset and source code can be found at\\nhttps://github.com/bczhangbczhang',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03853v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03853v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shangzhen Luan', 'Yan Li', 'Xiaodi Wang', 'Baochang Zhang'],\n",
       "  'tasks': ['Object Detection', 'Real-Time Object Detection'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/pseudo-task-augmentation-from-deep-multitask',\n",
       "  'arxiv_id': '1803.04062',\n",
       "  'title': 'Pseudo-task Augmentation: From Deep Multitask Learning to Intratask Sharing---and Back',\n",
       "  'abstract': 'Deep multitask learning boosts performance by sharing learned structure\\nacross related tasks. This paper adapts ideas from deep multitask learning to\\nthe setting where only a single task is available. The method is formalized as\\npseudo-task augmentation, in which models are trained with multiple decoders\\nfor each task. Pseudo-tasks simulate the effect of training towards\\nclosely-related tasks drawn from the same universe. In a suite of experiments,\\npseudo-task augmentation is shown to improve performance on single-task\\nlearning problems. When combined with multitask learning, further improvements\\nare achieved, including state-of-the-art performance on the CelebA dataset,\\nshowing that pseudo-task augmentation and multitask learning have complementary\\nvalue. All in all, pseudo-task augmentation is a broadly applicable and\\nefficient way to boost performance in deep learning systems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.04062v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.04062v2.pdf',\n",
       "  'proceeding': 'ICML 2018',\n",
       "  'authors': ['Elliot Meyerson', 'Risto Miikkulainen'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-03-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['celeba', 'imdb-movie-reviews'],\n",
       "  'datasets_used_full': ['CelebA', 'IMDb Movie Reviews'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/challenges-of-language-technologies-for-the',\n",
       "  'arxiv_id': '1806.04291',\n",
       "  'title': 'Challenges of language technologies for the indigenous languages of the Americas',\n",
       "  'abstract': 'Indigenous languages of the American continent are highly diverse. However,\\nthey have received little attention from the technological perspective. In this\\npaper, we review the research, the digital resources and the available NLP\\nsystems that focus on these languages. We present the main challenges and\\nresearch questions that arise when distant languages and low-resource scenarios\\nare faced. We would like to encourage NLP research in linguistically rich and\\ndiverse areas like the Americas.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04291v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04291v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Manuel Mager',\n",
       "   'Ximena Gutierrez-Vasques',\n",
       "   'Gerardo Sierra',\n",
       "   'Ivan Meza'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dpatch-an-adversarial-patch-attack-on-object',\n",
       "  'arxiv_id': '1806.02299',\n",
       "  'title': 'DPatch: An Adversarial Patch Attack on Object Detectors',\n",
       "  'abstract': \"Object detectors have emerged as an indispensable module in modern computer\\nvision systems. In this work, we propose DPatch -- a black-box\\nadversarial-patch-based attack towards mainstream object detectors (i.e. Faster\\nR-CNN and YOLO). Unlike the original adversarial patch that only manipulates\\nimage-level classifier, our DPatch simultaneously attacks the bounding box\\nregression and object classification so as to disable their predictions.\\nCompared to prior works, DPatch has several appealing properties: (1) DPatch\\ncan perform both untargeted and targeted effective attacks, degrading the mAP\\nof Faster R-CNN and YOLO from 75.10% and 65.7% down to below 1%, respectively.\\n(2) DPatch is small in size and its attacking effect is location-independent,\\nmaking it very practical to implement real-world attacks. (3) DPatch\\ndemonstrates great transferability among different detectors as well as\\ntraining datasets. For example, DPatch that is trained on Faster R-CNN can\\neffectively attack YOLO, and vice versa. Extensive evaluations imply that\\nDPatch can perform effective attacks under black-box setup, i.e., even without\\nthe knowledge of the attacked network's architectures and parameters.\\nSuccessful realization of DPatch also illustrates the intrinsic vulnerability\\nof the modern detector architectures to such patch-based adversarial attacks.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.02299v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.02299v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xin Liu',\n",
       "   'Huanrui Yang',\n",
       "   'Ziwei Liu',\n",
       "   'Linghao Song',\n",
       "   'Hai Li',\n",
       "   'Yiran Chen'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-05',\n",
       "  'methods': [{'name': 'RPN',\n",
       "    'full_name': 'Region Proposal Network',\n",
       "    'description': 'A **Region Proposal Network**, or **RPN**, is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals. RPN and algorithms like [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn) can be merged into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with attention mechanisms, the RPN component tells the unified network where to look.\\r\\n\\r\\nRPNs are designed to efficiently predict region proposals with a wide range of scales and aspect ratios. RPNs use anchor boxes that serve as references at multiple scales and aspect ratios. The scheme can be thought of as a pyramid of regression references, which avoids enumerating images or filters of multiple scales or aspect ratios.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1506.01497v3',\n",
       "    'source_title': 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Region Proposal',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'RoIPool',\n",
       "    'full_name': 'RoIPool',\n",
       "    'description': '**Region of Interest Pooling**, or **RoIPool**, is an operation for extracting a small feature map (e.g., $7×7$) from each RoI in detection and segmentation based tasks. Features are extracted from each candidate box, and thereafter in models like [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn), are then classified and bounding box regression performed.\\r\\n\\r\\nThe actual scaling to, e.g., $7×7$, occurs by dividing the region proposal into equally sized sections, finding the largest value in each section, and then copying these max values to the output buffer. In essence, **RoIPool** is [max pooling](https://paperswithcode.com/method/max-pooling) on a discrete grid based on a box.\\r\\n\\r\\nImage Source: [Joyce Xu](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1311.2524v5',\n",
       "    'source_title': 'Rich feature hierarchies for accurate object detection and semantic segmentation',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/5e9ebe8dadc0ea2841a46cfcd82a93b4ce0d4519/torchvision/ops/roi_pool.py#L10',\n",
       "    'main_collection': {'name': 'RoI Feature Extractors',\n",
       "     'description': '**RoI Feature Extractors** are used to extract regions of interest features for tasks such as object detection. Below you can find a continuously updating list of RoI Feature Extractors.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Faster R-CNN',\n",
       "    'full_name': 'Faster R-CNN',\n",
       "    'description': '**Faster R-CNN** is an object detection model that improves on [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn) by utilising a region proposal network ([RPN](https://paperswithcode.com/method/rpn)) with the CNN model. The RPN shares full-image convolutional features with the detection network, enabling nearly cost-free region proposals. It is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn) for detection. RPN and Fast [R-CNN](https://paperswithcode.com/method/r-cnn) are merged into a single network by sharing their convolutional features: the RPN component tells the unified network where to look.\\r\\n\\r\\nAs a whole, Faster R-CNN consists of two modules. The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector that uses the proposed regions.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1506.01497v3',\n",
       "    'source_title': 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks',\n",
       "    'code_snippet_url': 'https://github.com/chenyuntc/simple-faster-rcnn-pytorch/blob/367db367834efd8a2bc58ee0023b2b628a0e474d/model/faster_rcnn.py#L22',\n",
       "    'main_collection': {'name': 'Object Detection Models',\n",
       "     'description': '**Object Detection Models** are architectures used to perform the task of object detection. Below you can find a continuously updating list of object detection models.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/twin-regularization-for-online-speech',\n",
       "  'arxiv_id': '1804.05374',\n",
       "  'title': 'Twin Regularization for online speech recognition',\n",
       "  'abstract': 'Online speech recognition is crucial for developing natural human-machine\\ninterfaces. This modality, however, is significantly more challenging than\\noff-line ASR, since real-time/low-latency constraints inevitably hinder the use\\nof future information, that is known to be very helpful to perform robust\\npredictions. A popular solution to mitigate this issue consists of feeding\\nneural acoustic models with context windows that gather some future frames.\\nThis introduces a latency which depends on the number of employed look-ahead\\nfeatures. This paper explores a different approach, based on estimating the\\nfuture rather than waiting for it. Our technique encourages the hidden\\nrepresentations of a unidirectional recurrent network to embed some useful\\ninformation about the future. Inspired by a recently proposed technique called\\nTwin Networks, we add a regularization term that forces forward hidden states\\nto be as close as possible to cotemporal backward ones, computed by a \"twin\"\\nneural network running backwards in time. The experiments, conducted on a\\nnumber of datasets, recurrent architectures, input features, and acoustic\\nconditions, have shown the effectiveness of this approach. One important\\nadvantage is that our method does not introduce any additional computation at\\ntest time if compared to standard unidirectional recurrent networks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.05374v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.05374v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mirco Ravanelli', 'Dmitriy Serdyuk', 'Yoshua Bengio'],\n",
       "  'tasks': ['Speech Recognition'],\n",
       "  'date': '2018-04-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['librispeech', 'dirha'],\n",
       "  'datasets_used_full': ['LibriSpeech', 'DIRHA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/iparaphrasing-extracting-visually-grounded',\n",
       "  'arxiv_id': '1806.04284',\n",
       "  'title': 'iParaphrasing: Extracting Visually Grounded Paraphrases via an Image',\n",
       "  'abstract': 'A paraphrase is a restatement of the meaning of a text in other words.\\nParaphrases have been studied to enhance the performance of many natural\\nlanguage processing tasks. In this paper, we propose a novel task iParaphrasing\\nto extract visually grounded paraphrases (VGPs), which are different phrasal\\nexpressions describing the same visual concept in an image. These extracted\\nVGPs have the potential to improve language and image multimodal tasks such as\\nvisual question answering and image captioning. How to model the similarity\\nbetween VGPs is the key of iParaphrasing. We apply various existing methods as\\nwell as propose a novel neural network-based method with image attention, and\\nreport the results of the first attempt toward iParaphrasing.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04284v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04284v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Chenhui Chu', 'Mayu Otani', 'Yuta Nakashima'],\n",
       "  'tasks': ['Image Captioning',\n",
       "   'Question Answering',\n",
       "   'Visual Question Answering'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['flickr30k'],\n",
       "  'datasets_used_full': ['Flickr30k'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/distance-free-modeling-of-multi-predicate',\n",
       "  'arxiv_id': '1806.03869',\n",
       "  'title': 'Distance-Free Modeling of Multi-Predicate Interactions in End-to-End Japanese Predicate-Argument Structure Analysis',\n",
       "  'abstract': 'Capturing interactions among multiple predicate-argument structures (PASs) is\\na crucial issue in the task of analyzing PAS in Japanese. In this paper, we\\npropose new Japanese PAS analysis models that integrate the label prediction\\ninformation of arguments in multiple PASs by extending the input and last\\nlayers of a standard deep bidirectional recurrent neural network (bi-RNN)\\nmodel. In these models, using the mechanisms of pooling and attention, we aim\\nto directly capture the potential interactions among multiple PASs, without\\nbeing disturbed by the word order and distance. Our experiments show that the\\nproposed models improve the prediction accuracy specifically for cases where\\nthe predicate and argument are in an indirect dependency relation and achieve a\\nnew state of the art in the overall $F_1$ on a standard benchmark corpus.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03869v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03869v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Yuichiroh Matsubayashi', 'Kentaro Inui'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/complete-analysis-of-a-random-forest-model',\n",
       "  'arxiv_id': '1805.02587',\n",
       "  'title': 'Sharp Analysis of a Simple Model for Random Forests',\n",
       "  'abstract': \"Random forests have become an important tool for improving accuracy in regression and classification problems since their inception by Leo Breiman in 2001. In this paper, we revisit a historically important random forest model originally proposed by Breiman in 2004 and later studied by G\\\\'erard Biau in 2012, where a feature is selected at random and the splits occurs at the midpoint of the node along the chosen feature. If the regression function is Lipschitz and depends only on a small subset of $ S $ out of $ d $ features, we show that, given access to $ n $ observations and properly tuned split probabilities, the mean-squared prediction error is $ O((n(\\\\log n)^{(S-1)/2})^{-\\\\frac{1}{S\\\\log2+1}}) $. This positively answers an outstanding question of Biau about whether the rate of convergence for this random forest model could be improved. Furthermore, by a refined analysis of the approximation and estimation errors for linear models, we show that this rate cannot be improved in general. Finally, we generalize our analysis and improve extant prediction error bounds for another random forest model in which each tree is constructed from subsampled data and the splits are performed at the empirical median along a chosen feature.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1805.02587v7',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1805.02587v7.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jason M. Klusowski'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-nes-music-database-a-multi-instrumental',\n",
       "  'arxiv_id': '1806.04278',\n",
       "  'title': 'The NES Music Database: A multi-instrumental dataset with expressive performance attributes',\n",
       "  'abstract': \"Existing research on music generation focuses on composition, but often\\nignores the expressive performance characteristics required for plausible\\nrenditions of resultant pieces. In this paper, we introduce the Nintendo\\nEntertainment System Music Database (NES-MDB), a large corpus allowing for\\nseparate examination of the tasks of composition and performance. NES-MDB\\ncontains thousands of multi-instrumental songs composed for playback by the\\ncompositionally-constrained NES audio synthesizer. For each song, the dataset\\ncontains a musical score for four instrument voices as well as expressive\\nattributes for the dynamics and timbre of each voice. Unlike datasets comprised\\nof General MIDI files, NES-MDB includes all of the information needed to render\\nexact acoustic performances of the original compositions. Alongside the\\ndataset, we provide a tool that renders generated compositions as NES-style\\naudio by emulating the device's audio processor. Additionally, we establish\\nbaselines for the tasks of composition, which consists of learning the\\nsemantics of composing for the NES synthesizer, and performance, which involves\\nfinding a mapping between a composition and realistic expressive attributes.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04278v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04278v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Chris Donahue', 'Huanru Henry Mao', 'Julian McAuley'],\n",
       "  'tasks': ['Music Generation'],\n",
       "  'date': '2018-06-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['musedata'],\n",
       "  'datasets_used_full': ['MuseData'],\n",
       "  'datasets_introduced_lower': ['nes-mdb'],\n",
       "  'datasets_introduced_full': ['NES-MDB']},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/generalized-zero-shot-learning-via',\n",
       "  'arxiv_id': '1712.03878',\n",
       "  'title': 'Generalized Zero-Shot Learning via Synthesized Examples',\n",
       "  'abstract': \"We present a generative framework for generalized zero-shot learning where\\nthe training and test classes are not necessarily disjoint. Built upon a\\nvariational autoencoder based architecture, consisting of a probabilistic\\nencoder and a probabilistic conditional decoder, our model can generate novel\\nexemplars from seen/unseen classes, given their respective class attributes.\\nThese exemplars can subsequently be used to train any off-the-shelf\\nclassification model. One of the key aspects of our encoder-decoder\\narchitecture is a feedback-driven mechanism in which a discriminator (a\\nmultivariate regressor) learns to map the generated exemplars to the\\ncorresponding class attribute vectors, leading to an improved generator. Our\\nmodel's ability to generate and leverage examples from unseen classes to train\\nthe classification model naturally helps to mitigate the bias towards\\npredicting seen classes in generalized zero-shot learning settings. Through a\\ncomprehensive set of experiments, we show that our model outperforms several\\nstate-of-the-art methods, on several benchmark datasets, for both standard as\\nwell as generalized zero-shot learning.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.03878v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.03878v5.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Vinay Kumar Verma',\n",
       "   'Gundeep Arora',\n",
       "   'Ashish Mishra',\n",
       "   'Piyush Rai'],\n",
       "  'tasks': ['General Classification',\n",
       "   'Generalized Zero-Shot Learning',\n",
       "   'Zero-Shot Learning'],\n",
       "  'date': '2017-12-11',\n",
       "  'methods': [{'name': 'AutoEncoder',\n",
       "    'full_name': 'AutoEncoder',\n",
       "    'description': 'An **Autoencoder** is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then performs a reconstruction of the input with this latent code (the decoder).\\r\\n\\r\\nImage: [Michael Massi](https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://science.sciencemag.org/content/313/5786/504',\n",
       "    'source_title': 'Reducing the Dimensionality of Data with Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/L1aoXingyu/pytorch-beginner/blob/9c86be785c7c318a09cf29112dd1f1a58613239b/08-AutoEncoder/simple_autoencoder.py#L38',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['awa-1'],\n",
       "  'datasets_used_full': ['AwA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-multilingual-topics-from',\n",
       "  'arxiv_id': '1806.04270',\n",
       "  'title': 'Learning Multilingual Topics from Incomparable Corpus',\n",
       "  'abstract': 'Multilingual topic models enable crosslingual tasks by extracting consistent\\ntopics from multilingual corpora. Most models require parallel or comparable\\ntraining corpora, which limits their ability to generalize. In this paper, we\\nfirst demystify the knowledge transfer mechanism behind multilingual topic\\nmodels by defining an alternative but equivalent formulation. Based on this\\nanalysis, we then relax the assumption of training data required by most\\nexisting models, creating a model that only requires a dictionary for training.\\nExperiments show that our new method effectively learns coherent multilingual\\ntopics from partially and fully incomparable corpora with limited amounts of\\ndictionary resources.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04270v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04270v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shudong Hao', 'Michael J. Paul'],\n",
       "  'tasks': ['Topic Models', 'Transfer Learning'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/accurate-and-robust-neural-networks-for',\n",
       "  'arxiv_id': '1806.04265',\n",
       "  'title': 'Accurate and Robust Neural Networks for Security Related Applications Exampled by Face Morphing Attacks',\n",
       "  'abstract': 'Artificial neural networks tend to learn only what they need for a task. A\\nmanipulation of the training data can counter this phenomenon. In this paper,\\nwe study the effect of different alterations of the training data, which limit\\nthe amount and position of information that is available for the decision\\nmaking. We analyze the accuracy and robustness against semantic and black box\\nattacks on the networks that were trained on different training data\\nmodifications for the particular example of morphing attacks. A morphing attack\\nis an attack on a biometric facial recognition system where the system is\\nfooled to match two different individuals with the same synthetic face image.\\nSuch a synthetic image can be created by aligning and blending images of the\\ntwo individuals that should be matched with this image.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04265v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04265v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Clemens Seibold',\n",
       "   'Wojciech Samek',\n",
       "   'Anna Hilsmann',\n",
       "   'Peter Eisert'],\n",
       "  'tasks': ['Decision Making'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/smoothed-action-value-functions-for-learning',\n",
       "  'arxiv_id': '1803.02348',\n",
       "  'title': 'Smoothed Action Value Functions for Learning Gaussian Policies',\n",
       "  'abstract': 'State-action value functions (i.e., Q-values) are ubiquitous in reinforcement\\nlearning (RL), giving rise to popular algorithms such as SARSA and Q-learning.\\nWe propose a new notion of action value defined by a Gaussian smoothed version\\nof the expected Q-value. We show that such smoothed Q-values still satisfy a\\nBellman equation, making them learnable from experience sampled from an\\nenvironment. Moreover, the gradients of expected reward with respect to the\\nmean and covariance of a parameterized Gaussian policy can be recovered from\\nthe gradient and Hessian of the smoothed Q-value function. Based on these\\nrelationships, we develop new algorithms for training a Gaussian policy\\ndirectly from a learned smoothed Q-value approximator. The approach is\\nadditionally amenable to proximal optimization by augmenting the objective with\\na penalty on KL-divergence from a previous policy. We find that the ability to\\nlearn both a mean and covariance during training leads to significantly\\nimproved results on standard continuous control benchmarks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.02348v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.02348v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Ofir Nachum',\n",
       "   'Mohammad Norouzi',\n",
       "   'George Tucker',\n",
       "   'Dale Schuurmans'],\n",
       "  'tasks': ['Continuous Control', 'Q-Learning'],\n",
       "  'date': '2018-03-06',\n",
       "  'methods': [{'name': 'Sarsa',\n",
       "    'full_name': 'Sarsa',\n",
       "    'description': '**Sarsa** is an on-policy TD control algorithm:\\r\\n\\r\\n$$Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) \\\\leftarrow Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) + \\\\alpha\\\\left[R_{t+1} + \\\\gamma{Q}\\\\left(S\\\\_{t+1}, A\\\\_{t+1}\\\\right) - Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right)\\\\right] $$\\r\\n\\r\\nThis update is done after every transition from a nonterminal state $S\\\\_{t}$. if $S\\\\_{t+1}$ is terminal, then $Q\\\\left(S\\\\_{t+1}, A\\\\_{t+1}\\\\right)$ is defined as zero.\\r\\n\\r\\nTo design an on-policy control algorithm using Sarsa, we estimate $q\\\\_{\\\\pi}$ for a behaviour policy $\\\\pi$ and then change $\\\\pi$ towards greediness with respect to $q\\\\_{\\\\pi}$.\\r\\n\\r\\nSource: Sutton and Barto, Reinforcement Learning, 2nd Edition',\n",
       "    'introduced_year': 1994,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'On-Policy TD Control',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Reinforcement Learning'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/group-normalization',\n",
       "  'arxiv_id': '1803.08494',\n",
       "  'title': 'Group Normalization',\n",
       "  'abstract': \"FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.08494v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.08494v3.pdf',\n",
       "  'proceeding': 'ECCV 2018 9',\n",
       "  'authors': ['Yuxin Wu', 'Kaiming He'],\n",
       "  'tasks': ['Object Detection', 'Video Classification'],\n",
       "  'date': '2018-03-22',\n",
       "  'methods': [{'name': 'Focal Loss',\n",
       "    'full_name': 'Focal Loss',\n",
       "    'description': 'A **Focal Loss** function addresses class imbalance during training in tasks like object detection. Focal loss applies a modulating term to the cross entropy loss in order to focus learning on hard misclassified examples. It is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. \\r\\n\\r\\nFormally, the Focal Loss adds a factor $(1 - p\\\\_{t})^\\\\gamma$ to the standard cross entropy criterion. Setting $\\\\gamma>0$ reduces the relative loss for well-classified examples ($p\\\\_{t}>.5$), putting more focus on hard, misclassified examples. Here there is tunable *focusing* parameter $\\\\gamma \\\\ge 0$. \\r\\n\\r\\n$$ {\\\\text{FL}(p\\\\_{t}) = - (1 - p\\\\_{t})^\\\\gamma \\\\log\\\\left(p\\\\_{t}\\\\right)} $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1708.02002v2',\n",
       "    'source_title': 'Focal Loss for Dense Object Detection',\n",
       "    'code_snippet_url': 'https://github.com/clcarwin/focal_loss_pytorch/blob/e11e75bad957aecf641db6998a1016204722c1bb/focalloss.py#L6',\n",
       "    'main_collection': {'name': 'Loss Functions',\n",
       "     'description': '**Loss Functions** are used to frame the problem to be optimized within deep learning. Below you will find a continuously updating list of (specialized) loss functions for neutral networks.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'FPN',\n",
       "    'full_name': 'Feature Pyramid Network',\n",
       "    'description': 'A **Feature Pyramid Network**, or **FPN**, is a feature extractor that takes a single-scale image of an arbitrary size as input, and outputs proportionally sized feature maps at multiple levels, in a fully convolutional fashion. This process is independent of the backbone convolutional architectures. It therefore acts as a generic solution for building feature pyramids inside deep convolutional networks to be used in tasks like object detection.\\r\\n\\r\\nThe construction of the pyramid involves a bottom-up pathway and a top-down pathway.\\r\\n\\r\\nThe bottom-up pathway is the feedforward computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2. For the feature\\r\\npyramid, one pyramid level is defined for each stage. The output of the last layer of each stage is used as a reference set of feature maps. For [ResNets](https://paperswithcode.com/method/resnet) we use the feature activations output by each stage’s last [residual block](https://paperswithcode.com/method/residual-block). \\r\\n\\r\\nThe top-down pathway hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels. These features are then enhanced with features from the bottom-up pathway via lateral connections. Each lateral connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down pathway. The bottom-up feature map is of lower-level semantics, but its activations are more accurately localized as it was subsampled fewer times.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1612.03144v2',\n",
       "    'source_title': 'Feature Pyramid Networks for Object Detection',\n",
       "    'code_snippet_url': 'https://github.com/facebookresearch/Detectron/blob/8170b25b425967f8f1c7d715bea3c5b8d9536cd8/detectron/modeling/FPN.py#L117',\n",
       "    'main_collection': {'name': 'Feature Extractors',\n",
       "     'description': '**Feature Extractors** for object detection are modules used to construct features that can be used for detecting objects. They address issues such as the need to detect multiple-sized objects in an image (and the need to have representations that are suitable for the different scales).',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Average Pooling',\n",
       "    'full_name': 'Average Pooling',\n",
       "    'description': '**Average Pooling** is a pooling operation that calculates the average value for patches of a feature map, and uses it to create a downsampled (pooled) feature map. It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs. It extracts features more smoothly than [Max Pooling](https://paperswithcode.com/method/max-pooling), whereas max pooling extracts more pronounced features like edges.\\r\\n\\r\\nImage Source: [here](https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'RetinaNet',\n",
       "    'full_name': 'RetinaNet',\n",
       "    'description': \"**RetinaNet** is a one-stage object detection model that utilizes a [focal loss](https://paperswithcode.com/method/focal-loss) function to address class imbalance during training. Focal loss applies a modulating term to the cross entropy loss in order to focus learning on hard negative examples. RetinaNet is a single, unified network composed of a *backbone* network and two task-specific *subnetworks*. The backbone is responsible for computing a convolutional feature map over an entire input image and is an off-the-self convolutional network. The first subnet performs convolutional object classification on the backbone's output; the second subnet performs convolutional bounding box regression. The two subnetworks feature a simple design that the authors propose specifically for one-stage, dense detection. \\r\\n\\r\\nWe can see the motivation for focal loss by comparing with two-stage object detectors. Here class imbalance is addressed by a two-stage cascade and sampling heuristics. The proposal stage (e.g., [Selective Search](https://paperswithcode.com/method/selective-search), [EdgeBoxes](https://paperswithcode.com/method/edgeboxes), [DeepMask](https://paperswithcode.com/method/deepmask), [RPN](https://paperswithcode.com/method/rpn)) rapidly narrows down the number of candidate object locations to a small number (e.g., 1-2k), filtering out most background samples. In the second classification stage, sampling heuristics, such as a fixed foreground-to-background ratio, or online hard example mining ([OHEM](https://paperswithcode.com/method/ohem)), are performed to maintain a\\r\\nmanageable balance between foreground and background.\\r\\n\\r\\nIn contrast, a one-stage detector must process a much larger set of candidate object locations regularly sampled across an image. To tackle this, RetinaNet uses a focal loss function, a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. \\r\\n\\r\\nFormally, the Focal Loss adds a factor $(1 - p\\\\_{t})^\\\\gamma$ to the standard cross entropy criterion. Setting $\\\\gamma>0$ reduces the relative loss for well-classified examples ($p\\\\_{t}>.5$), putting more focus on hard, misclassified examples. Here there is tunable *focusing* parameter $\\\\gamma \\\\ge 0$. \\r\\n\\r\\n$$ {\\\\text{FL}(p\\\\_{t}) = - (1 - p\\\\_{t})^\\\\gamma \\\\log\\\\left(p\\\\_{t}\\\\right)} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1708.02002v2',\n",
       "    'source_title': 'Focal Loss for Dense Object Detection',\n",
       "    'code_snippet_url': 'https://github.com/facebookresearch/Detectron/blob/8170b25b425967f8f1c7d715bea3c5b8d9536cd8/detectron/modeling/retinanet_heads.py',\n",
       "    'main_collection': {'name': 'Object Detection Models',\n",
       "     'description': '**Object Detection Models** are architectures used to perform the task of object detection. Below you can find a continuously updating list of object detection models.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Residual Connection',\n",
       "    'full_name': 'Residual Connection',\n",
       "    'description': '**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': '1x1 Convolution',\n",
       "    'full_name': '1x1 Convolution',\n",
       "    'description': 'A **1 x 1 Convolution** is a [convolution](https://paperswithcode.com/method/convolution) with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an [MLP](https://paperswithcode.com/method/feedforward-network) looking at a particular pixel location.\\r\\n\\r\\nImage Credit: [http://deeplearning.ai](http://deeplearning.ai)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://www.healthnutra.org/es/maxup/',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Batch Normalization',\n",
       "    'full_name': 'Batch Normalization',\n",
       "    'description': '**Batch Normalization** aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for [Dropout](https://paperswithcode.com/method/dropout).\\r\\n\\r\\nWe apply a batch normalization layer as follows for a minibatch $\\\\mathcal{B}$:\\r\\n\\r\\n$$ \\\\mu\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}x\\\\_{i} $$\\r\\n\\r\\n$$ \\\\sigma^{2}\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}\\\\left(x\\\\_{i}-\\\\mu\\\\_{\\\\mathcal{B}}\\\\right)^{2} $$\\r\\n\\r\\n$$ \\\\hat{x}\\\\_{i} = \\\\frac{x\\\\_{i} - \\\\mu\\\\_{\\\\mathcal{B}}}{\\\\sqrt{\\\\sigma^{2}\\\\_{\\\\mathcal{B}}+\\\\epsilon}} $$\\r\\n\\r\\n$$ y\\\\_{i} = \\\\gamma\\\\hat{x}\\\\_{i} + \\\\beta = \\\\text{BN}\\\\_{\\\\gamma, \\\\beta}\\\\left(x\\\\_{i}\\\\right) $$\\r\\n\\r\\nWhere $\\\\gamma$ and $\\\\beta$ are learnable parameters.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.03167v3',\n",
       "    'source_title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/36f91261099b00194922bd93ed1286fe1c199724/jax/experimental/stax.py#L116',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Bottleneck Residual Block',\n",
       "    'full_name': 'Bottleneck Residual Block',\n",
       "    'description': 'A **Bottleneck Residual Block** is a variant of the [residual block](https://paperswithcode.com/method/residual-block) that utilises 1x1 convolutions to create a bottleneck. The use of a bottleneck reduces the number of parameters and matrix multiplications. The idea is to make residual blocks as thin as possible to increase depth and have less parameters. They were introduced as part of the [ResNet](https://paperswithcode.com/method/resnet) architecture, and are used as part of deeper ResNets such as ResNet-50 and ResNet-101.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L75',\n",
       "    'main_collection': {'name': 'Skip Connection Blocks',\n",
       "     'description': \"**Skip Connection Blocks** are building blocks for neural networks that feature skip connections. These skip connections 'skip' some layers allowing gradients to better flow through the network. Below you will find a continuously updating list of skip connection blocks:\",\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Global Average Pooling',\n",
       "    'full_name': 'Global Average Pooling',\n",
       "    'description': '**Global Average Pooling** is a pooling operation designed to replace fully connected layers in classical CNNs. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the [softmax](https://paperswithcode.com/method/softmax) layer. \\r\\n\\r\\nOne advantage of global [average pooling](https://paperswithcode.com/method/average-pooling) over the fully connected layers is that it is more native to the [convolution](https://paperswithcode.com/method/convolution) structure by enforcing correspondences between feature maps and categories. Thus the feature maps can be easily interpreted as categories confidence maps. Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. Furthermore, global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/baa592b215804927e28638f6a7f3318cbc411d49/torchvision/models/resnet.py#L157',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Residual Block',\n",
       "    'full_name': 'Residual Block',\n",
       "    'description': \"**Residual Blocks** are skip-connection blocks that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. They were introduced as part of the [ResNet](https://paperswithcode.com/method/resnet) architecture.\\r\\n \\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$. The $\\\\mathcal{F}({x})$ acts like a residual, hence the name 'residual block'.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. Having skip connections allows the network to more easily learn identity-like mappings.\\r\\n\\r\\nNote that in practice, [Bottleneck Residual Blocks](https://paperswithcode.com/method/bottleneck-residual-block) are used for deeper ResNets, such as ResNet-50 and ResNet-101, as these bottleneck blocks are less computationally intensive.\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L35',\n",
       "    'main_collection': {'name': 'Skip Connection Blocks',\n",
       "     'description': \"**Skip Connection Blocks** are building blocks for neural networks that feature skip connections. These skip connections 'skip' some layers allowing gradients to better flow through the network. Below you will find a continuously updating list of skip connection blocks:\",\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Kaiming Initialization',\n",
       "    'full_name': 'Kaiming Initialization',\n",
       "    'description': '**Kaiming Initialization**, or **He Initialization**, is an initialization method for neural networks that takes into account the non-linearity of activation functions, such as [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nA proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. Using a derivation they work out that the condition to stop this happening is:\\r\\n\\r\\n$$\\\\frac{1}{2}n\\\\_{l}\\\\text{Var}\\\\left[w\\\\_{l}\\\\right] = 1 $$\\r\\n\\r\\nThis implies an initialization scheme of:\\r\\n\\r\\n$$ w\\\\_{l} \\\\sim \\\\mathcal{N}\\\\left(0,  2/n\\\\_{l}\\\\right)$$\\r\\n\\r\\nThat is, a zero-centered Gaussian with standard deviation of $\\\\sqrt{2/{n}\\\\_{l}}$ (variance shown in equation above). Biases are initialized at $0$.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.01852v1',\n",
       "    'source_title': 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/0adb5843766092fba584791af76383125fd0d01c/torch/nn/init.py#L389',\n",
       "    'main_collection': {'name': 'Initialization',\n",
       "     'description': '**Initialization** methods are used to initialize the weights in a neural network. Below can you find a continuously updating list of initialization methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'ResNet',\n",
       "    'full_name': 'Residual Network',\n",
       "    'description': '**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack [residual blocks](https://paperswithcode.com/method/residual-block) ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}(x)$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}(x):=\\\\mathcal{H}(x)-x$. The original mapping is recast into $\\\\mathcal{F}(x)+x$.\\r\\n\\r\\nThere is empirical evidence that these types of network are easier to optimize, and can gain accuracy from considerably increased depth.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/resnet.py#L124',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'RoIAlign',\n",
       "    'full_name': 'RoIAlign',\n",
       "    'description': '**Region of Interest Align**, or **RoIAlign**, is an operation for extracting a small feature map from each RoI in detection and segmentation based tasks. It removes the harsh quantization of [RoI Pool](https://paperswithcode.com/method/roi-pooling), properly *aligning* the extracted features with the input. To avoid any quantization of the RoI boundaries or bins (using $x/16$ instead of $[x/16]$), RoIAlign uses bilinear interpolation to compute the exact values of the input features at four regularly sampled locations in each RoI bin, and the result is then aggregated (using max or average).',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1703.06870v3',\n",
       "    'source_title': 'Mask R-CNN',\n",
       "    'code_snippet_url': 'https://github.com/facebookresearch/detectron2/blob/bb9f5d8e613358519c9865609ab3fe7b6571f2ba/detectron2/layers/roi_align.py#L51',\n",
       "    'main_collection': {'name': 'RoI Feature Extractors',\n",
       "     'description': '**RoI Feature Extractors** are used to extract regions of interest features for tasks such as object detection. Below you can find a continuously updating list of RoI Feature Extractors.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Group Normalization',\n",
       "    'full_name': 'Group Normalization',\n",
       "    'description': '**Group Normalization** is a normalization layer that divides channels into groups and normalizes the features within each group. GN does not exploit the batch dimension, and its computation is independent of batch sizes. In the case where the group size is 1, it is equivalent to [Instance Normalization](https://paperswithcode.com/method/instance-normalization).\\r\\n\\r\\nAs motivation for the method, many classical features like SIFT and HOG had *group-wise* features and involved *group-wise normalization*. For example, a HOG vector is the outcome of several spatial cells where each cell is represented by a normalized orientation histogram.\\r\\n\\r\\nFormally, Group Normalization is defined as:\\r\\n\\r\\n$$ \\\\mu\\\\_{i} = \\\\frac{1}{m}\\\\sum\\\\_{k\\\\in\\\\mathcal{S}\\\\_{i}}x\\\\_{k} $$\\r\\n\\r\\n$$ \\\\sigma^{2}\\\\_{i} = \\\\frac{1}{m}\\\\sum\\\\_{k\\\\in\\\\mathcal{S}\\\\_{i}}\\\\left(x\\\\_{k}-\\\\mu\\\\_{i}\\\\right)^{2} $$\\r\\n\\r\\n$$ \\\\hat{x}\\\\_{i} = \\\\frac{x\\\\_{i} - \\\\mu\\\\_{i}}{\\\\sqrt{\\\\sigma^{2}\\\\_{i}+\\\\epsilon}} $$\\r\\n\\r\\nHere $x$ is the feature computed by a layer, and $i$ is an index. Formally, a Group Norm layer computes $\\\\mu$ and $\\\\sigma$ in a set $\\\\mathcal{S}\\\\_{i}$ defined as: $\\\\mathcal{S}\\\\_{i} = ${$k \\\\mid k\\\\_{N} = i\\\\_{N} ,\\\\lfloor\\\\frac{k\\\\_{C}}{C/G}\\\\rfloor = \\\\lfloor\\\\frac{I\\\\_{C}}{C/G}\\\\rfloor $}.\\r\\n\\r\\nHere $G$ is the number of groups, which is a pre-defined hyper-parameter ($G = 32$ by default). $C/G$ is the number of channels per group. $\\\\lfloor$ is the floor operation, and the final term means that the indexes $i$ and $k$ are in the same group of channels, assuming each group of channels are stored in a sequential order along the $C$ axis.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1803.08494v3',\n",
       "    'source_title': 'Group Normalization',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/1c5c289b6218eb1026dcb5fd9738231401cfccea/torch/nn/modules/normalization.py#L177',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Mask R-CNN',\n",
       "    'full_name': 'Mask R-CNN',\n",
       "    'description': \"**Mask R-CNN** extends [Faster R-CNN](http://paperswithcode.com/method/faster-r-cnn) to solve instance segmentation tasks. It achieves this by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. In principle, Mask R-CNN is an intuitive extension of Faster [R-CNN](https://paperswithcode.com/method/r-cnn), but constructing the mask branch properly is critical for good results. \\r\\n\\r\\nMost importantly, Faster R-CNN was not designed for pixel-to-pixel alignment between network inputs and outputs. This is evident in how [RoIPool](http://paperswithcode.com/method/roi-pooling), the *de facto* core operation for attending to instances, performs coarse spatial quantization for feature extraction. To fix the misalignment, Mask R-CNN utilises a simple, quantization-free layer, called [RoIAlign](http://paperswithcode.com/method/roi-align), that faithfully preserves exact spatial locations. \\r\\n\\r\\nSecondly, Mask R-CNN *decouples* mask and class prediction: it predicts a binary mask for each class independently, without competition among classes, and relies on the network's RoI classification branch to predict the category. In contrast, an [FCN](http://paperswithcode.com/method/fcn) usually perform per-pixel multi-class categorization, which couples segmentation and classification.\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1703.06870v3',\n",
       "    'source_title': 'Mask R-CNN',\n",
       "    'code_snippet_url': 'https://github.com/facebookresearch/detectron2/blob/601d7666faaf7eb0ba64c9f9ce5811b13861fe12/detectron2/modeling/roi_heads/mask_head.py#L154',\n",
       "    'main_collection': {'name': 'Instance Segmentation Models',\n",
       "     'description': '**Instance Segmentation** models are models that perform the task of [Instance Segmentation](https://paperswithcode.com/task/instance-segmentation).',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['coco', 'kinetics'],\n",
       "  'datasets_used_full': ['COCO', 'Kinetics'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/linear-convergence-of-gradient-and-proximal',\n",
       "  'arxiv_id': '1608.04636',\n",
       "  'title': 'Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Łojasiewicz Condition',\n",
       "  'abstract': 'In 1963, Polyak proposed a simple condition that is sufficient to show a global linear convergence rate for gradient descent. This condition is a special case of the \\\\L{}ojasiewicz inequality proposed in the same year, and it does not require strong convexity (or even convexity). In this work, we show that this much-older Polyak-\\\\L{}ojasiewicz (PL) inequality is actually weaker than the main conditions that have been explored to show linear convergence rates without strong convexity over the last 25 years. We also use the PL inequality to give new analyses of randomized and greedy coordinate descent methods, sign-based gradient descent methods, and stochastic gradient methods in the classic setting (with decreasing or constant step-sizes) as well as the variance-reduced setting. We further propose a generalization that applies to proximal-gradient methods for non-smooth optimization, leading to simple proofs of linear convergence of these methods. Along the way, we give simple convergence results for a wide variety of problems in machine learning: least squares, logistic regression, boosting, resilient backpropagation, L1-regularization, support vector machines, stochastic dual coordinate ascent, and stochastic variance-reduced gradient methods.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1608.04636v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1608.04636v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hamed Karimi', 'Julie Nutini', 'Mark Schmidt'],\n",
       "  'tasks': [],\n",
       "  'date': '2016-08-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/lets-do-it-again-a-first-computational',\n",
       "  'arxiv_id': '1806.04262',\n",
       "  'title': 'Let\\'s do it \"again\": A First Computational Approach to Detecting Adverbial Presupposition Triggers',\n",
       "  'abstract': 'We introduce the task of predicting adverbial presupposition triggers such as\\nalso and again. Solving such a task requires detecting recurring or similar\\nevents in the discourse context, and has applications in natural language\\ngeneration tasks such as summarization and dialogue systems. We create two new\\ndatasets for the task, derived from the Penn Treebank and the Annotated English\\nGigaword corpora, as well as a novel attention mechanism tailored to this task.\\nOur attention mechanism augments a baseline recurrent neural network without\\nthe need for additional trainable parameters, minimizing the added\\ncomputational cost of our mechanism. We demonstrate that our model\\nstatistically outperforms a number of baselines, including an LSTM-based\\nlanguage model.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04262v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04262v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Andre Cianflone',\n",
       "   'Yulan Feng',\n",
       "   'Jad Kabbara',\n",
       "   'Jackie Chi Kit Cheung'],\n",
       "  'tasks': ['Language Modelling', 'Text Generation'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['penn-treebank', 'cbt'],\n",
       "  'datasets_used_full': ['Penn Treebank', 'CBT'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/improving-whole-slide-segmentation-through',\n",
       "  'arxiv_id': '1806.04259',\n",
       "  'title': 'Improving Whole Slide Segmentation Through Visual Context - A Systematic Study',\n",
       "  'abstract': 'While challenging, the dense segmentation of histology images is a necessary\\nfirst step to assess changes in tissue architecture and cellular morphology.\\nAlthough specific convolutional neural network architectures have been applied\\nwith great success to the problem, few effectively incorporate visual context\\ninformation from multiple scales. With this paper, we present a systematic\\ncomparison of different architectures to assess how including multi-scale\\ninformation affects segmentation performance. A publicly available breast\\ncancer and a locally collected prostate cancer datasets are being utilised for\\nthis study. The results support our hypothesis that visual context and scale\\nplay a crucial role in histology image classification problems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04259v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04259v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Korsuk Sirinukunwattana',\n",
       "   'Nasullah Khalid Alham',\n",
       "   'Clare Verrill',\n",
       "   'Jens Rittscher'],\n",
       "  'tasks': ['General Classification', 'Image Classification'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/can-machine-learning-identify-interesting',\n",
       "  'arxiv_id': '1805.07431',\n",
       "  'title': 'Can machine learning identify interesting mathematics? An exploration using empirically observed laws',\n",
       "  'abstract': \"We explore the possibility of using machine learning to identify interesting\\nmathematical structures by using certain quantities that serve as fingerprints.\\nIn particular, we extract features from integer sequences using two empirical\\nlaws: Benford's law and Taylor's law and experiment with various classifiers to\\nidentify whether a sequence is, for example, nice, important, multiplicative,\\neasy to compute or related to primes or palindromes.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.07431v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.07431v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Chai Wah Wu'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multi-step-reinforcement-learning-a-unifying',\n",
       "  'arxiv_id': '1703.01327',\n",
       "  'title': 'Multi-step Reinforcement Learning: A Unifying Algorithm',\n",
       "  'abstract': 'Unifying seemingly disparate algorithmic ideas to produce better performing\\nalgorithms has been a longstanding goal in reinforcement learning. As a primary\\nexample, TD($\\\\lambda$) elegantly unifies one-step TD prediction with Monte\\nCarlo methods through the use of eligibility traces and the trace-decay\\nparameter $\\\\lambda$. Currently, there are a multitude of algorithms that can be\\nused to perform TD control, including Sarsa, $Q$-learning, and Expected Sarsa.\\nThese methods are often studied in the one-step case, but they can be extended\\nacross multiple time steps to achieve better performance. Each of these\\nalgorithms is seemingly distinct, and no one dominates the others for all\\nproblems. In this paper, we study a new multi-step action-value algorithm\\ncalled $Q(\\\\sigma)$ which unifies and generalizes these existing algorithms,\\nwhile subsuming them as special cases. A new parameter, $\\\\sigma$, is introduced\\nto allow the degree of sampling performed by the algorithm at each step during\\nits backup to be continuously varied, with Sarsa existing at one extreme (full\\nsampling), and Expected Sarsa existing at the other (pure expectation).\\n$Q(\\\\sigma)$ is generally applicable to both on- and off-policy learning, but in\\nthis work we focus on experiments in the on-policy case. Our results show that\\nan intermediate value of $\\\\sigma$, which results in a mixture of the existing\\nalgorithms, performs better than either extreme. The mixture can also be varied\\ndynamically which can result in even greater performance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1703.01327v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1703.01327v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kristopher De Asis',\n",
       "   'J. Fernando Hernandez-Garcia',\n",
       "   'G. Zacharias Holland',\n",
       "   'Richard S. Sutton'],\n",
       "  'tasks': ['Q-Learning'],\n",
       "  'date': '2017-03-03',\n",
       "  'methods': [{'name': 'Expected Sarsa',\n",
       "    'full_name': 'Expected Sarsa',\n",
       "    'description': '**Expected Sarsa** is like [Q-learning](https://paperswithcode.com/method/q-learning) but instead of taking the maximum over next state-action pairs, we use the expected value, taking into account how likely each action is under the current policy.\\r\\n\\r\\n$$Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) \\\\leftarrow Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) + \\\\alpha\\\\left[R_{t+1} + \\\\gamma\\\\sum\\\\_{a}\\\\pi\\\\left(a\\\\mid{S\\\\_{t+1}}\\\\right)Q\\\\left(S\\\\_{t+1}, a\\\\right) - Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right)\\\\right] $$\\r\\n\\r\\nExcept for this change to the update rule, the algorithm otherwise follows the scheme of Q-learning. It is more computationally expensive than [Sarsa](https://paperswithcode.com/method/sarsa) but it eliminates the variance due to the random selection of $A\\\\_{t+1}$.\\r\\n\\r\\nSource: Sutton and Barto, Reinforcement Learning, 2nd Edition',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'On-Policy TD Control',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Reinforcement Learning'}},\n",
       "   {'name': 'Sarsa',\n",
       "    'full_name': 'Sarsa',\n",
       "    'description': '**Sarsa** is an on-policy TD control algorithm:\\r\\n\\r\\n$$Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) \\\\leftarrow Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) + \\\\alpha\\\\left[R_{t+1} + \\\\gamma{Q}\\\\left(S\\\\_{t+1}, A\\\\_{t+1}\\\\right) - Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right)\\\\right] $$\\r\\n\\r\\nThis update is done after every transition from a nonterminal state $S\\\\_{t}$. if $S\\\\_{t+1}$ is terminal, then $Q\\\\left(S\\\\_{t+1}, A\\\\_{t+1}\\\\right)$ is defined as zero.\\r\\n\\r\\nTo design an on-policy control algorithm using Sarsa, we estimate $q\\\\_{\\\\pi}$ for a behaviour policy $\\\\pi$ and then change $\\\\pi$ towards greediness with respect to $q\\\\_{\\\\pi}$.\\r\\n\\r\\nSource: Sutton and Barto, Reinforcement Learning, 2nd Edition',\n",
       "    'introduced_year': 1994,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'On-Policy TD Control',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Reinforcement Learning'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/complexity-theory-for-discrete-black-box',\n",
       "  'arxiv_id': '1801.02037',\n",
       "  'title': 'Complexity Theory for Discrete Black-Box Optimization Heuristics',\n",
       "  'abstract': 'A predominant topic in the theory of evolutionary algorithms and, more\\ngenerally, theory of randomized black-box optimization techniques is running\\ntime analysis. Running time analysis aims at understanding the performance of a\\ngiven heuristic on a given problem by bounding the number of function\\nevaluations that are needed by the heuristic to identify a solution of a\\ndesired quality. As in general algorithms theory, this running time perspective\\nis most useful when it is complemented by a meaningful complexity theory that\\nstudies the limits of algorithmic solutions.\\n  In the context of discrete black-box optimization, several black-box\\ncomplexity models have been developed to analyze the best possible performance\\nthat a black-box optimization algorithm can achieve on a given problem. The\\nmodels differ in the classes of algorithms to which these lower bounds apply.\\nThis way, black-box complexity contributes to a better understanding of how\\ncertain algorithmic choices (such as the amount of memory used by a heuristic,\\nits selective pressure, or properties of the strategies that it uses to create\\nnew solution candidates) influences performance.\\n  In this chapter we review the different black-box complexity models that have\\nbeen proposed in the literature, survey the bounds that have been obtained for\\nthese models, and discuss how the interplay of running time analysis and\\nblack-box complexity can inspire new algorithmic solutions to well-researched\\nproblems in evolutionary computation. We also discuss in this chapter several\\ninteresting open questions for future work.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1801.02037v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1801.02037v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Carola Doerr'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-01-06',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/revisiting-adversarial-risk',\n",
       "  'arxiv_id': '1806.02924',\n",
       "  'title': 'Revisiting Adversarial Risk',\n",
       "  'abstract': 'Recent works on adversarial perturbations show that there is an inherent\\ntrade-off between standard test accuracy and adversarial accuracy.\\nSpecifically, they show that no classifier can simultaneously be robust to\\nadversarial perturbations and achieve high standard test accuracy. However,\\nthis is contrary to the standard notion that on tasks such as image\\nclassification, humans are robust classifiers with low error rate. In this\\nwork, we show that the main reason behind this confusion is the inexact\\ndefinition of adversarial perturbation that is used in the literature. To fix\\nthis issue, we propose a slight, yet important modification to the existing\\ndefinition of adversarial perturbation. Based on the modified definition, we\\nshow that there is no trade-off between adversarial and standard accuracies;\\nthere exist classifiers that are robust and achieve high standard accuracy. We\\nfurther study several properties of this new definition of adversarial risk and\\nits relation to the existing definition.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.02924v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.02924v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Arun Sai Suggala',\n",
       "   'Adarsh Prasad',\n",
       "   'Vaishnavh Nagarajan',\n",
       "   'Pradeep Ravikumar'],\n",
       "  'tasks': ['Image Classification'],\n",
       "  'date': '2018-06-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10'],\n",
       "  'datasets_used_full': ['CIFAR-10'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/theory-of-parameter-control-for-discrete',\n",
       "  'arxiv_id': '1804.05650',\n",
       "  'title': 'Theory of Parameter Control for Discrete Black-Box Optimization: Provable Performance Gains Through Dynamic Parameter Choices',\n",
       "  'abstract': 'Parameter control aims at realizing performance gains through a dynamic choice of the parameters which determine the behavior of the underlying optimization algorithm. In the context of evolutionary algorithms this research line has for a long time been dominated by empirical approaches. With the significant advances in running time analysis achieved in the last ten years, the parameter control question has become accessible to theoretical investigations. A number of running time results for a broad range of different parameter control mechanisms have been obtained in recent years. This book chapter surveys these works, and puts them into context, by proposing an updated classification scheme for parameter control.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1804.05650v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1804.05650v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Benjamin Doerr', 'Carola Doerr'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-04-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-to-speed-up-structured-output',\n",
       "  'arxiv_id': '1806.04245',\n",
       "  'title': 'Learning to Speed Up Structured Output Prediction',\n",
       "  'abstract': 'Predicting structured outputs can be computationally onerous due to the\\ncombinatorially large output spaces. In this paper, we focus on reducing the\\nprediction time of a trained black-box structured classifier without losing\\naccuracy. To do so, we train a speedup classifier that learns to mimic a\\nblack-box classifier under the learning-to-search approach. As the structured\\nclassifier predicts more examples, the speedup classifier will operate as a\\nlearned heuristic to guide search to favorable regions of the output space. We\\npresent a mistake bound for the speedup classifier and identify inference\\nsituations where it can independently make correct judgments without input\\nfeatures. We evaluate our method on the task of entity and relation extraction\\nand show that the speedup classifier outperforms even greedy search in terms of\\nspeed without loss of accuracy.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04245v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04245v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Xingyuan Pan', 'Vivek Srikumar'],\n",
       "  'tasks': ['Relation Extraction'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-potential-of-the-return-distribution-for',\n",
       "  'arxiv_id': '1806.04242',\n",
       "  'title': 'The Potential of the Return Distribution for Exploration in RL',\n",
       "  'abstract': 'This paper studies the potential of the return distribution for exploration\\nin deterministic reinforcement learning (RL) environments. We study network\\nlosses and propagation mechanisms for Gaussian, Categorical and Gaussian\\nmixture distributions. Combined with exploration policies that leverage this\\nreturn distribution, we solve, for example, a randomized Chain task of length\\n100, which has not been reported before when learning with neural networks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04242v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04242v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Thomas M. Moerland', 'Joost Broekens', 'Catholijn M. Jonker'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multi-turn-dialogue-response-generation-in-an',\n",
       "  'arxiv_id': '1805.11752',\n",
       "  'title': 'Multi-turn Dialogue Response Generation in an Adversarial Learning Framework',\n",
       "  'abstract': \"We propose an adversarial learning approach for generating multi-turn dialogue responses. Our proposed framework, hredGAN, is based on conditional generative adversarial networks (GANs). The GAN's generator is a modified hierarchical recurrent encoder-decoder network (HRED) and the discriminator is a word-level bidirectional RNN that shares context and word embeddings with the generator. During inference, noise samples conditioned on the dialogue history are used to perturb the generator's latent space to generate several possible responses. The final response is the one ranked best by the discriminator. The hredGAN shows improved performance over existing methods: (1) it generalizes better than networks trained using only the log-likelihood criterion, and (2) it generates longer, more informative and more diverse responses with high utterance and topic relevance even with limited training data. This improvement is demonstrated on the Movie triples and Ubuntu dialogue datasets using both automatic and human evaluations.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1805.11752v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1805.11752v5.pdf',\n",
       "  'proceeding': 'WS 2019 8',\n",
       "  'authors': ['Oluwatobi Olabiyi',\n",
       "   'Alan Salimov',\n",
       "   'Anish Khazane',\n",
       "   'Erik T. Mueller'],\n",
       "  'tasks': ['Response Generation', 'Word Embeddings'],\n",
       "  'date': '2018-05-30',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/lecture-notes-on-fair-division',\n",
       "  'arxiv_id': '1806.04234',\n",
       "  'title': 'Lecture Notes on Fair Division',\n",
       "  'abstract': 'Fair division is the problem of dividing one or several goods amongst two or\\nmore agents in a way that satisfies a suitable fairness criterion. These Notes\\nprovide a succinct introduction to the field. We cover three main topics.\\nFirst, we need to define what is to be understood by a \"fair\" allocation of\\ngoods to individuals. We present an overview of the most important fairness\\ncriteria (as well as the closely related criteria for economic efficiency)\\ndeveloped in the literature, together with a short discussion of their\\naxiomatic foundations. Second, we give an introduction to cake-cutting\\nprocedures as an example of methods for fairly dividing a single divisible\\nresource amongst a group of individuals. Third, we discuss the combinatorial\\noptimisation problem of fairly allocating a set of indivisible goods to a group\\nof agents, covering both centralised algorithms (similar to auctions) and a\\ndistributed approach based on negotiation.\\n  While the classical literature on fair division has largely developed within\\nEconomics, these Notes are specifically written for readers with a background\\nin Computer Science or similar, and who may be (or may wish to be) engaged in\\nresearch in Artificial Intelligence, Multiagent Systems, or Computational\\nSocial Choice. References for further reading, as well as a small number of\\nexercises, are included.\\n  Notes prepared for a tutorial at the 11th European Agent Systems Summer\\nSchool (EASSS-2009), Torino, Italy, 31 August and 1 September 2009. Updated for\\na tutorial at the COST-ADT Doctoral School on Computational Social Choice,\\nEstoril, Portugal, 9--14 April 2010.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04234v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04234v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ulle Endriss'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/physical-representation-based-predicate',\n",
       "  'arxiv_id': '1806.04226',\n",
       "  'title': 'Physical Representation-based Predicate Optimization for a Visual Analytics Database',\n",
       "  'abstract': \"Querying the content of images, video, and other non-textual data sources\\nrequires expensive content extraction methods. Modern extraction techniques are\\nbased on deep convolutional neural networks (CNNs) and can classify objects\\nwithin images with astounding accuracy. Unfortunately, these methods are slow:\\nprocessing a single image can take about 10 milliseconds on modern GPU-based\\nhardware. As massive video libraries become ubiquitous, running a content-based\\nquery over millions of video frames is prohibitive.\\n  One promising approach to reduce the runtime cost of queries of visual\\ncontent is to use a hierarchical model, such as a cascade, where simple cases\\nare handled by an inexpensive classifier. Prior work has sought to design\\ncascades that optimize the computational cost of inference by, for example,\\nusing smaller CNNs. However, we observe that there are critical factors besides\\nthe inference time that dramatically impact the overall query time. Notably, by\\ntreating the physical representation of the input image as part of our query\\noptimization---that is, by including image transforms, such as resolution\\nscaling or color-depth reduction, within the cascade---we can optimize data\\nhandling costs and enable drastically more efficient classifier cascades.\\n  In this paper, we propose Tahoma, which generates and evaluates many\\npotential classifier cascades that jointly optimize the CNN architecture and\\ninput data representation. Our experiments on a subset of ImageNet show that\\nTahoma's input transformations speed up cascades by up to 35 times. We also\\nfind up to a 98x speedup over the ResNet50 classifier with no loss in accuracy,\\nand a 280x speedup if some accuracy is sacrificed.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04226v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04226v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Michael R. Anderson',\n",
       "   'Michael Cafarella',\n",
       "   'German Ros',\n",
       "   'Thomas F. Wenisch'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/neuronet-fast-and-robust-reproduction-of',\n",
       "  'arxiv_id': '1806.04224',\n",
       "  'title': 'NeuroNet: Fast and Robust Reproduction of Multiple Brain Image Segmentation Pipelines',\n",
       "  'abstract': 'NeuroNet is a deep convolutional neural network mimicking multiple popular\\nand state-of-the-art brain segmentation tools including FSL, SPM, and MALPEM.\\nThe network is trained on 5,000 T1-weighted brain MRI scans from the UK Biobank\\nImaging Study that have been automatically segmented into brain tissue and\\ncortical and sub-cortical structures using the standard neuroimaging pipelines.\\nTraining a single model from these complementary and partially overlapping\\nlabel maps yields a new powerful \"all-in-one\", multi-output segmentation tool.\\nThe processing time for a single subject is reduced by an order of magnitude\\ncompared to running each individual software package. We demonstrate very good\\nreproducibility of the original outputs while increasing robustness to\\nvariations in the input data. We believe NeuroNet could be an important tool in\\nlarge-scale population imaging studies and serve as a new standard in\\nneuroscience by reducing the risk of introducing bias when choosing a specific\\nsoftware package.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04224v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04224v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Martin Rajchl',\n",
       "   'Nick Pawlowski',\n",
       "   'Daniel Rueckert',\n",
       "   'Paul M. Matthews',\n",
       "   'Ben Glocker'],\n",
       "  'tasks': ['Brain Image Segmentation',\n",
       "   'Brain Segmentation',\n",
       "   'Semantic Segmentation'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/collaborative-human-ai-chai-evidence-based',\n",
       "  'arxiv_id': '1805.12234',\n",
       "  'title': 'Collaborative Human-AI (CHAI): Evidence-Based Interpretable Melanoma Classification in Dermoscopic Images',\n",
       "  'abstract': 'Automated dermoscopic image analysis has witnessed rapid growth in diagnostic\\nperformance. Yet adoption faces resistance, in part, because no evidence is\\nprovided to support decisions. In this work, an approach for evidence-based\\nclassification is presented. A feature embedding is learned with CNNs,\\ntriplet-loss, and global average pooling, and used to classify via kNN search.\\nEvidence is provided as both the discovered neighbors, as well as localized\\nimage regions most relevant to measuring distance between query and neighbors.\\nTo ensure that results are relevant in terms of both label accuracy and human\\nvisual similarity for any skill level, a novel hierarchical triplet logic is\\nimplemented to jointly learn an embedding according to disease labels and\\nnon-expert similarity. Results are improved over baselines trained on disease\\nlabels alone, as well as standard multiclass loss. Quantitative relevance of\\nresults, according to non-expert similarity, as well as localized image\\nregions, are also significantly improved.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.12234v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.12234v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Noel C. F. Codella',\n",
       "   'Chung-Ching Lin',\n",
       "   'Allan Halpern',\n",
       "   'Michael Hind',\n",
       "   'Rogerio Feris',\n",
       "   'John R. Smith'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-05-30',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multi-agent-path-finding-with-deadlines',\n",
       "  'arxiv_id': '1806.04216',\n",
       "  'title': 'Multi-Agent Path Finding with Deadlines',\n",
       "  'abstract': 'We formalize Multi-Agent Path Finding with Deadlines (MAPF-DL). The objective\\nis to maximize the number of agents that can reach their given goal vertices\\nfrom their given start vertices within the deadline, without colliding with\\neach other. We first show that MAPF-DL is NP-hard to solve optimally. We then\\npresent two classes of optimal algorithms, one based on a reduction of MAPF-DL\\nto a flow problem and a subsequent compact integer linear programming\\nformulation of the resulting reduced abstracted multi-commodity flow network\\nand the other one based on novel combinatorial search algorithms. Our empirical\\nresults demonstrate that these MAPF-DL solvers scale well and each one\\ndominates the other ones in different scenarios.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04216v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04216v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hang Ma',\n",
       "   'Glenn Wagner',\n",
       "   'Ariel Felner',\n",
       "   'Jiaoyang Li',\n",
       "   'T. K. Satish Kumar',\n",
       "   'Sven Koenig'],\n",
       "  'tasks': ['Multi-Agent Path Finding'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/universality-of-the-stochastic-block-model',\n",
       "  'arxiv_id': '1806.04214',\n",
       "  'title': 'Universality of the stochastic block model',\n",
       "  'abstract': 'Mesoscopic pattern extraction (MPE) is the problem of finding a partition of\\nthe nodes of a complex network that maximizes some objective function. Many\\nwell-known network inference problems fall in this category, including, for\\ninstance, community detection, core-periphery identification, and imperfect\\ngraph coloring. In this paper, we show that the most popular algorithms\\ndesigned to solve MPE problems can in fact be understood as special cases of\\nthe maximum likelihood formulation of the stochastic block model (SBM), or one\\nof its direct generalizations. These equivalence relations show that the SBM is\\nnearly universal with respect to MPE problems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04214v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04214v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jean-Gabriel Young',\n",
       "   'Guillaume St-Onge',\n",
       "   'Patrick Desrosiers',\n",
       "   'Louis J. Dubé'],\n",
       "  'tasks': ['Community Detection', 'Stochastic Block Model'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/matching-with-text-data-an-experimental',\n",
       "  'arxiv_id': '1801.00644',\n",
       "  'title': 'Matching with Text Data: An Experimental Evaluation of Methods for Matching Documents and of Measuring Match Quality',\n",
       "  'abstract': 'Matching for causal inference is a well-studied problem, but standard methods\\nfail when the units to match are text documents: the high-dimensional and rich\\nnature of the data renders exact matching infeasible, causes propensity scores\\nto produce incomparable matches, and makes assessing match quality difficult.\\nIn this paper, we characterize a framework for matching text documents that\\ndecomposes existing methods into: (1) the choice of text representation, and\\n(2) the choice of distance metric. We investigate how different choices within\\nthis framework affect both the quantity and quality of matches identified\\nthrough a systematic multifactor evaluation experiment using human subjects.\\nAltogether we evaluate over 100 unique text matching methods along with 5\\ncomparison methods taken from the literature. Our experimental results identify\\nmethods that generate matches with higher subjective match quality than current\\nstate-of-the-art techniques. We enhance the precision of these results by\\ndeveloping a predictive model to estimate the match quality of pairs of text\\ndocuments as a function of our various distance scores. This model, which we\\nfind successfully mimics human judgment, also allows for approximate and\\nunsupervised evaluation of new procedures. We then employ the identified best\\nmethod to illustrate the utility of text matching in two applications. First,\\nwe engage with a substantive debate in the study of media bias by using text\\nmatching to control for topic selection when comparing news articles from\\nthirteen news sources. We then show how conditioning on text data leads to more\\nprecise causal inferences in an observational study examining the effects of a\\nmedical intervention.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1801.00644v7',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1801.00644v7.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Reagan Mozer',\n",
       "   'Luke Miratrix',\n",
       "   'Aaron Russell Kaufman',\n",
       "   'L. Jason Anastasopoulos'],\n",
       "  'tasks': ['Causal Inference', 'Text Matching'],\n",
       "  'date': '2018-01-02',\n",
       "  'methods': [{'name': 'Causal Inference',\n",
       "    'full_name': 'Causal Inference',\n",
       "    'description': 'Causal inference is the process of drawing a conclusion about a causal connection based on the conditions of the occurrence of an effect. The main difference between causal inference and inference of association is that the former analyzes the response of the effect variable when the cause is changed.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': None}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/how-curiosity-can-be-modeled-for-a-clickbait',\n",
       "  'arxiv_id': '1806.04212',\n",
       "  'title': 'How Curiosity can be modeled for a Clickbait Detector',\n",
       "  'abstract': 'The impact of continually evolving digital technologies and the proliferation\\nof communications and content has now been widely acknowledged to be central to\\nunderstanding our world. What is less acknowledged is that this is based on the\\nsuccessful arousing of curiosity both at the collective and individual levels.\\nAdvertisers, communication professionals and news editors are in constant\\ncompetition to capture attention of the digital population perennially shifty\\nand distracted. This paper, tries to understand how curiosity works in the\\ndigital world by attempting the first ever work done on quantifying human\\ncuriosity, basing itself on various theories drawn from humanities and social\\nsciences. Curious communication pushes people to spot, read and click the\\nmessage from their social feed or any other form of online presentation. Our\\napproach focuses on measuring the strength of the stimulus to generate reader\\ncuriosity by using unsupervised and supervised machine learning algorithms, but\\nis also informed by philosophical, psychological, neural and cognitive studies\\non this topic. Manually annotated news headlines - clickbaits - have been\\nselected for the study, which are known to have drawn huge reader response. A\\nbinary classifier was developed based on human curiosity (unlike the work done\\nso far using words and other linguistic features). Our classifier shows an\\naccuracy of 97% . This work is part of the research in computational humanities\\non digital politics quantifying the emotions of curiosity and outrage on\\ndigital media.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04212v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04212v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Lasya Venneti', 'Aniket Alam'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/temporal-difference-variational-auto-encoder',\n",
       "  'arxiv_id': '1806.03107',\n",
       "  'title': 'Temporal Difference Variational Auto-Encoder',\n",
       "  'abstract': 'To act and plan in complex environments, we posit that agents should have a\\nmental simulator of the world with three characteristics: (a) it should build\\nan abstract state representing the condition of the world; (b) it should form a\\nbelief which represents uncertainty on the world; (c) it should go beyond\\nsimple step-by-step simulation, and exhibit temporal abstraction. Motivated by\\nthe absence of a model satisfying all these requirements, we propose TD-VAE, a\\ngenerative sequence model that learns representations containing explicit\\nbeliefs about states several steps into the future, and that can be rolled out\\ndirectly without single-step transitions. TD-VAE is trained on pairs of\\ntemporally separated time points, using an analogue of temporal difference\\nlearning used in reinforcement learning.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03107v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03107v3.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Karol Gregor',\n",
       "   'George Papamakarios',\n",
       "   'Frederic Besse',\n",
       "   'Lars Buesing',\n",
       "   'Theophane Weber'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}},\n",
       "   {'name': 'TD-VAE',\n",
       "    'full_name': 'TD-VAE',\n",
       "    'description': '**TD-VAE**, or **Temporal Difference VAE**, is a generative sequence model that learns representations containing explicit beliefs about states several steps into the future, and that can be rolled out directly without single-step transitions. TD-VAE is trained on pairs of temporally separated time points, using an analogue of [temporal difference learning](https://paperswithcode.com/method/td-lambda) used in reinforcement learning.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1806.03107v3',\n",
       "    'source_title': 'Temporal Difference Variational Auto-Encoder',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Generative Sequence Models',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/in-ictu-oculi-exposing-ai-generated-fake-face',\n",
       "  'arxiv_id': '1806.02877',\n",
       "  'title': 'In Ictu Oculi: Exposing AI Generated Fake Face Videos by Detecting Eye Blinking',\n",
       "  'abstract': 'The new developments in deep generative networks have significantly improve\\nthe quality and efficiency in generating realistically-looking fake face\\nvideos. In this work, we describe a new method to expose fake face videos\\ngenerated with neural networks. Our method is based on detection of eye\\nblinking in the videos, which is a physiological signal that is not well\\npresented in the synthesized fake videos. Our method is tested over benchmarks\\nof eye-blinking detection datasets and also show promising performance on\\ndetecting videos generated with DeepFake.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.02877v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.02877v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yuezun Li', 'Ming-Ching Chang', 'Siwei Lyu'],\n",
       "  'tasks': ['Face Swapping'],\n",
       "  'date': '2018-06-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/swarming-for-faster-convergence-in-stochastic',\n",
       "  'arxiv_id': '1806.04207',\n",
       "  'title': 'Swarming for Faster Convergence in Stochastic Optimization',\n",
       "  'abstract': \"We study a distributed framework for stochastic optimization which is\\ninspired by models of collective motion found in nature (e.g., swarming) with\\nmild communication requirements. Specifically, we analyze a scheme in which\\neach one of $N > 1$ independent threads, implements in a distributed and\\nunsynchronized fashion, a stochastic gradient-descent algorithm which is\\nperturbed by a swarming potential. Assuming the overhead caused by\\nsynchronization is not negligible, we show the swarming-based approach exhibits\\nbetter performance than a centralized algorithm (based upon the average of $N$\\nobservations) in terms of (real-time) convergence speed. We also derive an\\nerror bound that is monotone decreasing in network size and connectivity. We\\ncharacterize the scheme's finite-time performances for both convex and\\nnon-convex objective functions.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04207v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04207v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shi Pu', 'Alfredo Garcia'],\n",
       "  'tasks': ['Stochastic Optimization'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-note-about-local-explanation-methods-for',\n",
       "  'arxiv_id': '1806.04205',\n",
       "  'title': 'A Note about: Local Explanation Methods for Deep Neural Networks lack Sensitivity to Parameter Values',\n",
       "  'abstract': \"Local explanation methods, also known as attribution methods, attribute a\\ndeep network's prediction to its input (cf. Baehrens et al. (2010)). We respond\\nto the claim from Adebayo et al. (2018) that local explanation methods lack\\nsensitivity, i.e., DNNs with randomly-initialized weights produce explanations\\nthat are both visually and quantitatively similar to those produced by DNNs\\nwith learned weights.\\n  Further investigation reveals that their findings are due to two choices in\\ntheir analysis: (a) ignoring the signs of the attributions; and (b) for\\nintegrated gradients (IG), including pixels in their analysis that have zero\\nattributions by choice of the baseline (an auxiliary input relative to which\\nthe attributions are computed). When both factors are accounted for, IG\\nattributions for a random network and the actual network are uncorrelated. Our\\ninvestigation also sheds light on how these issues affect visualizations,\\nalthough we note that more work is needed to understand how viewers interpret\\nthe difference between the random and the actual attributions.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04205v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04205v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mukund Sundararajan', 'Ankur Taly'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/calibrating-noise-to-variance-in-adaptive',\n",
       "  'arxiv_id': '1712.07196',\n",
       "  'title': 'Calibrating Noise to Variance in Adaptive Data Analysis',\n",
       "  'abstract': 'Datasets are often used multiple times and each successive analysis may\\ndepend on the outcome of previous analyses. Standard techniques for ensuring\\ngeneralization and statistical validity do not account for this adaptive\\ndependence. A recent line of work studies the challenges that arise from such\\nadaptive data reuse by considering the problem of answering a sequence of\\n\"queries\" about the data distribution where each query may depend arbitrarily\\non answers to previous queries.\\n  The strongest results obtained for this problem rely on differential privacy\\n-- a strong notion of algorithmic stability with the important property that it\\n\"composes\" well when data is reused. However the notion is rather strict, as it\\nrequires stability under replacement of an arbitrary data element. The simplest\\nalgorithm is to add Gaussian (or Laplace) noise to distort the empirical\\nanswers. However, analysing this technique using differential privacy yields\\nsuboptimal accuracy guarantees when the queries have low variance. Here we\\npropose a relaxed notion of stability that also composes adaptively. We\\ndemonstrate that a simple and natural algorithm based on adding noise scaled to\\nthe standard deviation of the query provides our notion of stability. This\\nimplies an algorithm that can answer statistical queries about the dataset with\\nsubstantially improved accuracy guarantees for low-variance queries. The only\\nprevious approach that provides such accuracy guarantees is based on a more\\ninvolved differentially private median-of-means algorithm and its analysis\\nexploits stronger \"group\" stability of the algorithm.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.07196v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.07196v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Vitaly Feldman', 'Thomas Steinke'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-12-19',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/degree-based-classification-of-harmful-speech',\n",
       "  'arxiv_id': '1806.04197',\n",
       "  'title': 'Degree based Classification of Harmful Speech using Twitter Data',\n",
       "  'abstract': 'Harmful speech has various forms and it has been plaguing the social media in\\ndifferent ways. If we need to crackdown different degrees of hate speech and\\nabusive behavior amongst it, the classification needs to be based on complex\\nramifications which needs to be defined and hold accountable for, other than\\nracist, sexist or against some particular group and community. This paper\\nprimarily describes how we created an ontological classification of harmful\\nspeech based on degree of hateful intent, and used it to annotate twitter data\\naccordingly. The key contribution of this paper is the new dataset of tweets we\\ncreated based on ontological classes and degrees of harmful speech found in the\\ntext. We also propose supervised classification system for recognizing these\\nrespective harmful speech classes in the texts hence.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04197v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04197v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Sanjana Sharma', 'Saksham Agrawal', 'Manish Shrivastava'],\n",
       "  'tasks': ['Classification', 'General Classification'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/enhancing-human-color-vision-by-breaking',\n",
       "  'arxiv_id': '1703.04392',\n",
       "  'title': 'Enhancing human color vision by breaking binocular redundancy',\n",
       "  'abstract': 'To see color, the human visual system combines the response of three types of\\ncone cells in the retina--a compressive process that discards a significant\\namount of spectral information. Here, we present an approach to enhance human\\ncolor vision by breaking its inherent binocular redundancy, providing different\\nspectral content to each eye. We fabricated a set of optical filters that\\n\"splits\" the response of the short-wavelength cone between the two eyes in\\nindividuals with typical trichromatic vision, simulating the presence of\\napproximately four distinct cone types (\"tetrachromacy\"). Such an increase in\\nthe number of effective cone types can reduce the prevalence of metamers--pairs\\nof distinct spectra that resolve to the same tristimulus values. This technique\\nmay result in an enhancement of spectral perception, with applications ranging\\nfrom camouflage detection and anti-counterfeiting to new types of artwork and\\ndata visualization.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1703.04392v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1703.04392v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Bradley S. Gundlach',\n",
       "   'Michel Frising',\n",
       "   'Alireza Shahsafi',\n",
       "   'Gregory Vershbow',\n",
       "   'Chenghao Wan',\n",
       "   'Jad Salman',\n",
       "   'Bas Rokers',\n",
       "   'Laurent Lessard',\n",
       "   'Mikhail A. Kats'],\n",
       "  'tasks': ['Data Visualization'],\n",
       "  'date': '2017-03-02',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/navigating-with-graph-representations-for',\n",
       "  'arxiv_id': '1806.04189',\n",
       "  'title': 'Navigating with Graph Representations for Fast and Scalable Decoding of Neural Language Models',\n",
       "  'abstract': 'Neural language models (NLMs) have recently gained a renewed interest by\\nachieving state-of-the-art performance across many natural language processing\\n(NLP) tasks. However, NLMs are very computationally demanding largely due to\\nthe computational cost of the softmax layer over a large vocabulary. We observe\\nthat, in decoding of many NLP tasks, only the probabilities of the top-K\\nhypotheses need to be calculated preciously and K is often much smaller than\\nthe vocabulary size. This paper proposes a novel softmax layer approximation\\nalgorithm, called Fast Graph Decoder (FGD), which quickly identifies, for a\\ngiven context, a set of K words that are most likely to occur according to a\\nNLM. We demonstrate that FGD reduces the decoding time by an order of magnitude\\nwhile attaining close to the full softmax baseline accuracy on neural machine\\ntranslation and language modeling tasks. We also prove the theoretical\\nguarantee on the softmax approximation quality.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04189v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04189v1.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Minjia Zhang',\n",
       "   'Xiaodong Liu',\n",
       "   'Wenhan Wang',\n",
       "   'Jianfeng Gao',\n",
       "   'Yuxiong He'],\n",
       "  'tasks': ['Language Modelling', 'Machine Translation', 'Translation'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [{'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['wikitext-2'],\n",
       "  'datasets_used_full': ['WikiText-2'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-corpus-with-multi-level-annotations-of',\n",
       "  'arxiv_id': '1806.04185',\n",
       "  'title': 'A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature',\n",
       "  'abstract': \"We present a corpus of 5,000 richly annotated abstracts of medical articles\\ndescribing clinical randomized controlled trials. Annotations include\\ndemarcations of text spans that describe the Patient population enrolled, the\\nInterventions studied and to what they were Compared, and the Outcomes measured\\n(the `PICO' elements). These spans are further annotated at a more granular\\nlevel, e.g., individual interventions within them are marked and mapped onto a\\nstructured medical vocabulary. We acquired annotations from a diverse set of\\nworkers with varying levels of expertise and cost. We describe our data\\ncollection process and the corpus itself in detail. We then outline a set of\\nchallenging NLP tasks that would aid searching of the medical literature and\\nthe practice of evidence-based medicine.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04185v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04185v1.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Benjamin Nye',\n",
       "   'Junyi Jessy Li',\n",
       "   'Roma Patel',\n",
       "   'Yinfei Yang',\n",
       "   'Iain J. Marshall',\n",
       "   'Ani Nenkova',\n",
       "   'Byron C. Wallace'],\n",
       "  'tasks': ['Participant Intervention Comparison Outcome Extraction', 'PICO'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/synthetic-depth-of-field-with-a-single-camera',\n",
       "  'arxiv_id': '1806.04171',\n",
       "  'title': 'Synthetic Depth-of-Field with a Single-Camera Mobile Phone',\n",
       "  'abstract': 'Shallow depth-of-field is commonly used by photographers to isolate a subject\\nfrom a distracting background. However, standard cell phone cameras cannot\\nproduce such images optically, as their short focal lengths and small apertures\\ncapture nearly all-in-focus images. We present a system to computationally\\nsynthesize shallow depth-of-field images with a single mobile camera and a\\nsingle button press. If the image is of a person, we use a person segmentation\\nnetwork to separate the person and their accessories from the background. If\\navailable, we also use dense dual-pixel auto-focus hardware, effectively a\\n2-sample light field with an approximately 1 millimeter baseline, to compute a\\ndense depth map. These two signals are combined and used to render a defocused\\nimage. Our system can process a 5.4 megapixel image in 4 seconds on a mobile\\nphone, is fully automatic, and is robust enough to be used by non-experts. The\\nmodular nature of our system allows it to degrade naturally in the absence of a\\ndual-pixel sensor or a human subject.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04171v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04171v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Neal Wadhwa',\n",
       "   'Rahul Garg',\n",
       "   'David E. Jacobs',\n",
       "   'Bryan E. Feldman',\n",
       "   'Nori Kanazawa',\n",
       "   'Robert Carroll',\n",
       "   'Yair Movshovitz-Attias',\n",
       "   'Jonathan T. Barron',\n",
       "   'Yael Pritch',\n",
       "   'Marc Levoy'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/defense-against-the-dark-arts-an-overview-of',\n",
       "  'arxiv_id': '1806.04169',\n",
       "  'title': 'Defense Against the Dark Arts: An overview of adversarial example security research and future research directions',\n",
       "  'abstract': 'This article presents a summary of a keynote lecture at the Deep Learning\\nSecurity workshop at IEEE Security and Privacy 2018. This lecture summarizes\\nthe state of the art in defenses against adversarial examples and provides\\nrecommendations for future research directions on this topic.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04169v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04169v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ian Goodfellow'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/straight-to-the-tree-constituency-parsing',\n",
       "  'arxiv_id': '1806.04168',\n",
       "  'title': 'Straight to the Tree: Constituency Parsing with Neural Syntactic Distance',\n",
       "  'abstract': 'In this work, we propose a novel constituency parsing scheme. The model\\npredicts a vector of real-valued scalars, named syntactic distances, for each\\nsplit position in the input sentence. The syntactic distances specify the order\\nin which the split points will be selected, recursively partitioning the input,\\nin a top-down fashion. Compared to traditional shift-reduce parsing schemes,\\nour approach is free from the potential problem of compounding errors, while\\nbeing faster and easier to parallelize. Our model achieves competitive\\nperformance amongst single model, discriminative parsers in the PTB dataset and\\noutperforms previous models in the CTB dataset.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04168v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04168v1.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Yikang Shen',\n",
       "   'Zhouhan Lin',\n",
       "   'Athul Paul Jacob',\n",
       "   'Alessandro Sordoni',\n",
       "   'Aaron Courville',\n",
       "   'Yoshua Bengio'],\n",
       "  'tasks': ['Constituency Parsing'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-an-approximate-model-predictive',\n",
       "  'arxiv_id': '1806.04167',\n",
       "  'title': 'Learning an Approximate Model Predictive Controller with Guarantees',\n",
       "  'abstract': \"A supervised learning framework is proposed to approximate a model predictive\\ncontroller (MPC) with reduced computational complexity and guarantees on\\nstability and constraint satisfaction. The framework can be used for a wide\\nclass of nonlinear systems. Any standard supervised learning technique (e.g.\\nneural networks) can be employed to approximate the MPC from samples. In order\\nto obtain closed-loop guarantees for the learned MPC, a robust MPC design is\\ncombined with statistical learning bounds. The MPC design ensures robustness to\\ninaccurate inputs within given bounds, and Hoeffding's Inequality is used to\\nvalidate that the learned MPC satisfies these bounds with high confidence. The\\nresult is a closed-loop statistical guarantee on stability and constraint\\nsatisfaction for the learned MPC. The proposed learning-based MPC framework is\\nillustrated on a nonlinear benchmark problem, for which we learn a neural\\nnetwork controller with guarantees.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04167v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04167v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Michael Hertneck',\n",
       "   'Johannes Köhler',\n",
       "   'Sebastian Trimpe',\n",
       "   'Frank Allgöwer'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/gesture-based-bootstrapping-for-egocentric',\n",
       "  'arxiv_id': '1612.02889',\n",
       "  'title': 'Gesture-based Bootstrapping for Egocentric Hand Segmentation',\n",
       "  'abstract': \"Accurately identifying hands in images is a key sub-task for human activity\\nunderstanding with wearable first-person point-of-view cameras. Traditional\\nhand segmentation approaches rely on a large corpus of manually labeled data to\\ngenerate robust hand detectors. However, these approaches still face challenges\\nas the appearance of the hand varies greatly across users, tasks, environments\\nor illumination conditions. A key observation in the case of many wearable\\napplications and interfaces is that, it is only necessary to accurately detect\\nthe user's hands in a specific situational context. Based on this observation,\\nwe introduce an interactive approach to learn a person-specific hand\\nsegmentation model that does not require any manually labeled training data.\\nOur approach proceeds in two steps, an interactive bootstrapping step for\\nidentifying moving hand regions, followed by learning a personalized user\\nspecific hand appearance model. Concretely, our approach uses two convolutional\\nneural networks: (1) a gesture network that uses pre-defined motion information\\nto detect the hand region; and (2) an appearance network that learns a person\\nspecific model of the hand region based on the output of the gesture network.\\nDuring training, to make the appearance network robust to errors in the gesture\\nnetwork, the loss function of the former network incorporates the confidence of\\nthe gesture network while learning. Experiments demonstrate the robustness of\\nour approach with an F1 score over 0.8 on all challenging datasets across a\\nwide range of illumination and hand appearance variations, improving over a\\nbaseline approach by over 10%.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1612.02889v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1612.02889v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yubo Zhang', 'Vishnu Naresh Boddeti', 'Kris M. Kitani'],\n",
       "  'tasks': ['Hand Segmentation'],\n",
       "  'date': '2016-12-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-to-decompose-and-disentangle',\n",
       "  'arxiv_id': '1806.04166',\n",
       "  'title': 'Learning to Decompose and Disentangle Representations for Video Prediction',\n",
       "  'abstract': 'Our goal is to predict future video frames given a sequence of input frames.\\nDespite large amounts of video data, this remains a challenging task because of\\nthe high-dimensionality of video frames. We address this challenge by proposing\\nthe Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework\\nthat combines structured probabilistic models and deep networks to\\nautomatically (i) decompose the high-dimensional video that we aim to predict\\ninto components, and (ii) disentangle each component to have low-dimensional\\ntemporal dynamics that are easier to predict. Crucially, with an appropriately\\nspecified generative model of video frames, our DDPAE is able to learn both the\\nlatent decomposition and disentanglement without explicit supervision. For the\\nMoving MNIST dataset, we show that DDPAE is able to recover the underlying\\ncomponents (individual digits) and disentanglement (appearance and location) as\\nwe would intuitively do. We further demonstrate that DDPAE can be applied to\\nthe Bouncing Balls dataset involving complex interactions between multiple\\nobjects to predict the video frame directly from the pixels and recover\\nphysical states without explicit supervision.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04166v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04166v2.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Jun-Ting Hsieh',\n",
       "   'Bingbin Liu',\n",
       "   'De-An Huang',\n",
       "   'Li Fei-Fei',\n",
       "   'Juan Carlos Niebles'],\n",
       "  'tasks': ['Disentanglement',\n",
       "   'Predict Future Video Frames',\n",
       "   'Video Prediction'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['moving-mnist'],\n",
       "  'datasets_used_full': ['Moving MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/roto-translation-covariant-convolutional',\n",
       "  'arxiv_id': '1804.03393',\n",
       "  'title': 'Roto-Translation Covariant Convolutional Networks for Medical Image Analysis',\n",
       "  'abstract': 'We propose a framework for rotation and translation covariant deep learning\\nusing $SE(2)$ group convolutions. The group product of the special Euclidean\\nmotion group $SE(2)$ describes how a concatenation of two roto-translations\\nresults in a net roto-translation. We encode this geometric structure into\\nconvolutional neural networks (CNNs) via $SE(2)$ group convolutional layers,\\nwhich fit into the standard 2D CNN framework, and which allow to generically\\ndeal with rotated input samples without the need for data augmentation.\\n  We introduce three layers: a lifting layer which lifts a 2D (vector valued)\\nimage to an $SE(2)$-image, i.e., 3D (vector valued) data whose domain is\\n$SE(2)$; a group convolution layer from and to an $SE(2)$-image; and a\\nprojection layer from an $SE(2)$-image to a 2D image. The lifting and group\\nconvolution layers are $SE(2)$ covariant (the output roto-translates with the\\ninput). The final projection layer, a maximum intensity projection over\\nrotations, makes the full CNN rotation invariant.\\n  We show with three different problems in histopathology, retinal imaging, and\\nelectron microscopy that with the proposed group CNNs, state-of-the-art\\nperformance can be achieved, without the need for data augmentation by rotation\\nand with increased performance compared to standard CNNs that do rely on\\naugmentation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.03393v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.03393v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Erik J. Bekkers',\n",
       "   'Maxime W. Lafarge',\n",
       "   'Mitko Veta',\n",
       "   'Koen AJ Eppenhof',\n",
       "   'Josien PW Pluim',\n",
       "   'Remco Duits'],\n",
       "  'tasks': ['Data Augmentation', 'Translation'],\n",
       "  'date': '2018-04-10',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/finding-syntax-in-human-encephalography-with',\n",
       "  'arxiv_id': '1806.04127',\n",
       "  'title': 'Finding Syntax in Human Encephalography with Beam Search',\n",
       "  'abstract': 'Recurrent neural network grammars (RNNGs) are generative models of\\n(tree,string) pairs that rely on neural networks to evaluate derivational\\nchoices. Parsing with them using beam search yields a variety of incremental\\ncomplexity metrics such as word surprisal and parser action count. When used as\\nregressors against human electrophysiological responses to naturalistic text,\\nthey derive two amplitude effects: an early peak and a P600-like later peak. By\\ncontrast, a non-syntactic neural language model yields no reliable effects.\\nModel comparisons attribute the early peak to syntactic composition within the\\nRNNG. This pattern of results recommends the RNNG+beam search combination as a\\nmechanistic model of the syntactic processing that occurs during normal human\\nlanguage comprehension.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04127v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04127v1.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['John Hale',\n",
       "   'Chris Dyer',\n",
       "   'Adhiguna Kuncoro',\n",
       "   'Jonathan R. Brennan'],\n",
       "  'tasks': ['Language Modelling'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/evaluating-robustness-of-neural-networks-with',\n",
       "  'arxiv_id': '1711.07356',\n",
       "  'title': 'Evaluating Robustness of Neural Networks with Mixed Integer Programming',\n",
       "  'abstract': 'Neural networks have demonstrated considerable success on a wide variety of\\nreal-world problems. However, networks trained only to optimize for training\\naccuracy can often be fooled by adversarial examples - slightly perturbed\\ninputs that are misclassified with high confidence. Verification of networks\\nenables us to gauge their vulnerability to such adversarial examples. We\\nformulate verification of piecewise-linear neural networks as a mixed integer\\nprogram. On a representative task of finding minimum adversarial distortions,\\nour verifier is two to three orders of magnitude quicker than the\\nstate-of-the-art. We achieve this computational speedup via tight formulations\\nfor non-linearities, as well as a novel presolve algorithm that makes full use\\nof all information available. The computational speedup allows us to verify\\nproperties on convolutional networks with an order of magnitude more ReLUs than\\nnetworks previously verified by any complete verifier. In particular, we\\ndetermine for the first time the exact adversarial accuracy of an MNIST\\nclassifier to perturbations with bounded $l_\\\\infty$ norm $\\\\epsilon=0.1$: for\\nthis classifier, we find an adversarial example for 4.38% of samples, and a\\ncertificate of robustness (to perturbations with bounded norm) for the\\nremainder. Across all robust training procedures and network architectures\\nconsidered, we are able to certify more samples than the state-of-the-art and\\nfind more adversarial examples than a strong first-order attack.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.07356v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.07356v3.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Vincent Tjeng', 'Kai Xiao', 'Russ Tedrake'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-11-20',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/constructing-datasets-for-multi-hop-reading',\n",
       "  'arxiv_id': '1710.06481',\n",
       "  'title': 'Constructing Datasets for Multi-hop Reading Comprehension Across Documents',\n",
       "  'abstract': 'Most Reading Comprehension methods limit themselves to queries which can be\\nanswered using a single sentence, paragraph, or document. Enabling models to\\ncombine disjoint pieces of textual evidence would extend the scope of machine\\ncomprehension methods, but currently there exist no resources to train and test\\nthis capability. We propose a novel task to encourage the development of models\\nfor text understanding across multiple documents and to investigate the limits\\nof existing methods. In our task, a model learns to seek and combine evidence -\\neffectively performing multi-hop (alias multi-step) inference. We devise a\\nmethodology to produce datasets for this task, given a collection of\\nquery-answer pairs and thematically linked documents. Two datasets from\\ndifferent domains are induced, and we identify potential pitfalls and devise\\ncircumvention strategies. We evaluate two previously proposed competitive\\nmodels and find that one can integrate information across documents. However,\\nboth models struggle to select relevant information, as providing documents\\nguaranteed to be relevant greatly improves their performance. While the models\\noutperform several strong baselines, their best accuracy reaches 42.9% compared\\nto human performance at 74.0% - leaving ample room for improvement.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1710.06481v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1710.06481v2.pdf',\n",
       "  'proceeding': 'TACL 2018 1',\n",
       "  'authors': ['Johannes Welbl', 'Pontus Stenetorp', 'Sebastian Riedel'],\n",
       "  'tasks': ['Multi-Hop Reading Comprehension', 'Reading Comprehension'],\n",
       "  'date': '2017-10-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['squad', 'triviaqa', 'ddi'],\n",
       "  'datasets_used_full': ['SQuAD', 'TriviaQA', 'DDI'],\n",
       "  'datasets_introduced_lower': ['wikihop', 'medhop'],\n",
       "  'datasets_introduced_full': ['WikiHop', 'MedHop']},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-convolutional-neural-networks-for-brain',\n",
       "  'arxiv_id': '1712.03747',\n",
       "  'title': 'Deep convolutional neural networks for brain image analysis on magnetic resonance imaging: a review',\n",
       "  'abstract': 'In recent years, deep convolutional neural networks (CNNs) have shown\\nrecord-shattering performance in a variety of computer vision problems, such as\\nvisual object recognition, detection and segmentation. These methods have also\\nbeen utilised in medical image analysis domain for lesion segmentation,\\nanatomical segmentation and classification. We present an extensive literature\\nreview of CNN techniques applied in brain magnetic resonance imaging (MRI)\\nanalysis, focusing on the architectures, pre-processing, data-preparation and\\npost-processing strategies available in these works. The aim of this study is\\nthree-fold. Our primary goal is to report how different CNN architectures have\\nevolved, discuss state-of-the-art strategies, condense their results obtained\\nusing public datasets and examine their pros and cons. Second, this paper is\\nintended to be a detailed reference of the research activity in deep CNN for\\nbrain MRI analysis. Finally, we present a perspective on the future of CNNs in\\nwhich we hint some of the research directions in subsequent years.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1712.03747v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1712.03747v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jose Bernal',\n",
       "   'Kaisar Kushibar',\n",
       "   'Daniel S. Asfaw',\n",
       "   'Sergi Valverde',\n",
       "   'Arnau Oliver',\n",
       "   'Robert Martí',\n",
       "   'Xavier Lladó'],\n",
       "  'tasks': ['Lesion Segmentation', 'Object Recognition'],\n",
       "  'date': '2017-12-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet',\n",
       "   'mnist',\n",
       "   'svhn',\n",
       "   'brats-2015-1',\n",
       "   'brats-2013-1',\n",
       "   'brats-2014-1'],\n",
       "  'datasets_used_full': ['ImageNet',\n",
       "   'MNIST',\n",
       "   'SVHN',\n",
       "   'BraTS 2015',\n",
       "   'BraTS 2013',\n",
       "   'BraTS 2014'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/atomo-communication-efficient-learning-via',\n",
       "  'arxiv_id': '1806.04090',\n",
       "  'title': 'ATOMO: Communication-efficient Learning via Atomic Sparsification',\n",
       "  'abstract': 'Distributed model training suffers from communication overheads due to\\nfrequent gradient updates transmitted between compute nodes. To mitigate these\\noverheads, several studies propose the use of sparsified stochastic gradients.\\nWe argue that these are facets of a general sparsification method that can\\noperate on any possible atomic decomposition. Notable examples include\\nelement-wise, singular value, and Fourier decompositions. We present ATOMO, a\\ngeneral framework for atomic sparsification of stochastic gradients. Given a\\ngradient, an atomic decomposition, and a sparsity budget, ATOMO gives a random\\nunbiased sparsification of the atoms minimizing variance. We show that recent\\nmethods such as QSGD and TernGrad are special cases of ATOMO and that\\nsparsifiying the singular value decomposition of neural networks gradients,\\nrather than their coordinates, can lead to significantly faster distributed\\ntraining.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04090v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04090v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Hongyi Wang',\n",
       "   'Scott Sievert',\n",
       "   'Zachary Charles',\n",
       "   'Shengchao Liu',\n",
       "   'Stephen Wright',\n",
       "   'Dimitris Papailiopoulos'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'svhn'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'SVHN'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-research-of-the-real-time-detection-and',\n",
       "  'arxiv_id': '1806.04070',\n",
       "  'title': 'The Research of the Real-time Detection and Recognition of Targets in Streetscape Videos',\n",
       "  'abstract': 'This study proposes a method for the real-time detection and recognition of\\ntargets in streetscape videos. The proposed method is based on separation\\nconfidence computation and scale synthesis optimization. We use the proposed\\nmethod to detect and recognize targets in streetscape videos with high frame\\nrates and high definition. Furthermore, we experimentally demonstrate that the\\naccuracy and robustness of our proposed method are superior to those of\\nconventional methods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04070v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04070v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Liu Jian-min'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-co-matching-model-for-multi-choice-reading',\n",
       "  'arxiv_id': '1806.04068',\n",
       "  'title': 'A Co-Matching Model for Multi-choice Reading Comprehension',\n",
       "  'abstract': 'Multi-choice reading comprehension is a challenging task, which involves the\\nmatching between a passage and a question-answer pair. This paper proposes a\\nnew co-matching approach to this problem, which jointly models whether a\\npassage can match both a question and a candidate answer. Experimental results\\non the RACE dataset demonstrate that our approach achieves state-of-the-art\\nperformance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04068v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04068v1.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Shuohang Wang', 'Mo Yu', 'Shiyu Chang', 'Jing Jiang'],\n",
       "  'tasks': ['Reading Comprehension'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['race'],\n",
       "  'datasets_used_full': ['RACE'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/adaptive-mechanism-design-learning-to-promote',\n",
       "  'arxiv_id': '1806.04067',\n",
       "  'title': 'Adaptive Mechanism Design: Learning to Promote Cooperation',\n",
       "  'abstract': \"In the future, artificial learning agents are likely to become increasingly widespread in our society. They will interact with both other learning agents and humans in a variety of complex settings including social dilemmas. We consider the problem of how an external agent can promote cooperation between artificial learners by distributing additional rewards and punishments based on observing the learners' actions. We propose a rule for automatically learning how to create right incentives by considering the players' anticipated parameter updates. Using this learning rule leads to cooperation with high social welfare in matrix games in which the agents would otherwise learn to defect with high probability. We show that the resulting cooperative outcome is stable in certain games even if the planning agent is turned off after a given number of episodes, while other games require ongoing intervention to maintain mutual cooperation. However, even in the latter case, the amount of necessary additional incentives decreases over time.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04067v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04067v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Tobias Baumann', 'Thore Graepel', 'John Shawe-Taylor'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/joint-learning-of-motion-estimation-and',\n",
       "  'arxiv_id': '1806.04066',\n",
       "  'title': 'Joint Learning of Motion Estimation and Segmentation for Cardiac MR Image Sequences',\n",
       "  'abstract': 'Cardiac motion estimation and segmentation play important roles in\\nquantitatively assessing cardiac function and diagnosing cardiovascular\\ndiseases. In this paper, we propose a novel deep learning method for joint\\nestimation of motion and segmentation from cardiac MR image sequences. The\\nproposed network consists of two branches: a cardiac motion estimation branch\\nwhich is built on a novel unsupervised Siamese style recurrent spatial\\ntransformer network, and a cardiac segmentation branch that is based on a fully\\nconvolutional network. In particular, a joint multi-scale feature encoder is\\nlearned by optimizing the segmentation branch and the motion estimation branch\\nsimultaneously. This enables the weakly-supervised segmentation by taking\\nadvantage of features that are unsupervisedly learned in the motion estimation\\nbranch from a large amount of unannotated data. Experimental results using\\ncardiac MRI images from 220 subjects show that the joint learning of both tasks\\nis complementary and the proposed models outperform the competing methods\\nsignificantly in terms of accuracy and speed.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04066v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04066v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Chen Qin',\n",
       "   'Wenjia Bai',\n",
       "   'Jo Schlemper',\n",
       "   'Steffen E. Petersen',\n",
       "   'Stefan K. Piechnik',\n",
       "   'Stefan Neubauer',\n",
       "   'Daniel Rueckert'],\n",
       "  'tasks': ['Cardiac Segmentation',\n",
       "   'Motion Estimation',\n",
       "   'Weakly supervised segmentation'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/which-training-methods-for-gans-do-actually',\n",
       "  'arxiv_id': '1801.04406',\n",
       "  'title': 'Which Training Methods for GANs do actually Converge?',\n",
       "  'abstract': 'Recent work has shown local convergence of GAN training for absolutely\\ncontinuous data and generator distributions. In this paper, we show that the\\nrequirement of absolute continuity is necessary: we describe a simple yet\\nprototypical counterexample showing that in the more realistic case of\\ndistributions that are not absolutely continuous, unregularized GAN training is\\nnot always convergent. Furthermore, we discuss regularization strategies that\\nwere recently proposed to stabilize GAN training. Our analysis shows that GAN\\ntraining with instance noise or zero-centered gradient penalties converges. On\\nthe other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number\\nof discriminator updates per generator update do not always converge to the\\nequilibrium point. We discuss these results, leading us to a new explanation\\nfor the stability problems of GAN training. Based on our analysis, we extend\\nour convergence results to more general GANs and prove local convergence for\\nsimplified gradient penalties even if the generator and data distribution lie\\non lower dimensional manifolds. We find these penalties to work well in\\npractice and use them to learn high-resolution generative image models for a\\nvariety of datasets with little hyperparameter tuning.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1801.04406v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1801.04406v4.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Lars Mescheder', 'Andreas Geiger', 'Sebastian Nowozin'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-01-13',\n",
       "  'methods': [{'name': 'R1 Regularization',\n",
       "    'full_name': 'R1 Regularization',\n",
       "    'description': '**R_INLINE_MATH_1 Regularization** is a regularization technique and gradient penalty for training [generative adversarial networks](https://paperswithcode.com/methods/category/generative-adversarial-networks). It penalizes the discriminator from deviating from the Nash Equilibrium via penalizing the gradient on real data alone: when the generator distribution produces the true data distribution and the discriminator is equal to 0 on the data manifold, the gradient penalty ensures that the discriminator cannot create a non-zero gradient orthogonal to the data manifold without suffering a loss in the [GAN](https://paperswithcode.com/method/gan) game.\\r\\n\\r\\nThis leads to the following regularization term:\\r\\n\\r\\n$$ R\\\\_{1}\\\\left(\\\\psi\\\\right) = \\\\frac{\\\\gamma}{2}E\\\\_{p\\\\_{D}\\\\left(x\\\\right)}\\\\left[||\\\\nabla{D\\\\_{\\\\psi}\\\\left(x\\\\right)}||^{2}\\\\right] $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1801.04406v4',\n",
       "    'source_title': 'Which Training Methods for GANs do actually Converge?',\n",
       "    'code_snippet_url': 'https://github.com/ChristophReich1996/Dirac-GAN/blob/decb8283d919640057c50ff5a1ba01b93ed86332/dirac_gan/loss.py#L292',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/efficient-model-based-deep-reinforcement',\n",
       "  'arxiv_id': '1802.04325',\n",
       "  'title': 'Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation',\n",
       "  'abstract': 'Modern reinforcement learning algorithms reach super-human performance on\\nmany board and video games, but they are sample inefficient, i.e. they\\ntypically require significantly more playing experience than humans to reach an\\nequal performance level. To improve sample efficiency, an agent may build a\\nmodel of the environment and use planning methods to update its policy. In this\\narticle we introduce Variational State Tabulation (VaST), which maps an\\nenvironment with a high-dimensional state space (e.g. the space of visual\\ninputs) to an abstract tabular model. Prioritized sweeping with small backups,\\na highly efficient planning method, can then be used to update state-action\\nvalues. We show how VaST can rapidly learn to maximize reward in tasks like 3D\\nnavigation and efficiently adapt to sudden changes in rewards or transition\\nprobabilities.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.04325v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.04325v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Dane Corneil', 'Wulfram Gerstner', 'Johanni Brea'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-12',\n",
       "  'methods': [{'name': 'Prioritized Sweeping',\n",
       "    'full_name': 'Prioritized Sweeping',\n",
       "    'description': '**Prioritized Sweeping** is a reinforcement learning technique for model-based algorithms that prioritizes updates according to a measure of urgency, and performs these updates first. A queue is maintained of every state-action pair whose estimated value would change nontrivially if updated, prioritized by the size of the change. When the top pair in the queue is updated, the effect on each of its predecessor pairs is computed. If the effect is greater than some threshold, then the pair is inserted in the queue with the new priority.\\r\\n\\r\\nSource: Sutton and Barto, Reinforcement Learning, 2nd Edition',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Efficient Planning',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Reinforcement Learning'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/ct-realistic-lung-nodule-simulation-from-3d',\n",
       "  'arxiv_id': '1806.04051',\n",
       "  'title': 'CT-Realistic Lung Nodule Simulation from 3D Conditional Generative Adversarial Networks for Robust Lung Segmentation',\n",
       "  'abstract': 'Data availability plays a critical role for the performance of deep learning\\nsystems. This challenge is especially acute within the medical image domain,\\nparticularly when pathologies are involved, due to two factors: 1) limited\\nnumber of cases, and 2) large variations in location, scale, and appearance. In\\nthis work, we investigate whether augmenting a dataset with artificially\\ngenerated lung nodules can improve the robustness of the progressive\\nholistically nested network (P-HNN) model for pathological lung segmentation of\\nCT scans. To achieve this goal, we develop a 3D generative adversarial network\\n(GAN) that effectively learns lung nodule property distributions in 3D space.\\nIn order to embed the nodules within their background context, we condition the\\nGAN based on a volume of interest whose central part containing the nodule has\\nbeen erased. To further improve realism and blending with the background, we\\npropose a novel multi-mask reconstruction loss. We train our method on over\\n1000 nodules from the LIDC dataset. Qualitative results demonstrate the\\neffectiveness of our method compared to the state-of-art. We then use our GAN\\nto generate simulated training images where nodules lie on the lung border,\\nwhich are cases where the published P-HNN model struggles. Qualitative and\\nquantitative results demonstrate that armed with these simulated images, the\\nP-HNN model learns to better segment lung regions under these challenging\\nsituations. As a result, our system provides a promising means to help overcome\\nthe data paucity that commonly afflicts medical imaging.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04051v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04051v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Dakai Jin',\n",
       "   'Ziyue Xu',\n",
       "   'You-Bao Tang',\n",
       "   'Adam P. Harrison',\n",
       "   'Daniel J. Mollura'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/adaptive-denoising-of-signals-with-shift',\n",
       "  'arxiv_id': '1806.04028',\n",
       "  'title': 'Adaptive Denoising of Signals with Local Shift-Invariant Structure',\n",
       "  'abstract': 'We discuss the problem of adaptive discrete-time signal denoising in the situation where the signal to be recovered admits a \"linear oracle\" -- an unknown linear estimate that takes the form of convolution of observations with a time-invariant filter. It was shown by Juditsky and Nemirovski (2009) that when the $\\\\ell_2$-norm of the oracle filter is small enough, such oracle can be \"mimicked\" by an efficiently computable adaptive estimate of the same structure with an observation-driven filter. The filter in question was obtained as a solution to the optimization problem in which the $\\\\ell_\\\\infty$-norm of the Discrete Fourier Transform (DFT) of the estimation residual is minimized under constraint on the $\\\\ell_1$-norm of the filter DFT. In this paper, we discuss a new family of adaptive estimates which rely upon minimizing the $\\\\ell_2$-norm of the estimation residual. We show that such estimators possess better statistical properties than those based on $\\\\ell_\\\\infty$-fit; in particular, we prove oracle inequalities for their $\\\\ell_2$-loss and improved bounds for $\\\\ell_2$- and pointwise losses. The oracle inequalities rely on the \"approximate shift-invariance\" assumption stating that the signal to be recovered is close to an (unknown) shift-invariant subspace. We also study the relationship of the approximate shift-invariance assumption with the \"signal simplicity\" assumption introduced in Juditsky and Nemirovski (2009) and discuss the application of the proposed approach to harmonic oscillations denoising.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04028v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04028v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Zaid Harchaoui',\n",
       "   'Anatoli Juditsky',\n",
       "   'Arkadi Nemirovski',\n",
       "   'Dmitrii Ostrovskii'],\n",
       "  'tasks': ['Denoising'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/baselines-and-a-datasheet-for-the-cerema-awp',\n",
       "  'arxiv_id': '1806.04016',\n",
       "  'title': 'Baselines and a datasheet for the Cerema AWP dataset',\n",
       "  'abstract': 'This paper presents the recently published Cerema AWP (Adverse Weather\\nPedestrian) dataset for various machine learning tasks and its exports in\\nmachine learning friendly format. We explain why this dataset can be\\ninteresting (mainly because it is a greatly controlled and fully annotated\\nimage dataset) and present baseline results for various tasks. Moreover, we\\ndecided to follow the very recent suggestions of datasheets for dataset, trying\\nto standardize all the available information of the dataset, with a\\ntransparency objective.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04016v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04016v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ismaïla Seck',\n",
       "   'Khouloud Dahmane',\n",
       "   'Pierre Duthon',\n",
       "   'Gaëlle Loosli'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/turning-your-weakness-into-a-strength',\n",
       "  'arxiv_id': '1802.04633',\n",
       "  'title': 'Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring',\n",
       "  'abstract': 'Deep Neural Networks have recently gained lots of success after enabling\\nseveral breakthroughs in notoriously challenging problems. Training these\\nnetworks is computationally expensive and requires vast amounts of training\\ndata. Selling such pre-trained models can, therefore, be a lucrative business\\nmodel. Unfortunately, once the models are sold they can be easily copied and\\nredistributed. To avoid this, a tracking mechanism to identify models as the\\nintellectual property of a particular vendor is necessary.\\n  In this work, we present an approach for watermarking Deep Neural Networks in\\na black-box way. Our scheme works for general classification tasks and can\\neasily be combined with current learning algorithms. We show experimentally\\nthat such a watermark has no noticeable impact on the primary task that the\\nmodel is designed for and evaluate the robustness of our proposal against a\\nmultitude of practical attacks. Moreover, we provide a theoretical analysis,\\nrelating our approach to previous work on backdooring.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.04633v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.04633v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yossi Adi',\n",
       "   'Carsten Baum',\n",
       "   'Moustapha Cisse',\n",
       "   'Benny Pinkas',\n",
       "   'Joseph Keshet'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-02-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'cifar-100', 'stl-10'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'CIFAR-100', 'STL-10'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/relational-inductive-biases-deep-learning-and',\n",
       "  'arxiv_id': '1806.01261',\n",
       "  'title': 'Relational inductive biases, deep learning, and graph networks',\n",
       "  'abstract': 'Artificial intelligence (AI) has undergone a renaissance recently, making\\nmajor progress in key domains such as vision, language, control, and\\ndecision-making. This has been due, in part, to cheap data and cheap compute\\nresources, which have fit the natural strengths of deep learning. However, many\\ndefining characteristics of human intelligence, which developed under much\\ndifferent pressures, remain out of reach for current approaches. In particular,\\ngeneralizing beyond one\\'s experiences--a hallmark of human intelligence from\\ninfancy--remains a formidable challenge for modern AI.\\n  The following is part position paper, part review, and part unification. We\\nargue that combinatorial generalization must be a top priority for AI to\\nachieve human-like abilities, and that structured representations and\\ncomputations are key to realizing this objective. Just as biology uses nature\\nand nurture cooperatively, we reject the false choice between\\n\"hand-engineering\" and \"end-to-end\" learning, and instead advocate for an\\napproach which benefits from their complementary strengths. We explore how\\nusing relational inductive biases within deep learning architectures can\\nfacilitate learning about entities, relations, and rules for composing them. We\\npresent a new building block for the AI toolkit with a strong relational\\ninductive bias--the graph network--which generalizes and extends various\\napproaches for neural networks that operate on graphs, and provides a\\nstraightforward interface for manipulating structured knowledge and producing\\nstructured behaviors. We discuss how graph networks can support relational\\nreasoning and combinatorial generalization, laying the foundation for more\\nsophisticated, interpretable, and flexible patterns of reasoning. As a\\ncompanion to this paper, we have released an open-source software library for\\nbuilding graph networks, with demonstrations of how to use them in practice.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.01261v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.01261v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Peter W. Battaglia',\n",
       "   'Jessica B. Hamrick',\n",
       "   'Victor Bapst',\n",
       "   'Alvaro Sanchez-Gonzalez',\n",
       "   'Vinicius Zambaldi',\n",
       "   'Mateusz Malinowski',\n",
       "   'Andrea Tacchetti',\n",
       "   'David Raposo',\n",
       "   'Adam Santoro',\n",
       "   'Ryan Faulkner',\n",
       "   'Caglar Gulcehre',\n",
       "   'Francis Song',\n",
       "   'Andrew Ballard',\n",
       "   'Justin Gilmer',\n",
       "   'George Dahl',\n",
       "   'Ashish Vaswani',\n",
       "   'Kelsey Allen',\n",
       "   'Charles Nash',\n",
       "   'Victoria Langston',\n",
       "   'Chris Dyer',\n",
       "   'Nicolas Heess',\n",
       "   'Daan Wierstra',\n",
       "   'Pushmeet Kohli',\n",
       "   'Matt Botvinick',\n",
       "   'Oriol Vinyals',\n",
       "   'Yujia Li',\n",
       "   'Razvan Pascanu'],\n",
       "  'tasks': ['Decision Making', 'Relational Reasoning'],\n",
       "  'date': '2018-06-04',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/prosody-modifications-for-question-answering',\n",
       "  'arxiv_id': '1806.03957',\n",
       "  'title': 'Prosody Modifications for Question-Answering in Voice-Only Settings',\n",
       "  'abstract': 'Many popular form factors of digital assistants---such as Amazon Echo, Apple Homepod, or Google Home---enable the user to hold a conversation with these systems based only on the speech modality. The lack of a screen presents unique challenges. To satisfy the information need of a user, the presentation of the answer needs to be optimized for such voice-only interactions. In this paper, we propose a task of evaluating the usefulness of audio transformations (i.e., prosodic modifications) for voice-only question answering. We introduce a crowdsourcing setup where we evaluate the quality of our proposed modifications along multiple dimensions corresponding to the informativeness, naturalness, and ability of the user to identify key parts of the answer. We offer a set of prosodic modifications that highlight potentially important parts of the answer using various acoustic cues. Our experiments show that some of these prosodic modifications lead to better comprehension at the expense of only slightly degraded naturalness of the audio.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03957v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03957v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Aleksandr Chuklin',\n",
       "   'Aliaksei Severyn',\n",
       "   'Johanne Trippas',\n",
       "   'Enrique Alfonseca',\n",
       "   'Hanna Silen',\n",
       "   'Damiano Spina'],\n",
       "  'tasks': ['Informativeness', 'Question Answering'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['squad'],\n",
       "  'datasets_used_full': ['SQuAD'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/psgan-a-generative-adversarial-network-for',\n",
       "  'arxiv_id': '1805.03371',\n",
       "  'title': 'PSGAN: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening',\n",
       "  'abstract': 'This paper addresses the problem of remote sensing image pan-sharpening from the perspective of generative adversarial learning. We propose a novel deep neural network based method named PSGAN. To the best of our knowledge, this is one of the first attempts at producing high-quality pan-sharpened images with GANs. The PSGAN consists of two components: a generative network (i.e., generator) and a discriminative network (i.e., discriminator). The generator is designed to accept panchromatic (PAN) and multispectral (MS) images as inputs and maps them to the desired high-resolution (HR) MS images and the discriminator implements the adversarial training strategy for generating higher fidelity pan-sharpened images. In this paper, we evaluate several architectures and designs, namely two-stream input, stacking input, batch normalization layer, and attention mechanism to find the optimal solution for pan-sharpening. Extensive experiments on QuickBird, GaoFen-2, and WorldView-2 satellite images demonstrate that the proposed PSGANs not only are effective in generating high-quality HR MS images and superior to state-of-the-art methods and also generalize well to full-scale images.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1805.03371v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1805.03371v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Qingjie Liu',\n",
       "   'Huanyu Zhou',\n",
       "   'Qizhi Xu',\n",
       "   'Xiangyu Liu',\n",
       "   'Yunhong Wang'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['3dfaw'],\n",
       "  'datasets_used_full': ['3DFAW'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-numerics-of-gans',\n",
       "  'arxiv_id': '1705.10461',\n",
       "  'title': 'The Numerics of GANs',\n",
       "  'abstract': 'In this paper, we analyze the numerics of common algorithms for training\\nGenerative Adversarial Networks (GANs). Using the formalism of smooth\\ntwo-player games we analyze the associated gradient vector field of GAN\\ntraining objectives. Our findings suggest that the convergence of current\\nalgorithms suffers due to two factors: i) presence of eigenvalues of the\\nJacobian of the gradient vector field with zero real-part, and ii) eigenvalues\\nwith big imaginary part. Using these findings, we design a new algorithm that\\novercomes some of these limitations and has better convergence properties.\\nExperimentally, we demonstrate its superiority on training common GAN\\narchitectures and show convergence on GAN architectures that are known to be\\nnotoriously hard to train.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1705.10461v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1705.10461v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2017 12',\n",
       "  'authors': ['Lars Mescheder', 'Sebastian Nowozin', 'Andreas Geiger'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-05-30',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-fast-and-easy-regression-technique-for-k-nn',\n",
       "  'arxiv_id': '1806.03945',\n",
       "  'title': 'A Fast and Easy Regression Technique for k-NN Classification Without Using Negative Pairs',\n",
       "  'abstract': 'This paper proposes an inexpensive way to learn an effective dissimilarity function to be used for $k$-nearest neighbor ($k$-NN) classification. Unlike Mahalanobis metric learning methods that map both query (unlabeled) objects and labeled objects to new coordinates by a single transformation, our method learns a transformation of labeled objects to new points in the feature space whereas query objects are kept in their original coordinates. This method has several advantages over existing distance metric learning methods: (i) In experiments with large document and image datasets, it achieves $k$-NN classification accuracy better than or at least comparable to the state-of-the-art metric learning methods. (ii) The transformation can be learned efficiently by solving a standard ridge regression problem. For document and image datasets, training is often more than two orders of magnitude faster than the fastest metric learning methods tested. This speed-up is also due to the fact that the proposed method eliminates the optimization over \"negative\" object pairs, i.e., objects whose class labels are different. (iii) The formulation has a theoretical justification in terms of reducing hubness in data.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03945v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03945v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yutaro Shigeto', 'Masashi Shimbo', 'Yuji Matsumoto'],\n",
       "  'tasks': ['General Classification', 'Metric Learning'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/image-denoising-with-generalized-gaussian',\n",
       "  'arxiv_id': '1802.01458',\n",
       "  'title': 'Image denoising with generalized Gaussian mixture model patch priors',\n",
       "  'abstract': 'Patch priors have become an important component of image restoration. A\\npowerful approach in this category of restoration algorithms is the popular\\nExpected Patch Log-Likelihood (EPLL) algorithm. EPLL uses a Gaussian mixture\\nmodel (GMM) prior learned on clean image patches as a way to regularize\\ndegraded patches. In this paper, we show that a generalized Gaussian mixture\\nmodel (GGMM) captures the underlying distribution of patches better than a GMM.\\nEven though GGMM is a powerful prior to combine with EPLL, the non-Gaussianity\\nof its components presents major challenges to be applied to a computationally\\nintensive process of image restoration. Specifically, each patch has to undergo\\na patch classification step and a shrinkage step. These two steps can be\\nefficiently solved with a GMM prior but are computationally impractical when\\nusing a GGMM prior. In this paper, we provide approximations and computational\\nrecipes for fast evaluation of these two steps, so that EPLL can embed a GGMM\\nprior on an image with more than tens of thousands of patches. Our main\\ncontribution is to analyze the accuracy of our approximations based on thorough\\ntheoretical analysis. Our evaluations indicate that the GGMM prior is\\nconsistently a better fit formodeling image patch distribution and performs\\nbetter on average in image denoising task.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.01458v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.01458v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Charles-Alban Deledalle',\n",
       "   'Shibin Parameswaran',\n",
       "   'Truong Q. Nguyen'],\n",
       "  'tasks': ['Denoising', 'Image Denoising', 'Image Restoration'],\n",
       "  'date': '2018-02-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/second-order-asymptotically-optimal',\n",
       "  'arxiv_id': '1806.00739',\n",
       "  'title': 'Second-Order Asymptotically Optimal Statistical Classification',\n",
       "  'abstract': 'Motivated by real-world machine learning applications, we analyze\\napproximations to the non-asymptotic fundamental limits of statistical\\nclassification. In the binary version of this problem, given two training\\nsequences generated according to two {\\\\em unknown} distributions $P_1$ and\\n$P_2$, one is tasked to classify a test sequence which is known to be generated\\naccording to either $P_1$ or $P_2$. This problem can be thought of as an\\nanalogue of the binary hypothesis testing problem but in the present setting,\\nthe generating distributions are unknown. Due to finite sample considerations,\\nwe consider the second-order asymptotics (or dispersion-type) tradeoff between\\ntype-I and type-II error probabilities for tests which ensure that (i) the\\ntype-I error probability for {\\\\em all} pairs of distributions decays\\nexponentially fast and (ii) the type-II error probability for a {\\\\em\\nparticular} pair of distributions is non-vanishing. We generalize our results\\nto classification of multiple hypotheses with the rejection option.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.00739v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.00739v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Lin Zhou', 'Vincent Y. F. Tan', 'Mehul Motani'],\n",
       "  'tasks': ['Classification', 'General Classification', 'Two-sample testing'],\n",
       "  'date': '2018-06-03',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/when-and-where-do-feed-forward-neural',\n",
       "  'arxiv_id': '1806.03934',\n",
       "  'title': 'When and where do feed-forward neural networks learn localist representations?',\n",
       "  'abstract': 'According to parallel distributed processing (PDP) theory in psychology,\\nneural networks (NN) learn distributed rather than interpretable localist\\nrepresentations. This view has been held so strongly that few researchers have\\nanalysed single units to determine if this assumption is correct. However,\\nrecent results from psychology, neuroscience and computer science have shown\\nthe occasional existence of local codes emerging in artificial and biological\\nneural networks. In this paper, we undertake the first systematic survey of\\nwhen local codes emerge in a feed-forward neural network, using generated input\\nand output data with known qualities. We find that the number of local codes\\nthat emerge from a NN follows a well-defined distribution across the number of\\nhidden layer neurons, with a peak determined by the size of input data, number\\nof examples presented and the sparsity of input data. Using a 1-hot output code\\ndrastically decreases the number of local codes on the hidden layer. The number\\nof emergent local codes increases with the percentage of dropout applied to the\\nhidden layer, suggesting that the localist encoding may offer a resilience to\\nnoisy networks. This data suggests that localist coding can emerge from\\nfeed-forward PDP networks and suggests some of the conditions that may lead to\\ninterpretable localist representations in the cortex. The findings highlight\\nhow local codes should not be dismissed out of hand.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03934v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03934v1.pdf',\n",
       "  'proceeding': 'ICLR 2018 1',\n",
       "  'authors': ['Ella M. Gale', 'Nicolas Martin', 'Jeffrey S. Bowers'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [{'name': 'Dropout',\n",
       "    'full_name': 'Dropout',\n",
       "    'description': '**Dropout** is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p=0.5$). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).\\r\\n\\r\\nThe idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://jmlr.org/papers/v15/srivastava14a.html',\n",
       "    'source_title': 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/7f3078b70d0ed9bea6228efa420879c56f72ef69/jax/experimental/stax.py#L271-L275',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/adversarial-variational-bayes-unifying',\n",
       "  'arxiv_id': '1701.04722',\n",
       "  'title': 'Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks',\n",
       "  'abstract': 'Variational Autoencoders (VAEs) are expressive latent variable models that\\ncan be used to learn complex probability distributions from training data.\\nHowever, the quality of the resulting model crucially relies on the\\nexpressiveness of the inference model. We introduce Adversarial Variational\\nBayes (AVB), a technique for training Variational Autoencoders with arbitrarily\\nexpressive inference models. We achieve this by introducing an auxiliary\\ndiscriminative network that allows to rephrase the maximum-likelihood-problem\\nas a two-player game, hence establishing a principled connection between VAEs\\nand Generative Adversarial Networks (GANs). We show that in the nonparametric\\nlimit our method yields an exact maximum-likelihood assignment for the\\nparameters of the generative model, as well as the exact posterior distribution\\nover the latent variables given an observation. Contrary to competing\\napproaches which combine VAEs with GANs, our approach has a clear theoretical\\njustification, retains most advantages of standard Variational Autoencoders and\\nis easy to implement.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1701.04722v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1701.04722v4.pdf',\n",
       "  'proceeding': 'ICML 2017 8',\n",
       "  'authors': ['Lars Mescheder', 'Sebastian Nowozin', 'Andreas Geiger'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-01-17',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist'],\n",
       "  'datasets_used_full': ['MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/kblrn-end-to-end-learning-of-knowledge-base',\n",
       "  'arxiv_id': '1709.04676',\n",
       "  'title': 'KBLRN : End-to-End Learning of Knowledge Base Representations with Latent, Relational, and Numerical Features',\n",
       "  'abstract': 'We present KBLRN, a framework for end-to-end learning of knowledge base\\nrepresentations from latent, relational, and numerical features. KBLRN\\nintegrates feature types with a novel combination of neural representation\\nlearning and probabilistic product of experts models. To the best of our\\nknowledge, KBLRN is the first approach that learns representations of knowledge\\nbases by integrating latent, relational, and numerical features. We show that\\ninstances of KBLRN outperform existing methods on a range of knowledge base\\ncompletion tasks. We contribute a novel data sets enriching commonly used\\nknowledge base completion benchmarks with numerical features. The data sets are\\navailable under a permissive BSD-3 license. We also investigate the impact\\nnumerical features have on the KB completion performance of KBLRN.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1709.04676v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1709.04676v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alberto Garcia-Duran', 'Mathias Niepert'],\n",
       "  'tasks': ['Knowledge Base Completion', 'Representation Learning'],\n",
       "  'date': '2017-09-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['fb15k', 'wn18', 'fb15k-237', 'fb122'],\n",
       "  'datasets_used_full': ['FB15k', 'WN18', 'FB15k-237', 'FB122'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/3d-face-reconstruction-with-geometry-details',\n",
       "  'arxiv_id': '1702.05619',\n",
       "  'title': '3D Face Reconstruction with Geometry Details from a Single Image',\n",
       "  'abstract': '3D face reconstruction from a single image is a classical and challenging\\nproblem, with wide applications in many areas. Inspired by recent works in face\\nanimation from RGB-D or monocular video inputs, we develop a novel method for\\nreconstructing 3D faces from unconstrained 2D images, using a coarse-to-fine\\noptimization strategy. First, a smooth coarse 3D face is generated from an\\nexample-based bilinear face model, by aligning the projection of 3D face\\nlandmarks with 2D landmarks detected from the input image. Afterwards, using\\nlocal corrective deformation fields, the coarse 3D face is refined using\\nphotometric consistency constraints, resulting in a medium face shape. Finally,\\na shape-from-shading method is applied on the medium face to recover fine\\ngeometric details. Our method outperforms state-of-the-art approaches in terms\\nof accuracy and detail recovery, which is demonstrated in extensive experiments\\nusing real world models and publicly available datasets.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1702.05619v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1702.05619v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Luo Jiang',\n",
       "   'Juyong Zhang',\n",
       "   'Bailin Deng',\n",
       "   'Hao Li',\n",
       "   'Ligang Liu'],\n",
       "  'tasks': ['3D Face Reconstruction', 'Face Model', 'Face Reconstruction'],\n",
       "  'date': '2017-02-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['facewarehouse'],\n",
       "  'datasets_used_full': ['FaceWarehouse'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/gear-training-a-new-way-to-implement-high',\n",
       "  'arxiv_id': '1806.03925',\n",
       "  'title': 'Gear Training: A new way to implement high-performance model-parallel training',\n",
       "  'abstract': 'The training of Deep Neural Networks usually needs tremendous computing\\nresources. Therefore many deep models are trained in large cluster instead of\\nsingle machine or GPU. Though major researchs at present try to run whole model\\non all machines by using asynchronous asynchronous stochastic gradient descent\\n(ASGD), we present a new approach to train deep model parallely -- split the\\nmodel and then seperately train different parts of it in different speed.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03925v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03925v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hao Dong', 'Shuai Li', 'Dongchang Xu', 'Yi Ren', 'Di Zhang'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/convergence-rates-for-projective-splitting',\n",
       "  'arxiv_id': '1806.03920',\n",
       "  'title': 'Convergence Rates for Projective Splitting',\n",
       "  'abstract': 'Projective splitting is a family of methods for solving inclusions involving\\nsums of maximal monotone operators. First introduced by Eckstein and Svaiter in\\n2008, these methods have enjoyed significant innovation in recent years,\\nbecoming one of the most flexible operator splitting frameworks available.\\nWhile weak convergence of the iterates to a solution has been established,\\nthere have been few attempts to study convergence rates of projective\\nsplitting. The purpose of this paper is to do so under various assumptions. To\\nthis end, there are three main contributions. First, in the context of convex\\noptimization, we establish an $O(1/k)$ ergodic function convergence rate.\\nSecond, for strongly monotone inclusions, strong convergence is established as\\nwell as an ergodic $O(1/\\\\sqrt{k})$ convergence rate for the distance of the\\niterates to the solution. Finally, for inclusions featuring strong monotonicity\\nand cocoercivity, linear convergence is established.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03920v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03920v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Patrick R. Johnstone', 'Jonathan Eckstein'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-classifiers-with-fenchel-young',\n",
       "  'arxiv_id': '1805.09717',\n",
       "  'title': 'Learning Classifiers with Fenchel-Young Losses: Generalized Entropies, Margins, and Algorithms',\n",
       "  'abstract': 'This paper studies Fenchel-Young losses, a generic way to construct convex\\nloss functions from a regularization function. We analyze their properties in\\ndepth, showing that they unify many well-known loss functions and allow to\\ncreate useful new ones easily. Fenchel-Young losses constructed from a\\ngeneralized entropy, including the Shannon and Tsallis entropies, induce\\npredictive probability distributions. We formulate conditions for a generalized\\nentropy to yield losses with a separation margin, and probability distributions\\nwith sparse support. Finally, we derive efficient algorithms, making\\nFenchel-Young losses appealing both in theory and practice.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.09717v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.09717v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mathieu Blondel', 'André F. T. Martins', 'Vlad Niculae'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-24',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/low-rank-inducing-norms-with-optimality',\n",
       "  'arxiv_id': '1612.03186',\n",
       "  'title': 'Low-Rank Inducing Norms with Optimality Interpretations',\n",
       "  'abstract': 'Optimization problems with rank constraints appear in many diverse fields\\nsuch as control, machine learning and image analysis. Since the rank constraint\\nis non-convex, these problems are often approximately solved via convex\\nrelaxations. Nuclear norm regularization is the prevailing convexifying\\ntechnique for dealing with these types of problem. This paper introduces a\\nfamily of low-rank inducing norms and regularizers which includes the nuclear\\nnorm as a special case. A posteriori guarantees on solving an underlying rank\\nconstrained optimization problem with these convex relaxations are provided. We\\nevaluate the performance of the low-rank inducing norms on three matrix\\ncompletion problems. In all examples, the nuclear norm heuristic is\\noutperformed by convex relaxations based on other low-rank inducing norms. For\\ntwo of the problems there exist low-rank inducing norms that succeed in\\nrecovering the partially unknown matrix, while the nuclear norm fails. These\\nlow-rank inducing norms are shown to be representable as semi-definite\\nprograms. Moreover, these norms have cheaply computable proximal mappings,\\nwhich makes it possible to also solve problems of large size using first-order\\nmethods.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1612.03186v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1612.03186v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Christian Grussler', 'Pontus Giselsson'],\n",
       "  'tasks': ['Matrix Completion'],\n",
       "  'date': '2016-12-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/retinal-optic-disc-segmentation-using',\n",
       "  'arxiv_id': '1806.03905',\n",
       "  'title': 'Retinal Optic Disc Segmentation using Conditional Generative Adversarial Network',\n",
       "  'abstract': 'This paper proposed a retinal image segmentation method based on conditional\\nGenerative Adversarial Network (cGAN) to segment optic disc. The proposed model\\nconsists of two successive networks: generator and discriminator. The generator\\nlearns to map information from the observing input (i.e., retinal fundus color\\nimage), to the output (i.e., binary mask). Then, the discriminator learns as a\\nloss function to train this mapping by comparing the ground-truth and the\\npredicted output with observing the input image as a condition.Experiments were\\nperformed on two publicly available dataset; DRISHTI GS1 and RIM-ONE. The\\nproposed model outperformed state-of-the-art-methods by achieving around 0.96%\\nand 0.98% of Jaccard and Dice coefficients, respectively. Moreover, an image\\nsegmentation is performed in less than a second on recent GPU.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03905v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03905v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Vivek Kumar Singh',\n",
       "   'Hatem Rashwan',\n",
       "   'Farhan Akram',\n",
       "   'Nidhi Pandey',\n",
       "   'Md. Mostaf Kamal Sarker',\n",
       "   'Adel Saleh',\n",
       "   'Saddam Abdulwahab',\n",
       "   'Najlaa Maaroof',\n",
       "   'Santiago Romani',\n",
       "   'Domenec Puig'],\n",
       "  'tasks': ['Optic Disc Segmentation', 'Semantic Segmentation'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multi-task-learning-of-daily-work-and-study',\n",
       "  'arxiv_id': '1806.03903',\n",
       "  'title': 'Multi-task learning of daily work and study round-trips from survey data',\n",
       "  'abstract': \"In this study, we present a machine learning approach to infer the worker and\\nstudent mobility flows on daily basis from static censuses. The rapid\\nurbanization has made the estimation of the human mobility flows a critical\\ntask for transportation and urban planners. The primary objective of this paper\\nis to complete individuals' census data with working and studying trips,\\nallowing its merging with other mobility data to better estimate the complete\\norigin-destination matrices. Worker and student mobility flows are among the\\nmost weekly regular displacements and consequently generate road congestion\\nproblems. Estimating their round-trips eases the decision-making processes for\\nlocal authorities. Worker and student censuses often contain home location,\\nwork places and educational institutions. We thus propose a neural network\\nmodel that learns the temporal distribution of displacements from other\\nmobility sources and tries to predict them on new censuses data. The inclusion\\nof multi-task learning in our neural network results in a significant error\\nrate control in comparison to single task learning.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03903v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03903v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mehdi Katranji',\n",
       "   'Sami Kraiem',\n",
       "   'Laurent Moalic',\n",
       "   'Guilhem Sanmarty',\n",
       "   'Alexandre Caminada',\n",
       "   'Fouad Hadj Selem'],\n",
       "  'tasks': ['Decision Making', 'Multi-Task Learning'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dual-pattern-learning-networks-by-empirical',\n",
       "  'arxiv_id': '1806.03902',\n",
       "  'title': 'Dual Pattern Learning Networks by Empirical Dual Prediction Risk Minimization',\n",
       "  'abstract': 'Motivated by the observation that humans can learn patterns from two given\\nimages at one time, we propose a dual pattern learning network architecture in\\nthis paper. Unlike conventional networks, the proposed architecture has two\\ninput branches and two loss functions. Instead of minimizing the empirical risk\\nof a given dataset, dual pattern learning networks is trained by minimizing the\\nempirical dual prediction loss. We show that this can improve the performance\\nfor single image classification. This architecture forces the network to learn\\ndiscriminative class-specific features by analyzing and comparing two input\\nimages. In addition, the dual input structure allows the network to have a\\nconsiderably large number of image pairs, which can help address the\\noverfitting issue due to limited training data. Moreover, we propose to\\nassociate each input branch with a random interest value for learning\\ncorresponding image during training. This method can be seen as a stochastic\\nregularization technique, and can further lead to generalization performance\\nimprovement. State-of-the-art deep networks can be adapted to dual pattern\\nlearning networks without increasing the same number of parameters. Extensive\\nexperiments on CIFAR-10, CIFAR- 100, FI-8, Google commands dataset, and MNIST\\ndemonstrate that our DPLNets exhibit better performance than original networks.\\nThe experimental results on subsets of CIFAR- 10, CIFAR-100, and MNIST\\ndemonstrate that dual pattern learning networks have good generalization\\nperformance on small datasets.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03902v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03902v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Haimin Zhang', 'Min Xu'],\n",
       "  'tasks': ['Image Classification'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'mnist', 'cifar-100'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'MNIST', 'CIFAR-100'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/natasha-2-faster-non-convex-optimization-than',\n",
       "  'arxiv_id': '1708.08694',\n",
       "  'title': 'Natasha 2: Faster Non-Convex Optimization Than SGD',\n",
       "  'abstract': 'We design a stochastic algorithm to train any smooth neural network to\\n$\\\\varepsilon$-approximate local minima, using $O(\\\\varepsilon^{-3.25})$\\nbackpropagations. The best result was essentially $O(\\\\varepsilon^{-4})$ by SGD.\\n  More broadly, it finds $\\\\varepsilon$-approximate local minima of any smooth\\nnonconvex function in rate $O(\\\\varepsilon^{-3.25})$, with only oracle access to\\nstochastic gradients.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1708.08694v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1708.08694v4.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Zeyuan Allen-Zhu'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-08-29',\n",
       "  'methods': [{'name': 'SGD',\n",
       "    'full_name': 'Stochastic Gradient Descent',\n",
       "    'description': '**Stochastic Gradient Descent** is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:\\r\\n\\r\\n$$ w\\\\_{t+1} = w\\\\_{t} - \\\\eta\\\\hat{\\\\nabla}\\\\_{w}{L(w\\\\_{t})} $$\\r\\n\\r\\nWhere $\\\\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.\\r\\n\\r\\n(Image Source: [here](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/))',\n",
       "    'introduced_year': 1951,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/4e0ac120e9a8b096069c2f892488d630a5c8f358/torch/optim/sgd.py#L97-L112',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/large-scale-bisample-learning-on-id-versus',\n",
       "  'arxiv_id': '1806.03018',\n",
       "  'title': 'Large-scale Bisample Learning on ID Versus Spot Face Recognition',\n",
       "  'abstract': 'In real-world face recognition applications, there is a tremendous amount of\\ndata with two images for each person. One is an ID photo for face enrollment,\\nand the other is a probe photo captured on spot. Most existing methods are\\ndesigned for training data with limited breadth (a relatively small number of\\nclasses) and sufficient depth (many samples for each class). They would meet\\ngreat challenges on ID versus Spot (IvS) data, including the under-represented\\nintra-class variations and an excessive demand on computing devices. In this\\npaper, we propose a deep learning based large-scale bisample learning (LBL)\\nmethod for IvS face recognition. To tackle the bisample problem with only two\\nsamples for each class, a classification-verification-classification (CVC)\\ntraining strategy is proposed to progressively enhance the IvS performance.\\nBesides, a dominant prototype softmax (DP-softmax) is incorporated to make the\\ndeep learning scalable on large-scale classes. We conduct LBL on a IvS face\\ndataset with more than two million identities. Experimental results show the\\nproposed method achieves superior performance to previous ones, validating the\\neffectiveness of LBL on IvS face recognition.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03018v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03018v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xiangyu Zhu',\n",
       "   'Hao liu',\n",
       "   'Zhen Lei',\n",
       "   'Hailin Shi',\n",
       "   'Fan Yang',\n",
       "   'Dong Yi',\n",
       "   'Guo-Jun Qi',\n",
       "   'Stan Z. Li'],\n",
       "  'tasks': ['Face Recognition', 'General Classification'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [{'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['casia-webface'],\n",
       "  'datasets_used_full': ['CASIA-WebFace'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multi-task-deep-networks-for-depth-based-6d',\n",
       "  'arxiv_id': '1806.03891',\n",
       "  'title': 'Multi-Task Deep Networks for Depth-Based 6D Object Pose and Joint Registration in Crowd Scenarios',\n",
       "  'abstract': 'In bin-picking scenarios, multiple instances of an object of interest are\\nstacked in a pile randomly, and hence, the instances are inherently subjected\\nto the challenges: severe occlusion, clutter, and similar-looking distractors.\\nMost existing methods are, however, for single isolated object instances, while\\nsome recent methods tackle crowd scenarios as post-refinement which accounts\\nmultiple object relations. In this paper, we address recovering 6D poses of\\nmultiple instances in bin-picking scenarios in depth modality by multi-task\\nlearning in deep neural networks. Our architecture jointly learns multiple\\nsub-tasks: 2D detection, depth, and 3D pose estimation of individual objects;\\nand joint registration of multiple objects. For training data generation, depth\\nimages of physically plausible object pose configurations are generated by a 3D\\nobject model in a physics simulation, which yields diverse occlusion patterns\\nto learn. We adopt a state-of-the-art object detector, and 2D offsets are\\nfurther estimated via a network to refine misaligned 2D detections. The depth\\nand 3D pose estimator is designed to generate multiple hypotheses per\\ndetection. This allows the joint registration network to learn occlusion\\npatterns and remove physically implausible pose hypotheses. We apply our\\narchitecture on both synthetic (our own and Sileane dataset) and real (a public\\nBin-Picking dataset) data, showing that it significantly outperforms\\nstate-of-the-art methods by 15-31% in average precision.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03891v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03891v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Juil Sock', 'Kwang In Kim', 'Caner Sahin', 'Tae-Kyun Kim'],\n",
       "  'tasks': ['3D Pose Estimation', 'Multi-Task Learning', 'Pose Estimation'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['linemod-1'],\n",
       "  'datasets_used_full': ['LINEMOD'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/first-experiments-with-neural-translation-of',\n",
       "  'arxiv_id': '1805.06502',\n",
       "  'title': 'First Experiments with Neural Translation of Informal to Formal Mathematics',\n",
       "  'abstract': \"We report on our experiments to train deep neural networks that automatically\\ntranslate informalized LaTeX-written Mizar texts into the formal Mizar\\nlanguage. To the best of our knowledge, this is the first time when neural\\nnetworks have been adopted in the formalization of mathematics. Using Luong et\\nal.'s neural machine translation model (NMT), we tested our aligned\\ninformal-formal corpora against various hyperparameters and evaluated their\\nresults. Our experiments show that our best performing model configurations are\\nable to generate correct Mizar statements on 65.73\\\\% of the inference data,\\nwith the union of all models covering 79.17\\\\%. These results indicate that\\nformalization through artificial neural network is a promising approach for\\nautomated formalization of mathematics. We present several case studies to\\nillustrate our results.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.06502v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.06502v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Qingxiang Wang', 'Cezary Kaliszyk', 'Josef Urban'],\n",
       "  'tasks': ['Machine Translation', 'Translation'],\n",
       "  'date': '2018-05-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/object-detection-using-domain-randomization',\n",
       "  'arxiv_id': '1805.11778',\n",
       "  'title': 'Object Detection using Domain Randomization and Generative Adversarial Refinement of Synthetic Images',\n",
       "  'abstract': 'In this work, we present an application of domain randomization and\\ngenerative adversarial networks (GAN) to train a near real-time object detector\\nfor industrial electric parts, entirely in a simulated environment. Large scale\\navailability of labelled real world data is typically rare and difficult to\\nobtain in many industrial settings. As such here, only a few hundred of\\nunlabelled real images are used to train a Cyclic-GAN network, in combination\\nwith various degree of domain randomization procedures. We demonstrate that\\nthis enables robust translation of synthetic images to the real world domain.\\nWe show that a combination of the original synthetic (simulation) and GAN\\ntranslated images, when used for training a Mask-RCNN object detection network\\nachieves greater than 0.95 mean average precision in detecting and classifying\\na collection of industrial electric parts. We evaluate the performance across\\ndifferent combinations of training data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.11778v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.11778v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Fernando Camaro Nogues', 'Andrew Huie', 'Sakyasingha Dasgupta'],\n",
       "  'tasks': ['Object Detection', 'Translation'],\n",
       "  'date': '2018-05-30',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/automatic-target-recovery-for-hindi-english',\n",
       "  'arxiv_id': '1806.04535',\n",
       "  'title': 'Automatic Target Recovery for Hindi-English Code Mixed Puns',\n",
       "  'abstract': 'In order for our computer systems to be more human-like, with a higher\\nemotional quotient, they need to be able to process and understand intrinsic\\nhuman language phenomena like humour. In this paper, we consider a subtype of\\nhumour - puns, which are a common type of wordplay-based jokes. In particular,\\nwe consider code-mixed puns which have become increasingly mainstream on social\\nmedia, in informal conversations and advertisements and aim to build a system\\nwhich can automatically identify the pun location and recover the target of\\nsuch puns. We first study and classify code-mixed puns into two categories\\nnamely intra-sentential and intra-word, and then propose a four-step algorithm\\nto recover the pun targets for puns belonging to the intra-sentential category.\\nOur algorithm uses language models, and phonetic similarity-based features to\\nget the desired results. We test our approach on a small set of code-mixed\\npunning advertisements, and observe that our system is successfully able to\\nrecover the targets for 67% of the puns.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04535v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04535v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Srishti Aggarwal', 'Kritik Mathur', 'Radhika Mamidi'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fast-approximate-natural-gradient-descent-in-1',\n",
       "  'arxiv_id': '1806.03884',\n",
       "  'title': 'Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis',\n",
       "  'abstract': 'Optimization algorithms that leverage gradient covariance information, such as variants of natural gradient descent (Amari, 1998), offer the prospect of yielding more effective descent directions. For models with many parameters, the covariance matrix they are based on becomes gigantic, making them inapplicable in their original form. This has motivated research into both simple diagonal approximations and more sophisticated factored approximations such as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In the present work we draw inspiration from both to propose a novel approximation that is provably better than KFAC and amendable to cheap partial updates. It consists in tracking a diagonal variance, not in parameter coordinates, but in a Kronecker-factored eigenbasis, in which the diagonal approximation is likely to be more effective. Experiments show improvements over KFAC in optimization speed for several deep network architectures.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03884v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03884v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Thomas George',\n",
       "   'César Laurent',\n",
       "   'Xavier Bouthillier',\n",
       "   'Nicolas Ballas',\n",
       "   'Pascal Vincent'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/massively-parallel-video-networks',\n",
       "  'arxiv_id': '1806.03863',\n",
       "  'title': 'Massively Parallel Video Networks',\n",
       "  'abstract': 'We introduce a class of causal video understanding models that aims to\\nimprove efficiency of video processing by maximising throughput, minimising\\nlatency, and reducing the number of clock cycles. Leveraging operation\\npipelining and multi-rate clocks, these models perform a minimal amount of\\ncomputation (e.g. as few as four convolutional layers) for each frame per\\ntimestep to produce an output. The models are still very deep, with dozens of\\nsuch operations being performed but in a pipelined fashion that enables\\ndepth-parallel computation. We illustrate the proposed principles by applying\\nthem to existing image architectures and analyse their behaviour on two video\\ntasks: action recognition and human keypoint localisation. The results show\\nthat a significant degree of parallelism, and implicitly speedup, can be\\nachieved with little loss in performance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03863v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03863v2.pdf',\n",
       "  'proceeding': 'ECCV 2018 9',\n",
       "  'authors': ['Joao Carreira',\n",
       "   'Viorica Patraucean',\n",
       "   'Laurent Mazare',\n",
       "   'Andrew Zisserman',\n",
       "   'Simon Osindero'],\n",
       "  'tasks': ['Action Recognition',\n",
       "   'Temporal Action Localization',\n",
       "   'Video Understanding'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['kinetics'],\n",
       "  'datasets_used_full': ['Kinetics'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/global-convergence-of-block-coordinate',\n",
       "  'arxiv_id': '1803.00225',\n",
       "  'title': 'Global Convergence of Block Coordinate Descent in Deep Learning',\n",
       "  'abstract': 'Deep learning has aroused extensive attention due to its great empirical success. The efficiency of the block coordinate descent (BCD) methods has been recently demonstrated in deep neural network (DNN) training. However, theoretical studies on their convergence properties are limited due to the highly nonconvex nature of DNN training. In this paper, we aim at providing a general methodology for provable convergence guarantees for this type of methods. In particular, for most of the commonly used DNN training models involving both two- and three-splitting schemes, we establish the global convergence to a critical point at a rate of ${\\\\cal O}(1/k)$, where $k$ is the number of iterations. The results extend to general loss functions which have Lipschitz continuous gradients and deep residual networks (ResNets). Our key development adds several new elements to the Kurdyka-{\\\\L}ojasiewicz inequality framework that enables us to carry out the global convergence analysis of BCD in the general scenario of deep learning.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1803.00225v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1803.00225v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jinshan Zeng', 'Tim Tsz-Kit Lau', 'Shao-Bo Lin', 'Yuan YAO'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-03-01',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-more-human-way-to-play-computer-chess',\n",
       "  'arxiv_id': '1503.04333',\n",
       "  'title': 'A More Human Way to Play Computer Chess',\n",
       "  'abstract': \"This paper suggests a forward-pruning technique for computer chess that uses\\n'Move Tables', which are like Transposition Tables, but for moves not\\npositions. They use an efficient memory structure and has put the design into\\nthe context of long and short-term memories. The long-term memory updates a\\nplay path with weight reinforcement, while the short-term memory can be\\nimmediately added or removed. With this, 'long branches' can play a short path,\\nbefore returning to a full search at the resulting leaf nodes. Re-using an\\nearlier search path allows the tree to be forward-pruned, which is known to be\\ndangerous, because it removes part of the search process. Additional checks are\\ntherefore made and moves can even be re-added when the search result is\\nunsatisfactory. Automatic feature analysis is now central to the algorithm,\\nwhere key squares and related squares can be generated automatically and used\\nto guide the search process. Using this analysis, if a search result is\\ninferior, it can re-insert un-played moves that cover these key squares only.\\nOn the tactical side, a type of move that the forward-pruning will fail on is\\nrecognised and a pattern-based solution to that problem is suggested. This has\\ncompleted the theory of an earlier paper and resulted in a more human-like\\napproach to searching for a chess move. Tests demonstrate that the obvious\\nblunders associated with forward pruning are no longer present and that it can\\ncompete at the top level with regard to playing strength.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1503.04333v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1503.04333v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kieran Greer'],\n",
       "  'tasks': [],\n",
       "  'date': '2015-03-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deepfirearm-learning-discriminative-feature',\n",
       "  'arxiv_id': '1806.02984',\n",
       "  'title': 'DeepFirearm: Learning Discriminative Feature Representation for Fine-grained Firearm Retrieval',\n",
       "  'abstract': 'There are great demands for automatically regulating inappropriate appearance\\nof shocking firearm images in social media or identifying firearm types in\\nforensics. Image retrieval techniques have great potential to solve these\\nproblems. To facilitate research in this area, we introduce Firearm 14k, a\\nlarge dataset consisting of over 14,000 images in 167 categories. It can be\\nused for both fine-grained recognition and retrieval of firearm images. Recent\\nadvances in image retrieval are mainly driven by fine-tuning state-of-the-art\\nconvolutional neural networks for retrieval task. The conventional single\\nmargin contrastive loss, known for its simplicity and good performance, has\\nbeen widely used. We find that it performs poorly on the Firearm 14k dataset\\ndue to: (1) Loss contributed by positive and negative image pairs is unbalanced\\nduring training process. (2) A huge domain gap exists between this dataset and\\nImageNet. We propose to deal with the unbalanced loss by employing a double\\nmargin contrastive loss. We tackle the domain gap issue with a two-stage\\ntraining strategy, where we first fine-tune the network for classification, and\\nthen fine-tune it for retrieval. Experimental results show that our approach\\noutperforms the conventional single margin approach by a large margin (up to\\n88.5% relative improvement) and even surpasses the strong triplet-loss-based\\napproach.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.02984v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.02984v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jiedong Hao', 'Jing Dong', 'Wei Wang', 'Tieniu Tan'],\n",
       "  'tasks': ['Image Retrieval'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-learning-for-classification-tasks-on',\n",
       "  'arxiv_id': '1806.03857',\n",
       "  'title': 'Deep Learning for Classification Tasks on Geospatial Vector Polygons',\n",
       "  'abstract': 'In this paper, we evaluate the accuracy of deep learning approaches on geospatial vector geometry classification tasks. The purpose of this evaluation is to investigate the ability of deep learning models to learn from geometry coordinates directly. Previous machine learning research applied to geospatial polygon data did not use geometries directly, but derived properties thereof. These are produced by way of extracting geometry properties such as Fourier descriptors. Instead, our introduced deep neural net architectures are able to learn on sequences of coordinates mapped directly from polygons. In three classification tasks we show that the deep learning architectures are competitive with common learning algorithms that require extracted features.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03857v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03857v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': [\"Rein van 't Veer\", 'Peter Bloem', 'Erwin Folmer'],\n",
       "  'tasks': ['Classification', 'General Classification'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/data-augmentation-instead-of-explicit',\n",
       "  'arxiv_id': '1806.03852',\n",
       "  'title': 'Data augmentation instead of explicit regularization',\n",
       "  'abstract': 'Contrary to most machine learning models, modern deep artificial neural networks typically include multiple components that contribute to regularization. Despite the fact that some (explicit) regularization techniques, such as weight decay and dropout, require costly fine-tuning of sensitive hyperparameters, the interplay between them and other elements that provide implicit regularization is not well understood yet. Shedding light upon these interactions is key to efficiently using computational resources and may contribute to solving the puzzle of generalization in deep learning. Here, we first provide formal definitions of explicit and implicit regularization that help understand essential differences between techniques. Second, we contrast data augmentation with weight decay and dropout. Our results show that visual object categorization models trained with data augmentation alone achieve the same performance or higher than models trained also with weight decay and dropout, as is common practice. We conclude that the contribution on generalization of weight decay and dropout is not only superfluous when sufficient implicit regularization is provided, but also such techniques can dramatically deteriorate the performance if the hyperparameters are not carefully tuned for the architecture and data set. In contrast, data augmentation systematically provides large generalization gains and does not require hyperparameter re-tuning. In view of our results, we suggest to optimize neural networks without weight decay and dropout to save computational resources, hence carbon emissions, and focus more on data augmentation and other inductive biases to improve performance and robustness.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03852v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03852v5.pdf',\n",
       "  'proceeding': 'ICLR 2018 1',\n",
       "  'authors': ['Alex Hernández-García', 'Peter König'],\n",
       "  'tasks': ['Data Augmentation'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [{'name': 'Weight Decay',\n",
       "    'full_name': 'Weight Decay',\n",
       "    'description': '**Weight Decay**, or **$L_{2}$ Regularization**, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L\\\\_{2}$ Norm of the weights:\\r\\n\\r\\n$$L\\\\_{new}\\\\left(w\\\\right) = L\\\\_{original}\\\\left(w\\\\right) + \\\\lambda{w^{T}w}$$\\r\\n\\r\\nwhere $\\\\lambda$ is a value determining the strength of the penalty (encouraging smaller weights). \\r\\n\\r\\nWeight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).\\r\\n\\r\\nImage Source: Deep Learning, Goodfellow et al',\n",
       "    'introduced_year': 1943,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['cifar-10', 'cifar-100'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'CIFAR-100'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/synthetic-perfusion-maps-imaging-perfusion',\n",
       "  'arxiv_id': '1806.03848',\n",
       "  'title': 'Synthetic Perfusion Maps: Imaging Perfusion Deficits in DSC-MRI with Deep Learning',\n",
       "  'abstract': 'In this work, we present a novel convolutional neural net- work based method\\nfor perfusion map generation in dynamic suscepti- bility contrast-enhanced\\nperfusion imaging. The proposed architecture is trained end-to-end and solely\\nrelies on raw perfusion data for inference. We used a dataset of 151 acute\\nischemic stroke cases for evaluation. Our method generates perfusion maps that\\nare comparable to the target maps used for clinical routine, while being\\nmodel-free, fast, and less noisy.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03848v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03848v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Andreas Hess',\n",
       "   'Raphael Meier',\n",
       "   'Johannes Kaesmacher',\n",
       "   'Simon Jung',\n",
       "   'Fabien Scalzo',\n",
       "   'David Liebeskind',\n",
       "   'Roland Wiest',\n",
       "   'Richard McKinley'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-multimodal-classifier-generative',\n",
       "  'arxiv_id': '1806.03847',\n",
       "  'title': 'A Multimodal Classifier Generative Adversarial Network for Carry and Place Tasks from Ambiguous Language Instructions',\n",
       "  'abstract': 'This paper focuses on a multimodal language understanding method for\\ncarry-and-place tasks with domestic service robots. We address the case of\\nambiguous instructions, that is, when the target area is not specified. For\\ninstance \"put away the milk and cereal\" is a natural instruction where there is\\nambiguity regarding the target area, considering environments in daily life.\\nConventionally, this instruction can be disambiguated from a dialogue system,\\nbut at the cost of time and cumbersome interaction. Instead, we propose a\\nmultimodal approach, in which the instructions are disambiguated using the\\nrobot\\'s state and environment context. We develop the Multi-Modal Classifier\\nGenerative Adversarial Network (MMC-GAN) to predict the likelihood of different\\ntarget areas considering the robot\\'s physical limitation and the target\\nclutter. Our approach, MMC-GAN, significantly improves accuracy compared with\\nbaseline methods that use instructions only or simple deep neural networks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03847v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03847v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Aly Magassouba', 'Komei Sugiura', 'Hisashi Kawai'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/intriguing-properties-of-learned',\n",
       "  'arxiv_id': '1804.07090',\n",
       "  'title': 'Robustness via Deep Low-Rank Representations',\n",
       "  'abstract': 'We investigate the effect of the dimensionality of the representations learned in Deep Neural Networks (DNNs) on their robustness to input perturbations, both adversarial and random. To achieve low dimensionality of learned representations, we propose an easy-to-use, end-to-end trainable, low-rank regularizer (LR) that can be applied to any intermediate layer representation of a DNN. This regularizer forces the feature representations to (mostly) lie in a low-dimensional linear subspace. We perform a wide range of experiments that demonstrate that the LR indeed induces low rank on the representations, while providing modest improvements to accuracy as an added benefit. Furthermore, the learned features make the trained model significantly more robust to input perturbations such as Gaussian and adversarial noise (even without adversarial training). Lastly, the low-dimensionality means that the learned features are highly compressible; thus discriminative features of the data can be stored using very little memory. Our experiments indicate that models trained using the LR learn robust classifiers by discovering subspaces that avoid non-robust features. Algorithmically, the LR is scalable, generic, and straightforward to implement into existing deep learning frameworks.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1804.07090v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1804.07090v5.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Amartya Sanyal',\n",
       "   'Varun Kanade',\n",
       "   'Philip H. S. Torr',\n",
       "   'Puneet K. Dokania'],\n",
       "  'tasks': ['General Classification',\n",
       "   'Image Classification',\n",
       "   'Transfer Learning'],\n",
       "  'date': '2018-04-19',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multi-document-summarization-using',\n",
       "  'arxiv_id': '1710.02745',\n",
       "  'title': 'Multi-Document Summarization using Distributed Bag-of-Words Model',\n",
       "  'abstract': 'As the number of documents on the web is growing exponentially,\\nmulti-document summarization is becoming more and more important since it can\\nprovide the main ideas in a document set in short time. In this paper, we\\npresent an unsupervised centroid-based document-level reconstruction framework\\nusing distributed bag of words model. Specifically, our approach selects\\nsummary sentences in order to minimize the reconstruction error between the\\nsummary and the documents. We apply sentence selection and beam search, to\\nfurther improve the performance of our model. Experimental results on two\\ndifferent datasets show significant performance gains compared with the\\nstate-of-the-art baselines.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1710.02745v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1710.02745v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kaustubh Mani', 'Ishan Verma', 'Hardik Meisheri', 'Lipika Dey'],\n",
       "  'tasks': ['Document Summarization', 'Multi-Document Summarization'],\n",
       "  'date': '2017-10-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dmcnn-dual-domain-multi-scale-convolutional',\n",
       "  'arxiv_id': '1806.03275',\n",
       "  'title': 'DMCNN: Dual-Domain Multi-Scale Convolutional Neural Network for Compression Artifacts Removal',\n",
       "  'abstract': 'JPEG is one of the most commonly used standards among lossy image compression\\nmethods. However, JPEG compression inevitably introduces various kinds of\\nartifacts, especially at high compression rates, which could greatly affect the\\nQuality of Experience (QoE). Recently, convolutional neural network (CNN) based\\nmethods have shown excellent performance for removing the JPEG artifacts. Lots\\nof efforts have been made to deepen the CNNs and extract deeper features, while\\nrelatively few works pay attention to the receptive field of the network. In\\nthis paper, we illustrate that the quality of output images can be\\nsignificantly improved by enlarging the receptive fields in many cases. One\\nstep further, we propose a Dual-domain Multi-scale CNN (DMCNN) to take full\\nadvantage of redundancies on both the pixel and DCT domains. Experiments show\\nthat DMCNN sets a new state-of-the-art for the task of JPEG artifact removal.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03275v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03275v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xiaoshuai Zhang', 'Wenhan Yang', 'Yueyu Hu', 'Jiaying Liu'],\n",
       "  'tasks': ['Image Compression',\n",
       "   'JPEG Artifact Correction',\n",
       "   'JPEG Artifact Removal'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['icb'],\n",
       "  'datasets_used_full': ['ICB'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/bayesian-model-agnostic-meta-learning',\n",
       "  'arxiv_id': '1806.03836',\n",
       "  'title': 'Bayesian Model-Agnostic Meta-Learning',\n",
       "  'abstract': 'Learning to infer Bayesian posterior from a few-shot dataset is an important\\nstep towards robust meta-learning due to the model uncertainty inherent in the\\nproblem. In this paper, we propose a novel Bayesian model-agnostic\\nmeta-learning method. The proposed method combines scalable gradient-based\\nmeta-learning with nonparametric variational inference in a principled\\nprobabilistic framework. During fast adaptation, the method is capable of\\nlearning complex uncertainty structure beyond a point estimate or a simple\\nGaussian approximation. In addition, a robust Bayesian meta-update mechanism\\nwith a new meta-loss prevents overfitting during meta-update. Remaining an\\nefficient gradient-based meta-learner, the method is also model-agnostic and\\nsimple to implement. Experiment results show the accuracy and robustness of the\\nproposed method in various tasks: sinusoidal regression, image classification,\\nactive learning, and reinforcement learning.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03836v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03836v4.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Taesup Kim',\n",
       "   'Jaesik Yoon',\n",
       "   'Ousmane Dia',\n",
       "   'Sungwoong Kim',\n",
       "   'Yoshua Bengio',\n",
       "   'Sungjin Ahn'],\n",
       "  'tasks': ['Active Learning',\n",
       "   'Image Classification',\n",
       "   'Meta-Learning',\n",
       "   'Variational Inference'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['miniimagenet-1'],\n",
       "  'datasets_used_full': ['miniImageNet'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/interactive-visual-grounding-of-referring',\n",
       "  'arxiv_id': '1806.03831',\n",
       "  'title': 'Interactive Visual Grounding of Referring Expressions for Human-Robot Interaction',\n",
       "  'abstract': 'This paper presents INGRESS, a robot system that follows human natural\\nlanguage instructions to pick and place everyday objects. The core issue here\\nis the grounding of referring expressions: infer objects and their\\nrelationships from input images and language expressions. INGRESS allows for\\nunconstrained object categories and unconstrained language expressions.\\nFurther, it asks questions to disambiguate referring expressions interactively.\\nTo achieve these, we take the approach of grounding by generation and propose a\\ntwo-stage neural network model for grounding. The first stage uses a neural\\nnetwork to generate visual descriptions of objects, compares them with the\\ninput language expression, and identifies a set of candidate objects. The\\nsecond stage uses another neural network to examine all pairwise relations\\nbetween the candidates and infers the most likely referred object. The same\\nneural networks are used for both grounding and question generation for\\ndisambiguation. Experiments show that INGRESS outperformed a state-of-the-art\\nmethod on the RefCOCO dataset and in robot experiments with humans.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03831v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03831v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mohit Shridhar', 'David Hsu'],\n",
       "  'tasks': ['Question Generation', 'Visual Grounding'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['visual-genome'],\n",
       "  'datasets_used_full': ['Visual Genome'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/analysis-and-design-of-convolutional-networks',\n",
       "  'arxiv_id': '1705.02302',\n",
       "  'title': 'Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions',\n",
       "  'abstract': 'The driving force behind convolutional networks - the most successful deep\\nlearning architecture to date, is their expressive power. Despite its wide\\nacceptance and vast empirical evidence, formal analyses supporting this belief\\nare scarce. The primary notions for formally reasoning about expressiveness are\\nefficiency and inductive bias. Expressive efficiency refers to the ability of a\\nnetwork architecture to realize functions that require an alternative\\narchitecture to be much larger. Inductive bias refers to the prioritization of\\nsome functions over others given prior knowledge regarding a task at hand. In\\nthis paper we overview a series of works written by the authors, that through\\nan equivalence to hierarchical tensor decompositions, analyze the expressive\\nefficiency and inductive bias of various convolutional network architectural\\nfeatures (depth, width, strides and more). The results presented shed light on\\nthe demonstrated effectiveness of convolutional networks, and in addition,\\nprovide new tools for network design.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1705.02302v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1705.02302v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Nadav Cohen',\n",
       "   'Or Sharir',\n",
       "   'Yoav Levine',\n",
       "   'Ronen Tamari',\n",
       "   'David Yakira',\n",
       "   'Amnon Shashua'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-05-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/autofocus-layer-for-semantic-segmentation',\n",
       "  'arxiv_id': '1805.08403',\n",
       "  'title': 'Autofocus Layer for Semantic Segmentation',\n",
       "  'abstract': \"We propose the autofocus convolutional layer for semantic segmentation with\\nthe objective of enhancing the capabilities of neural networks for multi-scale\\nprocessing. Autofocus layers adaptively change the size of the effective\\nreceptive field based on the processed context to generate more powerful\\nfeatures. This is achieved by parallelising multiple convolutional layers with\\ndifferent dilation rates, combined by an attention mechanism that learns to\\nfocus on the optimal scales driven by context. By sharing the weights of the\\nparallel convolutions we make the network scale-invariant, with only a modest\\nincrease in the number of parameters. The proposed autofocus layer can be\\neasily integrated into existing networks to improve a model's representational\\npower. We evaluate our models on the challenging tasks of multi-organ\\nsegmentation in pelvic CT and brain tumor segmentation in MRI and achieve very\\npromising performance.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.08403v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.08403v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yao Qin',\n",
       "   'Konstantinos Kamnitsas',\n",
       "   'Siddharth Ancha',\n",
       "   'Jay Nanavati',\n",
       "   'Garrison Cottrell',\n",
       "   'Antonio Criminisi',\n",
       "   'Aditya Nori'],\n",
       "  'tasks': ['Brain Tumor Segmentation',\n",
       "   'Medical Image Segmentation',\n",
       "   'Semantic Segmentation',\n",
       "   'Tumor Segmentation'],\n",
       "  'date': '2018-05-22',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['brats-2015-1'],\n",
       "  'datasets_used_full': ['BraTS 2015'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-the-optimization-of-deep-networks-implicit',\n",
       "  'arxiv_id': '1802.06509',\n",
       "  'title': 'On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization',\n",
       "  'abstract': 'Conventional wisdom in deep learning states that increasing depth improves\\nexpressiveness but complicates optimization. This paper suggests that,\\nsometimes, increasing depth can speed up optimization. The effect of depth on\\noptimization is decoupled from expressiveness by focusing on settings where\\nadditional layers amount to overparameterization - linear neural networks, a\\nwell-studied model. Theoretical analysis, as well as experiments, show that\\nhere depth acts as a preconditioner which may accelerate convergence. Even on\\nsimple convex problems such as linear regression with $\\\\ell_p$ loss, $p>2$,\\ngradient descent can benefit from transitioning to a non-convex\\noverparameterized objective, more than it would from some common acceleration\\nschemes. We also prove that it is mathematically impossible to obtain the\\nacceleration effect of overparametrization via gradients of any regularizer.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.06509v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.06509v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Sanjeev Arora', 'Nadav Cohen', 'Elad Hazan'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-19',\n",
       "  'methods': [{'name': 'Linear Regression',\n",
       "    'full_name': 'Linear Regression',\n",
       "    'description': '**Linear Regression** is a method for modelling a relationship between a dependent variable and independent variables. These models can be fit with numerous approaches. The most common is *least squares*, where we minimize the mean square error between the predicted values $\\\\hat{y} = \\\\textbf{X}\\\\hat{\\\\beta}$ and actual values $y$: $\\\\left(y-\\\\textbf{X}\\\\beta\\\\right)^{2}$.\\r\\n\\r\\nWe can also define the problem in probabilistic terms as a generalized linear model (GLM) where the pdf is a Gaussian distribution, and then perform maximum likelihood estimation to estimate $\\\\hat{\\\\beta}$.\\r\\n\\r\\nImage Source: [Wikipedia](https://en.wikipedia.org/wiki/Linear_regression)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Generalized Linear Models',\n",
       "     'description': '**Generalized Linear Models (GLMs)** are a class of models that generalize upon linear regression by allowing many more distributions to be modeled for the response variable via a link function. Below you can find a continuously updating list of GLMs.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/know-what-you-dont-know-unanswerable',\n",
       "  'arxiv_id': '1806.03822',\n",
       "  'title': \"Know What You Don't Know: Unanswerable Questions for SQuAD\",\n",
       "  'abstract': 'Extractive reading comprehension systems can often locate the correct answer\\nto a question in a context document, but they also tend to make unreliable\\nguesses on questions for which the correct answer is not stated in the context.\\nExisting datasets either focus exclusively on answerable questions, or use\\nautomatically generated unanswerable questions that are easy to identify. To\\naddress these weaknesses, we present SQuAD 2.0, the latest version of the\\nStanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD\\ndata with over 50,000 unanswerable questions written adversarially by\\ncrowdworkers to look similar to answerable ones. To do well on SQuAD 2.0,\\nsystems must not only answer questions when possible, but also determine when\\nno answer is supported by the paragraph and abstain from answering. SQuAD 2.0\\nis a challenging natural language understanding task for existing models: a\\nstrong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on\\nSQuAD 2.0.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03822v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03822v1.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Pranav Rajpurkar', 'Robin Jia', 'Percy Liang'],\n",
       "  'tasks': ['Natural Language Understanding',\n",
       "   'Question Answering',\n",
       "   'Reading Comprehension'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['squad'],\n",
       "  'datasets_used_full': ['SQuAD'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/addition-of-code-mixed-features-to-enhance',\n",
       "  'arxiv_id': '1806.03821',\n",
       "  'title': 'Addition of Code Mixed Features to Enhance the Sentiment Prediction of Song Lyrics',\n",
       "  'abstract': \"Sentiment analysis, also called opinion mining, is the field of study that\\nanalyzes people's opinions,sentiments, attitudes and emotions. Songs are\\nimportant to sentiment analysis since the songs and mood are mutually dependent\\non each other. Based on the selected song it becomes easy to find the mood of\\nthe listener, in future it can be used for recommendation. The song lyric is a\\nrich source of datasets containing words that are helpful in analysis and\\nclassification of sentiments generated from it. Now a days we observe a lot of\\ninter-sentential and intra-sentential code-mixing in songs which has a varying\\nimpact on audience. To study this impact we created a Telugu songs dataset\\nwhich contained both Telugu-English code-mixed and pure Telugu songs. In this\\npaper, we classify the songs based on its arousal as exciting or non-exciting.\\nWe develop a language identification tool and introduce code-mixing features\\nobtained from it as additional features. Our system with these additional\\nfeatures attains 4-5% accuracy greater than traditional approaches on our\\ndataset.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03821v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03821v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Gangula Rama Rohit Reddy', 'Radhika Mamidi'],\n",
       "  'tasks': ['Language Identification', 'Opinion Mining', 'Sentiment Analysis'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/an-efficient-generalized-bellman-update-for',\n",
       "  'arxiv_id': '1806.03820',\n",
       "  'title': 'An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning',\n",
       "  'abstract': \"Our goal is for AI systems to correctly identify and act according to their\\nhuman user's objectives. Cooperative Inverse Reinforcement Learning (CIRL)\\nformalizes this value alignment problem as a two-player game between a human\\nand robot, in which only the human knows the parameters of the reward function:\\nthe robot needs to learn them as the interaction unfolds. Previous work showed\\nthat CIRL can be solved as a POMDP, but with an action space size exponential\\nin the size of the reward parameter space. In this work, we exploit a specific\\nproperty of CIRL---the human is a full information agent---to derive an\\noptimality-preserving modification to the standard Bellman update; this reduces\\nthe complexity of the problem by an exponential factor and allows us to relax\\nCIRL's assumption of human rationality. We apply this update to a variety of\\nPOMDP solvers and find that it enables us to scale CIRL to non-trivial\\nproblems, with larger reward parameter spaces, and larger action spaces for\\nboth robot and human. In solutions to these larger problems, the human exhibits\\npedagogic (teaching) behavior, while the robot interprets it as such and\\nattains higher value for the human.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03820v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03820v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Dhruv Malik',\n",
       "   'Malayandi Palaniappan',\n",
       "   'Jaime F. Fisac',\n",
       "   'Dylan Hadfield-Menell',\n",
       "   'Stuart Russell',\n",
       "   'Anca D. Dragan'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/adaptive-mcmc-via-combining-local-samplers',\n",
       "  'arxiv_id': '1806.03816',\n",
       "  'title': 'Adaptive MCMC via Combining Local Samplers',\n",
       "  'abstract': 'Markov chain Monte Carlo (MCMC) methods are widely used in machine learning. One of the major problems with MCMC is the question of how to design chains that mix fast over the whole state space; in particular, how to select the parameters of an MCMC algorithm. Here we take a different approach and, similarly to parallel MCMC methods, instead of trying to find a single chain that samples from the whole distribution, we combine samples from several chains run in parallel, each exploring only parts of the state space (e.g., a few modes only). The chains are prioritized based on kernel Stein discrepancy, which provides a good measure of performance locally. The samples from the independent chains are combined using a novel technique for estimating the probability of different regions of the sample space. Experimental results demonstrate that the proposed algorithm may provide significant speedups in different sampling problems. Most importantly, when combined with the state-of-the-art NUTS algorithm as the base MCMC sampler, our method remained competitive with NUTS on sampling from unimodal distributions, while significantly outperforming state-of-the-art competitors on synthetic multimodal problems as well as on a challenging sensor localization task.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03816v6',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03816v6.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kiarash Shaloudegi', 'András György'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/compression-of-phase-only-holograms-with-jpeg',\n",
       "  'arxiv_id': '1806.03811',\n",
       "  'title': 'Compression of phase-only holograms with JPEG standard and deep learning',\n",
       "  'abstract': 'It is a critical issue to reduce the enormous amount of data in the\\nprocessing, storage and transmission of a hologram in digital format. In\\nphotograph compression, the JPEG standard is commonly supported by almost every\\nsystem and device. It will be favorable if JPEG standard is applicable to\\nhologram compression, with advantages of universal compatibility. However, the\\nreconstructed image from a JPEG compressed hologram suffers from severe quality\\ndegradation since some high frequency features in the hologram will be lost\\nduring the compression process. In this work, we employ a deep convolutional\\nneural network to reduce the artifacts in a JPEG compressed hologram.\\nSimulation and experimental results reveal that our proposed \"JPEG + deep\\nlearning\" hologram compression scheme can achieve satisfactory reconstruction\\nresults for a computer-generated phase-only hologram after compression.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03811v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03811v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shuming Jiao',\n",
       "   'Zhi Jin',\n",
       "   'Chenliang Chang',\n",
       "   'Changyuan Zhou',\n",
       "   'Wenbin Zou',\n",
       "   'Xia Li'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/greybox-fuzzing-as-a-contextual-bandits',\n",
       "  'arxiv_id': '1806.03806',\n",
       "  'title': 'Greybox fuzzing as a contextual bandits problem',\n",
       "  'abstract': \"Greybox fuzzing is one of the most useful and effective techniques for the\\nbug detection in large scale application programs. It uses minimal amount of\\ninstrumentation. American Fuzzy Lop (AFL) is a popular coverage based\\nevolutionary greybox fuzzing tool. AFL performs extremely well in fuzz testing\\nlarge applications and finding critical vulnerabilities, but AFL involves a lot\\nof heuristics while deciding the favored test case(s), skipping test cases\\nduring fuzzing, assigning fuzzing iterations to test case(s). In this work, we\\naim at replacing the heuristics the AFL uses while assigning the fuzzing\\niterations to a test case during the random fuzzing. We formalize this problem\\nas a `contextual bandit problem' and we propose an algorithm to solve this\\nproblem. We have implemented our approach on top of the AFL. We modify the\\nAFL's heuristics with our learned model through the policy gradient method. Our\\nlearning algorithm selects the multiplier of the number of fuzzing iterations\\nto be assigned to a test case during random fuzzing, given a fixed length\\nsubstring of the test case to be fuzzed. We fuzz the substring with this new\\nenergy value and continuously updates the policy based upon the interesting\\ntest cases it produces on fuzzing.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03806v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03806v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ketan Patil', 'Aditya Kanade'],\n",
       "  'tasks': ['Multi-Armed Bandits'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/chaining-mutual-information-and-tightening',\n",
       "  'arxiv_id': '1806.03803',\n",
       "  'title': 'Chaining Mutual Information and Tightening Generalization Bounds',\n",
       "  'abstract': \"Bounding the generalization error of learning algorithms has a long history, which yet falls short in explaining various generalization successes including those of deep learning. Two important difficulties are (i) exploiting the dependencies between the hypotheses, (ii) exploiting the dependence between the algorithm's input and output. Progress on the first point was made with the chaining method, originating from the work of Kolmogorov, and used in the VC-dimension bound. More recently, progress on the second point was made with the mutual information method by Russo and Zou '15. Yet, these two methods are currently disjoint. In this paper, we introduce a technique to combine the chaining and mutual information methods, to obtain a generalization bound that is both algorithm-dependent and that exploits the dependencies between the hypotheses. We provide an example in which our bound significantly outperforms both the chaining and the mutual information bounds. As a corollary, we tighten Dudley's inequality when the learning algorithm chooses its output from a small subset of hypotheses with high probability.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03803v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03803v2.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Amir R. Asadi', 'Emmanuel Abbe', 'Sergio Verdú'],\n",
       "  'tasks': ['Generalization Bounds'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/eve-a-gradient-based-optimization-method-with',\n",
       "  'arxiv_id': '1611.01505',\n",
       "  'title': 'Eve: A Gradient Based Optimization Method with Locally and Globally Adaptive Learning Rates',\n",
       "  'abstract': 'Adaptive gradient methods for stochastic optimization adjust the learning\\nrate for each parameter locally. However, there is also a global learning rate\\nwhich must be tuned in order to get the best performance. In this paper, we\\npresent a new algorithm that adapts the learning rate locally for each\\nparameter separately, and also globally for all parameters together.\\nSpecifically, we modify Adam, a popular method for training deep learning\\nmodels, with a coefficient that captures properties of the objective function.\\nEmpirically, we show that our method, which we call Eve, outperforms Adam and\\nother popular methods in training deep neural networks, like convolutional\\nneural networks for image classification, and recurrent neural networks for\\nlanguage tasks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1611.01505v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1611.01505v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hiroaki Hayashi', 'Jayanth Koushik', 'Graham Neubig'],\n",
       "  'tasks': ['General Classification',\n",
       "   'Image Classification',\n",
       "   'Stochastic Optimization'],\n",
       "  'date': '2016-11-04',\n",
       "  'methods': [{'name': 'Adam',\n",
       "    'full_name': 'Adam',\n",
       "    'description': '**Adam** is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of [RMSProp](https://paperswithcode.com/method/rmsprop) and [SGD w/th Momentum](https://paperswithcode.com/method/sgd-with-momentum). The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. \\r\\n\\r\\nThe weight updates are performed as:\\r\\n\\r\\n$$ w_{t} = w_{t-1} - \\\\eta\\\\frac{\\\\hat{m}\\\\_{t}}{\\\\sqrt{\\\\hat{v}\\\\_{t}} + \\\\epsilon}  $$\\r\\n\\r\\nwith\\r\\n\\r\\n$$ \\\\hat{m}\\\\_{t} = \\\\frac{m_{t}}{1-\\\\beta^{t}_{1}} $$\\r\\n\\r\\n$$ \\\\hat{v}\\\\_{t} = \\\\frac{v_{t}}{1-\\\\beta^{t}_{2}} $$\\r\\n\\r\\n$$ m_{t} = \\\\beta_{1}m_{t-1} + (1-\\\\beta_{1})g_{t} $$\\r\\n\\r\\n$$ v_{t} = \\\\beta_{2}v_{t-1} + (1-\\\\beta_{2})g_{t}^{2}  $$\\r\\n\\r\\n\\r\\n$ \\\\eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \\\\epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \\\\beta_{1} $ and $ \\\\beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1412.6980v9',\n",
       "    'source_title': 'Adam: A Method for Stochastic Optimization',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['cifar-100', 'penn-treebank'],\n",
       "  'datasets_used_full': ['CIFAR-100', 'Penn Treebank'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/generative-adversarial-network-architectures',\n",
       "  'arxiv_id': '1806.03796',\n",
       "  'title': 'Generative Adversarial Network Architectures For Image Synthesis Using Capsule Networks',\n",
       "  'abstract': \"In this paper, we propose Generative Adversarial Network (GAN) architectures\\nthat use Capsule Networks for image-synthesis. Based on the principal of\\npositional-equivariance of features, Capsule Network's ability to encode\\nspatial relationships between the features of the image helps it become a more\\npowerful critic in comparison to Convolutional Neural Networks (CNNs) used in\\ncurrent architectures for image synthesis. Our proposed GAN architectures learn\\nthe data manifold much faster and therefore, synthesize visually accurate\\nimages in significantly lesser number of training samples and training epochs\\nin comparison to GANs and its variants that use CNNs. Apart from analyzing the\\nquantitative results corresponding the images generated by different\\narchitectures, we also explore the reasons for the lower coverage and diversity\\nexplored by the GAN architectures that use CNN critics.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03796v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03796v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yash Upadhyay', 'Paul Schrater'],\n",
       "  'tasks': ['Image Generation'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['fashion-mnist'],\n",
       "  'datasets_used_full': ['Fashion-MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/cross-dataset-person-re-identification-using',\n",
       "  'arxiv_id': '1806.04533',\n",
       "  'title': 'Cross-dataset Person Re-Identification Using Similarity Preserved Generative Adversarial Networks',\n",
       "  'abstract': 'Person re-identification (Re-ID) aims to match the image frames which contain\\nthe same person in the surveillance videos. Most of the Re-ID algorithms\\nconduct supervised training in some small labeled datasets, so directly\\ndeploying these trained models to the real-world large camera networks may lead\\nto a poor performance due to underfitting. The significant difference between\\nthe source training dataset and the target testing dataset makes it challenging\\nto incrementally optimize the model. To address this challenge, we propose a\\nnovel solution by transforming the unlabeled images in the target domain to fit\\nthe original classifier by using our proposed similarity preserved generative\\nadversarial networks model, SimPGAN. Specifically, SimPGAN adopts the\\ngenerative adversarial networks with the cycle consistency constraint to\\ntransform the unlabeled images in the target domain to the style of the source\\ndomain. Meanwhile, SimPGAN uses the similarity consistency loss, which is\\nmeasured by a siamese deep convolutional neural network, to preserve the\\nsimilarity of the transformed images of the same person. Comprehensive\\nexperiments based on multiple real surveillance datasets are conducted, and the\\nresults show that our algorithm is better than the state-of-the-art\\ncross-dataset unsupervised person Re-ID algorithms.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04533v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04533v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jianming Lv', 'Xintong Wang'],\n",
       "  'tasks': ['Person Re-Identification'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['market-1501'],\n",
       "  'datasets_used_full': ['Market-1501'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-effect-of-network-width-on-the',\n",
       "  'arxiv_id': '1806.03791',\n",
       "  'title': 'The Effect of Network Width on the Performance of Large-batch Training',\n",
       "  'abstract': 'Distributed implementations of mini-batch stochastic gradient descent (SGD)\\nsuffer from communication overheads, attributed to the high frequency of\\ngradient updates inherent in small-batch training. Training with large batches\\ncan reduce these overheads; however, large batches can affect the convergence\\nproperties and generalization performance of SGD. In this work, we take a first\\nstep towards analyzing how the structure (width and depth) of a neural network\\naffects the performance of large-batch training. We present new theoretical\\nresults which suggest that--for a fixed number of parameters--wider networks\\nare more amenable to fast large-batch training compared to deeper ones. We\\nprovide extensive experiments on residual and fully-connected neural networks\\nwhich suggest that wider networks can be trained using larger batches without\\nincurring a convergence slow-down, unlike their deeper variants.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03791v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03791v1.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Lingjiao Chen',\n",
       "   'Hongyi Wang',\n",
       "   'Jinman Zhao',\n",
       "   'Dimitris Papailiopoulos',\n",
       "   'Paraschos Koutris'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [{'name': 'SGD',\n",
       "    'full_name': 'Stochastic Gradient Descent',\n",
       "    'description': '**Stochastic Gradient Descent** is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:\\r\\n\\r\\n$$ w\\\\_{t+1} = w\\\\_{t} - \\\\eta\\\\hat{\\\\nabla}\\\\_{w}{L(w\\\\_{t})} $$\\r\\n\\r\\nWhere $\\\\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.\\r\\n\\r\\n(Image Source: [here](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/))',\n",
       "    'introduced_year': 1951,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/4e0ac120e9a8b096069c2f892488d630a5c8f358/torch/optim/sgd.py#L97-L112',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['cifar-10', 'mnist', 'emnist'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'MNIST', 'EMNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dureader-a-chinese-machine-reading',\n",
       "  'arxiv_id': '1711.05073',\n",
       "  'title': 'DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications',\n",
       "  'abstract': 'This paper introduces DuReader, a new large-scale, open-domain Chinese ma-\\nchine reading comprehension (MRC) dataset, designed to address real-world MRC.\\nDuReader has three advantages over previous MRC datasets: (1) data sources:\\nquestions and documents are based on Baidu Search and Baidu Zhidao; answers are\\nmanually generated. (2) question types: it provides rich annotations for more\\nquestion types, especially yes-no and opinion questions, that leaves more\\nopportunity for the research community. (3) scale: it contains 200K questions,\\n420K answers and 1M documents; it is the largest Chinese MRC dataset so far.\\nExperiments show that human performance is well above current state-of-the-art\\nbaseline systems, leaving plenty of room for the community to make\\nimprovements. To help the community make these improvements, both DuReader and\\nbaseline systems have been posted online. We also organize a shared competition\\nto encourage the exploration of more models. Since the release of the task,\\nthere are significant improvements over the baselines.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.05073v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.05073v4.pdf',\n",
       "  'proceeding': 'WS 2018 7',\n",
       "  'authors': ['Wei He',\n",
       "   'Kai Liu',\n",
       "   'Jing Liu',\n",
       "   'Yajuan Lyu',\n",
       "   'Shiqi Zhao',\n",
       "   'Xinyan Xiao',\n",
       "   'Yu-An Liu',\n",
       "   'Yizhong Wang',\n",
       "   'Hua Wu',\n",
       "   'Qiaoqiao She',\n",
       "   'Xuan Liu',\n",
       "   'Tian Wu',\n",
       "   'Haifeng Wang'],\n",
       "  'tasks': ['Machine Reading Comprehension', 'Reading Comprehension'],\n",
       "  'date': '2017-11-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['squad', 'ms-marco', 'triviaqa', 'newsqa'],\n",
       "  'datasets_used_full': ['SQuAD', 'MS MARCO', 'TriviaQA', 'NewsQA'],\n",
       "  'datasets_introduced_lower': ['dureader'],\n",
       "  'datasets_introduced_full': ['DuReader']},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/assumed-density-filtering-q-learning',\n",
       "  'arxiv_id': '1712.03333',\n",
       "  'title': 'Assumed Density Filtering Q-learning',\n",
       "  'abstract': 'While off-policy temporal difference (TD) methods have widely been used in reinforcement learning due to their efficiency and simple implementation, their Bayesian counterparts have not been utilized as frequently. One reason is that the non-linear max operation in the Bellman optimality equation makes it difficult to define conjugate distributions over the value functions. In this paper, we introduce a novel Bayesian approach to off-policy TD methods, called as ADFQ, which updates beliefs on state-action values, Q, through an online Bayesian inference method known as Assumed Density Filtering. We formulate an efficient closed-form solution for the value update by approximately estimating analytic parameters of the posterior of the Q-beliefs. Uncertainty measures in the beliefs not only are used in exploration but also provide a natural regularization for the value update considering all next available actions. ADFQ converges to Q-learning as the uncertainty measures of the Q-beliefs decrease and improves common drawbacks of other Bayesian RL algorithms such as computational complexity. We extend ADFQ with a neural network. Our empirical results demonstrate that ADFQ outperforms comparable algorithms on various Atari 2600 games, with drastic improvements in highly stochastic domains or domains with a large action space.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1712.03333v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1712.03333v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Heejin Jeong',\n",
       "   'Clark Zhang',\n",
       "   'George J. Pappas',\n",
       "   'Daniel D. Lee'],\n",
       "  'tasks': ['Atari Games', 'Bayesian Inference', 'Q-Learning'],\n",
       "  'date': '2017-12-09',\n",
       "  'methods': [{'name': 'Q-Learning',\n",
       "    'full_name': 'Q-Learning',\n",
       "    'description': '**Q-Learning** is an off-policy temporal difference control algorithm:\\r\\n\\r\\n$$Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) \\\\leftarrow Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) + \\\\alpha\\\\left[R_{t+1} + \\\\gamma\\\\max\\\\_{a}Q\\\\left(S\\\\_{t+1}, a\\\\right) - Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right)\\\\right] $$\\r\\n\\r\\nThe learned action-value function $Q$ directly approximates $q\\\\_{*}$, the optimal action-value function, independent of the policy being followed.\\r\\n\\r\\nSource: Sutton and Barto, Reinforcement Learning, 2nd Edition',\n",
       "    'introduced_year': 1984,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Off-Policy TD Control',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Reinforcement Learning'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/doobnet-deep-object-occlusion-boundary',\n",
       "  'arxiv_id': '1806.03772',\n",
       "  'title': 'DOOBNet: Deep Object Occlusion Boundary Detection from an Image',\n",
       "  'abstract': 'Object occlusion boundary detection is a fundamental and crucial research\\nproblem in computer vision. This is challenging to solve as encountering the\\nextreme boundary/non-boundary class imbalance during training an object\\nocclusion boundary detector. In this paper, we propose to address this class\\nimbalance by up-weighting the loss contribution of false negative and false\\npositive examples with our novel Attention Loss function. We also propose a\\nunified end-to-end multi-task deep object occlusion boundary detection network\\n(DOOBNet) by sharing convolutional features to simultaneously predict object\\nboundary and occlusion orientation. DOOBNet adopts an encoder-decoder structure\\nwith skip connection in order to automatically learn multi-scale and\\nmulti-level features. We significantly surpass the state-of-the-art on the PIOD\\ndataset (ODS F-score of .702) and the BSDS ownership dataset (ODS F-score of\\n.555), as well as improving the detecting speed to as 0.037s per image on the\\nPIOD dataset.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03772v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03772v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Guoxia Wang', 'Xiaohui Liang', 'Frederick W. B. Li'],\n",
       "  'tasks': ['Boundary Detection'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/smoothed-analysis-of-the-low-rank-approach',\n",
       "  'arxiv_id': '1806.03763',\n",
       "  'title': 'Smoothed analysis of the low-rank approach for smooth semidefinite programs',\n",
       "  'abstract': 'We consider semidefinite programs (SDPs) of size n with equality constraints.\\nIn order to overcome scalability issues, Burer and Monteiro proposed a\\nfactorized approach based on optimizing over a matrix Y of size $n$ by $k$ such\\nthat $X = YY^*$ is the SDP variable. The advantages of such formulation are\\ntwofold: the dimension of the optimization variable is reduced and positive\\nsemidefiniteness is naturally enforced. However, the problem in Y is\\nnon-convex. In prior work, it has been shown that, when the constraints on the\\nfactorized variable regularly define a smooth manifold, provided k is large\\nenough, for almost all cost matrices, all second-order stationary points\\n(SOSPs) are optimal. Importantly, in practice, one can only compute points\\nwhich approximately satisfy necessary optimality conditions, leading to the\\nquestion: are such points also approximately optimal? To this end, and under\\nsimilar assumptions, we use smoothed analysis to show that approximate SOSPs\\nfor a randomly perturbed objective function are approximate global optima, with\\nk scaling like the square root of the number of constraints (up to log\\nfactors). Moreover, we bound the optimality gap at the approximate solution of\\nthe perturbed problem with respect to the original problem. We particularize\\nour results to an SDP relaxation of phase retrieval.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03763v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03763v2.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Thomas Pumir', 'Samy Jelassi', 'Nicolas Boumal'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/leveraging-translations-for-speech',\n",
       "  'arxiv_id': '1803.08991',\n",
       "  'title': 'Leveraging translations for speech transcription in low-resource settings',\n",
       "  'abstract': 'Recently proposed data collection frameworks for endangered language\\ndocumentation aim not only to collect speech in the language of interest, but\\nalso to collect translations into a high-resource language that will render the\\ncollected resource interpretable. We focus on this scenario and explore whether\\nwe can improve transcription quality under these extremely low-resource\\nsettings with the assistance of text translations. We present a neural\\nmulti-source model and evaluate several variations of it on three low-resource\\ndatasets. We find that our multi-source model with shared attention outperforms\\nthe baselines, reducing transcription character error rate by up to 12.3%.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.08991v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.08991v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Antonis Anastasopoulos', 'David Chiang'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-03-23',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/part-of-speech-tagging-on-an-endangered',\n",
       "  'arxiv_id': '1806.03757',\n",
       "  'title': 'Part-of-Speech Tagging on an Endangered Language: a Parallel Griko-Italian Resource',\n",
       "  'abstract': 'Most work on part-of-speech (POS) tagging is focused on high resource\\nlanguages, or examines low-resource and active learning settings through\\nsimulated studies. We evaluate POS tagging techniques on an actual endangered\\nlanguage, Griko. We present a resource that contains 114 narratives in Griko,\\nalong with sentence-level translations in Italian, and provides gold\\nannotations for the test set. Based on a previously collected small corpus, we\\ninvestigate several traditional methods, as well as methods that take advantage\\nof monolingual data or project cross-lingual POS tags. We show that the\\ncombination of a semi-supervised method with cross-lingual transfer is more\\nappropriate for this extremely challenging setting, with the best tagger\\nachieving an accuracy of 72.9%. With an applied active learning scheme, which\\nwe use to collect sentence-level annotations over the test set, we achieve\\nimprovements of more than 21 percentage points.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03757v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03757v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Antonis Anastasopoulos',\n",
       "   'Marika Lekakou',\n",
       "   'Josep Quer',\n",
       "   'Eleni Zimianiti',\n",
       "   'Justin DeBenedetto',\n",
       "   'David Chiang'],\n",
       "  'tasks': ['Active Learning',\n",
       "   'Cross-Lingual Transfer',\n",
       "   'Part-Of-Speech Tagging',\n",
       "   'POS'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/robust-object-tracking-with-crow-search',\n",
       "  'arxiv_id': '1806.03753',\n",
       "  'title': 'Robust Object Tracking with Crow Search Optimized Multi-cue Particle Filter',\n",
       "  'abstract': 'Particle Filter(PF) is used extensively for estimation of target Non-linear\\nand Non-gaussian state. However, its performance suffers due to inherent\\nproblem of sample degeneracy and impoverishment. In order to address this, we\\npropose a novel resampling method based upon Crow Search Optimization to\\novercome low performing particles detected as outlier. Proposed outlier\\ndetection mechanism with transductive reliability achieve faster convergence of\\nproposed PF tracking framework. In addition, we present an adaptive fuzzy\\nfusion model to integrate multi-cue extracted for each evaluated particle.\\nAutomatic boosting and suppression of particles using proposed fusion model not\\nonly enhances performance of resampling method but also achieve optimal state\\nestimation. Performance of the proposed tracker is evaluated over 12 benchmark\\nvideo sequences and compared with state-of-the-art solutions. Qualitative and\\nquantitative results reveals that the proposed tracker not only outperforms\\nexisting solutions but also efficiently handle various tracking challenges. On\\naverage of outcome, we achieve CLE of 7.98 and F-measure of 0.734.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03753v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03753v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kapil Sharma',\n",
       "   'Gurjit Singh Walia',\n",
       "   'Ashish Kumar',\n",
       "   'Astitwa Saxena',\n",
       "   'Kuldeep Singh'],\n",
       "  'tasks': ['Object Tracking', 'Outlier Detection'],\n",
       "  'date': '2018-06-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/improving-transferability-of-adversarial',\n",
       "  'arxiv_id': '1803.06978',\n",
       "  'title': 'Improving Transferability of Adversarial Examples with Input Diversity',\n",
       "  'abstract': 'Though CNNs have achieved the state-of-the-art performance on various vision tasks, they are vulnerable to adversarial examples --- crafted by adding human-imperceptible perturbations to clean images. However, most of the existing adversarial attacks only achieve relatively low success rates under the challenging black-box setting, where the attackers have no knowledge of the model structure and parameters. To this end, we propose to improve the transferability of adversarial examples by creating diverse input patterns. Instead of only using the original images to generate adversarial examples, our method applies random transformations to the input images at each iteration. Extensive experiments on ImageNet show that the proposed attack method can generate adversarial examples that transfer much better to different networks than existing baselines. By evaluating our method against top defense solutions and official baselines from NIPS 2017 adversarial competition, the enhanced attack reaches an average success rate of 73.0%, which outperforms the top-1 attack submission in the NIPS competition by a large margin of 6.6%. We hope that our proposed attack strategy can serve as a strong benchmark baseline for evaluating the robustness of networks to adversaries and the effectiveness of different defense methods in the future. Code is available at https://github.com/cihangxie/DI-2-FGSM.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1803.06978v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1803.06978v4.pdf',\n",
       "  'proceeding': 'CVPR 2019 6',\n",
       "  'authors': ['Cihang Xie',\n",
       "   'Zhishuai Zhang',\n",
       "   'Yuyin Zhou',\n",
       "   'Song Bai',\n",
       "   'Jian-Yu Wang',\n",
       "   'Zhou Ren',\n",
       "   'Alan Yuille'],\n",
       "  'tasks': ['Adversarial Attack', 'Image Classification'],\n",
       "  'date': '2018-03-19',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-gpu-based-wfst-decoder-with-exact-lattice',\n",
       "  'arxiv_id': '1804.03243',\n",
       "  'title': 'A GPU-based WFST Decoder with Exact Lattice Generation',\n",
       "  'abstract': 'We describe initial work on an extension of the Kaldi toolkit that supports\\nweighted finite-state transducer (WFST) decoding on Graphics Processing Units\\n(GPUs). We implement token recombination as an atomic GPU operation in order to\\nfully parallelize the Viterbi beam search, and propose a dynamic load balancing\\nstrategy for more efficient token passing scheduling among GPU threads. We also\\nredesign the exact lattice generation and lattice pruning algorithms for better\\nutilization of the GPUs. Experiments on the Switchboard corpus show that the\\nproposed method achieves identical 1-best results and lattice quality in\\nrecognition and confidence measure tasks, while running 3 to 15 times faster\\nthan the single process Kaldi decoder. The above results are reported on\\ndifferent GPU architectures. Additionally we obtain a 46-fold speedup with\\nsequence parallelism and multi-process service (MPS) in GPU.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.03243v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.03243v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Zhehuai Chen',\n",
       "   'Justin Luitjens',\n",
       "   'Hainan Xu',\n",
       "   'Yiming Wang',\n",
       "   'Daniel Povey',\n",
       "   'Sanjeev Khudanpur'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-04-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-structured-variational-autoencoder-for',\n",
       "  'arxiv_id': '1806.03746',\n",
       "  'title': 'A Structured Variational Autoencoder for Contextual Morphological Inflection',\n",
       "  'abstract': 'Statistical morphological inflectors are typically trained on fully supervised, type-level data. One remaining open research question is the following: How can we effectively exploit raw, token-level data to improve their performance? To this end, we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation. To enable posterior inference over the latent variables, we derive an efficient variational inference procedure based on the wake-sleep algorithm. We experiment on 23 languages, using the Universal Dependencies corpora in a simulated low-resource setting, and find improvements of over 10% absolute accuracy in some cases.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03746v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03746v2.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Lawrence Wolf-Sonkin',\n",
       "   'Jason Naradowsky',\n",
       "   'Sabrina J. Mielke',\n",
       "   'Ryan Cotterell'],\n",
       "  'tasks': ['Morphological Inflection', 'Variational Inference'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/object-detection-in-videos-by-high-quality',\n",
       "  'arxiv_id': '1801.09823',\n",
       "  'title': 'Object Detection in Videos by High Quality Object Linking',\n",
       "  'abstract': 'Compared with object detection in static images, object detection in videos\\nis more challenging due to degraded image qualities. An effective way to\\naddress this problem is to exploit temporal contexts by linking the same object\\nacross video to form tubelets and aggregating classification scores in the\\ntubelets. In this paper, we focus on obtaining high quality object linking\\nresults for better classification. Unlike previous methods that link objects by\\nchecking boxes between neighboring frames, we propose to link in the same\\nframe. To achieve this goal, we extend prior methods in following aspects: (1)\\na cuboid proposal network that extracts spatio-temporal candidate cuboids which\\nbound the movement of objects; (2) a short tubelet detection network that\\ndetects short tubelets in short video segments; (3) a short tubelet linking\\nalgorithm that links temporally-overlapping short tubelets to form long\\ntubelets. Experiments on the ImageNet VID dataset show that our method\\noutperforms both the static image detector and the previous state of the art.\\nIn particular, our method improves results by 8.8% over the static image\\ndetector for fast moving objects.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1801.09823v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1801.09823v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Peng Tang',\n",
       "   'Chunyu Wang',\n",
       "   'Xinggang Wang',\n",
       "   'Wenyu Liu',\n",
       "   'Wen-Jun Zeng',\n",
       "   'Jingdong Wang'],\n",
       "  'tasks': ['General Classification', 'Object Detection'],\n",
       "  'date': '2018-01-30',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/are-all-languages-equally-hard-to-language',\n",
       "  'arxiv_id': '1806.03743',\n",
       "  'title': 'Are All Languages Equally Hard to Language-Model?',\n",
       "  'abstract': 'For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both $n$-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03743v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03743v2.pdf',\n",
       "  'proceeding': 'NAACL 2018 6',\n",
       "  'authors': ['Ryan Cotterell',\n",
       "   'Sabrina J. Mielke',\n",
       "   'Jason Eisner',\n",
       "   'Brian Roark'],\n",
       "  'tasks': ['Language Modelling'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [{'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Tanh Activation',\n",
       "    'full_name': 'Tanh Activation',\n",
       "    'description': '**Tanh Activation** is an activation function used for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\\r\\n\\r\\nHistorically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nImage Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'LSTM',\n",
       "    'full_name': 'Long Short-Term Memory',\n",
       "    'description': 'An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\\r\\n\\r\\n(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\\r\\n\\r\\n(Introduced by Hochreiter and Schmidhuber)',\n",
       "    'introduced_year': 1997,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Recurrent Neural Networks',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/unsupervised-disambiguation-of-syncretism-in',\n",
       "  'arxiv_id': '1806.03740',\n",
       "  'title': 'Unsupervised Disambiguation of Syncretism in Inflected Lexicons',\n",
       "  'abstract': \"Lexical ambiguity makes it difficult to compute various useful statistics of a corpus. A given word form might represent any of several morphological feature bundles. One can, however, use unsupervised learning (as in EM) to fit a model that probabilistically disambiguates word forms. We present such an approach, which employs a neural network to smoothly model a prior distribution over feature bundles (even rare ones). Although this basic model does not consider a token's context, that very property allows it to operate on a simple list of unigram type counts, partitioning each count among different analyses of that unigram. We discuss evaluation metrics for this novel task and report results on 5 languages.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03740v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03740v2.pdf',\n",
       "  'proceeding': 'NAACL 2018 6',\n",
       "  'authors': ['Ryan Cotterell',\n",
       "   'Christo Kirov',\n",
       "   'Sabrina J. Mielke',\n",
       "   'Jason Eisner'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['universal-dependencies'],\n",
       "  'datasets_used_full': ['Universal Dependencies'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/polya-urn-latent-dirichlet-allocation-a',\n",
       "  'arxiv_id': '1704.03581',\n",
       "  'title': 'Pólya Urn Latent Dirichlet Allocation: a doubly sparse massively parallel sampler',\n",
       "  'abstract': \"Latent Dirichlet Allocation (LDA) is a topic model widely used in natural language processing and machine learning. Most approaches to training the model rely on iterative algorithms, which makes it difficult to run LDA on big corpora that are best analyzed in parallel and distributed computational environments. Indeed, current approaches to parallel inference either don't converge to the correct posterior or require storage of large dense matrices in memory. We present a novel sampler that overcomes both problems, and we show that this sampler is faster, both empirically and theoretically, than previous Gibbs samplers for LDA. We do so by employing a novel P\\\\'olya-urn-based approximation in the sparse partially collapsed sampler for LDA. We prove that the approximation error vanishes with data size, making our algorithm asymptotically exact, a property of importance for large-scale topic models. In addition, we show, via an explicit example, that - contrary to popular belief in the topic modeling literature - partially collapsed samplers can be more efficient than fully collapsed samplers. We conclude by comparing the performance of our algorithm with that of other approaches on well-known corpora.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1704.03581v7',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1704.03581v7.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alexander Terenin',\n",
       "   'Måns Magnusson',\n",
       "   'Leif Jonsson',\n",
       "   'David Draper'],\n",
       "  'tasks': ['Topic Models'],\n",
       "  'date': '2017-04-12',\n",
       "  'methods': [{'name': 'LDA',\n",
       "    'full_name': 'Linear Discriminant Analysis',\n",
       "    'description': \"**Linear discriminant analysis** (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\\r\\n\\r\\nExtracted from [Wikipedia](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)\\r\\n\\r\\n**Source**:\\r\\n\\r\\nPaper: [Linear Discriminant Analysis: A Detailed Tutorial](https://dx.doi.org/10.3233/AIC-170729)\\r\\n\\r\\nPublic version: [Linear Discriminant Analysis: A Detailed Tutorial](https://usir.salford.ac.uk/id/eprint/52074/)\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Dimensionality Reduction',\n",
       "     'description': '**Dimensionality Reduction** methods transform data from a high-dimensional space into a low-dimensional space so that the low-dimensional space retains the most important properties of the original data. Below you can find a continuously updating list of dimensionality reduction methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/an-enhanced-bpso-based-approach-for-service',\n",
       "  'arxiv_id': '1806.05971',\n",
       "  'title': 'An Enhanced Binary Particle-Swarm Optimization (E-BPSO) Algorithm for Service Placement in Hybrid Cloud Platforms',\n",
       "  'abstract': 'Nowadays, hybrid cloud platforms stand as an attractive solution for organizations intending to implement combined private and public cloud applications, in order to meet their profitability requirements. However, this can only be achieved through the utilization of available resources while speeding up execution processes. Accordingly, deploying new applications entails dedicating some of these processes to a private cloud solution, while allocating others to the public cloud. In this context, the present work is set to help minimize relevant costs and deliver effective choices for an optimal service placement solution within minimal execution time. Several evolutionary algorithms have been applied to solve the service placement problem and are used when dealing with complex solution spaces to provide an optimal placement and often produce a short execution time. The standard BPSO algorithm is found to display a significant disadvantage, namely, of easily trapping into local optima, in addition to demonstrating a noticeable lack of robustness in dealing with service placement problems. Hence, to overcome critical shortcomings associated with the standard BPSO, an Enhanced Binary Particle Swarm Optimization (E-BPSO) algorithm is proposed, consisting of a modification of the particle position updating equation, initially inspired from the continuous PSO. Our proposed E-BPSO algorithm is shown to outperform state-of-the-art approaches in terms of both cost and execution time, using a real benchmark.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.05971v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.05971v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Wissem Abbes',\n",
       "   'Zied Kechaou',\n",
       "   'Amir Hussain',\n",
       "   'Abdulrahman M. Qahtani',\n",
       "   'Omar Aimutiry',\n",
       "   'Habib Dhahri',\n",
       "   'Adel M. ALIMI'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/cross-dataset-adaptation-for-visual-question',\n",
       "  'arxiv_id': '1806.03726',\n",
       "  'title': 'Cross-Dataset Adaptation for Visual Question Answering',\n",
       "  'abstract': 'We investigate the problem of cross-dataset adaptation for visual question\\nanswering (Visual QA). Our goal is to train a Visual QA model on a source\\ndataset but apply it to another target one. Analogous to domain adaptation for\\nvisual recognition, this setting is appealing when the target dataset does not\\nhave a sufficient amount of labeled data to learn an \"in-domain\" model. The key\\nchallenge is that the two datasets are constructed differently, resulting in\\nthe cross-dataset mismatch on images, questions, or answers.\\n  We overcome this difficulty by proposing a novel domain adaptation algorithm.\\nOur method reduces the difference in statistical distributions by transforming\\nthe feature representation of the data in the target dataset. Moreover, it\\nmaximizes the likelihood of answering questions (in the target dataset)\\ncorrectly using the Visual QA model trained on the source dataset. We\\nempirically studied the effectiveness of the proposed approach on adapting\\namong several popular Visual QA datasets. We show that the proposed method\\nimproves over baselines where there is no adaptation and several other\\nadaptation methods. We both quantitatively and qualitatively analyze when the\\nadaptation can be mostly effective.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03726v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03726v1.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Wei-Lun Chao', 'Hexiang Hu', 'Fei Sha'],\n",
       "  'tasks': ['Domain Adaptation',\n",
       "   'Question Answering',\n",
       "   'Visual Question Answering'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['coco',\n",
       "   'visual-question-answering',\n",
       "   'visual-genome',\n",
       "   'visual7w',\n",
       "   'coco-qa'],\n",
       "  'datasets_used_full': ['COCO',\n",
       "   'Visual Question Answering',\n",
       "   'Visual Genome',\n",
       "   'Visual7W',\n",
       "   'COCO-QA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-answer-embeddings-for-visual',\n",
       "  'arxiv_id': '1806.03724',\n",
       "  'title': 'Learning Answer Embeddings for Visual Question Answering',\n",
       "  'abstract': 'We propose a novel probabilistic model for visual question answering (Visual\\nQA). The key idea is to infer two sets of embeddings: one for the image and the\\nquestion jointly and the other for the answers. The learning objective is to\\nlearn the best parameterization of those embeddings such that the correct\\nanswer has higher likelihood among all possible answers. In contrast to several\\nexisting approaches of treating Visual QA as multi-way classification, the\\nproposed approach takes the semantic relationships (as characterized by the\\nembeddings) among answers into consideration, instead of viewing them as\\nindependent ordinal numbers. Thus, the learned embedded function can be used to\\nembed unseen answers (in the training dataset). These properties make the\\napproach particularly appealing for transfer learning for open-ended Visual QA,\\nwhere the source dataset on which the model is learned has limited overlapping\\nwith the target dataset in the space of answers. We have also developed\\nlarge-scale optimization techniques for applying the model to datasets with a\\nlarge number of answers, where the challenge is to properly normalize the\\nproposed probabilistic models. We validate our approach on several Visual QA\\ndatasets and investigate its utility for transferring models across datasets.\\nThe empirical results have shown that the approach performs well not only on\\nin-domain learning but also on transfer learning.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03724v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03724v1.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Hexiang Hu', 'Wei-Lun Chao', 'Fei Sha'],\n",
       "  'tasks': ['Question Answering',\n",
       "   'Transfer Learning',\n",
       "   'Visual Question Answering'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['coco',\n",
       "   'visual-question-answering',\n",
       "   'visual-genome',\n",
       "   'visual7w'],\n",
       "  'datasets_used_full': ['COCO',\n",
       "   'Visual Question Answering',\n",
       "   'Visual Genome',\n",
       "   'Visual7W'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/smallify-learning-network-size-while-training',\n",
       "  'arxiv_id': '1806.03723',\n",
       "  'title': 'Smallify: Learning Network Size while Training',\n",
       "  'abstract': 'As neural networks become widely deployed in different applications and on\\ndifferent hardware, it has become increasingly important to optimize inference\\ntime and model size along with model accuracy. Most current techniques optimize\\nmodel size, model accuracy and inference time in different stages, resulting in\\nsuboptimal results and computational inefficiency. In this work, we propose a\\nnew technique called Smallify that optimizes all three of these metrics at the\\nsame time. Specifically we present a new method to simultaneously optimize\\nnetwork size and model performance by neuron-level pruning during training.\\nNeuron-level pruning not only produces much smaller networks but also produces\\ndense weight matrices that are amenable to efficient inference. By applying our\\ntechnique to convolutional as well as fully connected models, we show that\\nSmallify can reduce network size by 35X with a 6X improvement in inference time\\nwith similar accuracy as models found by traditional training techniques.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03723v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03723v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Guillaume Leclerc',\n",
       "   'Manasi Vartak',\n",
       "   'Raul Castro Fernandez',\n",
       "   'Tim Kraska',\n",
       "   'Samuel Madden'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist', 'fashion-mnist'],\n",
       "  'datasets_used_full': ['MNIST', 'Fashion-MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/stochastic-seismic-waveform-inversion-using',\n",
       "  'arxiv_id': '1806.03720',\n",
       "  'title': 'Stochastic seismic waveform inversion using generative adversarial networks as a geological prior',\n",
       "  'abstract': \"We present an application of deep generative models in the context of\\npartial-differential equation (PDE) constrained inverse problems. We combine a\\ngenerative adversarial network (GAN) representing an a priori model that\\ncreates subsurface geological structures and their petrophysical properties,\\nwith the numerical solution of the PDE governing the propagation of acoustic\\nwaves within the earth's interior. We perform Bayesian inversion using an\\napproximate Metropolis-adjusted Langevin algorithm (MALA) to sample from the\\nposterior given seismic observations. Gradients with respect to the model\\nparameters governing the forward problem are obtained by solving the adjoint of\\nthe acoustic wave equation. Gradients of the mismatch with respect to the\\nlatent variables are obtained by leveraging the differentiable nature of the\\ndeep neural network used to represent the generative model. We show that\\napproximate MALA sampling allows efficient Bayesian inversion of model\\nparameters obtained from a prior represented by a deep generative model,\\nobtaining a diverse set of realizations that reflect the observed seismic\\nresponse.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03720v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03720v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Lukas Mosser', 'Olivier Dubrule', 'Martin J. Blunt'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/being-negative-but-constructively-lessons',\n",
       "  'arxiv_id': '1704.07121',\n",
       "  'title': 'Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets',\n",
       "  'abstract': 'Visual question answering (Visual QA) has attracted a lot of attention\\nlately, seen essentially as a form of (visual) Turing test that artificial\\nintelligence should strive to achieve. In this paper, we study a crucial\\ncomponent of this task: how can we design good datasets for the task? We focus\\non the design of multiple-choice based datasets where the learner has to select\\nthe right answer from a set of candidate ones including the target (\\\\ie the\\ncorrect one) and the decoys (\\\\ie the incorrect ones). Through careful analysis\\nof the results attained by state-of-the-art learning models and human\\nannotators on existing datasets, we show that the design of the decoy answers\\nhas a significant impact on how and what the learning models learn from the\\ndatasets. In particular, the resulting learner can ignore the visual\\ninformation, the question, or both while still doing well on the task. Inspired\\nby this, we propose automatic procedures to remedy such design deficiencies. We\\napply the procedures to re-construct decoy answers for two popular Visual QA\\ndatasets as well as to create a new Visual QA dataset from the Visual Genome\\nproject, resulting in the largest dataset for this task. Extensive empirical\\nstudies show that the design deficiencies have been alleviated in the remedied\\ndatasets and the performance on them is likely a more faithful indicator of the\\ndifference among learning models. The datasets are released and publicly\\navailable via http://www.teds.usc.edu/website_vqa/.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1704.07121v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1704.07121v2.pdf',\n",
       "  'proceeding': 'NAACL 2018 6',\n",
       "  'authors': ['Wei-Lun Chao', 'Hexiang Hu', 'Fei Sha'],\n",
       "  'tasks': ['Question Answering', 'Visual Question Answering'],\n",
       "  'date': '2017-04-24',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['coco',\n",
       "   'visual-question-answering',\n",
       "   'visual-genome',\n",
       "   'visual7w',\n",
       "   'coco-qa'],\n",
       "  'datasets_used_full': ['COCO',\n",
       "   'Visual Question Answering',\n",
       "   'Visual Genome',\n",
       "   'Visual7W',\n",
       "   'COCO-QA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/conditional-generative-adversarial-and',\n",
       "  'arxiv_id': '1805.10207',\n",
       "  'title': 'Conditional Generative Adversarial and Convolutional Networks for X-ray Breast Mass Segmentation and Shape Classification',\n",
       "  'abstract': 'This paper proposes a novel approach based on conditional Generative\\nAdversarial Networks (cGAN) for breast mass segmentation in mammography. We\\nhypothesized that the cGAN structure is well-suited to accurately outline the\\nmass area, especially when the training data is limited. The generative network\\nlearns intrinsic features of tumors while the adversarial network enforces\\nsegmentations to be similar to the ground truth. Experiments performed on\\ndozens of malignant tumors extracted from the public DDSM dataset and from our\\nin-house private dataset confirm our hypothesis with very high Dice coefficient\\nand Jaccard index (>94% and >89%, respectively) outperforming the scores\\nobtained by other state-of-the-art approaches. Furthermore, in order to detect\\nportray significant morphological features of the segmented tumor, a specific\\nConvolutional Neural Network (CNN) have also been designed for classifying the\\nsegmented tumor areas into four types (irregular, lobular, oval and round),\\nwhich provides an overall accuracy about 72% with the DDSM dataset.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.10207v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.10207v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Vivek Kumar Singh',\n",
       "   'Santiago Romani',\n",
       "   'Hatem A. Rashwan',\n",
       "   'Farhan Akram',\n",
       "   'Nidhi Pandey',\n",
       "   'Md. Mostafa Kamal Sarker',\n",
       "   'Jordina Torrents Barrena',\n",
       "   'Saddam Abdulwahab',\n",
       "   'Adel Saleh',\n",
       "   'Miguel Arquez',\n",
       "   'Meritxell Arenas',\n",
       "   'Domenec Puig'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2018-05-25',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/all-in-one-multi-task-learning-for-rumour',\n",
       "  'arxiv_id': '1806.03713',\n",
       "  'title': 'All-in-one: Multi-task Learning for Rumour Verification',\n",
       "  'abstract': 'Automatic resolution of rumours is a challenging task that can be broken down\\ninto smaller components that make up a pipeline, including rumour detection,\\nrumour tracking and stance classification, leading to the final outcome of\\ndetermining the veracity of a rumour. In previous work, these steps in the\\nprocess of rumour verification have been developed as separate components where\\nthe output of one feeds into the next. We propose a multi-task learning\\napproach that allows joint training of the main and auxiliary tasks, improving\\nthe performance of rumour verification. We examine the connection between the\\ndataset properties and the outcomes of the multi-task learning models used.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03713v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03713v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Elena Kochkina', 'Maria Liakata', 'Arkaitz Zubiaga'],\n",
       "  'tasks': ['General Classification',\n",
       "   'Multi-Task Learning',\n",
       "   'Rumour Detection',\n",
       "   'Stance Classification'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/light-field-super-resolution-through',\n",
       "  'arxiv_id': '1709.09422',\n",
       "  'title': 'Light field super resolution through controlled micro-shifts of light field sensor',\n",
       "  'abstract': 'Light field cameras enable new capabilities, such as post-capture refocusing\\nand aperture control, through capturing directional and spatial distribution of\\nlight rays in space. Micro-lens array based light field camera design is often\\npreferred due to its light transmission efficiency, cost-effectiveness and\\ncompactness. One drawback of the micro-lens array based light field cameras is\\nlow spatial resolution due to the fact that a single sensor is shared to\\ncapture both spatial and angular information. To address the low spatial\\nresolution issue, we present a light field imaging approach, where multiple\\nlight fields are captured and fused to improve the spatial resolution. For each\\ncapture, the light field sensor is shifted by a pre-determined fraction of a\\nmicro-lens size using an XY translation stage for optimal performance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1709.09422v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1709.09422v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['M. Umair Mukati', 'Bahadir K. Gunturk'],\n",
       "  'tasks': ['Super-Resolution', 'Translation'],\n",
       "  'date': '2017-09-27',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-reinforcement-learning-for-chinese-zero',\n",
       "  'arxiv_id': '1806.03711',\n",
       "  'title': 'Deep Reinforcement Learning for Chinese Zero pronoun Resolution',\n",
       "  'abstract': 'Deep neural network models for Chinese zero pronoun resolution learn semantic\\ninformation for zero pronoun and candidate antecedents, but tend to be\\nshort-sighted---they often make local decisions. They typically predict\\ncoreference chains between the zero pronoun and one single candidate antecedent\\none link at a time, while overlooking their long-term influence on future\\ndecisions. Ideally, modeling useful information of preceding potential\\nantecedents is critical when later predicting zero pronoun-candidate antecedent\\npairs. In this study, we show how to integrate local and global decision-making\\nby exploiting deep reinforcement learning models. With the help of the\\nreinforcement learning agent, our model learns the policy of selecting\\nantecedents in a sequential manner, where useful information provided by\\nearlier predicted antecedents could be utilized for making later coreference\\ndecisions. Experimental results on OntoNotes 5.0 dataset show that our\\ntechnique surpasses the state-of-the-art models.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03711v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03711v2.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Qingyu Yin',\n",
       "   'Yu Zhang',\n",
       "   'Wei-Nan Zhang',\n",
       "   'Ting Liu',\n",
       "   'William Yang Wang'],\n",
       "  'tasks': ['Chinese Zero Pronoun Resolution', 'Decision Making'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/unsupervised-video-to-video-translation',\n",
       "  'arxiv_id': '1806.03698',\n",
       "  'title': 'Unsupervised Video-to-Video Translation',\n",
       "  'abstract': 'Unsupervised image-to-image translation is a recently proposed task of\\ntranslating an image to a different style or domain given only unpaired image\\nexamples at training time. In this paper, we formulate a new task of\\nunsupervised video-to-video translation, which poses its own unique challenges.\\nTranslating video implies learning not only the appearance of objects and\\nscenes but also realistic motion and transitions between consecutive frames.We\\ninvestigate the performance of per-frame video-to-video translation using\\nexisting image-to-image translation networks, and propose a spatio-temporal 3D\\ntranslator as an alternative solution to this problem. We evaluate our 3D\\nmethod on multiple synthetic datasets, such as moving colorized digits, as well\\nas the realistic segmentation-to-video GTA dataset and a new CT-to-MRI\\nvolumetric images translation dataset. Our results show that frame-wise\\ntranslation produces realistic results on a single frame level but\\nunderperforms significantly on the scale of the whole video compared to our\\nthree-dimensional translation approach, which is better able to learn the\\ncomplex structure of video and motion and continuity of object appearance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03698v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03698v1.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Dina Bashkirova', 'Ben Usman', 'Kate Saenko'],\n",
       "  'tasks': ['Image-to-Image Translation',\n",
       "   'Translation',\n",
       "   'Unsupervised Image-To-Image Translation'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['moving-mnist'],\n",
       "  'datasets_used_full': ['Moving MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/attention-based-guided-structured-sparsity-of',\n",
       "  'arxiv_id': '1802.09902',\n",
       "  'title': 'Attention-Based Guided Structured Sparsity of Deep Neural Networks',\n",
       "  'abstract': 'Network pruning is aimed at imposing sparsity in a neural network\\narchitecture by increasing the portion of zero-valued weights for reducing its\\nsize regarding energy-efficiency consideration and increasing evaluation speed.\\nIn most of the conducted research efforts, the sparsity is enforced for network\\npruning without any attention to the internal network characteristics such as\\nunbalanced outputs of the neurons or more specifically the distribution of the\\nweights and outputs of the neurons. That may cause severe accuracy drop due to\\nuncontrolled sparsity. In this work, we propose an attention mechanism that\\nsimultaneously controls the sparsity intensity and supervised network pruning\\nby keeping important information bottlenecks of the network to be active. On\\nCIFAR-10, the proposed method outperforms the best baseline method by 6% and\\nreduced the accuracy drop by 2.6x at the same level of sparsity.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.09902v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.09902v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Amirsina Torfi',\n",
       "   'Rouzbeh A. Shirvani',\n",
       "   'Sobhan Soleymani',\n",
       "   'Nasser M. Nasrabadi'],\n",
       "  'tasks': ['Network Pruning'],\n",
       "  'date': '2018-02-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'mnist'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/continuous-time-visual-inertial-odometry-for',\n",
       "  'arxiv_id': '1702.07389',\n",
       "  'title': 'Continuous-Time Visual-Inertial Odometry for Event Cameras',\n",
       "  'abstract': \"Event cameras are bio-inspired vision sensors that output pixel-level\\nbrightness changes instead of standard intensity frames. They offer significant\\nadvantages over standard cameras, namely a very high dynamic range, no motion\\nblur, and a latency in the order of microseconds. However, due to the\\nfundamentally different structure of the sensor's output, new algorithms that\\nexploit the high temporal resolution and the asynchronous nature of the sensor\\nare required. Recent work has shown that a continuous-time representation of\\nthe event camera pose can deal with the high temporal resolution and\\nasynchronous nature of this sensor in a principled way. In this paper, we\\nleverage such a continuous-time representation to perform visual-inertial\\nodometry with an event camera. This representation allows direct integration of\\nthe asynchronous events with micro-second accuracy and the inertial\\nmeasurements at high frequency. The event camera trajectory is approximated by\\na smooth curve in the space of rigid-body motions using cubic splines. This\\nformulation significantly reduces the number of variables in trajectory\\nestimation problems. We evaluate our method on real data from several scenes\\nand compare the results against ground truth from a motion-capture system. We\\nshow that our method provides improved accuracy over the result of a\\nstate-of-the-art visual odometry method for event cameras. We also show that\\nboth the map orientation and scale can be recovered accurately by fusing events\\nand inertial data. To the best of our knowledge, this is the first work on\\nvisual-inertial fusion with event cameras using a continuous-time framework.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1702.07389v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1702.07389v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Elias Mueggler',\n",
       "   'Guillermo Gallego',\n",
       "   'Henri Rebecq',\n",
       "   'Davide Scaramuzza'],\n",
       "  'tasks': ['Visual Odometry'],\n",
       "  'date': '2017-02-23',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/segmentation-of-arterial-walls-in',\n",
       "  'arxiv_id': '1806.03695',\n",
       "  'title': 'Segmentation of Arterial Walls in Intravascular Ultrasound Cross-Sectional Images Using Extremal Region Selection',\n",
       "  'abstract': 'Intravascular Ultrasound (IVUS) is an intra-operative imaging modality that\\nfacilitates observing and appraising the vessel wall structure of the human\\ncoronary arteries. Segmentation of arterial wall boundaries from the IVUS\\nimages is not only crucial for quantitative analysis of the vessel walls and\\nplaque characteristics, but is also necessary for generating 3D reconstructed\\nmodels of the artery. The aim of this study is twofold. Firstly, we investigate\\nthe feasibility of using a recently proposed region detector, namely Extremal\\nRegion of Extremum Level (EREL) to delineate the luminal and media-adventitia\\nborders in IVUS frames acquired by 20 MHz probes. Secondly, we propose a region\\nselection strategy to label two ERELs as lumen and media based on the stability\\nof their textural information. We extensively evaluated our selection strategy\\non the test set of a standard publicly available dataset containing 326 IVUS\\nB-mode images. We showed that in the best case, the average Hausdorff Distances\\n(HD) between the extracted ERELs and the actual lumen and media were $0.22$ mm\\nand $0.45$ mm, respectively. The results of our experiments revealed that our\\nselection strategy was able to segment the lumen with $\\\\le 0.3$ mm HD to the\\ngold standard even though the images contained major artifacts such as\\nbifurcations, shadows, and side branches. Moreover, when there was no artifact,\\nour proposed method was able to delineate media-adventitia boundaries with\\n$0.31$ mm HD to the gold standard. Furthermore, our proposed segmentation\\nmethod runs in time that is linear in the number of pixels in each frame. Based\\non the results of this work, by using a 20 MHz IVUS probe with controlled\\npullback, not only can we now analyze the internal structure of human arteries\\nmore accurately, but also segment each frame during the pullback procedure\\nbecause of the low run time of our proposed segmentation method.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03695v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03695v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mehdi Faraji', 'Irene Cheng', 'Iris Naudin', 'Anup Basu'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deconvolution-based-global-decoding-for',\n",
       "  'arxiv_id': '1806.03692',\n",
       "  'title': 'Deconvolution-Based Global Decoding for Neural Machine Translation',\n",
       "  'abstract': 'A great proportion of sequence-to-sequence (Seq2Seq) models for Neural\\nMachine Translation (NMT) adopt Recurrent Neural Network (RNN) to generate\\ntranslation word by word following a sequential order. As the studies of\\nlinguistics have proved that language is not linear word sequence but sequence\\nof complex structure, translation at each step should be conditioned on the\\nwhole target-side context. To tackle the problem, we propose a new NMT model\\nthat decodes the sequence with the guidance of its structural prediction of the\\ncontext of the target sequence. Our model generates translation based on the\\nstructural prediction of the target-side context so that the translation can be\\nfreed from the bind of sequential order. Experimental results demonstrate that\\nour model is more competitive compared with the state-of-the-art methods, and\\nthe analysis reflects that our model is also robust to translating sentences of\\ndifferent lengths and it also reduces repetition with the instruction from the\\ntarget-side context for decoding.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03692v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03692v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Junyang Lin',\n",
       "   'Xu sun',\n",
       "   'Xuancheng Ren',\n",
       "   'Shuming Ma',\n",
       "   'Jinsong Su',\n",
       "   'Qi Su'],\n",
       "  'tasks': ['Machine Translation', 'Translation'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/lexnlp-natural-language-processing-and',\n",
       "  'arxiv_id': '1806.03688',\n",
       "  'title': 'LexNLP: Natural language processing and information extraction for legal and regulatory texts',\n",
       "  'abstract': 'LexNLP is an open source Python package focused on natural language\\nprocessing and machine learning for legal and regulatory text. The package\\nincludes functionality to (i) segment documents, (ii) identify key text such as\\ntitles and section headings, (iii) extract over eighteen types of structured\\ninformation like distances and dates, (iv) extract named entities such as\\ncompanies and geopolitical entities, (v) transform text into features for model\\ntraining, and (vi) build unsupervised and supervised models such as word\\nembedding or tagging models. LexNLP includes pre-trained models based on\\nthousands of unit tests drawn from real documents available from the SEC EDGAR\\ndatabase as well as various judicial and regulatory proceedings. LexNLP is\\ndesigned for use in both academic research and industrial applications, and is\\ndistributed at https://github.com/LexPredict/lexpredict-lexnlp.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03688v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03688v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Michael J Bommarito II',\n",
       "   'Daniel Martin Katz',\n",
       "   'Eric M Detterman'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/embedding-words-as-distributions-with-a',\n",
       "  'arxiv_id': '1711.11027',\n",
       "  'title': 'Embedding Words as Distributions with a Bayesian Skip-gram Model',\n",
       "  'abstract': \"We introduce a method for embedding words as probability densities in a\\nlow-dimensional space. Rather than assuming that a word embedding is fixed\\nacross the entire text collection, as in standard word embedding methods, in\\nour Bayesian model we generate it from a word-specific prior density for each\\noccurrence of a given word. Intuitively, for each word, the prior density\\nencodes the distribution of its potential 'meanings'. These prior densities are\\nconceptually similar to Gaussian embeddings. Interestingly, unlike the Gaussian\\nembeddings, we can also obtain context-specific densities: they encode\\nuncertainty about the sense of a word given its context and correspond to\\nposterior distributions within our model. The context-dependent densities have\\nmany potential applications: for example, we show that they can be directly\\nused in the lexical substitution task. We describe an effective estimation\\nmethod based on the variational autoencoding framework. We also demonstrate\\nthat our embeddings achieve competitive results on standard benchmarks.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.11027v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.11027v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Arthur Bražinskas', 'Serhii Havrylov', 'Ivan Titov'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-11-29',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dissipativity-theory-for-accelerating',\n",
       "  'arxiv_id': '1806.03677',\n",
       "  'title': 'Dissipativity Theory for Accelerating Stochastic Variance Reduction: A Unified Analysis of SVRG and Katyusha Using Semidefinite Programs',\n",
       "  'abstract': 'Techniques for reducing the variance of gradient estimates used in stochastic\\nprogramming algorithms for convex finite-sum problems have received a great\\ndeal of attention in recent years. By leveraging dissipativity theory from\\ncontrol, we provide a new perspective on two important variance-reduction\\nalgorithms: SVRG and its direct accelerated variant Katyusha. Our perspective\\nprovides a physically intuitive understanding of the behavior of SVRG-like\\nmethods via a principle of energy conservation. The tools discussed here allow\\nus to automate the convergence analysis of SVRG-like methods by capturing their\\nessential properties in small semidefinite programs amenable to standard\\nanalysis and computational techniques. Our approach recovers existing\\nconvergence results for SVRG and Katyusha and generalizes the theory to\\nalternative parameter choices. We also discuss how our approach complements the\\nlinear coupling technique. Our combination of perspectives leads to a better\\nunderstanding of accelerated variance-reduced stochastic methods for finite-sum\\nproblems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03677v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03677v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Bin Hu', 'Stephen Wright', 'Laurent Lessard'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-the-covariance-hessian-relation-in',\n",
       "  'arxiv_id': '1806.03674',\n",
       "  'title': 'On the Covariance-Hessian Relation in Evolution Strategies',\n",
       "  'abstract': 'We consider Evolution Strategies operating only with isotropic Gaussian mutations on positive quadratic objective functions, and investigate the covariance matrix when constructed out of selected individuals by truncation. We prove that the covariance matrix over $(1,\\\\lambda)$-selected decision vectors becomes proportional to the inverse of the landscape Hessian as the population-size $\\\\lambda$ increases. This generalizes a previous result that proved an equivalent phenomenon when sampling was assumed to take place in the vicinity of the optimum. It further confirms the classical hypothesis that statistical learning of the landscape is an inherent characteristic of standard Evolution Strategies, and that this distinguishing capability stems only from the usage of isotropic Gaussian mutations and rank-based selection. We provide broad numerical validation for the proven results, and present empirical evidence for its generalization to $(\\\\mu,\\\\lambda)$-selection.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03674v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03674v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ofer M. Shir', 'Amir Yehudayoff'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/global-encoding-for-abstractive-summarization',\n",
       "  'arxiv_id': '1805.03989',\n",
       "  'title': 'Global Encoding for Abstractive Summarization',\n",
       "  'abstract': 'In neural abstractive summarization, the conventional sequence-to-sequence\\n(seq2seq) model often suffers from repetition and semantic irrelevance. To\\ntackle the problem, we propose a global encoding framework, which controls the\\ninformation flow from the encoder to the decoder based on the global\\ninformation of the source context. It consists of a convolutional gated unit to\\nperform global encoding to improve the representations of the source-side\\ninformation. Evaluations on the LCSTS and the English Gigaword both demonstrate\\nthat our model outperforms the baseline models, and the analysis shows that our\\nmodel is capable of reducing repetition.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.03989v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.03989v2.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Junyang Lin', 'Xu sun', 'Shuming Ma', 'Qi Su'],\n",
       "  'tasks': ['Abstractive Text Summarization'],\n",
       "  'date': '2018-05-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['lcsts'],\n",
       "  'datasets_used_full': ['LCSTS'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-impact-of-humanoid-affect-expression-on',\n",
       "  'arxiv_id': '1806.03671',\n",
       "  'title': 'The Impact of Humanoid Affect Expression on Human Behavior in a Game-Theoretic Setting',\n",
       "  'abstract': \"With the rapid development of robot and other intelligent and autonomous\\nagents, how a human could be influenced by a robot's expressed mood when making\\ndecisions becomes a crucial question in human-robot interaction. In this pilot\\nstudy, we investigate (1) in what way a robot can express a certain mood to\\ninfluence a human's decision making behavioral model; (2) how and to what\\nextent the human will be influenced in a game theoretic setting. More\\nspecifically, we create an NLP model to generate sentences that adhere to a\\nspecific affective expression profile. We use these sentences for a humanoid\\nrobot as it plays a Stackelberg security game against a human. We investigate\\nthe behavioral model of the human player.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03671v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03671v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Aaron M. Roth',\n",
       "   'Umang Bhatt',\n",
       "   'Tamara Amin',\n",
       "   'Afsaneh Doryab',\n",
       "   'Fei Fang',\n",
       "   'Manuela Veloso'],\n",
       "  'tasks': ['Decision Making'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/neural-architecture-search-with-bayesian',\n",
       "  'arxiv_id': '1802.07191',\n",
       "  'title': 'Neural Architecture Search with Bayesian Optimisation and Optimal Transport',\n",
       "  'abstract': 'Bayesian Optimisation (BO) refers to a class of methods for global\\noptimisation of a function $f$ which is only accessible via point evaluations.\\nIt is typically used in settings where $f$ is expensive to evaluate. A common\\nuse case for BO in machine learning is model selection, where it is not\\npossible to analytically model the generalisation performance of a statistical\\nmodel, and we resort to noisy and expensive training and validation procedures\\nto choose the best model. Conventional BO methods have focused on Euclidean and\\ncategorical domains, which, in the context of model selection, only permits\\ntuning scalar hyper-parameters of machine learning algorithms. However, with\\nthe surge of interest in deep learning, there is an increasing demand to tune\\nneural network \\\\emph{architectures}. In this work, we develop NASBOT, a\\nGaussian process based BO framework for neural architecture search. To\\naccomplish this, we develop a distance metric in the space of neural network\\narchitectures which can be computed efficiently via an optimal transport\\nprogram. This distance might be of independent interest to the deep learning\\ncommunity as it may find applications outside of BO. We demonstrate that NASBOT\\noutperforms other alternatives for architecture search in several cross\\nvalidation based model selection tasks on multi-layer perceptrons and\\nconvolutional neural networks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.07191v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.07191v3.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Kirthevasan Kandasamy',\n",
       "   'Willie Neiswanger',\n",
       "   'Jeff Schneider',\n",
       "   'Barnabas Poczos',\n",
       "   'Eric Xing'],\n",
       "  'tasks': ['Bayesian Optimisation',\n",
       "   'Model Selection',\n",
       "   'Neural Architecture Search'],\n",
       "  'date': '2018-02-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-10', 'imagenet', 'nas-bench-201'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'ImageNet', 'NAS-Bench-201'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/centrality-measures-for-graphons-accounting',\n",
       "  'arxiv_id': '1707.09350',\n",
       "  'title': 'Centrality measures for graphons: Accounting for uncertainty in networks',\n",
       "  'abstract': 'As relational datasets modeled as graphs keep increasing in size and their\\ndata-acquisition is permeated by uncertainty, graph-based analysis techniques\\ncan become computationally and conceptually challenging. In particular, node\\ncentrality measures rely on the assumption that the graph is perfectly known --\\na premise not necessarily fulfilled for large, uncertain networks. Accordingly,\\ncentrality measures may fail to faithfully extract the importance of nodes in\\nthe presence of uncertainty. To mitigate these problems, we suggest a\\nstatistical approach based on graphon theory: we introduce formal definitions\\nof centrality measures for graphons and establish their connections to\\nclassical graph centrality measures. A key advantage of this approach is that\\ncentrality measures defined at the modeling level of graphons are inherently\\nrobust to stochastic variations of specific graph realizations. Using the\\ntheory of linear integral operators, we define degree, eigenvector, Katz and\\nPageRank centrality functions for graphons and establish concentration\\ninequalities demonstrating that graphon centrality functions arise naturally as\\nlimits of their counterparts defined on sequences of graphs of increasing size.\\nThe same concentration inequalities also provide high-probability bounds\\nbetween the graphon centrality functions and the centrality measures on any\\nsampled graph, thereby establishing a measure of uncertainty of the measured\\ncentrality score. The same concentration inequalities also provide\\nhigh-probability bounds between the graphon centrality functions and the\\ncentrality measures on any sampled graph, thereby establishing a measure of\\nuncertainty of the measured centrality score.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1707.09350v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1707.09350v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Marco Avella-Medina',\n",
       "   'Francesca Parise',\n",
       "   'Michael T. Schaub',\n",
       "   'Santiago Segarra'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-07-28',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/towards-understanding-acceleration-tradeoff',\n",
       "  'arxiv_id': '1806.01660',\n",
       "  'title': 'Towards Understanding Acceleration Tradeoff between Momentum and Asynchrony in Nonconvex Stochastic Optimization',\n",
       "  'abstract': 'Asynchronous momentum stochastic gradient descent algorithms (Async-MSGD) is one of the most popular algorithms in distributed machine learning. However, its convergence properties for these complicated nonconvex problems is still largely unknown, because of the current technical limit. Therefore, in this paper, we propose to analyze the algorithm through a simpler but nontrivial nonconvex problem - streaming PCA, which helps us to understand Aync-MSGD better even for more general problems. Specifically, we establish the asymptotic rate of convergence of Async-MSGD for streaming PCA by diffusion approximation. Our results indicate a fundamental tradeoff between asynchrony and momentum: To ensure convergence and acceleration through asynchrony, we have to reduce the momentum (compared with Sync-MSGD). To the best of our knowledge, this is the first theoretical attempt on understanding Async-MSGD for distributed nonconvex stochastic optimization. Numerical experiments on both streaming PCA and training deep neural networks are provided to support our findings for Async-MSGD.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.01660v6',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.01660v6.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Tianyi Liu',\n",
       "   'Shiyang Li',\n",
       "   'Jianping Shi',\n",
       "   'Enlu Zhou',\n",
       "   'Tuo Zhao'],\n",
       "  'tasks': ['Stochastic Optimization'],\n",
       "  'date': '2018-06-04',\n",
       "  'methods': [{'name': 'PCA',\n",
       "    'full_name': 'Principal Components Analysis',\n",
       "    'description': '**Principle Components Analysis (PCA)** is an unsupervised method primary used for dimensionality reduction within machine learning.  PCA is calculated via a singular value decomposition (SVD) of the design matrix, or alternatively, by calculating the covariance matrix of the data and performing eigenvalue decomposition on the covariance matrix. The results of PCA provide a low-dimensional picture of the structure of the data and the leading (uncorrelated) latent factors determining variation in the data.\\r\\n\\r\\nImage Source: [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#/media/File:GaussianScatterPCA.svg)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Dimensionality Reduction',\n",
       "     'description': '**Dimensionality Reduction** methods transform data from a high-dimensional space into a low-dimensional space so that the low-dimensional space retains the most important properties of the original data. Below you can find a continuously updating list of dimensionality reduction methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/conditional-noise-contrastive-estimation-of',\n",
       "  'arxiv_id': '1806.03664',\n",
       "  'title': 'Conditional Noise-Contrastive Estimation of Unnormalised Models',\n",
       "  'abstract': 'Many parametric statistical models are not properly normalised and only\\nspecified up to an intractable partition function, which renders parameter\\nestimation difficult. Examples of unnormalised models are Gibbs distributions,\\nMarkov random fields, and neural network models in unsupervised deep learning.\\nIn previous work, the estimation principle called noise-contrastive estimation\\n(NCE) was introduced where unnormalised models are estimated by learning to\\ndistinguish between data and auxiliary noise. An open question is how to best\\nchoose the auxiliary noise distribution. We here propose a new method that\\naddresses this issue. The proposed method shares with NCE the idea of\\nformulating density estimation as a supervised learning problem but in contrast\\nto NCE, the proposed method leverages the observed data when generating noise\\nsamples. The noise can thus be generated in a semi-automated manner. We first\\npresent the underlying theory of the new method, show that score matching\\nemerges as a limiting case, validate the method on continuous and discrete\\nvalued synthetic data, and show that we can expect an improved performance\\ncompared to NCE when the data lie in a lower-dimensional manifold. Then we\\ndemonstrate its applicability in unsupervised deep learning by estimating a\\nfour-layer neural image model.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03664v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03664v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Ciwan Ceylan', 'Michael U. Gutmann'],\n",
       "  'tasks': ['Density Estimation'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/smart-novel-computer-based-analytical-tool',\n",
       "  'arxiv_id': '1806.04576',\n",
       "  'title': 'Smart Novel Computer-based Analytical Tool for Image Forgery Authentication',\n",
       "  'abstract': 'This paper presents an integration of image forgery detection with image\\nfacial recognition using black propagation neural network (BPNN). We observed\\nthat facial image recognition by itself will always give a matching output or\\nclosest possible output image for every input image irrespective of the\\nauthenticity or otherwise not of the testing input image. Based on this, we are\\nproposing the combination of the blind but powerful automation image forgery\\ndetection for entire input images for the BPNN recognition program. Hence, an\\ninput image must first be authenticated before being fed into the recognition\\nprogram. Thus, an image security identification and authentication requirement,\\nany image that fails the authentication/verification stage are not to be used\\nas an input/test image. In addition, the universal smart GUI tool is proposed\\nand designed to perform image forgery detection with the high accuracy of 2%\\nerror rate.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04576v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04576v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Rozita Teymourzadeh', 'Amirrize Alpha', 'VH Mok'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/incremental-decoding-and-training-methods-for',\n",
       "  'arxiv_id': '1806.03661',\n",
       "  'title': 'Incremental Decoding and Training Methods for Simultaneous Translation in Neural Machine Translation',\n",
       "  'abstract': 'We address the problem of simultaneous translation by modifying the Neural MT\\ndecoder to operate with dynamically built encoder and attention. We propose a\\ntunable agent which decides the best segmentation strategy for a user-defined\\nBLEU loss and Average Proportion (AP) constraint. Our agent outperforms\\npreviously proposed Wait-if-diff and Wait-if-worse agents (Cho and Esipova,\\n2016) on BLEU with a lower latency. Secondly we proposed data-driven changes to\\nNeural MT training to better match the incremental decoding framework.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03661v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03661v1.pdf',\n",
       "  'proceeding': 'NAACL 2018 6',\n",
       "  'authors': ['Fahim Dalvi',\n",
       "   'Nadir Durrani',\n",
       "   'Hassan Sajjad',\n",
       "   'Stephan Vogel'],\n",
       "  'tasks': ['Machine Translation', 'Translation'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-generic-deep-architecture-for-single-image',\n",
       "  'arxiv_id': '1708.03474',\n",
       "  'title': 'A Generic Deep Architecture for Single Image Reflection Removal and Image Smoothing',\n",
       "  'abstract': 'This paper proposes a deep neural network structure that exploits edge\\ninformation in addressing representative low-level vision tasks such as layer\\nseparation and image filtering. Unlike most other deep learning strategies\\napplied in this context, our approach tackles these challenging problems by\\nestimating edges and reconstructing images using only cascaded convolutional\\nlayers arranged such that no handcrafted or application-specific\\nimage-processing components are required. We apply the resulting transferrable\\npipeline to two different problem domains that are both sensitive to edges,\\nnamely, single image reflection removal and image smoothing. For the former,\\nusing a mild reflection smoothness assumption and a novel synthetic data\\ngeneration method that acts as a type of weak supervision, our network is able\\nto solve much more difficult reflection cases that cannot be handled by\\nprevious methods. For the latter, we also exceed the state-of-the-art\\nquantitative and qualitative results by wide margins. In all cases, the\\nproposed framework is simple, fast, and easy to transfer across disparate\\ndomains.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1708.03474v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1708.03474v2.pdf',\n",
       "  'proceeding': 'ICCV 2017 10',\n",
       "  'authors': ['Qingnan Fan',\n",
       "   'Jiaolong Yang',\n",
       "   'Gang Hua',\n",
       "   'Baoquan Chen',\n",
       "   'David Wipf'],\n",
       "  'tasks': ['image smoothing',\n",
       "   'Reflection Removal',\n",
       "   'Synthetic Data Generation'],\n",
       "  'date': '2017-08-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/scidtb-discourse-dependency-treebank-for',\n",
       "  'arxiv_id': '1806.03653',\n",
       "  'title': 'SciDTB: Discourse Dependency TreeBank for Scientific Abstracts',\n",
       "  'abstract': 'Annotation corpus for discourse relations benefits NLP tasks such as machine\\ntranslation and question answering. In this paper, we present SciDTB, a\\ndomain-specific discourse treebank annotated on scientific articles. Different\\nfrom widely-used RST-DT and PDTB, SciDTB uses dependency trees to represent\\ndiscourse structure, which is flexible and simplified to some extent but do not\\nsacrifice structural integrity. We discuss the labeling framework, annotation\\nworkflow and some statistics about SciDTB. Furthermore, our treebank is made as\\na benchmark for evaluating discourse dependency parsers, on which we provide\\nseveral baselines as fundamental work.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03653v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03653v1.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['An Yang', 'Sujian Li'],\n",
       "  'tasks': ['Machine Translation', 'Question Answering', 'Translation'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-learning-estimation-of-absorbed-dose-for',\n",
       "  'arxiv_id': '1805.09108',\n",
       "  'title': 'Deep Learning Estimation of Absorbed Dose for Nuclear Medicine Diagnostics',\n",
       "  'abstract': 'The distribution of energy dose from Lu$^{177}$ radiotherapy can be estimated by convolving an image of a time-integrated activity distribution with a dose voxel kernel (DVK) consisting of different types of tissues. This fast and inacurate approximation is inappropriate for personalized dosimetry as it neglects tissue heterogenity. The latter can be calculated using different imaging techniques such as CT and SPECT combined with a time consuming monte-carlo simulation. The aim of this study is, for the first time, an estimation of DVKs from CT-derived density kernels (DK) via deep learning in convolutional neural networks (CNNs). The proposed CNN achieved, on the test set, a mean intersection over union (IOU) of $= 0.86$ after $308$ epochs and a corresponding mean squared error (MSE) $= 1.24 \\\\cdot 10^{-4}$. This generalization ability shows that the trained CNN can indeed learn the difficult transfer function from DK to DVK. Future work will evaluate DVKs estimated by CNNs with full monte-carlo simulations of a whole body CT to predict patient specific voxel dose maps.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1805.09108v9',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1805.09108v9.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Luciano Melodia'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-23',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/neural-disease-named-entity-extraction-with',\n",
       "  'arxiv_id': '1806.03648',\n",
       "  'title': 'Neural Disease Named Entity Extraction with Character-based BiLSTM+CRF in Japanese Medical Text',\n",
       "  'abstract': \"We propose an 'end-to-end' character-based recurrent neural network that\\nextracts disease named entities from a Japanese medical text and simultaneously\\njudges its modality as either positive or negative; i.e., the mentioned disease\\nor symptom is affirmed or negated. The motivation to adopt neural networks is\\nto learn effective lexical and structural representation features for Entity\\nRecognition and also for Positive/Negative classification from an annotated\\ncorpora without explicitly providing any rule-based or manual feature sets. We\\nconfirmed the superiority of our method over previous char-based CRF or SVM\\nmethods in the results.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03648v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03648v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ken Yano'],\n",
       "  'tasks': ['Entity Extraction using GAN', 'General Classification'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [{'name': 'CRF',\n",
       "    'full_name': 'Conditional Random Field',\n",
       "    'description': '**Conditional Random Fields** or **CRFs** are a type of probabilistic graph model that take neighboring sample context into account for tasks like classification. Prediction is modeled as a graphical model, which implements dependencies between the predictions. Graph choice depends on the application, for example linear chain CRFs are popular in natural language processing, whereas in image-based tasks, the graph would connect to neighboring locations in an image to enforce that they have similar predictions.\\r\\n\\r\\nImage Credit: [Charles Sutton and Andrew McCallum, An Introduction to Conditional Random Fields](https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Structured Prediction',\n",
       "     'description': '**Structured Prediction** methods deal with structured outputs with multiple interdependent outputs. Below you can find a continuously updating list of structured prediction methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/scalable-magnetic-field-slam-in-3d-using',\n",
       "  'arxiv_id': '1804.01926',\n",
       "  'title': 'Scalable Magnetic Field SLAM in 3D Using Gaussian Process Maps',\n",
       "  'abstract': 'We present a method for scalable and fully 3D magnetic field simultaneous\\nlocalisation and mapping (SLAM) using local anomalies in the magnetic field as\\na source of position information. These anomalies are due to the presence of\\nferromagnetic material in the structure of buildings and in objects such as\\nfurniture. We represent the magnetic field map using a Gaussian process model\\nand take well-known physical properties of the magnetic field into account. We\\nbuild local maps using three-dimensional hexagonal block tiling. To make our\\napproach computationally tractable we use reduced-rank Gaussian process\\nregression in combination with a Rao-Blackwellised particle filter. We show\\nthat it is possible to obtain accurate position and orientation estimates using\\nmeasurements from a smartphone, and that our approach provides a scalable\\nmagnetic field SLAM algorithm in terms of both computational complexity and map\\nstorage.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.01926v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.01926v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Manon Kok', 'Arno Solin'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-04-05',\n",
       "  'methods': [{'name': 'Gaussian Process',\n",
       "    'full_name': 'Gaussian Process',\n",
       "    'description': '**Gaussian Processes** are non-parametric models for approximating functions. They rely upon a measure of similarity between points (the kernel function) to predict the value for an unseen point from training data. The models are fully probabilistic so uncertainty bounds are baked in with the model.\\r\\n\\r\\nImage Source: Gaussian Processes for Machine Learning, C. E. Rasmussen & C. K. I. Williams',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Non-Parametric Classification',\n",
       "     'description': '**Non-Parametric Classification** methods perform classification where we use non-parametric methods to approximate the functional form of the relationship. Below you can find a continuously updating list of non-parametric classification methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-curiosity-loops-in-social-environments',\n",
       "  'arxiv_id': '1806.03645',\n",
       "  'title': 'Deep Curiosity Loops in Social Environments',\n",
       "  'abstract': \"Inspired by infants' intrinsic motivation to learn, which values informative\\nsensory channels contingent on their immediate social environment, we developed\\na deep curiosity loop (DCL) architecture. The DCL is composed of a learner,\\nwhich attempts to learn a forward model of the agent's state-action transition,\\nand a novel reinforcement-learning (RL) component, namely, an\\nAction-Convolution Deep Q-Network, which uses the learner's prediction error as\\nreward. The environment for our agent is composed of visual social scenes,\\ncomposed of sitcom video streams, thereby both the learner and the RL are\\nconstructed as deep convolutional neural networks. The agent's learner learns\\nto predict the zero-th order of the dynamics of visual scenes, resulting in\\nintrinsic rewards proportional to changes within its social environment. The\\nsources of these socially informative changes within the sitcom are\\npredominantly motions of faces and hands, leading to the unsupervised\\ncuriosity-based learning of social interaction features. The face and hand\\ndetection is represented by the value function and the social interaction\\noptical-flow is represented by the policy. Our results suggest that face and\\nhand detection are emergent properties of curiosity-based learning embedded in\\nsocial environments.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03645v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03645v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jonatan Barkan', 'Goren Gordon'],\n",
       "  'tasks': ['Hand Detection', 'Optical Flow Estimation'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/transformationally-identical-and-invariant-1',\n",
       "  'arxiv_id': '1806.03636',\n",
       "  'title': 'Transformationally Identical and Invariant Convolutional Neural Networks through Symmetric Element Operators',\n",
       "  'abstract': 'Mathematically speaking, a transformationally invariant operator, such as a\\ntransformationally identical (TI) matrix kernel (i.e., K= T{K}), commutes with\\nthe transformation (T{.}) itself when they operate on the first operand matrix.\\nWe found that by consistently applying the same type of TI kernels in a\\nconvolutional neural networks (CNN) system, the commutative property holds\\nthroughout all layers of convolution processes with and without involving an\\nactivation function and/or a 1D convolution across channels within a layer. We\\nfurther found that any CNN possessing the same TI kernel property for all\\nconvolution layers followed by a flatten layer with weight sharing among their\\ntransformation corresponding elements would output the same result for all\\ntransformation versions of the original input vector. In short, CNN[ Vi ] =\\nCNN[ T{Vi} ] providing every K = T{K} in CNN, where Vi denotes input vector and\\nCNN[.] represents the whole CNN process as a function of input vector that\\nproduces an output vector. With such a transformationally identical CNN\\n(TI-CNN) system, each transformation, that is not associated with a predefined\\nTI used in data augmentation, would inherently include all of its corresponding\\ntransformation versions of the input vector for the training. Hence the use of\\nsame TI property for every kernel in the CNN would serve as an orientation or a\\ntranslation independent training guide in conjunction with the\\nerror-backpropagation during the training. This TI kernel property is desirable\\nfor applications requiring a highly consistent output result from corresponding\\ntransformation versions of an input. Several C programming routines are\\nprovided to facilitate interested parties of using the TI-CNN technique which\\nis expected to produce a better generalization performance than its ordinary\\nCNN counterpart.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03636v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03636v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shih Chung B. Lo',\n",
       "   'Matthew T. Freedman',\n",
       "   'Seong K. Mun',\n",
       "   'Shuo Gu'],\n",
       "  'tasks': ['Data Augmentation'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/segmentation-of-instances-by-hashing',\n",
       "  'arxiv_id': '1702.08160',\n",
       "  'title': 'Segmentation of Instances by Hashing',\n",
       "  'abstract': 'We propose a novel approach to address the Simultaneous Detection and\\nSegmentation problem. Using hierarchical structures we use an efficient and\\naccurate procedure that exploits the hierarchy feature information using\\nLocality Sensitive Hashing. We build on recent work that utilizes convolutional\\nneural networks to detect bounding boxes in an image and then use the top\\nsimilar hierarchical region that best fits each bounding box after hashing, we\\ncall this approach CZ Segmentation. We then refine our final segmentation\\nresults by automatic hierarchy pruning. CZ Segmentation introduces a train-free\\nalternative to Hypercolumns. We conduct extensive experiments on PASCAL VOC\\n2012 segmentation dataset, showing that CZ gives competitive state-of-the-art\\nobject segmentations.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1702.08160v9',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1702.08160v9.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['J. D. Curtó', 'I. C. Zarza', 'A. Smola', 'L. Van Gool'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-02-27',\n",
       "  'methods': [{'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'SVM',\n",
       "    'full_name': 'Support Vector Machine',\n",
       "    'description': 'A **Support Vector Machine**, or **SVM**, is a non-parametric supervised learning model. For non-linear classification and regression, they utilise the kernel trick to map inputs to high-dimensional feature spaces. SVMs construct a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure to the right shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called “support vectors”. \\r\\n\\r\\nSource: [scikit-learn](https://scikit-learn.org/stable/modules/svm.html)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Non-Parametric Classification',\n",
       "     'description': '**Non-Parametric Classification** methods perform classification where we use non-parametric methods to approximate the functional form of the relationship. Below you can find a continuously updating list of non-parametric classification methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'R-CNN',\n",
       "    'full_name': 'R-CNN',\n",
       "    'description': '**R-CNN**, or **Regions with CNN Features**, is an object detection model that uses high-capacity CNNs to bottom-up region proposals in order to localize and segment objects. It uses [selective search](https://paperswithcode.com/method/selective-search) to identify a number of bounding-box object region candidates (“regions of interest”), and then extracts features from each region independently for classification.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1311.2524v5',\n",
       "    'source_title': 'Rich feature hierarchies for accurate object detection and semantic segmentation',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Object Detection Models',\n",
       "     'description': '**Object Detection Models** are architectures used to perform the task of object detection. Below you can find a continuously updating list of object detection models.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/mckernel-a-library-for-approximate-kernel',\n",
       "  'arxiv_id': '1702.08159',\n",
       "  'title': 'McKernel: A Library for Approximate Kernel Expansions in Log-linear Time',\n",
       "  'abstract': 'Kernel Methods Next Generation (KMNG) introduces a framework to use kernel\\napproximates in the mini-batch setting with SGD Optimizer as an alternative to\\nDeep Learning. McKernel is a C++ library for KMNG ML Large-scale. It contains a\\nCPU optimized implementation of the Fastfood algorithm that allows the\\ncomputation of approximated kernel expansions in log-linear time. The algorithm\\nrequires to compute the product of Walsh Hadamard Transform (WHT) matrices. A\\ncache friendly SIMD Fast Walsh Hadamard Transform (FWHT) that achieves\\ncompelling speed and outperforms current state-of-the-art methods has been\\ndeveloped. McKernel allows to obtain non-linear classification combining\\nFastfood and a linear classifier.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1702.08159v9',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1702.08159v9.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Joachim D. Curtó',\n",
       "   'Irene C. Zarza',\n",
       "   'Feng Yang',\n",
       "   'Alexander J. Smola',\n",
       "   'Fernando de la Torre',\n",
       "   'Chong-Wah Ngo',\n",
       "   'Luc van Gool'],\n",
       "  'tasks': ['General Classification'],\n",
       "  'date': '2017-02-27',\n",
       "  'methods': [{'name': 'MCKERNEL',\n",
       "    'full_name': 'MCKERNEL',\n",
       "    'description': 'McKernel introduces a framework to use kernel approximates in the mini-batch setting with Stochastic Gradient Descent ([SGD](https://paperswithcode.com/method/sgd)) as an alternative to Deep Learning.\\r\\n\\r\\nThe core library was developed in 2014 as integral part of a thesis of Master of Science [1,2] at Carnegie Mellon and City University of Hong Kong. The original intend was to implement a speedup of Random Kitchen Sinks (Rahimi and Recht 2007) by writing a very efficient HADAMARD tranform, which was the main bottleneck of the construction. The code though was later expanded at ETH Zürich (in McKernel by Curtó et al. 2017) to propose a framework that could explain both Kernel Methods and Neural Networks. This manuscript and the corresponding theses, constitute one of the first usages (if not the first) in the literature of FOURIER features and Deep Learning; which later got a lot of research traction and interest in the community.\\r\\n\\r\\nMore information can be found in this presentation that the first author gave at ICLR 2020 [iclr2020_DeCurto](https://www.decurto.tw/c/iclr2020_DeCurto.pdf).\\r\\n\\r\\n[1] [https://www.curto.hk/c/decurto.pdf](https://www.curto.hk/c/decurto.pdf)\\r\\n\\r\\n[2] [https://www.zarza.hk/z/dezarza.pdf](https://www.zarza.hk/z/dezarza.pdf)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1702.08159v9',\n",
       "    'source_title': 'McKernel: A Library for Approximate Kernel Expansions in Log-linear Time',\n",
       "    'code_snippet_url': 'https://github.com/curto2/mckernel',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'SGD',\n",
       "    'full_name': 'Stochastic Gradient Descent',\n",
       "    'description': '**Stochastic Gradient Descent** is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:\\r\\n\\r\\n$$ w\\\\_{t+1} = w\\\\_{t} - \\\\eta\\\\hat{\\\\nabla}\\\\_{w}{L(w\\\\_{t})} $$\\r\\n\\r\\nWhere $\\\\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.\\r\\n\\r\\n(Image Source: [here](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/))',\n",
       "    'introduced_year': 1951,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/4e0ac120e9a8b096069c2f892488d630a5c8f358/torch/optim/sgd.py#L97-L112',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/enhancing-convolutional-neural-networks-for',\n",
       "  'arxiv_id': '1707.07923',\n",
       "  'title': 'Enhancing Convolutional Neural Networks for Face Recognition with Occlusion Maps and Batch Triplet Loss',\n",
       "  'abstract': 'Despite the recent success of convolutional neural networks for computer\\nvision applications, unconstrained face recognition remains a challenge. In\\nthis work, we make two contributions to the field. Firstly, we consider the\\nproblem of face recognition with partial occlusions and show how current\\napproaches might suffer significant performance degradation when dealing with\\nthis kind of face images. We propose a simple method to find out which parts of\\nthe human face are more important to achieve a high recognition rate, and use\\nthat information during training to force a convolutional neural network to\\nlearn discriminative features from all the face regions more equally, including\\nthose that typical approaches tend to pay less attention to. We test the\\naccuracy of the proposed method when dealing with real-life occlusions using\\nthe AR face database. Secondly, we propose a novel loss function called batch\\ntriplet loss that improves the performance of the triplet loss by adding an\\nextra term to the loss function to cause minimisation of the standard deviation\\nof both positive and negative scores. We show consistent improvement in the\\nLabeled Faces in the Wild (LFW) benchmark by applying both proposed adjustments\\nto the convolutional neural network training.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1707.07923v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1707.07923v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Daniel Sáez Trigueros', 'Li Meng', 'Margaret Hartnett'],\n",
       "  'tasks': ['Face Recognition'],\n",
       "  'date': '2017-07-25',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['lfw', 'casia-webface'],\n",
       "  'datasets_used_full': ['LFW', 'CASIA-WebFace'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/voxelatlasgan-3d-left-ventricle-segmentation',\n",
       "  'arxiv_id': '1806.03619',\n",
       "  'title': 'VoxelAtlasGAN: 3D Left Ventricle Segmentation on Echocardiography with Atlas Guided Generation and Voxel-to-voxel Discrimination',\n",
       "  'abstract': '3D left ventricle (LV) segmentation on echocardiography is very important for\\ndiagnosis and treatment of cardiac disease. It is not only because of that\\nechocardiography is a real-time imaging technology and widespread in clinical\\napplication, but also because of that LV segmentation on 3D echocardiography\\ncan provide more full volume information of heart than LV segmentation on 2D\\nechocardiography. However, 3D LV segmentation on echocardiography is still an\\nopen and challenging task owing to the lower contrast, higher noise and data\\ndimensionality, limited annotation of 3D echocardiography. In this paper, we\\nproposed a novel real-time framework, i.e., VoxelAtlasGAN, for 3D LV\\nsegmentation on 3D echocardiography. This framework has three contributions: 1)\\nIt is based on voxel-to-voxel conditional generative adversarial nets (cGAN).\\nFor the first time, cGAN is used for 3D LV segmentation on echocardiography.\\nAnd cGAN advantageously fuses substantial 3D spatial context information from\\n3D echocardiography by self-learning structured loss; 2) For the first time, it\\nembeds the atlas into an end-to-end optimization framework, which uses 3D LV\\natlas as a powerful prior knowledge to improve the inference speed, address the\\nlower contrast and the limited annotation problems of 3D echocardiography; 3)\\nIt combines traditional discrimination loss and the new proposed consistent\\nconstraint, which further improves the generalization of the proposed\\nframework. VoxelAtlasGAN was validated on 60 subjects on 3D echocardiography\\nand it achieved satisfactory segmentation results and high inference speed. The\\nmean surface distance is 1.85 mm, the mean hausdorff surface distance is 7.26\\nmm, mean dice is 0.953, the correlation of EF is 0.918, and the mean inference\\nspeed is 0.1s. These results have demonstrated that our proposed method has\\ngreat potential for clinical application',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03619v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03619v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Suyu Dong',\n",
       "   'Gongning Luo',\n",
       "   'Kuanquan Wang',\n",
       "   'Shaodong Cao',\n",
       "   'Ashley Mercado',\n",
       "   'Olga Shmuilovich',\n",
       "   'Henggui Zhang',\n",
       "   'Shuo Li'],\n",
       "  'tasks': ['Left Ventricle Segmentation', 'Self-Learning'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/capacity-releasing-diffusion-for-speed-and-1',\n",
       "  'arxiv_id': '1706.05826',\n",
       "  'title': 'Capacity Releasing Diffusion for Speed and Locality',\n",
       "  'abstract': 'Diffusions and related random walk procedures are of central importance in\\nmany areas of machine learning, data analysis, and applied mathematics. Because\\nthey spread mass agnostically at each step in an iterative manner, they can\\nsometimes spread mass \"too aggressively,\" thereby failing to find the \"right\"\\nclusters. We introduce a novel Capacity Releasing Diffusion (CRD) Process,\\nwhich is both faster and stays more local than the classical spectral diffusion\\nprocess. As an application, we use our CRD Process to develop an improved local\\nalgorithm for graph clustering. Our local graph clustering method can find\\nlocal clusters in a model of clustering where one begins the CRD Process in a\\ncluster whose vertices are connected better internally than externally by an\\n$O(\\\\log^2 n)$ factor, where $n$ is the number of nodes in the cluster. Thus,\\nour CRD Process is the first local graph clustering algorithm that is not\\nsubject to the well-known quadratic Cheeger barrier. Our result requires a\\ncertain smoothness condition, which we expect to be an artifact of our\\nanalysis. Our empirical evaluation demonstrates improved results, in particular\\nfor realistic social graphs where there are moderately good---but not very\\ngood---clusters.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1706.05826v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1706.05826v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Di Wang',\n",
       "   'Kimon Fountoulakis',\n",
       "   'Monika Henzinger',\n",
       "   'Michael W. Mahoney',\n",
       "   'Satish Rao'],\n",
       "  'tasks': ['Graph Clustering'],\n",
       "  'date': '2017-06-19',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/implicit-policy-for-reinforcement-learning',\n",
       "  'arxiv_id': '1806.06798',\n",
       "  'title': 'Implicit Policy for Reinforcement Learning',\n",
       "  'abstract': 'We introduce Implicit Policy, a general class of expressive policies that can\\nflexibly represent complex action distributions in reinforcement learning, with\\nefficient algorithms to compute entropy regularized policy gradients. We\\nempirically show that, despite its simplicity in implementation, entropy\\nregularization combined with a rich policy class can attain desirable\\nproperties displayed under maximum entropy reinforcement learning framework,\\nsuch as robustness and multi-modality.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.06798v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.06798v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yunhao Tang', 'Shipra Agrawal'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mujoco'],\n",
       "  'datasets_used_full': ['MuJoCo'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/k-space-deep-learning-for-reference-free-epi',\n",
       "  'arxiv_id': '1806.00153',\n",
       "  'title': 'k-Space Deep Learning for Reference-free EPI Ghost Correction',\n",
       "  'abstract': 'Nyquist ghost artifacts in EPI are originated from phase mismatch between the even and odd echoes. However, conventional correction methods using reference scans often produce erroneous results especially in high-field MRI due to the non-linear and time-varying local magnetic field changes. Recently, it was shown that the problem of ghost correction can be reformulated as k-space interpolation problem that can be solved using structured low-rank Hankel matrix approaches. Another recent work showed that data driven Hankel matrix decomposition can be reformulated to exhibit similar structures as deep convolutional neural network. By synergistically combining these findings, we propose a k-space deep learning approach that immediately corrects the phase mismatch without a reference scan in both accelerated and non-accelerated EPI acquisitions. To take advantage of the even and odd-phase directional redundancy, the k-space data is divided into two channels configured with even and odd phase encodings. The redundancies between coils are also exploited by stacking the multi-coil k-space data into additional input channels. Then, our k-space ghost correction network is trained to learn the interpolation kernel to estimate the missing virtual k-space data. For the accelerated EPI data, the same neural network is trained to directly estimate the interpolation kernels for missing k-space data from both ghost and subsampling. Reconstruction results using 3T and 7T in-vivo data showed that the proposed method outperformed the image quality compared to the existing methods, and the computing time is much faster.The proposed k-space deep learning for EPI ghost correction is highly robust and fast, and can be combined with acceleration, so that it can be used as a promising correction tool for high-field MRI without changing the current acquisition protocol.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.00153v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.00153v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Juyoung Lee',\n",
       "   'Yoseob Han',\n",
       "   'Jae-Kyun Ryu',\n",
       "   'Jang-Yeon Park',\n",
       "   'Jong Chul Ye'],\n",
       "  'tasks': ['Matrix Completion'],\n",
       "  'date': '2018-06-01',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/weighted-tanimoto-coefficient-for-3d-molecule',\n",
       "  'arxiv_id': '1806.05237',\n",
       "  'title': 'Weighted Tanimoto Coefficient for 3D Molecule Structure Similarity Measurement',\n",
       "  'abstract': 'Similarity searching of molecular structure has been an important application\\nin the Chemoinformatics, especially in drug discovery. Similarity searching is\\na common method used for identification of molecular structure. It involve\\nthree main principal component of similarity searching: structure\\nrepresentation; weighting scheme; and similarity coefficient. In this paper, we\\nintroduces Weighted Tanimoto Coefficient based on weighted Euclidean distance\\nin order to investigate the effect of weight function on the result for\\nsimilarity searching. The Tanimoto coefficient is one of the popular similarity\\ncoefficients used to measure the similarity between pairs of the molecule. The\\nmost of research area found that the similarity searching is based on binary or\\nfingerprint data. Meanwhile, we used non-binary data and was set amphetamine\\nstructure as a reference or targeted structure and the rest of the dataset\\nbecomes a database structure. Throughout this study, it showed that there is\\ndefinitely gives a different result between a similarity searching with and\\nwithout weight.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05237v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05237v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Siti Asmah Bero',\n",
       "   'Azah Kamilah Muda',\n",
       "   'Yun-Huoy Choo',\n",
       "   'Noor Azilah Muda',\n",
       "   'Satrya Fajri Pratama'],\n",
       "  'tasks': ['Drug Discovery'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/k-space-deep-learning-for-parallel-mri',\n",
       "  'arxiv_id': '1806.00806',\n",
       "  'title': 'k-Space Deep Learning for Parallel MRI: Application to Time-Resolved MR Angiography',\n",
       "  'abstract': 'Time-resolved angiography with interleaved stochastic trajectories (TWIST)\\nhas been widely used for dynamic contrast enhanced MRI (DCE-MRI). To achieve\\nhighly accelerated acquisitions, TWIST combines the periphery of the k-space\\ndata from several adjacent frames to reconstruct one temporal frame. However,\\nthis view-sharing scheme limits the true temporal resolution of TWIST.\\nMoreover, the k-space sampling patterns have been specially designed for a\\nspecific generalized autocalibrating partial parallel acquisition (GRAPPA)\\nfactor so that it is not possible to reduce the number of view-sharing once the\\nk-data is acquired. To address these issues, this paper proposes a novel\\nk-space deep learning approach for parallel MRI. In particular, we have\\ndesigned our neural network so that accurate k-space interpolations are\\nperformed simultaneously for multiple coils by exploiting the redundancies\\nalong the coils and images. Reconstruction results using in vivo TWIST data set\\nconfirm that the proposed method can immediately generate high-quality\\nreconstruction results with various choices of view- sharing, allowing us to\\nexploit the trade-off between spatial and temporal resolution in time-resolved\\nMR angiography.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.00806v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.00806v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Eunju Cha', 'Eung Yeop Kim', 'Jong Chul Ye'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-03',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-the-generalization-of-equivariance-and',\n",
       "  'arxiv_id': '1802.03690',\n",
       "  'title': 'On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups',\n",
       "  'abstract': 'Convolutional neural networks have been extremely successful in the image\\nrecognition domain because they ensure equivariance to translations. There have\\nbeen many recent attempts to generalize this framework to other domains,\\nincluding graphs and data lying on manifolds. In this paper we give a rigorous,\\ntheoretical treatment of convolution and equivariance in neural networks with\\nrespect to not just translations, but the action of any compact group. Our main\\nresult is to prove that (given some natural constraints) convolutional\\nstructure is not just a sufficient, but also a necessary condition for\\nequivariance to the action of a compact group. Our exposition makes use of\\nconcepts from representation theory and noncommutative harmonic analysis and\\nderives new generalized convolution formulae.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.03690v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.03690v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Risi Kondor', 'Shubhendu Trivedi'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-11',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/cross-lingual-task-specific-representation',\n",
       "  'arxiv_id': '1806.03590',\n",
       "  'title': 'Cross-Lingual Task-Specific Representation Learning for Text Classification in Resource Poor Languages',\n",
       "  'abstract': 'Neural network models have shown promising results for text classification.\\nHowever, these solutions are limited by their dependence on the availability of\\nannotated data.\\n  The prospect of leveraging resource-rich languages to enhance the text\\nclassification of resource-poor languages is fascinating. The performance on\\nresource-poor languages can significantly improve if the resource availability\\nconstraints can be offset. To this end, we present a twin Bidirectional Long\\nShort Term Memory (Bi-LSTM) network with shared parameters consolidated by a\\ncontrastive loss function (based on a similarity metric). The model learns the\\nrepresentation of resource-poor and resource-rich sentences in a common space\\nby using the similarity between their assigned annotation tags. Hence, the\\nmodel projects sentences with similar tags closer and those with different tags\\nfarther from each other. We evaluated our model on the classification tasks of\\nsentiment analysis and emoji prediction for resource-poor languages - Hindi and\\nTelugu and resource-rich languages - English and Spanish. Our model\\nsignificantly outperforms the state-of-the-art approaches in both the tasks\\nacross all metrics.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03590v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03590v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Nurendra Choudhary', 'Rajat Singh', 'Manish Shrivastava'],\n",
       "  'tasks': ['Classification',\n",
       "   'General Classification',\n",
       "   'Representation Learning',\n",
       "   'Sentiment Analysis',\n",
       "   'Text Classification'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/free-form-image-inpainting-with-gated',\n",
       "  'arxiv_id': '1806.03589',\n",
       "  'title': 'Free-Form Image Inpainting with Gated Convolution',\n",
       "  'abstract': 'We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: https://github.com/JiahuiYu/generative_inpainting',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03589v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03589v2.pdf',\n",
       "  'proceeding': 'ICCV 2019 10',\n",
       "  'authors': ['Jiahui Yu',\n",
       "   'Zhe Lin',\n",
       "   'Jimei Yang',\n",
       "   'Xiaohui Shen',\n",
       "   'Xin Lu',\n",
       "   'Thomas Huang'],\n",
       "  'tasks': ['Image Inpainting'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [{'name': 'GLU',\n",
       "    'full_name': 'Gated Linear Unit',\n",
       "    'description': 'A **Gated Linear Unit**, or **GLU** computes:\\r\\n\\r\\n$$ \\\\text{GLU}\\\\left(a, b\\\\right) = a\\\\otimes \\\\sigma\\\\left(b\\\\right) $$\\r\\n\\r\\nIt is used in natural language processing architectures, for example the [Gated CNN](https://paperswithcode.com/method/gated-convolution-network), because here $b$ is the gate that control what information from $a$ is passed up to the following layer. Intuitively, for a language modeling task, the gating mechanism allows selection of words or features that are important for predicting the next word. The GLU also has non-linear capabilities, but has a linear path for the gradient so diminishes the vanishing gradient problem.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1612.08083v3',\n",
       "    'source_title': 'Language Modeling with Gated Convolutional Networks',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L551',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': '1x1 Convolution',\n",
       "    'full_name': '1x1 Convolution',\n",
       "    'description': 'A **1 x 1 Convolution** is a [convolution](https://paperswithcode.com/method/convolution) with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an [MLP](https://paperswithcode.com/method/feedforward-network) looking at a particular pixel location.\\r\\n\\r\\nImage Credit: [http://deeplearning.ai](http://deeplearning.ai)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://www.healthnutra.org/es/maxup/',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Gated Convolution',\n",
       "    'full_name': 'Gated Convolution',\n",
       "    'description': 'A **Gated Convolution** is a type of temporal [convolution](https://paperswithcode.com/method/convolution) with a gating mechanism. Zero-padding is used to ensure that future context can not be seen.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1612.08083v3',\n",
       "    'source_title': 'Language Modeling with Gated Convolutional Networks',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Temporal Convolutions',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Sequential'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['places', 'celeba-hq'],\n",
       "  'datasets_used_full': ['Places', 'CelebA-HQ'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-simplified-active-calibration-algorithm-for',\n",
       "  'arxiv_id': '1806.03584',\n",
       "  'title': 'A Simplified Active Calibration algorithm for Focal Length Estimation',\n",
       "  'abstract': 'We introduce new linear mathematical formulations to calculate the focal\\nlength of a camera in an active platform. Through mathematical derivations, we\\nshow that the focal lengths in each direction can be estimated using only one\\npoint correspondence that relates images taken before and after a degenerate\\nrotation of the camera. The new formulations will be beneficial in robotic and\\ndynamic surveillance environments when the camera needs to be calibrated while\\nit freely moves and zooms. By establishing a correspondence between only two\\nimages taken after slightly panning and tilting the camera and a reference\\nimage, our proposed Simplified Calibration Method is able to calculate the\\nfocal length of the camera. We extensively evaluate the derived formulations on\\na simulated camera, 3D scenes and real-world images. Our error analysis over\\nsimulated and real images indicates that the proposed Simplified Active\\nCalibration formulation estimates the parameters of a camera with low error\\nrates.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03584v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03584v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mehdi Faraji', 'Anup Basu'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-scalable-framework-for-trajectory',\n",
       "  'arxiv_id': '1806.03582',\n",
       "  'title': 'A Scalable Framework for Trajectory Prediction',\n",
       "  'abstract': 'Trajectory prediction (TP) is of great importance for a wide range of\\nlocation-based applications in intelligent transport systems such as\\nlocation-based advertising, route planning, traffic management, and early\\nwarning systems. In the last few years, the widespread use of GPS navigation\\nsystems and wireless communication technology enabled vehicles has resulted in\\nhuge volumes of trajectory data. The task of utilizing this data employing\\nspatio-temporal techniques for trajectory prediction in an efficient and\\naccurate manner is an ongoing research problem. Existing TP approaches are\\nlimited to short-term predictions. Moreover, they cannot handle a large volume\\nof trajectory data for long-term prediction. To address these limitations, we\\npropose a scalable clustering and Markov chain based hybrid framework, called\\nTraj-clusiVAT-based TP, for both short-term and long-term trajectory\\nprediction, which can handle a large number of overlapping trajectories in a\\ndense road network. Traj-clusiVAT can also determine the number of clusters,\\nwhich represent different movement behaviours in input trajectory data. In our\\nexperiments, we compare our proposed approach with a mixed Markov model\\n(MMM)-based scheme, and a trajectory clustering, NETSCAN-based TP method for\\nboth short- and long-term trajectory predictions. We performed our experiments\\non two real, vehicle trajectory datasets, including a large-scale trajectory\\ndataset consisting of 3.28 million trajectories obtained from 15,061 taxis in\\nSingapore over a period of one month. Experimental results on two real\\ntrajectory datasets show that our proposed approach outperforms the existing\\napproaches in terms of both short- and long-term prediction performances, based\\non prediction accuracy and distance error (in km).',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03582v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03582v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Punit Rathore',\n",
       "   'Dheeraj Kumar',\n",
       "   'Sutharshan Rajasegarar',\n",
       "   'Marimuthu Palaniswami',\n",
       "   'James C. Bezdek'],\n",
       "  'tasks': ['Trajectory Prediction'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/language-based-image-editing-with-recurrent',\n",
       "  'arxiv_id': '1711.06288',\n",
       "  'title': 'Language-Based Image Editing with Recurrent Attentive Models',\n",
       "  'abstract': 'We investigate the problem of Language-Based Image Editing (LBIE). Given a\\nsource image and a natural language description, we want to generate a target\\nimage by editing the source image based on the description. We propose a\\ngeneric modeling framework for two sub-tasks of LBIE: language-based image\\nsegmentation and image colorization. The framework uses recurrent attentive\\nmodels to fuse image and language features. Instead of using a fixed step size,\\nwe introduce for each region of the image a termination gate to dynamically\\ndetermine after each inference step whether to continue extrapolating\\nadditional information from the textual description. The effectiveness of the\\nframework is validated on three datasets. First, we introduce a synthetic\\ndataset, called CoSaL, to evaluate the end-to-end performance of our LBIE\\nsystem. Second, we show that the framework leads to state-of-the-art\\nperformance on image segmentation on the ReferIt dataset. Third, we present the\\nfirst language-based colorization result on the Oxford-102 Flowers dataset.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.06288v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.06288v2.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Jianbo Chen',\n",
       "   'Yelong Shen',\n",
       "   'Jianfeng Gao',\n",
       "   'Jingjing Liu',\n",
       "   'Xiaodong Liu'],\n",
       "  'tasks': ['Colorization', 'Semantic Segmentation'],\n",
       "  'date': '2017-11-16',\n",
       "  'methods': [{'name': 'Colorization',\n",
       "    'full_name': 'Colorization',\n",
       "    'description': '**Colorization** is a self-supervision approach that relies on colorization as the pretext task in order to learn image representations.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1603.08511v5',\n",
       "    'source_title': 'Colorful Image Colorization',\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Self-Supervised Learning',\n",
       "     'description': '**Self-Supervised Learning** refers to a category of methods where we learn representations in a self-supervised way (i.e without labels). These methods generally involve a pretext task that is solved to learn a good representation and a loss function to learn with. Below you can find a continuously updating list of self-supervised methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['visual-question-answering', 'oxford-102-flower'],\n",
       "  'datasets_used_full': ['Visual Question Answering', 'Oxford 102 Flower'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/erel-selection-using-morphological-relation',\n",
       "  'arxiv_id': '1806.03580',\n",
       "  'title': 'EREL Selection using Morphological Relation',\n",
       "  'abstract': 'This work concentrates on Extremal Regions of Extremum Level (EREL)\\nselection. EREL is a recently proposed feature detector aiming at detecting\\nregions from a set of extremal regions. This is a branching problem derived\\nfrom segmentation of arterial wall boundaries from Intravascular Ultrasound\\n(IVUS) images. For each IVUS frame, a set of EREL regions is generated to\\ndescribe the luminal area of human coronary. Each EREL is then fitted by an\\nellipse to represent the luminal border. The goal is to assign the most\\nappropriate EREL as the lumen. In this work, EREL selection carries out in two\\nrounds. In the first round, the pattern in a set of EREL regions is analyzed\\nand used to generate an approximate luminal region. Then, the two-dimensional\\n(2D) correlation coefficients are computed between this approximate region and\\neach EREL to keep the ones with tightest relevance. In the second round, a\\ncompactness measure is calculated for each EREL and its fitted ellipse to\\nguarantee that the resulting EREL has not affected by the common artifacts such\\nas bifurcations, shadows, and side branches. We evaluated the selected ERELs in\\nterms of Hausdorff Distance (HD) and Jaccard Measure (JM) on the train and test\\nset of a publicly available dataset. The results show that our selection\\nstrategy outperforms the current state-of-the-art.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03580v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03580v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yuying Li', 'Mehdi Faraji'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/adaptations-of-rouge-and-bleu-to-better',\n",
       "  'arxiv_id': '1806.03578',\n",
       "  'title': 'Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading Comprehension Task',\n",
       "  'abstract': 'Current evaluation metrics to question answering based machine reading\\ncomprehension (MRC) systems generally focus on the lexical overlap between the\\ncandidate and reference answers, such as ROUGE and BLEU. However, bias may\\nappear when these metrics are used for specific question types, especially\\nquestions inquiring yes-no opinions and entity lists. In this paper, we make\\nadaptations on the metrics to better correlate n-gram overlap with the human\\njudgment for answers to these two question types. Statistical analysis proves\\nthe effectiveness of our approach. Our adaptations may provide positive\\nguidance for the development of real-scene MRC systems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03578v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03578v1.pdf',\n",
       "  'proceeding': 'WS 2018 7',\n",
       "  'authors': ['An Yang', 'Kai Liu', 'Jing Liu', 'Yajuan Lyu', 'Sujian Li'],\n",
       "  'tasks': ['Machine Reading Comprehension',\n",
       "   'Question Answering',\n",
       "   'Reading Comprehension'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['squad', 'ms-marco', 'dureader'],\n",
       "  'datasets_used_full': ['SQuAD', 'MS MARCO', 'DuReader'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/generative-adversarial-nets-for-information',\n",
       "  'arxiv_id': '1806.03577',\n",
       "  'title': 'Generative Adversarial Nets for Information Retrieval: Fundamentals and Advances',\n",
       "  'abstract': 'Generative adversarial nets (GANs) have been widely studied during the recent\\ndevelopment of deep learning and unsupervised learning. With an adversarial\\ntraining mechanism, GAN manages to train a generative model to fit the\\nunderlying unknown real data distribution under the guidance of the\\ndiscriminative model estimating whether a data instance is real or generated.\\nSuch a framework is originally proposed for fitting continuous data\\ndistribution such as images, thus it is not straightforward to be directly\\napplied to information retrieval scenarios where the data is mostly discrete,\\nsuch as IDs, text and graphs. In this tutorial, we focus on discussing the GAN\\ntechniques and the variants on discrete data fitting in various information\\nretrieval scenarios. (i) We introduce the fundamentals of GAN framework and its\\ntheoretic properties; (ii) we carefully study the promising solutions to extend\\nGAN onto discrete data generation; (iii) we introduce IRGAN, the fundamental\\nGAN framework of fitting single ID data distribution and the direct application\\non information retrieval; (iv) we further discuss the task of sequential\\ndiscrete data generation tasks, e.g., text generation, and the corresponding\\nGAN solutions; (v) we present the most recent work on graph/network data\\nfitting with node embedding techniques by GANs. Meanwhile, we also introduce\\nthe relevant open-source platforms such as IRGAN and Texygen to help audience\\nconduct research experiments on GANs in information retrieval. Finally, we\\nconclude this tutorial with a comprehensive summarization and a prospect of\\nfurther research directions for GANs in information retrieval.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03577v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03577v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Wei-Nan Zhang'],\n",
       "  'tasks': ['Information Retrieval', 'Text Generation'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/instance-search-via-instance-level',\n",
       "  'arxiv_id': '1806.03576',\n",
       "  'title': 'Instance Search via Instance Level Segmentation and Feature Representation',\n",
       "  'abstract': 'Instance search is an interesting task as well as a challenging issue due to the lack of effective feature representation. In this paper, an instance level feature representation built upon fully convolutional instance-aware segmentation is proposed. The feature is ROI-pooled from the segmented instance region. So that instances in various sizes and layouts are represented by deep features in uniform length. This representation is further enhanced by the use of deformable ResNeXt blocks. Superior performance is observed in terms of its distinctiveness and scalability on a challenging evaluation dataset built by ourselves. In addition, the proposed enhancement on the network structure also shows superior performance on the instance segmentation task.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03576v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03576v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yu Zhan', 'Wan-Lei Zhao'],\n",
       "  'tasks': ['Instance Search',\n",
       "   'Instance Segmentation',\n",
       "   'Semantic Segmentation'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [{'name': 'Average Pooling',\n",
       "    'full_name': 'Average Pooling',\n",
       "    'description': '**Average Pooling** is a pooling operation that calculates the average value for patches of a feature map, and uses it to create a downsampled (pooled) feature map. It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs. It extracts features more smoothly than [Max Pooling](https://paperswithcode.com/method/max-pooling), whereas max pooling extracts more pronounced features like edges.\\r\\n\\r\\nImage Source: [here](https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'ResNeXt Block',\n",
       "    'full_name': 'ResNeXt Block',\n",
       "    'description': 'A **ResNeXt Block** is a type of [residual block](https://paperswithcode.com/method/residual-block) used as part of the [ResNeXt](https://paperswithcode.com/method/resnext) CNN architecture. It uses a \"split-transform-merge\" strategy (branched paths within a single module) similar to an [Inception module](https://paperswithcode.com/method/inception-module), i.e. it aggregates a set of transformations. Compared to a Residual Block, it exposes a new dimension,  *cardinality* (size of set of transformations) $C$, as an essential factor in addition to depth and width. \\r\\n\\r\\nFormally, a set of aggregated transformations can be represented as: $\\\\mathcal{F}(x)=\\\\sum_{i=1}^{C}\\\\mathcal{T}_i(x)$, where $\\\\mathcal{T}_i(x)$ can be an arbitrary function. Analogous to a simple neuron, $\\\\mathcal{T}_i$ should project $x$ into an (optionally low-dimensional) embedding and then transform it.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1611.05431v2',\n",
       "    'source_title': 'Aggregated Residual Transformations for Deep Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L75',\n",
       "    'main_collection': {'name': 'Skip Connection Blocks',\n",
       "     'description': \"**Skip Connection Blocks** are building blocks for neural networks that feature skip connections. These skip connections 'skip' some layers allowing gradients to better flow through the network. Below you will find a continuously updating list of skip connection blocks:\",\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Grouped Convolution',\n",
       "    'full_name': 'Grouped Convolution',\n",
       "    'description': 'A **Grouped Convolution** uses a group of convolutions - multiple kernels per layer - resulting in multiple channel outputs per layer. This leads to wider networks helping a network learn a varied set of low level and high level features. The original motivation of using Grouped Convolutions in [AlexNet](https://paperswithcode.com/method/alexnet) was to distribute the model over multiple GPUs as an engineering compromise. But later, with models such as [ResNeXt](https://paperswithcode.com/method/resnext), it was shown this module could be used to improve classification accuracy. Specifically by exposing a new dimension through grouped convolutions, *cardinality* (the size of set of transformations), we can increase accuracy by increasing it.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks',\n",
       "    'source_title': 'ImageNet Classification with Deep Convolutional Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/prlz77/ResNeXt.pytorch/blob/39fb8d03847f26ec02fb9b880ecaaa88db7a7d16/models/model.py#L42',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Global Average Pooling',\n",
       "    'full_name': 'Global Average Pooling',\n",
       "    'description': '**Global Average Pooling** is a pooling operation designed to replace fully connected layers in classical CNNs. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the [softmax](https://paperswithcode.com/method/softmax) layer. \\r\\n\\r\\nOne advantage of global [average pooling](https://paperswithcode.com/method/average-pooling) over the fully connected layers is that it is more native to the [convolution](https://paperswithcode.com/method/convolution) structure by enforcing correspondences between feature maps and categories. Thus the feature maps can be easily interpreted as categories confidence maps. Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. Furthermore, global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/baa592b215804927e28638f6a7f3318cbc411d49/torchvision/models/resnet.py#L157',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Residual Connection',\n",
       "    'full_name': 'Residual Connection',\n",
       "    'description': '**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Kaiming Initialization',\n",
       "    'full_name': 'Kaiming Initialization',\n",
       "    'description': '**Kaiming Initialization**, or **He Initialization**, is an initialization method for neural networks that takes into account the non-linearity of activation functions, such as [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nA proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. Using a derivation they work out that the condition to stop this happening is:\\r\\n\\r\\n$$\\\\frac{1}{2}n\\\\_{l}\\\\text{Var}\\\\left[w\\\\_{l}\\\\right] = 1 $$\\r\\n\\r\\nThis implies an initialization scheme of:\\r\\n\\r\\n$$ w\\\\_{l} \\\\sim \\\\mathcal{N}\\\\left(0,  2/n\\\\_{l}\\\\right)$$\\r\\n\\r\\nThat is, a zero-centered Gaussian with standard deviation of $\\\\sqrt{2/{n}\\\\_{l}}$ (variance shown in equation above). Biases are initialized at $0$.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.01852v1',\n",
       "    'source_title': 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/0adb5843766092fba584791af76383125fd0d01c/torch/nn/init.py#L389',\n",
       "    'main_collection': {'name': 'Initialization',\n",
       "     'description': '**Initialization** methods are used to initialize the weights in a neural network. Below can you find a continuously updating list of initialization methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': '1x1 Convolution',\n",
       "    'full_name': '1x1 Convolution',\n",
       "    'description': 'A **1 x 1 Convolution** is a [convolution](https://paperswithcode.com/method/convolution) with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an [MLP](https://paperswithcode.com/method/feedforward-network) looking at a particular pixel location.\\r\\n\\r\\nImage Credit: [http://deeplearning.ai](http://deeplearning.ai)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://www.healthnutra.org/es/maxup/',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Batch Normalization',\n",
       "    'full_name': 'Batch Normalization',\n",
       "    'description': '**Batch Normalization** aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for [Dropout](https://paperswithcode.com/method/dropout).\\r\\n\\r\\nWe apply a batch normalization layer as follows for a minibatch $\\\\mathcal{B}$:\\r\\n\\r\\n$$ \\\\mu\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}x\\\\_{i} $$\\r\\n\\r\\n$$ \\\\sigma^{2}\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}\\\\left(x\\\\_{i}-\\\\mu\\\\_{\\\\mathcal{B}}\\\\right)^{2} $$\\r\\n\\r\\n$$ \\\\hat{x}\\\\_{i} = \\\\frac{x\\\\_{i} - \\\\mu\\\\_{\\\\mathcal{B}}}{\\\\sqrt{\\\\sigma^{2}\\\\_{\\\\mathcal{B}}+\\\\epsilon}} $$\\r\\n\\r\\n$$ y\\\\_{i} = \\\\gamma\\\\hat{x}\\\\_{i} + \\\\beta = \\\\text{BN}\\\\_{\\\\gamma, \\\\beta}\\\\left(x\\\\_{i}\\\\right) $$\\r\\n\\r\\nWhere $\\\\gamma$ and $\\\\beta$ are learnable parameters.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.03167v3',\n",
       "    'source_title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/36f91261099b00194922bd93ed1286fe1c199724/jax/experimental/stax.py#L116',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ResNeXt',\n",
       "    'full_name': 'ResNeXt',\n",
       "    'description': 'A **ResNeXt** repeats a building block that aggregates a set of transformations with the same topology. Compared to a [ResNet](https://paperswithcode.com/method/resnet), it exposes a new dimension,  *cardinality* (the size of the set of transformations) $C$, as an essential factor in addition to the dimensions of depth and width. \\r\\n\\r\\nFormally, a set of aggregated transformations can be represented as: $\\\\mathcal{F}(x)=\\\\sum_{i=1}^{C}\\\\mathcal{T}_i(x)$, where $\\\\mathcal{T}_i(x)$ can be an arbitrary function. Analogous to a simple neuron, $\\\\mathcal{T}_i$ should project $x$ into an (optionally low-dimensional) embedding and then transform it.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1611.05431v2',\n",
       "    'source_title': 'Aggregated Residual Transformations for Deep Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/resnet.py#L124',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['coco', 'otb', 'otb-2015'],\n",
       "  'datasets_used_full': ['COCO', 'OTB', 'OTB-2015'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fmhash-deep-hashing-of-in-air-handwriting-for',\n",
       "  'arxiv_id': '1806.03574',\n",
       "  'title': 'FMHash: Deep Hashing of In-Air-Handwriting for User Identification',\n",
       "  'abstract': 'Many mobile systems and wearable devices, such as Virtual Reality (VR) or Augmented Reality (AR) headsets, lack a keyboard or touchscreen to type an ID and password for signing into a virtual website. However, they are usually equipped with gesture capture interfaces to allow the user to interact with the system directly with hand gestures. Although gesture-based authentication has been well-studied, less attention is paid to the gesture-based user identification problem, which is essentially an input method of account ID and an efficient searching and indexing method of a database of gesture signals. In this paper, we propose FMHash (i.e., Finger Motion Hash), a user identification framework that can generate a compact binary hash code from a piece of in-air-handwriting of an ID string. This hash code enables indexing and fast search of a large account database using the in-air-handwriting by a hash table. To demonstrate the effectiveness of the framework, we implemented a prototype and achieved >99.5% precision and >92.6% recall with exact hash code match on a dataset of 200 accounts collected by us. The ability of hashing in-air-handwriting pattern to binary code can be used to achieve convenient sign-in and sign-up with in-air-handwriting gesture ID on future mobile and wearable systems connected to the Internet.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03574v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03574v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Duo Lu', 'Dijiang Huang', 'Anshul Rai'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/stationary-geometric-graphical-model',\n",
       "  'arxiv_id': '1806.03571',\n",
       "  'title': 'Stationary Geometric Graphical Model Selection',\n",
       "  'abstract': 'We consider the problem of model selection in Gaussian Markov fields in the\\nsample deficient scenario. In many practically important cases, the underlying\\nnetworks are embedded into Euclidean spaces. Using the natural geometric\\nstructure, we introduce the notion of spatially stationary distributions over\\ngeometric graphs. This directly generalizes the notion of stationary time\\nseries to the multidimensional setting lacking time axis. We show that the idea\\nof spatial stationarity leads to a dramatic decrease in the sample complexity\\nof the model selection compared to abstract graphs with the same level of\\nsparsity. For geometric graphs on randomly spread vertices and edges of bounded\\nlength, we develop tight information-theoretic bounds on sample complexity and\\nshow that a finite number of independent samples is sufficient for a consistent\\nrecovery. Finally, we develop an efficient technique capable of reliably and\\nconsistently reconstructing graphs with a bounded number of measurements.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03571v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03571v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ilya Soloveychik', 'Vahid Tarokh'],\n",
       "  'tasks': ['Model Selection', 'Time Series'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/neural-factor-graph-models-for-cross-lingual',\n",
       "  'arxiv_id': '1805.04570',\n",
       "  'title': 'Neural Factor Graph Models for Cross-lingual Morphological Tagging',\n",
       "  'abstract': 'Morphological analysis involves predicting the syntactic traits of a word\\n(e.g. {POS: Noun, Case: Acc, Gender: Fem}). Previous work in morphological\\ntagging improves performance for low-resource languages (LRLs) through\\ncross-lingual training with a high-resource language (HRL) from the same\\nfamily, but is limited by the strict, often false, assumption that tag sets\\nexactly overlap between the HRL and LRL. In this paper we propose a method for\\ncross-lingual morphological tagging that aims to improve information sharing\\nbetween languages by relaxing this assumption. The proposed model uses\\nfactorial conditional random fields with neural network potentials, making it\\npossible to (1) utilize the expressive power of neural network representations\\nto smooth over superficial differences in the surface forms, (2) model pairwise\\nand transitive relationships between tags, and (3) accurately generate tag sets\\nthat are unseen or rare in the training data. Experiments on four languages\\nfrom the Universal Dependencies Treebank demonstrate superior tagging\\naccuracies over existing cross-lingual approaches.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.04570v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.04570v3.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Chaitanya Malaviya', 'Matthew R. Gormley', 'Graham Neubig'],\n",
       "  'tasks': ['Morphological Analysis', 'Morphological Tagging', 'POS', 'TAG'],\n",
       "  'date': '2018-05-11',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/explainable-recommendation-via-multi-task',\n",
       "  'arxiv_id': '1806.03568',\n",
       "  'title': 'Explainable Recommendation via Multi-Task Learning in Opinionated Text Data',\n",
       "  'abstract': \"Explaining automatically generated recommendations allows users to make more\\ninformed and accurate decisions about which results to utilize, and therefore\\nimproves their satisfaction. In this work, we develop a multi-task learning\\nsolution for explainable recommendation. Two companion learning tasks of user\\npreference modeling for recommendation} and \\\\textit{opinionated content\\nmodeling for explanation are integrated via a joint tensor factorization. As a\\nresult, the algorithm predicts not only a user's preference over a list of\\nitems, i.e., recommendation, but also how the user would appreciate a\\nparticular item at the feature level, i.e., opinionated textual explanation.\\nExtensive experiments on two large collections of Amazon and Yelp reviews\\nconfirmed the effectiveness of our solution in both recommendation and\\nexplanation tasks, compared with several existing recommendation algorithms.\\nAnd our extensive user study clearly demonstrates the practical value of the\\nexplainable recommendations generated by our algorithm.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03568v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03568v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Nan Wang', 'Hongning Wang', 'Yiling Jia', 'Yue Yin'],\n",
       "  'tasks': ['Multi-Task Learning'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/accurate-building-detection-in-vhr-remote',\n",
       "  'arxiv_id': '1806.00908',\n",
       "  'title': 'Accurate Building Detection in VHR Remote Sensing Images using Geometric Saliency',\n",
       "  'abstract': 'This paper aims to address the problem of detecting buildings from remote\\nsensing images with very high resolution (VHR). Inspired by the observation\\nthat buildings are always more distinguishable in geometries than in texture or\\nspectral, we propose a new geometric building index (GBI) for accurate building\\ndetection, which relies on the geometric saliency of building structures. The\\ngeometric saliency of buildings is derived from a mid-level geometric\\nrepresentations based on meaningful junctions that can locally describe\\nanisotropic geometrical structures of images. The resulting GBI is measured by\\nintegrating the derived geometric saliency of buildings. Experiments on three\\npublic datasets demonstrate that the proposed GBI achieves very promising\\nperformance, and meanwhile shows impressive generalization capability.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.00908v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.00908v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jin Huang', 'Gui-Song Xia', 'Fan Hu', 'Liangpei Zhang'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-04',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/rtseg-real-time-semantic-segmentation',\n",
       "  'arxiv_id': '1803.02758',\n",
       "  'title': 'RTSeg: Real-time Semantic Segmentation Comparative Study',\n",
       "  'abstract': 'Semantic segmentation benefits robotics related applications especially autonomous driving. Most of the research on semantic segmentation is only on increasing the accuracy of segmentation models with little attention to computationally efficient solutions. The few work conducted in this direction does not provide principled methods to evaluate the different design choices for segmentation. In this paper, we address this gap by presenting a real-time semantic segmentation benchmarking framework with a decoupled design for feature extraction and decoding methods. The framework is comprised of different network architectures for feature extraction such as VGG16, Resnet18, MobileNet, and ShuffleNet. It is also comprised of multiple meta-architectures for segmentation that define the decoding methodology. These include SkipNet, UNet, and Dilation Frontend. Experimental results are presented on the Cityscapes dataset for urban scenes. The modular design allows novel architectures to emerge, that lead to 143x GFLOPs reduction in comparison to SegNet. This benchmarking framework is publicly available at \"https://github.com/MSiam/TFSegmentation\".',\n",
       "  'url_abs': 'https://arxiv.org/abs/1803.02758v5',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1803.02758v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mennatullah Siam',\n",
       "   'Mostafa Gamal',\n",
       "   'Moemen Abdel-Razek',\n",
       "   'Senthil Yogamani',\n",
       "   'Martin Jagersand'],\n",
       "  'tasks': ['Autonomous Driving',\n",
       "   'Real-Time Semantic Segmentation',\n",
       "   'Semantic Segmentation'],\n",
       "  'date': '2018-03-07',\n",
       "  'methods': [{'name': '1x1 Convolution',\n",
       "    'full_name': '1x1 Convolution',\n",
       "    'description': 'A **1 x 1 Convolution** is a [convolution](https://paperswithcode.com/method/convolution) with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an [MLP](https://paperswithcode.com/method/feedforward-network) looking at a particular pixel location.\\r\\n\\r\\nImage Credit: [http://deeplearning.ai](http://deeplearning.ai)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://www.healthnutra.org/es/maxup/',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Depthwise Separable Convolution',\n",
       "    'full_name': 'Depthwise Separable Convolution',\n",
       "    'description': 'While [standard convolution](https://paperswithcode.com/method/convolution) performs the channelwise and spatial-wise computation in one step, **Depthwise Separable Convolution**  splits the computation into two steps: [depthwise convolution](https://paperswithcode.com/method/depthwise-convolution) applies a single convolutional filter per each input channel and [pointwise convolution](https://paperswithcode.com/method/pointwise-convolution) is used to create a linear combination of the output of the depthwise convolution. The comparison of standard convolution and depthwise separable convolution is shown to the right.\\r\\n\\r\\nCredit: [Depthwise Convolution Is All You Need for Learning Multiple Visual Domains](https://paperswithcode.com/paper/depthwise-convolution-is-all-you-need-for)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://openaccess.thecvf.com/content_cvpr_2017/html/Chollet_Xception_Deep_Learning_CVPR_2017_paper.html',\n",
       "    'source_title': 'Xception: Deep Learning With Depthwise Separable Convolutions',\n",
       "    'code_snippet_url': 'https://github.com/kwotsin/TensorFlow-Xception/blob/c42ad8cab40733f9150711be3537243278612b22/xception.py#L67',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'MobileNetV1',\n",
       "    'full_name': 'MobileNetV1',\n",
       "    'description': '**MobileNet** is a type of convolutional neural network designed for mobile and embedded vision applications. They are based on a streamlined architecture that uses depthwise separable convolutions to build lightweight deep neural networks that can have low latency for mobile and embedded devices.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1704.04861v1',\n",
       "    'source_title': 'MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications',\n",
       "    'code_snippet_url': 'https://github.com/osmr/imgclsmob/blob/956b4ebab0bbf98de4e1548287df5197a3c7154e/pytorch/pytorchcv/models/mobilenet.py#L14',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Bottleneck Residual Block',\n",
       "    'full_name': 'Bottleneck Residual Block',\n",
       "    'description': 'A **Bottleneck Residual Block** is a variant of the [residual block](https://paperswithcode.com/method/residual-block) that utilises 1x1 convolutions to create a bottleneck. The use of a bottleneck reduces the number of parameters and matrix multiplications. The idea is to make residual blocks as thin as possible to increase depth and have less parameters. They were introduced as part of the [ResNet](https://paperswithcode.com/method/resnet) architecture, and are used as part of deeper ResNets such as ResNet-50 and ResNet-101.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L75',\n",
       "    'main_collection': {'name': 'Skip Connection Blocks',\n",
       "     'description': \"**Skip Connection Blocks** are building blocks for neural networks that feature skip connections. These skip connections 'skip' some layers allowing gradients to better flow through the network. Below you will find a continuously updating list of skip connection blocks:\",\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Residual Block',\n",
       "    'full_name': 'Residual Block',\n",
       "    'description': \"**Residual Blocks** are skip-connection blocks that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. They were introduced as part of the [ResNet](https://paperswithcode.com/method/resnet) architecture.\\r\\n \\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$. The $\\\\mathcal{F}({x})$ acts like a residual, hence the name 'residual block'.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. Having skip connections allows the network to more easily learn identity-like mappings.\\r\\n\\r\\nNote that in practice, [Bottleneck Residual Blocks](https://paperswithcode.com/method/bottleneck-residual-block) are used for deeper ResNets, such as ResNet-50 and ResNet-101, as these bottleneck blocks are less computationally intensive.\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L35',\n",
       "    'main_collection': {'name': 'Skip Connection Blocks',\n",
       "     'description': \"**Skip Connection Blocks** are building blocks for neural networks that feature skip connections. These skip connections 'skip' some layers allowing gradients to better flow through the network. Below you will find a continuously updating list of skip connection blocks:\",\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ResNet',\n",
       "    'full_name': 'Residual Network',\n",
       "    'description': '**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack [residual blocks](https://paperswithcode.com/method/residual-block) ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}(x)$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}(x):=\\\\mathcal{H}(x)-x$. The original mapping is recast into $\\\\mathcal{F}(x)+x$.\\r\\n\\r\\nThere is empirical evidence that these types of network are easier to optimize, and can gain accuracy from considerably increased depth.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/resnet.py#L124',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Depthwise Convolution',\n",
       "    'full_name': 'Depthwise Convolution',\n",
       "    'description': '**Depthwise Convolution** is a type of convolution where we apply a single convolutional filter for each input channel. In the regular 2D [convolution](https://paperswithcode.com/method/convolution) performed over multiple input channels, the filter is as deep as the input and lets us freely mix channels to generate each element in the output. In contrast, depthwise convolutions keep each channel separate. To summarize the steps, we:\\r\\n\\r\\n1. Split the input and filter into channels.\\r\\n2. We convolve each input with the respective filter.\\r\\n3. We stack the convolved outputs together.\\r\\n\\r\\nImage Credit: [Chi-Feng Wang](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728)',\n",
       "    'introduced_year': 2016,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Pointwise Convolution',\n",
       "    'full_name': 'Pointwise Convolution',\n",
       "    'description': '**Pointwise Convolution** is a type of [convolution](https://paperswithcode.com/method/convolution) that uses a 1x1 kernel: a kernel that iterates through every single point. This kernel has a depth of however many channels the input image has. It can be used in conjunction with [depthwise convolutions](https://paperswithcode.com/method/depthwise-convolution) to produce an efficient class of convolutions known as [depthwise-separable convolutions](https://paperswithcode.com/method/depthwise-separable-convolution).\\r\\n\\r\\nImage Credit: [Chi-Feng Wang](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728)',\n",
       "    'introduced_year': 2016,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Residual Connection',\n",
       "    'full_name': 'Residual Connection',\n",
       "    'description': '**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \\r\\n\\r\\nFormally, denoting the desired underlying mapping as $\\\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\\\mathcal{F}({x}):=\\\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\\\mathcal{F}({x})+{x}$.\\r\\n\\r\\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1512.03385v1',\n",
       "    'source_title': 'Deep Residual Learning for Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118',\n",
       "    'main_collection': {'name': 'Skip Connections',\n",
       "     'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Average Pooling',\n",
       "    'full_name': 'Average Pooling',\n",
       "    'description': '**Average Pooling** is a pooling operation that calculates the average value for patches of a feature map, and uses it to create a downsampled (pooled) feature map. It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs. It extracts features more smoothly than [Max Pooling](https://paperswithcode.com/method/max-pooling), whereas max pooling extracts more pronounced features like edges.\\r\\n\\r\\nImage Source: [here](https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Channel Shuffle',\n",
       "    'full_name': 'Channel Shuffle',\n",
       "    'description': '**Channel Shuffle** is an operation to help information flow across feature channels in convolutional neural networks. It was used as part of the [ShuffleNet](https://paperswithcode.com/method/shufflenet) architecture. \\r\\n\\r\\nIf we allow a group [convolution](https://paperswithcode.com/method/convolution) to obtain input data from different groups, the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. \\r\\n\\r\\nThe above can be efficiently and elegantly implemented by a channel shuffle operation: suppose a convolutional layer with $g$ groups whose output has $g \\\\times n$ channels; we first reshape the output channel dimension into $\\\\left(g, n\\\\right)$, transposing and then flattening it back as the input of next layer. Channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1707.01083v2',\n",
       "    'source_title': 'ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices',\n",
       "    'code_snippet_url': 'https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/common.py#L862',\n",
       "    'main_collection': {'name': 'Miscellaneous Components',\n",
       "     'description': 'The following is a list of miscellaneous components used in neural networks.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ShuffleNet Block',\n",
       "    'full_name': 'ShuffleNet Block',\n",
       "    'description': 'A **ShuffleNet Block** is an image model block that utilises a [channel shuffle](https://paperswithcode.com/method/channel-shuffle) operation, along with depthwise convolutions, for an efficient architectural design. It was proposed as part of the [ShuffleNet](https://paperswithcode.com/method/shufflenet) architecture. The starting point is the [Residual Block](https://paperswithcode.com/method/residual-block) unit from [ResNets](https://paperswithcode.com/method/resnet), which is then modified with a pointwise group [convolution](https://paperswithcode.com/method/convolution) and a channel shuffle operation.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1707.01083v2',\n",
       "    'source_title': 'ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices',\n",
       "    'code_snippet_url': 'https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/shufflenet.py#L18',\n",
       "    'main_collection': {'name': 'Image Model Blocks',\n",
       "     'description': '**Image Model Blocks** are building blocks used in image models such as convolutional neural networks. Below you can find a continuously updating list of image model blocks.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Global Average Pooling',\n",
       "    'full_name': 'Global Average Pooling',\n",
       "    'description': '**Global Average Pooling** is a pooling operation designed to replace fully connected layers in classical CNNs. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the [softmax](https://paperswithcode.com/method/softmax) layer. \\r\\n\\r\\nOne advantage of global [average pooling](https://paperswithcode.com/method/average-pooling) over the fully connected layers is that it is more native to the [convolution](https://paperswithcode.com/method/convolution) structure by enforcing correspondences between feature maps and categories. Thus the feature maps can be easily interpreted as categories confidence maps. Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. Furthermore, global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.4400v3',\n",
       "    'source_title': 'Network In Network',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/baa592b215804927e28638f6a7f3318cbc411d49/torchvision/models/resnet.py#L157',\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Kaiming Initialization',\n",
       "    'full_name': 'Kaiming Initialization',\n",
       "    'description': '**Kaiming Initialization**, or **He Initialization**, is an initialization method for neural networks that takes into account the non-linearity of activation functions, such as [ReLU](https://paperswithcode.com/method/relu) activations.\\r\\n\\r\\nA proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. Using a derivation they work out that the condition to stop this happening is:\\r\\n\\r\\n$$\\\\frac{1}{2}n\\\\_{l}\\\\text{Var}\\\\left[w\\\\_{l}\\\\right] = 1 $$\\r\\n\\r\\nThis implies an initialization scheme of:\\r\\n\\r\\n$$ w\\\\_{l} \\\\sim \\\\mathcal{N}\\\\left(0,  2/n\\\\_{l}\\\\right)$$\\r\\n\\r\\nThat is, a zero-centered Gaussian with standard deviation of $\\\\sqrt{2/{n}\\\\_{l}}$ (variance shown in equation above). Biases are initialized at $0$.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.01852v1',\n",
       "    'source_title': 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/0adb5843766092fba584791af76383125fd0d01c/torch/nn/init.py#L389',\n",
       "    'main_collection': {'name': 'Initialization',\n",
       "     'description': '**Initialization** methods are used to initialize the weights in a neural network. Below can you find a continuously updating list of initialization methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Batch Normalization',\n",
       "    'full_name': 'Batch Normalization',\n",
       "    'description': '**Batch Normalization** aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for [Dropout](https://paperswithcode.com/method/dropout).\\r\\n\\r\\nWe apply a batch normalization layer as follows for a minibatch $\\\\mathcal{B}$:\\r\\n\\r\\n$$ \\\\mu\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}x\\\\_{i} $$\\r\\n\\r\\n$$ \\\\sigma^{2}\\\\_{\\\\mathcal{B}} = \\\\frac{1}{m}\\\\sum^{m}\\\\_{i=1}\\\\left(x\\\\_{i}-\\\\mu\\\\_{\\\\mathcal{B}}\\\\right)^{2} $$\\r\\n\\r\\n$$ \\\\hat{x}\\\\_{i} = \\\\frac{x\\\\_{i} - \\\\mu\\\\_{\\\\mathcal{B}}}{\\\\sqrt{\\\\sigma^{2}\\\\_{\\\\mathcal{B}}+\\\\epsilon}} $$\\r\\n\\r\\n$$ y\\\\_{i} = \\\\gamma\\\\hat{x}\\\\_{i} + \\\\beta = \\\\text{BN}\\\\_{\\\\gamma, \\\\beta}\\\\left(x\\\\_{i}\\\\right) $$\\r\\n\\r\\nWhere $\\\\gamma$ and $\\\\beta$ are learnable parameters.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1502.03167v3',\n",
       "    'source_title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/36f91261099b00194922bd93ed1286fe1c199724/jax/experimental/stax.py#L116',\n",
       "    'main_collection': {'name': 'Normalization',\n",
       "     'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Dense Connections',\n",
       "    'full_name': 'Dense Connections',\n",
       "    'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\\\_{\\\\text{inputs}}*n\\\\_{\\\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\\r\\n\\r\\n$$h\\\\_{l} = g\\\\left(\\\\textbf{W}^{T}h\\\\_{l-1}\\\\right)$$\\r\\n\\r\\nwhere $g$ is an activation function.\\r\\n\\r\\nImage Source: Deep Learning by Goodfellow, Bengio and Courville',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'SegNet',\n",
       "    'full_name': 'SegNet',\n",
       "    'description': '**SegNet** is a semantic segmentation model. This core trainable segmentation architecture consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the\\r\\nVGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature maps. Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to\\r\\nperform non-linear upsampling.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1511.00561v3',\n",
       "    'source_title': 'SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation',\n",
       "    'code_snippet_url': 'https://github.com/yassouali/pytorch_segmentation/blob/8b8e3ee20a3aa733cb19fc158ad5d7773ed6da7f/models/segnet.py#L9',\n",
       "    'main_collection': {'name': 'Semantic Segmentation Models',\n",
       "     'description': '**Semantic Segmentation Models** are a class of methods that address the task of semantically segmenting an image into different object classes. Below you can find a continuously updating list of semantic segmentation models. ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ShuffleNet',\n",
       "    'full_name': 'ShuffleNet',\n",
       "    'description': '**ShuffleNet** is a convolutional neural network designed specially for mobile devices with very limited computing power. The architecture utilizes two new operations, pointwise group [convolution](https://paperswithcode.com/method/convolution) and [channel shuffle](https://paperswithcode.com/method/channel-shuffle), to reduce computation cost while maintaining accuracy.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1707.01083v2',\n",
       "    'source_title': 'ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices',\n",
       "    'code_snippet_url': 'https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/shufflenet.py#L123',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['cityscapes'],\n",
       "  'datasets_used_full': ['Cityscapes'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/generic-coreset-for-scalable-learning-of',\n",
       "  'arxiv_id': '1802.07382',\n",
       "  'title': 'Generic Coreset for Scalable Learning of Monotonic Kernels: Logistic Regression, Sigmoid and more',\n",
       "  'abstract': 'Coreset (or core-set) is a small weighted \\\\emph{subset} $Q$ of an input set $P$ with respect to a given \\\\emph{monotonic} function $f:\\\\mathbb{R}\\\\to\\\\mathbb{R}$ that \\\\emph{provably} approximates its fitting loss $\\\\sum_{p\\\\in P}f(p\\\\cdot x)$ to \\\\emph{any} given $x\\\\in\\\\mathbb{R}^d$. Using $Q$ we can obtain approximation of $x^*$ that minimizes this loss, by running \\\\emph{existing} optimization algorithms on $Q$. In this work we provide: (i) A lower bound which proves that there are sets with no coresets smaller than $n=|P|$ for general monotonic loss functions. (ii) A proof that, under a natural assumption that holds e.g. for logistic regression and the sigmoid activation functions, a small coreset exists for \\\\emph{any} input $P$. (iii) A generic coreset construction algorithm that computes such a small coreset $Q$ in $O(nd+n\\\\log n)$ time, and (iv) Experimental results which demonstrate that our coresets are effective and are much smaller in practice than predicted in theory.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1802.07382v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1802.07382v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Elad Tolochinsky', 'Ibrahim Jubran', 'Dan Feldman'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-21',\n",
       "  'methods': [{'name': 'Coresets',\n",
       "    'full_name': 'Coresets',\n",
       "    'description': '',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1708.00489v4',\n",
       "    'source_title': 'Active Learning for Convolutional Neural Networks: A Core-Set Approach',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Clustering',\n",
       "     'description': '**Clustering** methods cluster a dataset so that similar datapoints are located in the same group. Below you can find a continuously updating list of clustering methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Sigmoid Activation',\n",
       "    'full_name': 'Sigmoid Activation',\n",
       "    'description': '**Sigmoid Activations** are a type of activation function for neural networks:\\r\\n\\r\\n$$f\\\\left(x\\\\right) = \\\\frac{1}{\\\\left(1+\\\\exp\\\\left(-x\\\\right)\\\\right)}$$\\r\\n\\r\\nSome drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/building-bayesian-neural-networks-with-blocks',\n",
       "  'arxiv_id': '1806.03563',\n",
       "  'title': 'Building Bayesian Neural Networks with Blocks: On Structure, Interpretability and Uncertainty',\n",
       "  'abstract': 'We provide simple schemes to build Bayesian Neural Networks (BNNs), block by\\nblock, inspired by a recent idea of computation skeletons. We show how by\\nadjusting the types of blocks that are used within the computation skeleton, we\\ncan identify interesting relationships with Deep Gaussian Processes (DGPs),\\ndeep kernel learning (DKL), random features type approximation and other\\ntopics. We give strategies to approximate the posterior via doubly stochastic\\nvariational inference for such models which yield uncertainty estimates. We\\ngive a detailed theoretical analysis and point out extensions that may be of\\nindependent interest. As a special case, we instantiate our procedure to define\\na Bayesian {\\\\em additive} Neural network -- a promising strategy to identify\\nstatistical interactions and has direct benefits for obtaining interpretable\\nmodels.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03563v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03563v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hao Henry Zhou', 'Yunyang Xiong', 'Vikas Singh'],\n",
       "  'tasks': ['Gaussian Processes', 'Variational Inference'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/what-knowledge-is-needed-to-solve-the-rte5',\n",
       "  'arxiv_id': '1806.03561',\n",
       "  'title': 'What Knowledge is Needed to Solve the RTE5 Textual Entailment Challenge?',\n",
       "  'abstract': 'This document gives a knowledge-oriented analysis of about 20 interesting\\nRecognizing Textual Entailment (RTE) examples, drawn from the 2005 RTE5\\ncompetition test set. The analysis ignores shallow statistical matching\\ntechniques between T and H, and rather asks: What would it take to reasonably\\ninfer that T implies H? What world knowledge would be needed for this task?\\nAlthough such knowledge-intensive techniques have not had much success in RTE\\nevaluations, ultimately an intelligent system should be expected to know and\\ndeploy this kind of world knowledge required to perform this kind of reasoning.\\n  The selected examples are typically ones which our RTE system (called BLUE)\\ngot wrong and ones which require world knowledge to answer. In particular, the\\nanalysis covers cases where there was near-perfect lexical overlap between T\\nand H, yet the entailment was NO, i.e., examples that most likely all current\\nRTE systems will have got wrong. A nice example is #341 (page 26), that\\nrequires inferring from \"a river floods\" that \"a river overflows its banks\".\\nSeems it should be easy, right? Enjoy!',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03561v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03561v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Peter Clark'],\n",
       "  'tasks': ['Natural Language Inference'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [{'name': 'Affine Coupling',\n",
       "    'full_name': 'Affine Coupling',\n",
       "    'description': '**Affine Coupling** is a method for implementing a normalizing flow (where we stack a sequence of invertible bijective transformation functions). Affine coupling is one of these bijective transformation functions. Specifically, it is an example of a reversible transformation where the forward function, the reverse function and the log-determinant are computationally efficient. For the forward function, we split the input dimension into two parts:\\r\\n\\r\\n$$ \\\\mathbf{x}\\\\_{a}, \\\\mathbf{x}\\\\_{b} = \\\\text{split}\\\\left(\\\\mathbf{x}\\\\right) $$\\r\\n\\r\\nThe second part stays the same $\\\\mathbf{x}\\\\_{b} = \\\\mathbf{y}\\\\_{b}$, while the first part  $\\\\mathbf{x}\\\\_{a}$ undergoes an affine transformation, where the parameters for this transformation are learnt using the second part $\\\\mathbf{x}\\\\_{b}$ being put through a neural network. Together we have:\\r\\n\\r\\n$$ \\\\left(\\\\log{\\\\mathbf{s}, \\\\mathbf{t}}\\\\right) = \\\\text{NN}\\\\left(\\\\mathbf{x}\\\\_{b}\\\\right) $$\\r\\n\\r\\n$$ \\\\mathbf{s} = \\\\exp\\\\left(\\\\log{\\\\mathbf{s}}\\\\right) $$\\r\\n\\r\\n$$ \\\\mathbf{y}\\\\_{a} = \\\\mathbf{s} \\\\odot \\\\mathbf{x}\\\\_{a} + \\\\mathbf{t}  $$\\r\\n\\r\\n$$ \\\\mathbf{y}\\\\_{b} = \\\\mathbf{x}\\\\_{b} $$\\r\\n\\r\\n$$ \\\\mathbf{y} = \\\\text{concat}\\\\left(\\\\mathbf{y}\\\\_{a}, \\\\mathbf{y}\\\\_{b}\\\\right) $$\\r\\n\\r\\nImage: [GLOW](https://paperswithcode.com/method/glow)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1410.8516v6',\n",
       "    'source_title': 'NICE: Non-linear Independent Components Estimation',\n",
       "    'code_snippet_url': 'https://github.com/paultsw/nice_pytorch/blob/15cfc543fc3dc81ee70398b8dfc37b67269ede95/nice/layers.py#L109',\n",
       "    'main_collection': {'name': 'Bijective Transformation',\n",
       "     'description': '**Bijective Transformations** are transformations that are bijective, i.e. they can be reversed. They are used within the context of normalizing flow models. Below you can find a continuously updating list of bijective transformation methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Normalizing Flows',\n",
       "    'full_name': 'Normalizing Flows',\n",
       "    'description': \"**Normalizing Flows** are a method for constructing complex distributions by transforming a\\r\\nprobability density through a series of invertible mappings. By repeatedly applying the rule for change of variables, the initial density ‘flows’ through the sequence of invertible mappings. At the end of this sequence we obtain a valid probability distribution and hence this type of flow is referred to as a normalizing flow.\\r\\n\\r\\nIn the case of finite flows, the basic rule for the transformation of densities considers an invertible, smooth mapping $f : \\\\mathbb{R}^{d} \\\\rightarrow \\\\mathbb{R}^{d}$ with inverse $f^{-1} = g$, i.e. the composition $g \\\\cdot f\\\\left(z\\\\right) = z$. If we use this mapping to transform a random variable $z$ with distribution $q\\\\left(z\\\\right)$, the resulting random variable $z' = f\\\\left(z\\\\right)$ has a distribution:\\r\\n\\r\\n$$ q\\\\left(\\\\mathbf{z}'\\\\right) = q\\\\left(\\\\mathbf{z}\\\\right)\\\\bigl\\\\vert{\\\\text{det}}\\\\frac{\\\\delta{f}^{-1}}{\\\\delta{\\\\mathbf{z'}}}\\\\bigr\\\\vert = q\\\\left(\\\\mathbf{z}\\\\right)\\\\bigl\\\\vert{\\\\text{det}}\\\\frac{\\\\delta{f}}{\\\\delta{\\\\mathbf{z}}}\\\\bigr\\\\vert ^{-1} $$\\r\\n\\x0c\\r\\nwhere the last equality can be seen by applying the chain rule (inverse function theorem) and is a property of Jacobians of invertible functions. We can construct arbitrarily complex densities by composing several simple maps and successively applying the above equation. The density $q\\\\_{K}\\\\left(\\\\mathbf{z}\\\\right)$ obtained by successively transforming a random variable $z\\\\_{0}$ with distribution $q\\\\_{0}$ through a chain of $K$ transformations $f\\\\_{k}$ is:\\r\\n\\r\\n$$ z\\\\_{K} = f\\\\_{K} \\\\cdot \\\\dots \\\\cdot f\\\\_{2} \\\\cdot f\\\\_{1}\\\\left(z\\\\_{0}\\\\right) $$\\r\\n\\r\\n$$ \\\\ln{q}\\\\_{K}\\\\left(z\\\\_{K}\\\\right) = \\\\ln{q}\\\\_{0}\\\\left(z\\\\_{0}\\\\right) − \\\\sum^{K}\\\\_{k=1}\\\\ln\\\\vert\\\\det\\\\frac{\\\\delta{f\\\\_{k}}}{\\\\delta{\\\\mathbf{z\\\\_{k-1}}}}\\\\vert $$\\r\\n\\x0c\\r\\nThe path traversed by the random variables $z\\\\_{k} = f\\\\_{k}\\\\left(z\\\\_{k-1}\\\\right)$ with initial distribution $q\\\\_{0}\\\\left(z\\\\_{0}\\\\right)$ is called the flow and the path formed by the successive distributions $q\\\\_{k}$ is a normalizing flow.\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1505.05770v6',\n",
       "    'source_title': 'Variational Inference with Normalizing Flows',\n",
       "    'code_snippet_url': 'https://github.com/ex4sperans/variational-inference-with-normalizing-flows/blob/922b569f851e02fa74700cd0754fe2ef5c1f3180/flow.py#L9',\n",
       "    'main_collection': {'name': 'Distribution Approximation',\n",
       "     'description': '**Distribution Approximation** methods aim to approximate a complex distribution. Below you can find a continuously updating list of distribution approximation methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/semantic-correspondence-a-hierarchical',\n",
       "  'arxiv_id': '1806.03560',\n",
       "  'title': 'Semantic Correspondence: A Hierarchical Approach',\n",
       "  'abstract': 'Establishing semantic correspondence across images when the objects in the\\nimages have undergone complex deformations remains a challenging task in the\\nfield of computer vision. In this paper, we propose a hierarchical method to\\ntackle this problem by first semantically targeting the foreground objects to\\nlocalize the search space and then looking deeply into multiple levels of the\\nfeature representation to search for point-level correspondence. In contrast to\\nexisting approaches, which typically penalize large discrepancies, our approach\\nallows for significant displacements, with the aim to accommodate large\\ndeformations of the objects in scene. Localizing the search space by\\nsemantically matching object-level correspondence, our method robustly handles\\nlarge deformations of objects. Representing the target region by concatenated\\nhypercolumn features which take into account the hierarchical levels of the\\nsurrounding context, helps to clear the ambiguity to further improve the\\naccuracy. By conducting multiple experiments across scenes with non-rigid\\nobjects, we validate the proposed approach, and show that it outperforms the\\nstate of the art methods for semantic correspondence establishment.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03560v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03560v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Akila Pemasiri',\n",
       "   'Kien Nguyen',\n",
       "   'Sridha Sridhara',\n",
       "   'and Clinton Fookes'],\n",
       "  'tasks': ['Semantic correspondence'],\n",
       "  'date': '2018-06-10',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/genesis-of-basic-and-multi-layer-echo-state',\n",
       "  'arxiv_id': '1804.08996',\n",
       "  'title': 'Genesis of Basic and Multi-Layer Echo State Network Recurrent Autoencoders for Efficient Data Representations',\n",
       "  'abstract': \"It is a widely accepted fact that data representations intervene noticeably\\nin machine learning tools. The more they are well defined the better the\\nperformance results are. Feature extraction-based methods such as autoencoders\\nare conceived for finding more accurate data representations from the original\\nones. They efficiently perform on a specific task in terms of 1) high accuracy,\\n2) large short term memory and 3) low execution time. Echo State Network (ESN)\\nis a recent specific kind of Recurrent Neural Network which presents very rich\\ndynamics thanks to its reservoir-based hidden layer. It is widely used in\\ndealing with complex non-linear problems and it has outperformed classical\\napproaches in a number of tasks including regression, classification, etc. In\\nthis paper, the noticeable dynamism and the large memory provided by ESN and\\nthe strength of Autoencoders in feature extraction are gathered within an ESN\\nRecurrent Autoencoder (ESN-RAE). In order to bring up sturdier alternative to\\nconventional reservoir-based networks, not only single layer basic ESN is used\\nas an autoencoder, but also Multi-Layer ESN (ML-ESN-RAE). The new features,\\nonce extracted from ESN's hidden layer, are applied to classification tasks.\\nThe classification rates rise considerably compared to those obtained when\\napplying the original data features. An accuracy-based comparison is performed\\nbetween the proposed recurrent AEs and two variants of an ELM feed-forward AEs\\n(Basic and ML) in both of noise free and noisy environments. The empirical\\nstudy reveals the main contribution of recurrent connections in improving the\\nclassification performance results.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.08996v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.08996v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Naima Chouikhi', 'Boudour Ammar', 'Adel M. ALIMI'],\n",
       "  'tasks': ['Classification', 'General Classification'],\n",
       "  'date': '2018-04-24',\n",
       "  'methods': [{'name': 'AutoEncoder',\n",
       "    'full_name': 'AutoEncoder',\n",
       "    'description': 'An **Autoencoder** is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then performs a reconstruction of the input with this latent code (the decoder).\\r\\n\\r\\nImage: [Michael Massi](https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://science.sciencemag.org/content/313/5786/504',\n",
       "    'source_title': 'Reducing the Dimensionality of Data with Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/L1aoXingyu/pytorch-beginner/blob/9c86be785c7c318a09cf29112dd1f1a58613239b/08-AutoEncoder/simple_autoencoder.py#L38',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/sparse-over-complete-patch-matching',\n",
       "  'arxiv_id': '1806.03556',\n",
       "  'title': 'Sparse Over-complete Patch Matching',\n",
       "  'abstract': 'Image patch matching, which is the process of identifying corresponding\\npatches across images, has been used as a subroutine for many computer vision\\nand image processing tasks. State -of-the-art patch matching techniques take\\nimage patches as input to a convolutional neural network to extract the patch\\nfeatures and evaluate their similarity. Our aim in this paper is to improve on\\nthe state of the art patch matching techniques by observing the fact that a\\nsparse-overcomplete representation of an image posses statistical properties of\\nnatural visual scenes which can be exploited for patch matching. We propose a\\nnew paradigm which encodes image patch details by encoding the patch and\\nsubsequently using this sparse representation as input to a neural network to\\ncompare the patches. As sparse coding is based on a generative model of natural\\nimage patches, it can represent the patch in terms of the fundamental visual\\ncomponents from which it has been composed of, leading to similar sparse codes\\nfor patches which are built from similar components. Once the sparse coded\\nfeatures are extracted, we employ a fully-connected neural network, which\\ncaptures the non-linear relationships between features, for comparison. We have\\nevaluated our approach using the Liberty and Notredame subsets of the popular\\nUBC patch dataset and set a new benchmark outperforming all state-of-the-art\\npatch matching techniques for these datasets.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03556v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03556v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Akila Pemasiri',\n",
       "   'Kien Nguyen',\n",
       "   'Sridha Sridharan',\n",
       "   'Clinton Fookes'],\n",
       "  'tasks': ['Patch Matching'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/consistent-position-bias-estimation-without',\n",
       "  'arxiv_id': '1806.03555',\n",
       "  'title': 'Consistent Position Bias Estimation without Online Interventions for Learning-to-Rank',\n",
       "  'abstract': 'Presentation bias is one of the key challenges when learning from implicit\\nfeedback in search engines, as it confounds the relevance signal with\\nuninformative signals due to position in the ranking, saliency, and other\\npresentation factors. While it was recently shown how counterfactual\\nlearning-to-rank (LTR) approaches \\\\cite{Joachims/etal/17a} can provably\\novercome presentation bias if observation propensities are known, it remains to\\nshow how to accurately estimate these propensities. In this paper, we propose\\nthe first method for producing consistent propensity estimates without manual\\nrelevance judgments, disruptive interventions, or restrictive relevance\\nmodeling assumptions. We merely require that we have implicit feedback data\\nfrom multiple different ranking functions. Furthermore, we argue that our\\nestimation technique applies to an extended class of Contextual Position-Based\\nPropensity Models, where propensities not only depend on position but also on\\nobservable features of the query and document. Initial simulation studies\\nconfirm that the approach is scalable, accurate, and robust.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03555v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03555v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Aman Agarwal', 'Ivan Zaitsev', 'Thorsten Joachims'],\n",
       "  'tasks': ['Learning-To-Rank'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/an-estimation-and-analysis-framework-for-the',\n",
       "  'arxiv_id': '1806.03551',\n",
       "  'title': 'An Estimation and Analysis Framework for the Rasch Model',\n",
       "  'abstract': 'The Rasch model is widely used for item response analysis in applications\\nranging from recommender systems to psychology, education, and finance. While a\\nnumber of estimators have been proposed for the Rasch model over the last\\ndecades, the available analytical performance guarantees are mostly asymptotic.\\nThis paper provides a framework that relies on a novel linear minimum\\nmean-squared error (L-MMSE) estimator which enables an exact, nonasymptotic,\\nand closed-form analysis of the parameter estimation error under the Rasch\\nmodel. The proposed framework provides guidelines on the number of items and\\nresponses required to attain low estimation errors in tests or surveys. We\\nfurthermore demonstrate its efficacy on a number of real-world collaborative\\nfiltering datasets, which reveals that the proposed L-MMSE estimator performs\\non par with state-of-the-art nonlinear estimators in terms of predictive\\nperformance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03551v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03551v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Andrew S. Lan', 'Mung Chiang', 'Christoph Studer'],\n",
       "  'tasks': ['Collaborative Filtering', 'Recommendation Systems'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/linear-spectral-estimators-and-an-application',\n",
       "  'arxiv_id': '1806.03547',\n",
       "  'title': 'Linear Spectral Estimators and an Application to Phase Retrieval',\n",
       "  'abstract': 'Phase retrieval refers to the problem of recovering real- or complex-valued\\nvectors from magnitude measurements. The best-known algorithms for this problem\\nare iterative in nature and rely on so-called spectral initializers that\\nprovide accurate initialization vectors. We propose a novel class of estimators\\nsuitable for general nonlinear measurement systems, called linear spectral\\nestimators (LSPEs), which can be used to compute accurate initialization\\nvectors for phase retrieval problems. The proposed LSPEs not only provide\\naccurate initialization vectors for noisy phase retrieval systems with\\nstructured or random measurement matrices, but also enable the derivation of\\nsharp and nonasymptotic mean-squared error bounds. We demonstrate the efficacy\\nof LSPEs on synthetic and real-world phase retrieval problems, and show that\\nour estimators significantly outperform existing methods for structured\\nmeasurement systems that arise in practice.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03547v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03547v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Ramina Ghods',\n",
       "   'Andrew S. Lan',\n",
       "   'Tom Goldstein',\n",
       "   'Christoph Studer'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/hierarchical-bi-level-multi-objective',\n",
       "  'arxiv_id': '1806.01016',\n",
       "  'title': 'Hierarchical Bi-level Multi-Objective Evolution of Single- and Multi-layer Echo State Network Autoencoders for Data Representations',\n",
       "  'abstract': \"Echo State Network (ESN) presents a distinguished kind of recurrent neural\\nnetworks. It is built upon a sparse, random and large hidden infrastructure\\ncalled reservoir. ESNs have succeeded in dealing with several non-linear\\nproblems such as prediction, classification, etc. Thanks to its rich dynamics,\\nESN is used as an Autoencoder (AE) to extract features from original data\\nrepresentations. ESN is not only used with its basic single layer form but also\\nwith the recently proposed Multi-Layer (ML) architecture. The well setting of\\nESN (basic and ML) architectures and training parameters is a crucial and hard\\nlabor task. Generally, a number of parameters (hidden neurons, sparsity rates,\\ninput scaling) is manually altered to achieve minimum learning error. However,\\nthis randomly hand crafted task, on one hand, may not guarantee best training\\nresults and on the other hand, it can raise the network's complexity. In this\\npaper, a hierarchical bi-level evolutionary optimization is proposed to deal\\nwith these issues. The first level includes a multi-objective architecture\\noptimization providing maximum learning accuracy while sustaining the\\ncomplexity at a reduced standard. Multi-objective Particle Swarm Optimization\\n(MOPSO) is used to optimize ESN structure in a way to provide a trade-off\\nbetween the network complexity decreasing and the accuracy increasing. A\\npareto-front of optimal solutions is generated by the end of the MOPSO process.\\nThese solutions present the set of candidates that succeeded in providing a\\ncompromise between different objectives (learning error and network\\ncomplexity). At the second level, each of the solutions already found undergo a\\nmono-objective weights optimization to enhance the obtained pareto-front.\\nEmpirical results ensure the effectiveness of the evolved ESN recurrent AEs\\n(basic and ML) for noisy and noise free data.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.01016v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.01016v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Naima Chouikhi', 'Boudour Ammar', 'Adel M. ALIMI'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-04',\n",
       "  'methods': [{'name': 'AutoEncoder',\n",
       "    'full_name': 'AutoEncoder',\n",
       "    'description': 'An **Autoencoder** is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then performs a reconstruction of the input with this latent code (the decoder).\\r\\n\\r\\nImage: [Michael Massi](https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://science.sciencemag.org/content/313/5786/504',\n",
       "    'source_title': 'Reducing the Dimensionality of Data with Neural Networks',\n",
       "    'code_snippet_url': 'https://github.com/L1aoXingyu/pytorch-beginner/blob/9c86be785c7c318a09cf29112dd1f1a58613239b/08-AutoEncoder/simple_autoencoder.py#L38',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/not-all-samples-are-created-equal-deep',\n",
       "  'arxiv_id': '1803.00942',\n",
       "  'title': 'Not All Samples Are Created Equal: Deep Learning with Importance Sampling',\n",
       "  'abstract': 'Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on \"informative\" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5% and 17%.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1803.00942v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1803.00942v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Angelos Katharopoulos', 'François Fleuret'],\n",
       "  'tasks': ['Image Classification'],\n",
       "  'date': '2018-03-02',\n",
       "  'methods': [{'name': 'SGD',\n",
       "    'full_name': 'Stochastic Gradient Descent',\n",
       "    'description': '**Stochastic Gradient Descent** is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:\\r\\n\\r\\n$$ w\\\\_{t+1} = w\\\\_{t} - \\\\eta\\\\hat{\\\\nabla}\\\\_{w}{L(w\\\\_{t})} $$\\r\\n\\r\\nWhere $\\\\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.\\r\\n\\r\\n(Image Source: [here](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/))',\n",
       "    'introduced_year': 1951,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/4e0ac120e9a8b096069c2f892488d630a5c8f358/torch/optim/sgd.py#L97-L112',\n",
       "    'main_collection': {'name': 'Stochastic Optimization',\n",
       "     'description': \"**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms.\",\n",
       "     'parent': 'Optimization',\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['cifar-10', 'imagenet', 'cifar-100'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'ImageNet', 'CIFAR-100'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/representation-learning-on-graphs-with',\n",
       "  'arxiv_id': '1806.03536',\n",
       "  'title': 'Representation Learning on Graphs with Jumping Knowledge Networks',\n",
       "  'abstract': 'Recent deep learning approaches for representation learning on graphs follow\\na neighborhood aggregation procedure. We analyze some important properties of\\nthese models, and propose a strategy to overcome those. In particular, the\\nrange of \"neighboring\" nodes that a node\\'s representation draws from strongly\\ndepends on the graph structure, analogous to the spread of a random walk. To\\nadapt to local neighborhood properties and tasks, we explore an architecture --\\njumping knowledge (JK) networks -- that flexibly leverages, for each node,\\ndifferent neighborhood ranges to enable better structure-aware representation.\\nIn a number of experiments on social, bioinformatics and citation networks, we\\ndemonstrate that our model achieves state-of-the-art performance. Furthermore,\\ncombining the JK framework with models like Graph Convolutional Networks,\\nGraphSAGE and Graph Attention Networks consistently improves those models\\'\\nperformance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03536v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03536v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Keyulu Xu',\n",
       "   'Chengtao Li',\n",
       "   'Yonglong Tian',\n",
       "   'Tomohiro Sonobe',\n",
       "   'Ken-ichi Kawarabayashi',\n",
       "   'Stefanie Jegelka'],\n",
       "  'tasks': ['Graph Attention',\n",
       "   'Node Classification',\n",
       "   'Node Property Prediction',\n",
       "   'Representation Learning'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [{'name': 'Graph Convolutional Networks',\n",
       "    'full_name': 'Graph Convolutional Networks',\n",
       "    'description': 'A Graph Convolutional Network, or GCN, is an approach for semi-supervised learning on graph-structured data. It is based on an efficient variant of convolutional neural networks which operate directly on graphs.\\r\\n\\r\\nImage source: [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/pdf/1609.02907v4.pdf)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1609.02907v4',\n",
       "    'source_title': 'Semi-Supervised Classification with Graph Convolutional Networks',\n",
       "    'code_snippet_url': '',\n",
       "    'main_collection': {'name': 'Graph Models',\n",
       "     'description': 'The Graph Methods include neural network architectures for learning on graphs with prior structure information, popularly called as Graph Neural Networks (GNNs).\\r\\n\\r\\nRecently, deep learning approaches are being extended to work on graph-structured data, giving rise to a series of graph neural networks addressing different challenges. Graph neural networks are particularly useful in applications where data are generated from non-Euclidean domains and represented as graphs with complex relationships. \\r\\n\\r\\nSome tasks where GNNs are widely used include [node classification](https://paperswithcode.com/task/node-classification), [graph classification](https://paperswithcode.com/task/graph-classification), [link prediction](https://paperswithcode.com/task/link-prediction), and much more. \\r\\n\\r\\nIn the taxonomy presented by [Wu et al. (2019)](https://paperswithcode.com/paper/a-comprehensive-survey-on-graph-neural), graph neural networks can be divided into four categories: **recurrent graph neural networks**, **convolutional graph neural networks**, **graph autoencoders**, and **spatial-temporal graph neural networks**.\\r\\n\\r\\nImage source: [A Comprehensive Survey on Graph NeuralNetworks](https://arxiv.org/pdf/1901.00596.pdf)',\n",
       "     'parent': None,\n",
       "     'area': 'Graphs'}}],\n",
       "  'datasets_used_lower': ['ogb', 'reddit', 'ppi'],\n",
       "  'datasets_used_full': ['OGB', 'Reddit', 'PPI'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/cell-detection-with-star-convex-polygons',\n",
       "  'arxiv_id': '1806.03535',\n",
       "  'title': 'Cell Detection with Star-convex Polygons',\n",
       "  'abstract': 'Automatic detection and segmentation of cells and nuclei in microscopy images\\nis important for many biological applications. Recent successful learning-based\\napproaches include per-pixel cell segmentation with subsequent pixel grouping,\\nor localization of bounding boxes with subsequent shape refinement. In\\nsituations of crowded cells, these can be prone to segmentation errors, such as\\nfalsely merging bordering cells or suppressing valid cell instances due to the\\npoor approximation with bounding boxes. To overcome these issues, we propose to\\nlocalize cell nuclei via star-convex polygons, which are a much better shape\\nrepresentation as compared to bounding boxes and thus do not need shape\\nrefinement. To that end, we train a convolutional neural network that predicts\\nfor every pixel a polygon for the cell instance at that position. We\\ndemonstrate the merits of our approach on two synthetic datasets and one\\nchallenging dataset of diverse fluorescence microscopy images.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03535v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03535v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Uwe Schmidt',\n",
       "   'Martin Weigert',\n",
       "   'Coleman Broaddus',\n",
       "   'Gene Myers'],\n",
       "  'tasks': ['Cell Segmentation'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-to-search-in-long-documents-using',\n",
       "  'arxiv_id': '1806.03529',\n",
       "  'title': 'Learning to Search in Long Documents Using Document Structure',\n",
       "  'abstract': 'Reading comprehension models are based on recurrent neural networks that\\nsequentially process the document tokens. As interest turns to answering more\\ncomplex questions over longer documents, sequential reading of large portions\\nof text becomes a substantial bottleneck. Inspired by how humans use document\\nstructure, we propose a novel framework for reading comprehension. We represent\\ndocuments as trees, and model an agent that learns to interleave quick\\nnavigation through the document tree with more expensive answer extraction. To\\nencourage exploration of the document tree, we propose a new algorithm, based\\non Deep Q-Network (DQN), which strategically samples tree nodes at training\\ntime. Empirically we find our algorithm improves question answering performance\\ncompared to DQN and a strong information-retrieval (IR) baseline, and that\\nensembling our model with the IR baseline results in further gains in\\nperformance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03529v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03529v2.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Mor Geva', 'Jonathan Berant'],\n",
       "  'tasks': ['Information Retrieval',\n",
       "   'Question Answering',\n",
       "   'Reading Comprehension'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [{'name': 'Q-Learning',\n",
       "    'full_name': 'Q-Learning',\n",
       "    'description': '**Q-Learning** is an off-policy temporal difference control algorithm:\\r\\n\\r\\n$$Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) \\\\leftarrow Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right) + \\\\alpha\\\\left[R_{t+1} + \\\\gamma\\\\max\\\\_{a}Q\\\\left(S\\\\_{t+1}, a\\\\right) - Q\\\\left(S\\\\_{t}, A\\\\_{t}\\\\right)\\\\right] $$\\r\\n\\r\\nThe learned action-value function $Q$ directly approximates $q\\\\_{*}$, the optimal action-value function, independent of the policy being followed.\\r\\n\\r\\nSource: Sutton and Barto, Reinforcement Learning, 2nd Edition',\n",
       "    'introduced_year': 1984,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Off-Policy TD Control',\n",
       "     'description': '',\n",
       "     'parent': None,\n",
       "     'area': 'Reinforcement Learning'}},\n",
       "   {'name': 'Dense Connections',\n",
       "    'full_name': 'Dense Connections',\n",
       "    'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\\\_{\\\\text{inputs}}*n\\\\_{\\\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\\r\\n\\r\\n$$h\\\\_{l} = g\\\\left(\\\\textbf{W}^{T}h\\\\_{l-1}\\\\right)$$\\r\\n\\r\\nwhere $g$ is an activation function.\\r\\n\\r\\nImage Source: Deep Learning by Goodfellow, Bengio and Courville',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'DQN',\n",
       "    'full_name': 'Deep Q-Network',\n",
       "    'description': 'A **DQN**, or Deep Q-Network, approximates a state-value function in a [Q-Learning](https://paperswithcode.com/method/q-learning) framework with a neural network. In the Atari Games case, they take in several frames of the game as an input and output state values for each action as an output. \\r\\n\\r\\nIt is usually used in conjunction with [Experience Replay](https://paperswithcode.com/method/experience-replay), for storing the episode steps in memory for off-policy learning, where samples are drawn from the replay memory at random. Additionally, the Q-Network is usually optimized towards a frozen target network that is periodically updated with the latest weights every $k$ steps (where $k$ is a hyperparameter). The latter makes training more stable by preventing short-term oscillations from a moving target. The former tackles autocorrelation that would occur from on-line learning, and having a replay memory makes the problem more like a supervised learning problem.\\r\\n\\r\\nImage Source: [here](https://www.researchgate.net/publication/319643003_Autonomous_Quadrotor_Landing_using_Deep_Reinforcement_Learning)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1312.5602v1',\n",
       "    'source_title': 'Playing Atari with Deep Reinforcement Learning',\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Q-Learning Networks',\n",
       "     'description': '',\n",
       "     'parent': 'Off-Policy TD Control',\n",
       "     'area': 'Reinforcement Learning'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/second-language-acquisition-modeling-an',\n",
       "  'arxiv_id': '1806.04525',\n",
       "  'title': 'Second Language Acquisition Modeling: An Ensemble Approach',\n",
       "  'abstract': 'Accurate prediction of students knowledge is a fundamental building block of\\npersonalized learning systems. Here, we propose a novel ensemble model to\\npredict student knowledge gaps. Applying our approach to student trace data\\nfrom the online educational platform Duolingo we achieved highest score on both\\nevaluation metrics for all three datasets in the 2018 Shared Task on Second\\nLanguage Acquisition Modeling. We describe our model and discuss relevance of\\nthe task compared to how it would be setup in a production environment for\\npersonalized education.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04525v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04525v1.pdf',\n",
       "  'proceeding': 'WS 2018 6',\n",
       "  'authors': ['Anton Osika',\n",
       "   'Susanna Nilsson',\n",
       "   'Andrii Sydorchuk',\n",
       "   'Faruk Sahin',\n",
       "   'Anders Huss'],\n",
       "  'tasks': ['Language Acquisition'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-taxonomy-and-survey-of-intrusion-detection',\n",
       "  'arxiv_id': '1806.03517',\n",
       "  'title': 'A Taxonomy of Network Threats and the Effect of Current Datasets on Intrusion Detection Systems',\n",
       "  'abstract': \"As the world moves towards being increasingly dependent on computers and automation, building secure applications, systems and networks are some of the main challenges faced in the current decade. The number of threats that individuals and businesses face is rising exponentially due to the increasing complexity of networks and services of modern networks. To alleviate the impact of these threats, researchers have proposed numerous solutions for anomaly detection; however, current tools often fail to adapt to ever-changing architectures, associated threats and zero-day attacks. This manuscript aims to pinpoint research gaps and shortcomings of current datasets, their impact on building Network Intrusion Detection Systems (NIDS) and the growing number of sophisticated threats. To this end, this manuscript provides researchers with two key pieces of information; a survey of prominent datasets, analyzing their use and impact on the development of the past decade's Intrusion Detection Systems (IDS) and a taxonomy of network threats and associated tools to carry out these attacks. The manuscript highlights that current IDS research covers only 33.3% of our threat taxonomy. Current datasets demonstrate a clear lack of real-network threats, attack representation and include a large number of deprecated threats, which together limit the detection accuracy of current machine learning IDS approaches. The unique combination of the taxonomy and the analysis of the datasets provided in this manuscript aims to improve the creation of datasets and the collection of real-world data. As a result, this will improve the efficiency of the next generation IDS and reflect network threats more accurately within new datasets.\",\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03517v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03517v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hanan Hindy',\n",
       "   'David Brosset',\n",
       "   'Ethan Bayne',\n",
       "   'Amar Seeam',\n",
       "   'Christos Tachtatzis',\n",
       "   'Robert Atkinson',\n",
       "   'Xavier Bellekens'],\n",
       "  'tasks': ['Anomaly Detection',\n",
       "   'Intrusion Detection',\n",
       "   'Network Intrusion Detection'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/field-weighted-factorization-machines-for',\n",
       "  'arxiv_id': '1806.03514',\n",
       "  'title': 'Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising',\n",
       "  'abstract': 'Click-through rate (CTR) prediction is a critical task in online display advertising. The data involved in CTR prediction are typically multi-field categorical data, i.e., every feature is categorical and belongs to one and only one field. One of the interesting characteristics of such data is that features from one field often interact differently with features from different other fields. Recently, Field-aware Factorization Machines (FFMs) have been among the best performing models for CTR prediction by explicitly modeling such difference. However, the number of parameters in FFMs is in the order of feature number times field number, which is unacceptable in the real-world production systems. In this paper, we propose Field-weighted Factorization Machines (FwFMs) to model the different feature interactions between different fields in a much more memory-efficient way. Our experimental evaluations show that FwFMs can achieve competitive prediction performance with only as few as 4% parameters of FFMs. When using the same number of parameters, FwFMs can bring 0.92% and 0.47% AUC lift over FFMs on two real CTR prediction data sets.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03514v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03514v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Junwei Pan',\n",
       "   'Jian Xu',\n",
       "   'Alfonso Lobos Ruiz',\n",
       "   'Wenliang Zhao',\n",
       "   'Shengjun Pan',\n",
       "   'Yu Sun',\n",
       "   'Quan Lu'],\n",
       "  'tasks': ['Click-Through Rate Prediction'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/feature-pyramid-network-for-multi-class-land',\n",
       "  'arxiv_id': '1806.03510',\n",
       "  'title': 'Feature Pyramid Network for Multi-Class Land Segmentation',\n",
       "  'abstract': 'Semantic segmentation is in-demand in satellite imagery processing. Because\\nof the complex environment, automatic categorization and segmentation of land\\ncover is a challenging problem. Solving it can help to overcome many obstacles\\nin urban planning, environmental engineering or natural landscape monitoring.\\nIn this paper, we propose an approach for automatic multi-class land\\nsegmentation based on a fully convolutional neural network of feature pyramid\\nnetwork (FPN) family. This network is consisted of pre-trained on ImageNet\\nResnet50 encoder and neatly developed decoder. Based on validation results,\\nleaderboard score and our own experience this network shows reliable results\\nfor the DEEPGLOBE - CVPR 2018 land cover classification sub-challenge.\\nMoreover, this network moderately uses memory that allows using GTX 1080 or\\n1080 TI video cards to perform whole training and makes pretty fast\\npredictions.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03510v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03510v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Selim S. Seferbekov',\n",
       "   'Vladimir I. Iglovikov',\n",
       "   'Alexander V. Buslaev',\n",
       "   'Alexey A. Shvets'],\n",
       "  'tasks': ['General Classification', 'Semantic Segmentation'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['deepglobe'],\n",
       "  'datasets_used_full': ['DeepGlobe'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/on-the-universal-approximation-property-and',\n",
       "  'arxiv_id': '1803.05391',\n",
       "  'title': 'On the Universal Approximation Property and Equivalence of Stochastic Computing-based Neural Networks and Binary Neural Networks',\n",
       "  'abstract': 'Large-scale deep neural networks are both memory intensive and\\ncomputation-intensive, thereby posing stringent requirements on the computing\\nplatforms. Hardware accelerations of deep neural networks have been extensively\\ninvestigated in both industry and academia. Specific forms of binary neural\\nnetworks (BNNs) and stochastic computing based neural networks (SCNNs) are\\nparticularly appealing to hardware implementations since they can be\\nimplemented almost entirely with binary operations. Despite the obvious\\nadvantages in hardware implementation, these approximate computing techniques\\nare questioned by researchers in terms of accuracy and universal applicability.\\nAlso it is important to understand the relative pros and cons of SCNNs and BNNs\\nin theory and in actual hardware implementations. In order to address these\\nconcerns, in this paper we prove that the \"ideal\" SCNNs and BNNs satisfy the\\nuniversal approximation property with probability 1 (due to the stochastic\\nbehavior). The proof is conducted by first proving the property for SCNNs from\\nthe strong law of large numbers, and then using SCNNs as a \"bridge\" to prove\\nfor BNNs. Based on the universal approximation property, we further prove that\\nSCNNs and BNNs exhibit the same energy complexity. In other words, they have\\nthe same asymptotic energy consumption with the growing of network size. We\\nalso provide a detailed analysis of the pros and cons of SCNNs and BNNs for\\nhardware implementations and conclude that SCNNs are more suitable for\\nhardware.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.05391v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.05391v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yanzhi Wang',\n",
       "   'Zheng Zhan',\n",
       "   'Jiayu Li',\n",
       "   'Jian Tang',\n",
       "   'Bo Yuan',\n",
       "   'Liang Zhao',\n",
       "   'Wujie Wen',\n",
       "   'Siyue Wang',\n",
       "   'Xue Lin'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-03-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-fast-and-scalable-joint-estimator-for-1',\n",
       "  'arxiv_id': '1806.00548',\n",
       "  'title': 'A Fast and Scalable Joint Estimator for Integrating Additional Knowledge in Learning Multiple Related Sparse Gaussian Graphical Models',\n",
       "  'abstract': 'We consider the problem of including additional knowledge in estimating\\nsparse Gaussian graphical models (sGGMs) from aggregated samples, arising often\\nin bioinformatics and neuroimaging applications. Previous joint sGGM estimators\\neither fail to use existing knowledge or cannot scale-up to many tasks (large\\n$K$) under a high-dimensional (large $p$) situation. In this paper, we propose\\na novel \\\\underline{J}oint \\\\underline{E}lementary \\\\underline{E}stimator\\nincorporating additional \\\\underline{K}nowledge (JEEK) to infer multiple related\\nsparse Gaussian Graphical models from large-scale heterogeneous data. Using\\ndomain knowledge as weights, we design a novel hybrid norm as the minimization\\nobjective to enforce the superposition of two weighted sparsity constraints,\\none on the shared interactions and the other on the task-specific structural\\npatterns. This enables JEEK to elegantly consider various forms of existing\\nknowledge based on the domain at hand and avoid the need to design\\nknowledge-specific optimization. JEEK is solved through a fast and entry-wise\\nparallelizable solution that largely improves the computational efficiency of\\nthe state-of-the-art $O(p^5K^4)$ to $O(p^2K^4)$. We conduct a rigorous\\nstatistical analysis showing that JEEK achieves the same convergence rate\\n$O(\\\\log(Kp)/n_{tot})$ as the state-of-the-art estimators that are much harder\\nto compute. Empirically, on multiple synthetic datasets and two real-world\\ndata, JEEK outperforms the speed of the state-of-arts significantly while\\nachieving the same level of prediction accuracy. Available as R tool @\\nhttp://jointnets.org/',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.00548v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.00548v4.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Beilun Wang', 'Arshdeep Sekhon', 'Yanjun Qi'],\n",
       "  'tasks': ['Structured Prediction'],\n",
       "  'date': '2018-06-01',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/emulating-dynamic-non-linear-simulators-using',\n",
       "  'arxiv_id': '1802.07575',\n",
       "  'title': 'Emulating dynamic non-linear simulators using Gaussian processes',\n",
       "  'abstract': 'The dynamic emulation of non-linear deterministic computer codes where the\\noutput is a time series, possibly multivariate, is examined. Such computer\\nmodels simulate the evolution of some real-world phenomenon over time, for\\nexample models of the climate or the functioning of the human brain. The models\\nwe are interested in are highly non-linear and exhibit tipping points,\\nbifurcations and chaotic behaviour. However, each simulation run could be too\\ntime-consuming to perform analyses that require many runs, including\\nquantifying the variation in model output with respect to changes in the\\ninputs. Therefore, Gaussian process emulators are used to approximate the\\noutput of the code. To do this, the flow map of the system under study is\\nemulated over a short time period. Then, it is used in an iterative way to\\npredict the whole time series. A number of ways are proposed to take into\\naccount the uncertainty of inputs to the emulators, after fixed initial\\nconditions, and the correlation between them through the time series. The\\nmethodology is illustrated with two examples: the highly non-linear dynamical\\nsystems described by the Lorenz and Van der Pol equations. In both cases, the\\npredictive performance is relatively high and the measure of uncertainty\\nprovided by the method reflects the extent of predictability in each system.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.07575v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.07575v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hossein Mohammadi', 'Peter Challenor', 'Marc Goodfellow'],\n",
       "  'tasks': ['Gaussian Processes', 'Time Series'],\n",
       "  'date': '2018-02-21',\n",
       "  'methods': [{'name': 'Gaussian Process',\n",
       "    'full_name': 'Gaussian Process',\n",
       "    'description': '**Gaussian Processes** are non-parametric models for approximating functions. They rely upon a measure of similarity between points (the kernel function) to predict the value for an unseen point from training data. The models are fully probabilistic so uncertainty bounds are baked in with the model.\\r\\n\\r\\nImage Source: Gaussian Processes for Machine Learning, C. E. Rasmussen & C. K. I. Williams',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Non-Parametric Classification',\n",
       "     'description': '**Non-Parametric Classification** methods perform classification where we use non-parametric methods to approximate the functional form of the relationship. Below you can find a continuously updating list of non-parametric classification methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/exploring-hidden-dimensions-in-parallelizing',\n",
       "  'arxiv_id': '1802.04924',\n",
       "  'title': 'Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks',\n",
       "  'abstract': 'The past few years have witnessed growth in the computational requirements\\nfor training deep convolutional neural networks. Current approaches parallelize\\ntraining onto multiple devices by applying a single parallelization strategy\\n(e.g., data or model parallelism) to all layers in a network. Although easy to\\nreason about, these approaches result in suboptimal runtime performance in\\nlarge-scale distributed training, since different layers in a network may\\nprefer different parallelization strategies. In this paper, we propose\\nlayer-wise parallelism that allows each layer in a network to use an individual\\nparallelization strategy. We jointly optimize how each layer is parallelized by\\nsolving a graph search problem. Our evaluation shows that layer-wise\\nparallelism outperforms state-of-the-art approaches by increasing training\\nthroughput, reducing communication costs, achieving better scalability to\\nmultiple GPUs, while maintaining original network accuracy.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.04924v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.04924v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Zhihao Jia', 'Sina Lin', 'Charles R. Qi', 'Alex Aiken'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-14',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/russe2018-a-shared-task-on-word-sense',\n",
       "  'arxiv_id': '1803.05795',\n",
       "  'title': \"RUSSE'2018: A Shared Task on Word Sense Induction for the Russian Language\",\n",
       "  'abstract': 'The paper describes the results of the first shared task on word sense\\ninduction (WSI) for the Russian language. While similar shared tasks were\\nconducted in the past for some Romance and Germanic languages, we explore the\\nperformance of sense induction and disambiguation methods for a Slavic language\\nthat shares many features with other Slavic languages, such as rich morphology\\nand virtually free word order. The participants were asked to group contexts of\\na given word in accordance with its senses that were not provided beforehand.\\nFor instance, given a word \"bank\" and a set of contexts for this word, e.g.\\n\"bank is a financial institution that accepts deposits\" and \"river bank is a\\nslope beside a body of water\", a participant was asked to cluster such contexts\\nin the unknown in advance number of clusters corresponding to, in this case,\\nthe \"company\" and the \"area\" senses of the word \"bank\". For the purpose of this\\nevaluation campaign, we developed three new evaluation datasets based on sense\\ninventories that have different sense granularity. The contexts in these\\ndatasets were sampled from texts of Wikipedia, the academic corpus of Russian,\\nand an explanatory dictionary of Russian. Overall, 18 teams participated in the\\ncompetition submitting 383 models. Multiple teams managed to substantially\\noutperform competitive state-of-the-art baselines from the previous years based\\non sense embeddings.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.05795v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.05795v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Alexander Panchenko',\n",
       "   'Anastasiya Lopukhina',\n",
       "   'Dmitry Ustalov',\n",
       "   'Konstantin Lopukhin',\n",
       "   'Nikolay Arefyev',\n",
       "   'Alexey Leontyev',\n",
       "   'Natalia Loukachevitch'],\n",
       "  'tasks': ['Word Sense Induction'],\n",
       "  'date': '2018-03-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': ['russe'],\n",
       "  'datasets_introduced_full': ['RUSSE']},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/generalized-earley-parser-bridging-symbolic',\n",
       "  'arxiv_id': '1806.03497',\n",
       "  'title': 'Generalized Earley Parser: Bridging Symbolic Grammars and Sequence Data for Future Prediction',\n",
       "  'abstract': 'Future predictions on sequence data (e.g., videos or audios) require the\\nalgorithms to capture non-Markovian and compositional properties of high-level\\nsemantics. Context-free grammars are natural choices to capture such\\nproperties, but traditional grammar parsers (e.g., Earley parser) only take\\nsymbolic sentences as inputs. In this paper, we generalize the Earley parser to\\nparse sequence data which is neither segmented nor labeled. This generalized\\nEarley parser integrates a grammar parser with a classifier to find the optimal\\nsegmentation and labels, and makes top-down future predictions. Experiments\\nshow that our method significantly outperforms other approaches for future\\nhuman activity prediction.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03497v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03497v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Siyuan Qi', 'Baoxiong Jia', 'Song-Chun Zhu'],\n",
       "  'tasks': ['Activity Prediction', 'Future prediction'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cad-120'],\n",
       "  'datasets_used_full': ['CAD-120'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/bridging-the-gap-between-2d-and-3d-organ',\n",
       "  'arxiv_id': '1804.00392',\n",
       "  'title': 'Bridging the Gap Between 2D and 3D Organ Segmentation with Volumetric Fusion Net',\n",
       "  'abstract': 'There has been a debate on whether to use 2D or 3D deep neural networks for\\nvolumetric organ segmentation. Both 2D and 3D models have their advantages and\\ndisadvantages. In this paper, we present an alternative framework, which trains\\n2D networks on different viewpoints for segmentation, and builds a 3D\\nVolumetric Fusion Net (VFN) to fuse the 2D segmentation results. VFN is\\nrelatively shallow and contains much fewer parameters than most 3D networks,\\nmaking our framework more efficient at integrating 3D information for\\nsegmentation. We train and test the segmentation and fusion modules\\nindividually, and propose a novel strategy, named cross-cross-augmentation, to\\nmake full use of the limited training data. We evaluate our framework on\\nseveral challenging abdominal organs, and verify its superiority in\\nsegmentation accuracy and stability over existing 2D and 3D approaches.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1804.00392v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1804.00392v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yingda Xia',\n",
       "   'Lingxi Xie',\n",
       "   'Fengze Liu',\n",
       "   'Zhuotun Zhu',\n",
       "   'Elliot K. Fishman',\n",
       "   'Alan L. Yuille'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-04-02',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/explainable-deterministic-mdps',\n",
       "  'arxiv_id': '1806.03492',\n",
       "  'title': 'Explainable Deterministic MDPs',\n",
       "  'abstract': 'We present a method for a certain class of Markov Decision Processes (MDPs)\\nthat can relate the optimal policy back to one or more reward sources in the\\nenvironment. For a given initial state, without fully computing the value\\nfunction, q-value function, or the optimal policy the algorithm can determine\\nwhich rewards will and will not be collected, whether a given reward will be\\ncollected only once or continuously, and which local maximum within the value\\nfunction the initial state will ultimately lead to. We demonstrate that the\\nmethod can be used to map the state space to identify regions that are\\ndominated by one reward source and can fully analyze the state space to explain\\nall actions. We provide a mathematical framework to show how all of this is\\npossible without first computing the optimal policy or value function.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03492v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03492v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Josh Bertram', 'Peng Wei'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/robust-lexical-features-for-improved-neural',\n",
       "  'arxiv_id': '1806.03489',\n",
       "  'title': 'Robust Lexical Features for Improved Neural Network Named-Entity Recognition',\n",
       "  'abstract': 'Neural network approaches to Named-Entity Recognition reduce the need for\\ncarefully hand-crafted features. While some features do remain in\\nstate-of-the-art systems, lexical features have been mostly discarded, with the\\nexception of gazetteers. In this work, we show that this is unfair: lexical\\nfeatures are actually quite useful. We propose to embed words and entity types\\ninto a low-dimensional vector space we train from annotated data produced by\\ndistant supervision thanks to Wikipedia. From this, we compute - offline - a\\nfeature vector representing each word. When used with a vanilla recurrent\\nneural network model, this representation yields substantial improvements. We\\nestablish a new state-of-the-art F1 score of 87.95 on ONTONOTES 5.0, while\\nmatching state-of-the-art performance with a F1 score of 91.73 on the\\nover-studied CONLL-2003 dataset.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03489v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03489v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Abbas Ghaddar', 'Philippe Langlais'],\n",
       "  'tasks': ['Named Entity Recognition'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['dbpedia', 'conll-2003', 'ontonotes-5-0'],\n",
       "  'datasets_used_full': ['DBpedia', 'CoNLL-2003', 'OntoNotes 5.0'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-to-grasp-from-a-single-demonstration',\n",
       "  'arxiv_id': '1806.03486',\n",
       "  'title': 'Learning to Grasp from a Single Demonstration',\n",
       "  'abstract': 'Learning-based approaches for robotic grasping using visual sensors typically\\nrequire collecting a large size dataset, either manually labeled or by many\\ntrial and errors of a robotic manipulator in the real or simulated world. We\\npropose a simpler learning-from-demonstration approach that is able to detect\\nthe object to grasp from merely a single demonstration using a convolutional\\nneural network we call GraspNet. In order to increase robustness and decrease\\nthe training time even further, we leverage data from previous demonstrations\\nto quickly fine-tune a GrapNet for each new demonstration. We present some\\npreliminary results on a grasping experiment with the Franka Panda cobot for\\nwhich we can train a GraspNet with only hundreds of train iterations.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03486v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03486v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Pieter Van Molle',\n",
       "   'Tim Verbelen',\n",
       "   'Elias De Coninck',\n",
       "   'Cedric De Boom',\n",
       "   'Pieter Simoens',\n",
       "   'Bart Dhoedt'],\n",
       "  'tasks': ['Robotic Grasping'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dir-st2-delineation-of-imprecise-regions',\n",
       "  'arxiv_id': '1806.03482',\n",
       "  'title': 'DIR-ST$^2$: Delineation of Imprecise Regions Using Spatio--Temporal--Textual Information',\n",
       "  'abstract': 'An imprecise region is referred to as a geographical area without a\\nclearly-defined boundary in the literature. Previous clustering-based\\napproaches exploit spatial information to find such regions. However, the prior\\nstudies suffer from the following two problems: the subjectivity in selecting\\nclustering parameters and the inclusion of a large portion of the undesirable\\nregion (i.e., a large number of noise points). To overcome these problems, we\\npresent DIR-ST$^2$, a novel framework for delineating an imprecise region by\\niteratively performing density-based clustering, namely DBSCAN, along with not\\nonly spatio--textual information but also temporal information on social media.\\nSpecifically, we aim at finding a proper radius of a circle used in the\\niterative DBSCAN process by gradually reducing the radius for each iteration in\\nwhich the temporal information acquired from all resulting clusters are\\nleveraged. Then, we propose an efficient and automated algorithm delineating\\nthe imprecise region via hierarchical clustering. Experiment results show that\\nby virtue of the significant noise reduction in the region, our DIR-ST$^2$\\nmethod outperforms the state-of-the-art approach employing one-class support\\nvector machine in terms of the $\\\\mathcal{F}_1$ score from comparison with\\nprecisely-defined regions regarded as a ground truth, and returns apparently\\nbetter delineation of imprecise regions. The computational complexity of\\nDIR-ST$^2$ is also analytically and numerically shown.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03482v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03482v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Cong Tran', 'Won-Yong Shin', 'Sang-Il Choi'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/geometry-score-a-method-for-comparing',\n",
       "  'arxiv_id': '1802.02664',\n",
       "  'title': 'Geometry Score: A Method For Comparing Generative Adversarial Networks',\n",
       "  'abstract': 'One of the biggest challenges in the research of generative adversarial\\nnetworks (GANs) is assessing the quality of generated samples and detecting\\nvarious levels of mode collapse. In this work, we construct a novel measure of\\nperformance of a GAN by comparing geometrical properties of the underlying data\\nmanifold and the generated one, which provides both qualitative and\\nquantitative means for evaluation. Our algorithm can be applied to datasets of\\nan arbitrary nature and is not limited to visual data. We test the obtained\\nmetric on various real-life models and datasets and demonstrate that our method\\nprovides new insights into properties of GANs.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.02664v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.02664v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Valentin Khrulkov', 'Ivan Oseledets'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-07',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['celeba'],\n",
       "  'datasets_used_full': ['CelebA'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/holographic-automata-for-ambient-immersive-a',\n",
       "  'arxiv_id': '1806.05108',\n",
       "  'title': 'Holographic Automata for Ambient Immersive A. I. via Reservoir Computing',\n",
       "  'abstract': 'We prove the existence of a semilinear representation of Cellular Automata\\n(CA) with the introduction of multiple convolution kernels. Examples of the\\ntechnique are presented for rules akin to the \"edge-of-chaos\" including the\\nTuring universal rule 110 for further utilization in the area of reservoir\\ncomputing. We also examine the significance of their dual representation on a\\nfrequency or wavelength domain as a superposition of plane waves for\\ndistributed computing applications including a new proposal for a \"Hologrid\"\\nthat could be realized with present Wi-Fi,Li-Fi technologies.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.05108v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.05108v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Theophanes E. Raptis'],\n",
       "  'tasks': ['Distributed Computing'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/orthogonal-random-forest-for-causal-inference',\n",
       "  'arxiv_id': '1806.03467',\n",
       "  'title': 'Orthogonal Random Forest for Causal Inference',\n",
       "  'abstract': 'We propose the orthogonal random forest, an algorithm that combines Neyman-orthogonality to reduce sensitivity with respect to estimation error of nuisance parameters with generalized random forests (Athey et al., 2017)--a flexible non-parametric method for statistical estimation of conditional moment models using random forests. We provide a consistency rate and establish asymptotic normality for our estimator. We show that under mild assumptions on the consistency rate of the nuisance estimator, we can achieve the same error rate as an oracle with a priori knowledge of these nuisance parameters. We show that when the nuisance functions have a locally sparse parametrization, then a local $\\\\ell_1$-penalized regression achieves the required rate. We apply our method to estimate heterogeneous treatment effects from observational data with discrete treatments or continuous treatments, and we show that, unlike prior work, our method provably allows to control for a high-dimensional set of variables under standard sparsity conditions. We also provide a comprehensive empirical evaluation of our algorithm on both synthetic and real data.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03467v4',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03467v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Miruna Oprescu', 'Vasilis Syrgkanis', 'Zhiwei Steven Wu'],\n",
       "  'tasks': ['Causal Inference'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/robust-semantic-segmentation-with-ladder',\n",
       "  'arxiv_id': '1806.03465',\n",
       "  'title': 'Robust Semantic Segmentation with Ladder-DenseNet Models',\n",
       "  'abstract': 'We present semantic segmentation experiments with a model capable to perform\\npredictions on four benchmark datasets: Cityscapes, ScanNet, WildDash and\\nKITTI. We employ a ladder-style convolutional architecture featuring a modified\\nDenseNet-169 model in the downsampling datapath, and only one convolution in\\neach stage of the upsampling datapath. Due to limited computing resources, we\\nperform the training only on Cityscapes Fine train+val, ScanNet train, WildDash\\nval and KITTI train. We evaluate the trained model on the test subsets of the\\nfour benchmarks in concordance with the guidelines of the Robust Vision\\nChallenge ROB 2018. The performed experiments reveal several interesting\\nfindings which we describe and discuss.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03465v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03465v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ivan Krešo', 'Marin Oršić', 'Petra Bevandić', 'Siniša Šegvić'],\n",
       "  'tasks': ['Semantic Segmentation'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['kitti', 'scannet'],\n",
       "  'datasets_used_full': ['KITTI', 'ScanNet'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/tapas-tricks-to-accelerate-encrypted',\n",
       "  'arxiv_id': '1806.03461',\n",
       "  'title': 'TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service',\n",
       "  'abstract': 'Machine learning methods are widely used for a variety of prediction\\nproblems. \\\\emph{Prediction as a service} is a paradigm in which service\\nproviders with technological expertise and computational resources may perform\\npredictions for clients. However, data privacy severely restricts the\\napplicability of such services, unless measures to keep client data private\\n(even from the service provider) are designed. Equally important is to minimize\\nthe amount of computation and communication required between client and server.\\nFully homomorphic encryption offers a possible way out, whereby clients may\\nencrypt their data, and on which the server may perform arithmetic\\ncomputations. The main drawback of using fully homomorphic encryption is the\\namount of time required to evaluate large machine learning models on encrypted\\ndata. We combine ideas from the machine learning literature, particularly work\\non binarization and sparsification of neural networks, together with\\nalgorithmic tools to speed-up and parallelize computation using encrypted data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03461v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03461v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Amartya Sanyal',\n",
       "   'Matt J. Kusner',\n",
       "   'Adrià Gascón',\n",
       "   'Varun Kanade'],\n",
       "  'tasks': ['Binarization'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-preliminary-exploration-of-floating-point',\n",
       "  'arxiv_id': '1806.03455',\n",
       "  'title': 'A Preliminary Exploration of Floating Point Grammatical Evolution',\n",
       "  'abstract': 'Current GP frameworks are highly effective on a range of real and simulated\\nbenchmarks. However, due to the high dimensionality of the genotypes for GP,\\nthe task of visualising the fitness landscape for GP search can be difficult.\\nThis paper describes a new framework: Floating Point Grammatical Evolution\\n(FP-GE) which uses a single floating point genotype to encode an individual\\nprogram. This encoding permits easier visualisation of the fitness landscape\\narbitrary problems by providing a way to map fitness against a single\\ndimension. The new framework also makes it trivially easy to apply continuous\\nsearch algorithms, such as Differential Evolution, to the search problem. In\\nthis work, the FP-GE framework is tested against several regression problems,\\nvisualising the search landscape for these and comparing different search\\nmeta-heuristics.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03455v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03455v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Brad Alexander'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-learning-topological-invariants-of-band',\n",
       "  'arxiv_id': '1805.10503',\n",
       "  'title': 'Deep Learning Topological Invariants of Band Insulators',\n",
       "  'abstract': 'In this work we design and train deep neural networks to predict topological\\ninvariants for one-dimensional four-band insulators in AIII class whose\\ntopological invariant is the winding number, and two-dimensional two-band\\ninsulators in A class whose topological invariant is the Chern number. Given\\nHamiltonians in the momentum space as the input, neural networks can predict\\ntopological invariants for both classes with accuracy close to or higher than\\n90%, even for Hamiltonians whose invariants are beyond the training data set.\\nDespite the complexity of the neural network, we find that the output of\\ncertain intermediate hidden layers resembles either the winding angle for\\nmodels in AIII class or the solid angle (Berry curvature) for models in A\\nclass, indicating that neural networks essentially capture the mathematical\\nformula of topological invariants. Our work demonstrates the ability of neural\\nnetworks to predict topological invariants for complicated models with local\\nHamiltonians as the only input, and offers an example that even a deep neural\\nnetwork is understandable.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.10503v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.10503v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ning Sun',\n",
       "   'Jinmin Yi',\n",
       "   'Pengfei Zhang',\n",
       "   'Huitao Shen',\n",
       "   'Hui Zhai'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-26',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/bounding-and-counting-linear-regions-of-deep',\n",
       "  'arxiv_id': '1711.02114',\n",
       "  'title': 'Bounding and Counting Linear Regions of Deep Neural Networks',\n",
       "  'abstract': 'We investigate the complexity of deep neural networks (DNN) that represent\\npiecewise linear (PWL) functions. In particular, we study the number of linear\\nregions, i.e. pieces, that a PWL function represented by a DNN can attain, both\\ntheoretically and empirically. We present (i) tighter upper and lower bounds\\nfor the maximum number of linear regions on rectifier networks, which are exact\\nfor inputs of dimension one; (ii) a first upper bound for multi-layer maxout\\nnetworks; and (iii) a first method to perform exact enumeration or counting of\\nthe number of regions by modeling the DNN with a mixed-integer linear\\nformulation. These bounds come from leveraging the dimension of the space\\ndefining each linear region. The results also indicate that a deep rectifier\\nnetwork can only have more linear regions than every shallow counterpart with\\nsame number of neurons if that number exceeds the dimension of the input.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.02114v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.02114v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Thiago Serra',\n",
       "   'Christian Tjandraatmadja',\n",
       "   'Srikumar Ramalingam'],\n",
       "  'tasks': [],\n",
       "  'date': '2017-11-06',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-hybrid-econometric-machine-learning',\n",
       "  'arxiv_id': '1806.04517',\n",
       "  'title': 'A hybrid econometric-machine learning approach for relative importance analysis: Prioritizing food policy',\n",
       "  'abstract': 'A measure of relative importance of variables is often desired by researchers when the explanatory aspects of econometric methods are of interest. To this end, the author briefly reviews the limitations of conventional econometrics in constructing a reliable measure of variable importance. The author highlights the relative stature of explanatory and predictive analysis in economics and the emergence of fruitful collaborations between econometrics and computer science. Learning lessons from both, the author proposes a hybrid approach based on conventional econometrics and advanced machine learning (ML) algorithms, which are otherwise, used in predictive analytics. The purpose of this article is two-fold, to propose a hybrid approach to assess relative importance and demonstrate its applicability in addressing policy priority issues with an example of food inflation in India, followed by a broader aim to introduce the possibility of conflation of ML and conventional econometrics to an audience of researchers in economics and social sciences, in general.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.04517v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.04517v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Akash Malhotra'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/abstaining-classification-when-error-costs',\n",
       "  'arxiv_id': '1806.03445',\n",
       "  'title': 'Abstaining Classification When Error Costs are Unequal and Unknown',\n",
       "  'abstract': 'Abstaining classificaiton aims to reject to classify the easily misclassified\\nexamples, so it is an effective approach to increase the clasificaiton\\nreliability and reduce the misclassification risk in the cost-sensitive\\napplications. In such applications, different types of errors (false positive\\nor false negative) usaully have unequal costs. And the error costs, which\\ndepend on specific applications, are usually unknown. However, current\\nabstaining classification methods either do not distinguish the error types, or\\nthey need the cost information of misclassification and rejection, which are\\nrealized in the framework of cost-sensitive learning. In this paper, we propose\\na bounded-abstention method with two constraints of reject rates (BA2), which\\nperforms abstaining classification when error costs are unequal and unknown.\\nBA2 aims to obtain the optimal area under the ROC curve (AUC) by constraining\\nthe reject rates of the positive and negative classes respectively.\\nSpecifically, we construct the receiver operating characteristic (ROC) curve,\\nand stepwise search the optimal reject thresholds from both ends of the curve,\\nuntill the two constraints are satisfied. Experimental results show that BA2\\nobtains higher AUC and lower total cost than the state-of-the-art abstaining\\nclassification methods. Meanwhile, BA2 achieves controllable reject rates of\\nthe positive and negative classes.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03445v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03445v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hongjiao Guan',\n",
       "   'Yingtao Zhang',\n",
       "   'H. D. Cheng',\n",
       "   'Xianglong Tang'],\n",
       "  'tasks': ['Classification', 'General Classification'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/hierarchical-imitation-and-reinforcement',\n",
       "  'arxiv_id': '1803.00590',\n",
       "  'title': 'Hierarchical Imitation and Reinforcement Learning',\n",
       "  'abstract': \"We study how to effectively leverage expert feedback to learn sequential\\ndecision-making policies. We focus on problems with sparse rewards and long\\ntime horizons, which typically pose significant challenges in reinforcement\\nlearning. We propose an algorithmic framework, called hierarchical guidance,\\nthat leverages the hierarchical structure of the underlying problem to\\nintegrate different modes of expert interaction. Our framework can incorporate\\ndifferent combinations of imitation learning (IL) and reinforcement learning\\n(RL) at different levels, leading to dramatic reductions in both expert effort\\nand cost of exploration. Using long-horizon benchmarks, including Montezuma's\\nRevenge, we demonstrate that our approach can learn significantly faster than\\nhierarchical RL, and be significantly more label-efficient than standard IL. We\\nalso theoretically analyze labeling cost for certain instantiations of our\\nframework.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.00590v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.00590v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Hoang M. Le',\n",
       "   'Nan Jiang',\n",
       "   'Alekh Agarwal',\n",
       "   'Miroslav Dudík',\n",
       "   'Yisong Yue',\n",
       "   'Hal Daumé III'],\n",
       "  'tasks': ['Decision Making', 'Imitation Learning', \"Montezuma's Revenge\"],\n",
       "  'date': '2018-03-01',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/towards-multifocal-displays-with-dense-focal',\n",
       "  'arxiv_id': '1805.10664',\n",
       "  'title': 'Towards Multifocal Displays with Dense Focal Stacks',\n",
       "  'abstract': \"We present a virtual reality display that is capable of generating a dense\\ncollection of depth/focal planes. This is achieved by driving a focus-tunable\\nlens to sweep a range of focal lengths at a high frequency and, subsequently,\\ntracking the focal length precisely at microsecond time resolutions using an\\noptical module. Precise tracking of the focal length, coupled with a high-speed\\ndisplay, enables our lab prototype to generate 1600 focal planes per second.\\nThis enables a novel first-of-its-kind virtual reality multifocal display that\\nis capable of resolving the vergence-accommodation conflict endemic to today's\\ndisplays.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.10664v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.10664v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jen-Hao Rick Chang',\n",
       "   'B. V. K. Vijaya Kumar',\n",
       "   'Aswin C. Sankaranarayanan'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-27',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/algorithmic-causal-deconvolution-of',\n",
       "  'arxiv_id': '1802.09904',\n",
       "  'title': 'Algorithmic Causal Deconvolution of Intertwined Programs and Networks by Generative Mechanism',\n",
       "  'abstract': 'Complex data usually results from the interaction of objects produced by different generating mechanisms. Here we introduce a universal, unsupervised and parameter-free model-oriented approach, based upon the seminal concept of algorithmic probability, that decomposes an observation into its most likely algorithmic generative sources. Our approach uses a causal calculus to infer model representations. We demonstrate its ability to deconvolve interacting mechanisms regardless of whether the resultant objects are strings, space-time evolution diagrams, images or networks. While this is mostly a conceptual contribution and a novel framework, we provide numerical evidence evaluating the ability of our methods to separate data from observations produced by discrete dynamical systems such as cellular automata and complex networks. We think that these separating techniques can contribute to tackling the challenge of causation, thus complementing other statistically oriented approaches.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1802.09904v8',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1802.09904v8.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Hector Zenil',\n",
       "   'Narsis A. Kiani',\n",
       "   'Allan A. Zea',\n",
       "   'Jesper Tegnér'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-18',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/hierarchical-clustering-with-prior-knowledge',\n",
       "  'arxiv_id': '1806.03432',\n",
       "  'title': 'Hierarchical Clustering with Prior Knowledge',\n",
       "  'abstract': 'Hierarchical clustering is a class of algorithms that seeks to build a\\nhierarchy of clusters. It has been the dominant approach to constructing\\nembedded classification schemes since it outputs dendrograms, which capture the\\nhierarchical relationship among members at all levels of granularity,\\nsimultaneously. Being greedy in the algorithmic sense, a hierarchical\\nclustering partitions data at every step solely based on a similarity /\\ndissimilarity measure. The clustering results oftentimes depend on not only the\\ndistribution of the underlying data, but also the choice of dissimilarity\\nmeasure and the clustering algorithm. In this paper, we propose a method to\\nincorporate prior domain knowledge about entity relationship into the\\nhierarchical clustering. Specifically, we use a distance function in\\nultrametric space to encode the external ontological information. We show that\\npopular linkage-based algorithms can faithfully recover the encoded structure.\\nSimilar to some regularized machine learning techniques, we add this distance\\nas a penalty term to the original pairwise distance to regulate the final\\nstructure of the dendrogram. As a case study, we applied this method on real\\ndata in the building of a customer behavior based product taxonomy for an\\nAmazon service, leveraging the information from a larger Amazon-wide browse\\nstructure. The method is useful when one wants to leverage the relational\\ninformation from external sources, or the data used to generate the distance\\nmatrix is noisy and sparse. Our work falls in the category of semi-supervised\\nor constrained clustering.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03432v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03432v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Xiaofei Ma', 'Satya Dhavala'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/word-familiarity-and-frequency',\n",
       "  'arxiv_id': '1806.03431',\n",
       "  'title': 'Word Familiarity and Frequency',\n",
       "  'abstract': 'Word frequency is assumed to correlate with word familiarity, but the\\nstrength of this correlation has not been thoroughly investigated. In this\\npaper, we report on our analysis of the correlation between a word familiarity\\nrating list obtained through a psycholinguistic experiment and the\\nlog-frequency obtained from various corpora of different kinds and sizes (up to\\nthe terabyte scale) for English and Japanese. Major findings are threefold:\\nFirst, for a given corpus, familiarity is necessary for a word to achieve high\\nfrequency, but familiar words are not necessarily frequent. Second, correlation\\nincreases with the corpus data size. Third, a corpus of spoken language\\ncorrelates better than one of written language. These findings suggest that\\ncognitive familiarity ratings are correlated to frequency, but more highly to\\nthat of spoken rather than written language.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03431v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03431v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kumiko Tanaka-Ishii', 'Hiroshi Terada'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/an-encoder-decoder-framework-translating',\n",
       "  'arxiv_id': '1711.06061',\n",
       "  'title': 'An Encoder-Decoder Framework Translating Natural Language to Database Queries',\n",
       "  'abstract': 'Machine translation is going through a radical revolution, driven by the\\nexplosive development of deep learning techniques using Convolutional Neural\\nNetwork (CNN) and Recurrent Neural Network (RNN). In this paper, we consider a\\nspecial case in machine translation problems, targeting to convert natural\\nlanguage into Structured Query Language (SQL) for data retrieval over\\nrelational database. Although generic CNN and RNN learn the grammar structure\\nof SQL when trained with sufficient samples, the accuracy and training\\nefficiency of the model could be dramatically improved, when the translation\\nmodel is deeply integrated with the grammar rules of SQL. We present a new\\nencoder-decoder framework, with a suite of new approaches, including new\\nsemantic features fed into the encoder, grammar-aware states injected into the\\nmemory of decoder, as well as recursive state management for sub-queries. These\\ntechniques help the neural network better focus on understanding semantics of\\noperations in natural language and save the efforts on SQL grammar learning.\\nThe empirical evaluation on real world database and queries show that our\\napproach outperform state-of-the-art solution by a significant margin.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.06061v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.06061v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Ruichu Cai',\n",
       "   'Boyan Xu',\n",
       "   'Xiaoyan Yang',\n",
       "   'Zhenjie Zhang',\n",
       "   'Zijian Li',\n",
       "   'Zhihao Liang'],\n",
       "  'tasks': ['Machine Translation', 'Translation'],\n",
       "  'date': '2017-11-16',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/speech2vec-a-sequence-to-sequence-framework',\n",
       "  'arxiv_id': '1803.08976',\n",
       "  'title': 'Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech',\n",
       "  'abstract': 'In this paper, we propose a novel deep neural network architecture,\\nSpeech2Vec, for learning fixed-length vector representations of audio segments\\nexcised from a speech corpus, where the vectors contain semantic information\\npertaining to the underlying spoken words, and are close to other vectors in\\nthe embedding space if their corresponding underlying spoken words are\\nsemantically similar. The proposed model can be viewed as a speech version of\\nWord2Vec. Its design is based on a RNN Encoder-Decoder framework, and borrows\\nthe methodology of skipgrams or continuous bag-of-words for training. Learning\\nword embeddings directly from speech enables Speech2Vec to make use of the\\nsemantic information carried by speech that does not exist in plain text. The\\nlearned word embeddings are evaluated and analyzed on 13 widely used word\\nsimilarity benchmarks, and outperform word embeddings learned by Word2Vec from\\nthe transcriptions.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.08976v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.08976v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yu-An Chung', 'James Glass'],\n",
       "  'tasks': ['Learning Word Embeddings', 'Word Embeddings', 'Word Similarity'],\n",
       "  'date': '2018-03-23',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['librispeech'],\n",
       "  'datasets_used_full': ['LibriSpeech'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/efficient-optimization-algorithms-for-robust',\n",
       "  'arxiv_id': '1806.03430',\n",
       "  'title': 'Efficient Optimization Algorithms for Robust Principal Component Analysis and Its Variants',\n",
       "  'abstract': 'Robust PCA has drawn significant attention in the last decade due to its\\nsuccess in numerous application domains, ranging from bio-informatics,\\nstatistics, and machine learning to image and video processing in computer\\nvision. Robust PCA and its variants such as sparse PCA and stable PCA can be\\nformulated as optimization problems with exploitable special structures. Many\\nspecialized efficient optimization methods have been proposed to solve robust\\nPCA and related problems. In this paper we review existing optimization methods\\nfor solving convex and nonconvex relaxations/variants of robust PCA, discuss\\ntheir advantages and disadvantages, and elaborate on their convergence\\nbehaviors. We also provide some insights for possible future research\\ndirections including new algorithmic frameworks that might be suitable for\\nimplementing on multi-processor setting to handle large-scale problems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03430v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03430v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Shiqian Ma', 'Necdet Serhat Aybat'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [{'name': 'PCA',\n",
       "    'full_name': 'Principal Components Analysis',\n",
       "    'description': '**Principle Components Analysis (PCA)** is an unsupervised method primary used for dimensionality reduction within machine learning.  PCA is calculated via a singular value decomposition (SVD) of the design matrix, or alternatively, by calculating the covariance matrix of the data and performing eigenvalue decomposition on the covariance matrix. The results of PCA provide a low-dimensional picture of the structure of the data and the leading (uncorrelated) latent factors determining variation in the data.\\r\\n\\r\\nImage Source: [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#/media/File:GaussianScatterPCA.svg)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Dimensionality Reduction',\n",
       "     'description': '**Dimensionality Reduction** methods transform data from a high-dimensional space into a low-dimensional space so that the low-dimensional space retains the most important properties of the original data. Below you can find a continuously updating list of dimensionality reduction methods.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/efficient-and-accurate-mri-super-resolution',\n",
       "  'arxiv_id': '1803.01417',\n",
       "  'title': 'Efficient and Accurate MRI Super-Resolution using a Generative Adversarial Network and 3D Multi-Level Densely Connected Network',\n",
       "  'abstract': 'High-resolution (HR) magnetic resonance images (MRI) provide detailed\\nanatomical information important for clinical application and quantitative\\nimage analysis. However, HR MRI conventionally comes at the cost of longer scan\\ntime, smaller spatial coverage, and lower signal-to-noise ratio (SNR). Recent\\nstudies have shown that single image super-resolution (SISR), a technique to\\nrecover HR details from one single low-resolution (LR) input image, could\\nprovide high-quality image details with the help of advanced deep convolutional\\nneural networks (CNN). However, deep neural networks consume memory heavily and\\nrun slowly, especially in 3D settings. In this paper, we propose a novel 3D\\nneural network design, namely a multi-level densely connected super-resolution\\nnetwork (mDCSRN) with generative adversarial network (GAN)-guided training. The\\nmDCSRN quickly trains and inferences and the GAN promotes realistic output\\nhardly distinguishable from original HR images. Our results from experiments on\\na dataset with 1,113 subjects show that our new architecture beats other\\npopular deep learning methods in recovering 4x resolution-downgraded im-ages\\nand runs 6x faster.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.01417v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.01417v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Yuhua Chen',\n",
       "   'Feng Shi',\n",
       "   'Anthony G. Christodoulou',\n",
       "   'Zhengwei Zhou',\n",
       "   'Yibin Xie',\n",
       "   'Debiao Li'],\n",
       "  'tasks': ['Image Super-Resolution', 'Super-Resolution'],\n",
       "  'date': '2018-03-04',\n",
       "  'methods': [{'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'GAN',\n",
       "    'full_name': 'Generative Adversarial Network',\n",
       "    'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\\r\\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\\r\\nprobability that a sample came from the training data rather than $G$.\\r\\n\\r\\nThe training procedure for $G$ is to maximize the probability of $D$ making\\r\\na mistake. This framework corresponds to a minimax two-player game. In the\\r\\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\\r\\nrecovering the training data distribution and $D$ equal to $\\\\frac{1}{2}$\\r\\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\\r\\nthe entire system can be trained with backpropagation. \\r\\n\\r\\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'https://arxiv.org/abs/1406.2661v1',\n",
       "    'source_title': 'Generative Adversarial Networks',\n",
       "    'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py',\n",
       "    'main_collection': {'name': 'Generative Models',\n",
       "     'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-continuous-hierarchies-in-the',\n",
       "  'arxiv_id': '1806.03417',\n",
       "  'title': 'Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry',\n",
       "  'abstract': \"We are concerned with the discovery of hierarchical relationships from\\nlarge-scale unstructured similarity scores. For this purpose, we study\\ndifferent models of hyperbolic space and find that learning embeddings in the\\nLorentz model is substantially more efficient than in the Poincar\\\\'e-ball\\nmodel. We show that the proposed approach allows us to learn high-quality\\nembeddings of large taxonomies which yield improvements over Poincar\\\\'e\\nembeddings, especially in low dimensions. Lastly, we apply our model to\\ndiscover hierarchies in two real-world datasets: we show that an embedding in\\nhyperbolic space can reveal important aspects of a company's organizational\\nstructure as well as reveal historical relationships between language families.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03417v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03417v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Maximilian Nickel', 'Douwe Kiela'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/analysis-of-minimax-error-rate-for',\n",
       "  'arxiv_id': '1802.04551',\n",
       "  'title': 'Analysis of Minimax Error Rate for Crowdsourcing and Its Application to Worker Clustering Model',\n",
       "  'abstract': 'While crowdsourcing has become an important means to label data, there is\\ngreat interest in estimating the ground truth from unreliable labels produced\\nby crowdworkers. The Dawid and Skene (DS) model is one of the most well-known\\nmodels in the study of crowdsourcing. Despite its practical popularity,\\ntheoretical error analysis for the DS model has been conducted only under\\nrestrictive assumptions on class priors, confusion matrices, or the number of\\nlabels each worker provides. In this paper, we derive a minimax error rate\\nunder more practical setting for a broader class of crowdsourcing models\\nincluding the DS model as a special case. We further propose the worker\\nclustering model, which is more practical than the DS model under real\\ncrowdsourcing settings. The wide applicability of our theoretical analysis\\nallows us to immediately investigate the behavior of this proposed model, which\\ncan not be analyzed by existing studies. Experimental results showed that there\\nis a strong similarity between the lower bound of the minimax error rate\\nderived by our theoretical analysis and the empirical error of the estimated\\nvalue.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.04551v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.04551v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Hideaki Imamura', 'Issei Sato', 'Masashi Sugiyama'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/joint-stem-detection-and-crop-weed',\n",
       "  'arxiv_id': '1806.03413',\n",
       "  'title': 'Joint Stem Detection and Crop-Weed Classification for Plant-specific Treatment in Precision Farming',\n",
       "  'abstract': 'Applying agrochemicals is the default procedure for conventional weed control\\nin crop production, but has negative impacts on the environment. Robots have\\nthe potential to treat every plant in the field individually and thus can\\nreduce the required use of such chemicals. To achieve that, robots need the\\nability to identify crops and weeds in the field and must additionally select\\neffective treatments. While certain types of weed can be treated mechanically,\\nother types need to be treated by (selective) spraying. In this paper, we\\npresent an approach that provides the necessary information for effective\\nplant-specific treatment. It outputs the stem location for weeds, which allows\\nfor mechanical treatments, and the covered area of the weed for selective\\nspraying. Our approach uses an end-to-end trainable fully convolutional network\\nthat simultaneously estimates stem positions as well as the covered area of\\ncrops and weeds. It jointly learns the class-wise stem detection and the\\npixel-wise semantic segmentation. Experimental evaluations on different\\nreal-world datasets show that our approach is able to reliably solve this\\nproblem. Compared to state-of-the-art approaches, our approach not only\\nsubstantially improves the stem detection accuracy, i.e., distinguishing crop\\nand weed stems, but also provides an improvement in the semantic segmentation\\nperformance.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03413v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03413v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Philipp Lottes',\n",
       "   'Jens Behley',\n",
       "   'Nived Chebrolu',\n",
       "   'Andres Milioto',\n",
       "   'Cyrill Stachniss'],\n",
       "  'tasks': ['General Classification', 'Semantic Segmentation'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/fully-convolutional-networks-with-sequential',\n",
       "  'arxiv_id': '1806.03412',\n",
       "  'title': 'Fully Convolutional Networks with Sequential Information for Robust Crop and Weed Detection in Precision Farming',\n",
       "  'abstract': 'Reducing the use of agrochemicals is an important component towards\\nsustainable agriculture. Robots that can perform targeted weed control offer\\nthe potential to contribute to this goal, for example, through specialized\\nweeding actions such as selective spraying or mechanical weed removal. A\\nprerequisite of such systems is a reliable and robust plant classification\\nsystem that is able to distinguish crop and weed in the field. A major\\nchallenge in this context is the fact that different fields show a large\\nvariability. Thus, classification systems have to robustly cope with\\nsubstantial environmental changes with respect to weed pressure and weed types,\\ngrowth stages of the crop, visual appearance, and soil conditions. In this\\npaper, we propose a novel crop-weed classification system that relies on a\\nfully convolutional network with an encoder-decoder structure and incorporates\\nspatial information by considering image sequences. Exploiting the crop\\narrangement information that is observable from the image sequences enables our\\nsystem to robustly estimate a pixel-wise labeling of the images into crop and\\nweed, i.e., a semantic segmentation. We provide a thorough experimental\\nevaluation, which shows that our system generalizes well to previously unseen\\nfields under varying environmental conditions --- a key capability to actually\\nuse such systems in precision framing. We provide comparisons to other\\nstate-of-the-art approaches and show that our system substantially improves the\\naccuracy of crop-weed classification without requiring a retraining of the\\nmodel.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03412v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03412v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Philipp Lottes',\n",
       "   'Jens Behley',\n",
       "   'Andres Milioto',\n",
       "   'Cyrill Stachniss'],\n",
       "  'tasks': ['Classification',\n",
       "   'General Classification',\n",
       "   'Semantic Segmentation'],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-scene-gist-with-convolutional-neural',\n",
       "  'arxiv_id': '1803.01967',\n",
       "  'title': 'Learning Scene Gist with Convolutional Neural Networks to Improve Object Recognition',\n",
       "  'abstract': \"Advancements in convolutional neural networks (CNNs) have made significant\\nstrides toward achieving high performance levels on multiple object recognition\\ntasks. While some approaches utilize information from the entire scene to\\npropose regions of interest, the task of interpreting a particular region or\\nobject is still performed independently of other objects and features in the\\nimage. Here we demonstrate that a scene's 'gist' can significantly contribute\\nto how well humans can recognize objects. These findings are consistent with\\nthe notion that humans foveate on an object and incorporate information from\\nthe periphery to aid in recognition. We use a biologically inspired two-part\\nconvolutional neural network ('GistNet') that models the fovea and periphery to\\nprovide a proof-of-principle demonstration that computational object\\nrecognition can significantly benefit from the gist of the scene as contextual\\ninformation. Our model yields accuracy improvements of up to 50% in certain\\nobject categories when incorporating contextual gist, while only increasing the\\noriginal model size by 5%. This proposed model mirrors our intuition about how\\nthe human visual system recognizes objects, suggesting specific biologically\\nplausible constraints to improve machine vision and building initial steps\\ntowards the challenge of scene understanding.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.01967v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.01967v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kevin Wu', 'Eric Wu', 'Gabriel Kreiman'],\n",
       "  'tasks': ['Object Recognition', 'Scene Understanding'],\n",
       "  'date': '2018-03-06',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deterministic-stretchy-regression',\n",
       "  'arxiv_id': '1806.03404',\n",
       "  'title': 'Deterministic Stretchy Regression',\n",
       "  'abstract': 'An extension of the regularized least-squares in which the estimation\\nparameters are stretchable is introduced and studied in this paper. The\\nsolution of this ridge regression with stretchable parameters is given in\\nprimal and dual spaces and in closed-form. Essentially, the proposed solution\\nstretches the covariance computation by a power term, thereby compressing or\\namplifying the estimation parameters. To maintain the computation of power root\\nterms within the real space, an input transformation is proposed. The results\\nof an empirical evaluation in both synthetic and real-world data illustrate\\nthat the proposed method is effective for compressive learning with\\nhigh-dimensional data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03404v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03404v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Kar-Ann Toh', 'Lei Sun', 'Zhiping Lin'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-09',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-probabilistic-framework-for-multi-view',\n",
       "  'arxiv_id': '1802.04630',\n",
       "  'title': 'A probabilistic framework for multi-view feature learning with many-to-many associations via neural networks',\n",
       "  'abstract': \"A simple framework Probabilistic Multi-view Graph Embedding (PMvGE) is\\nproposed for multi-view feature learning with many-to-many associations so that\\nit generalizes various existing multi-view methods. PMvGE is a probabilistic\\nmodel for predicting new associations via graph embedding of the nodes of data\\nvectors with links of their associations. Multi-view data vectors with\\nmany-to-many associations are transformed by neural networks to feature vectors\\nin a shared space, and the probability of new association between two data\\nvectors is modeled by the inner product of their feature vectors. While\\nexisting multi-view feature learning techniques can treat only either of\\nmany-to-many association or non-linear transformation, PMvGE can treat both\\nsimultaneously. By combining Mercer's theorem and the universal approximation\\ntheorem, we prove that PMvGE learns a wide class of similarity measures across\\nviews. Our likelihood-based estimator enables efficient computation of\\nnon-linear transformations of data vectors in large-scale datasets by minibatch\\nSGD, and numerical experiments illustrate that PMvGE outperforms existing\\nmulti-view methods.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.04630v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.04630v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Akifumi Okuno', 'Tetsuya Hada', 'Hidetoshi Shimodaira'],\n",
       "  'tasks': ['Graph Embedding'],\n",
       "  'date': '2018-02-13',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/unsupervised-learning-of-depth-and-ego-motion-2',\n",
       "  'arxiv_id': '1802.05522',\n",
       "  'title': 'Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints',\n",
       "  'abstract': 'We present a novel approach for unsupervised learning of depth and ego-motion\\nfrom monocular video. Unsupervised learning removes the need for separate\\nsupervisory signals (depth or ego-motion ground truth, or multi-view video).\\nPrior work in unsupervised depth learning uses pixel-wise or gradient-based\\nlosses, which only consider pixels in small local neighborhoods. Our main\\ncontribution is to explicitly consider the inferred 3D geometry of the scene,\\nenforcing consistency of the estimated 3D point clouds and ego-motion across\\nconsecutive frames. This is a challenging task and is solved by a novel\\n(approximate) backpropagation algorithm for aligning 3D structures.\\n  We combine this novel 3D-based loss with 2D losses based on photometric\\nquality of frame reconstructions using estimated depth and ego-motion from\\nadjacent frames. We also incorporate validity masks to avoid penalizing areas\\nin which no useful information exists.\\n  We test our algorithm on the KITTI dataset and on a video dataset captured on\\nan uncalibrated mobile phone camera. Our proposed approach consistently\\nimproves depth estimates on both datasets, and outperforms the state-of-the-art\\nfor both depth and ego-motion. Because we only require a simple video, learning\\ndepth and ego-motion on large and varied datasets becomes possible. We\\ndemonstrate this by training on the low quality uncalibrated video dataset and\\nevaluating on KITTI, ranking among top performing prior methods which are\\ntrained on KITTI itself.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.05522v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.05522v2.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Reza Mahjourian', 'Martin Wicke', 'Anelia Angelova'],\n",
       "  'tasks': ['Depth And Camera Motion', 'Depth Estimation'],\n",
       "  'date': '2018-02-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cityscapes', 'kitti'],\n",
       "  'datasets_used_full': ['Cityscapes', 'KITTI'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/going-deeper-in-spiking-neural-networks-vgg',\n",
       "  'arxiv_id': '1802.02627',\n",
       "  'title': 'Going Deeper in Spiking Neural Networks: VGG and Residual Architectures',\n",
       "  'abstract': 'Over the past few years, Spiking Neural Networks (SNNs) have become popular\\nas a possible pathway to enable low-power event-driven neuromorphic hardware.\\nHowever, their application in machine learning have largely been limited to\\nvery shallow neural network architectures for simple problems. In this paper,\\nwe propose a novel algorithmic technique for generating an SNN with a deep\\narchitecture, and demonstrate its effectiveness on complex visual recognition\\nproblems such as CIFAR-10 and ImageNet. Our technique applies to both VGG and\\nResidual network architectures, with significantly better accuracy than the\\nstate-of-the-art. Finally, we present analysis of the sparse event-driven\\ncomputations to demonstrate reduced hardware overhead when operating in the\\nspiking domain.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.02627v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.02627v4.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Abhronil Sengupta',\n",
       "   'Yuting Ye',\n",
       "   'Robert Wang',\n",
       "   'Chiao Liu',\n",
       "   'Kaushik Roy'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-02-07',\n",
       "  'methods': [{'name': 'Dropout',\n",
       "    'full_name': 'Dropout',\n",
       "    'description': '**Dropout** is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p=0.5$). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).\\r\\n\\r\\nThe idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://jmlr.org/papers/v15/srivastava14a.html',\n",
       "    'source_title': 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting',\n",
       "    'code_snippet_url': 'https://github.com/google/jax/blob/7f3078b70d0ed9bea6228efa420879c56f72ef69/jax/experimental/stax.py#L271-L275',\n",
       "    'main_collection': {'name': 'Regularization',\n",
       "     'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Dense Connections',\n",
       "    'full_name': 'Dense Connections',\n",
       "    'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\\\_{\\\\text{inputs}}*n\\\\_{\\\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\\r\\n\\r\\n$$h\\\\_{l} = g\\\\left(\\\\textbf{W}^{T}h\\\\_{l-1}\\\\right)$$\\r\\n\\r\\nwhere $g$ is an activation function.\\r\\n\\r\\nImage Source: Deep Learning by Goodfellow, Bengio and Courville',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Feedforward Networks',\n",
       "     'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'ReLU',\n",
       "    'full_name': 'Rectified Linear Units',\n",
       "    'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\\r\\n\\r\\n$$ f\\\\left(x\\\\right) = \\\\max\\\\left(0, x\\\\right) $$',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13',\n",
       "    'main_collection': {'name': 'Activation Functions',\n",
       "     'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Softmax',\n",
       "    'full_name': 'Softmax',\n",
       "    'description': \"The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\\r\\n\\r\\n$$ P(y=j \\\\mid{x}) = \\\\frac{e^{x^{T}w_{j}}}{\\\\sum^{K}_{k=1}e^{x^{T}wk}} $$\",\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Output Functions',\n",
       "     'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'VGG',\n",
       "    'full_name': 'VGG',\n",
       "    'description': '**VGG** is a classical convolutional neural network architecture. It was based on an analysis of how to increase the depth of such networks. The network utilises small 3 x 3 filters. Otherwise the network is characterized by its simplicity: the only other components being pooling layers and a fully connected layer.\\r\\n\\r\\nImage: [Davi Frossard](https://www.cs.toronto.edu/frossard/post/vgg16/)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1409.1556v6',\n",
       "    'source_title': 'Very Deep Convolutional Networks for Large-Scale Image Recognition',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/vgg.py#L24',\n",
       "    'main_collection': {'name': 'Convolutional Neural Networks',\n",
       "     'description': '**Convolutional Neural Networks** are used to extract features from images (and videos), employing convolutions as their primary operator. Below you can find a continuously updating list of convolutional neural networks.',\n",
       "     'parent': 'Image Models',\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': ['cifar-10', 'mnist'],\n",
       "  'datasets_used_full': ['CIFAR-10', 'MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/cs-vqa-visual-question-answering-with',\n",
       "  'arxiv_id': '1806.03379',\n",
       "  'title': 'CS-VQA: Visual Question Answering with Compressively Sensed Images',\n",
       "  'abstract': 'Visual Question Answering (VQA) is a complex semantic task requiring both\\nnatural language processing and visual recognition. In this paper, we explore\\nwhether VQA is solvable when images are captured in a sub-Nyquist compressive\\nparadigm. We develop a series of deep-network architectures that exploit\\navailable compressive data to increasing degrees of accuracy, and show that VQA\\nis indeed solvable in the compressed domain. Our results show that there is\\nnominal degradation in VQA performance when using compressive measurements, but\\nthat accuracy can be recovered when VQA pipelines are used in conjunction with\\nstate-of-the-art deep neural networks for CS reconstruction. The results\\npresented yield important implications for resource-constrained VQA\\napplications.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03379v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03379v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Li-Chi Huang',\n",
       "   'Kuldeep Kulkarni',\n",
       "   'Anik Jha',\n",
       "   'Suhas Lohit',\n",
       "   'Suren Jayasuriya',\n",
       "   'Pavan Turaga'],\n",
       "  'tasks': ['Question Answering', 'Visual Question Answering'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['visual-question-answering'],\n",
       "  'datasets_used_full': ['Visual Question Answering'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/self-supervisory-signals-for-object-discovery',\n",
       "  'arxiv_id': '1806.03370',\n",
       "  'title': 'Self-supervisory Signals for Object Discovery and Detection',\n",
       "  'abstract': 'In robotic applications, we often face the challenge of discovering new\\nobjects while having very little or no labelled training data. In this paper we\\nexplore the use of self-supervision provided by a robot traversing an\\nenvironment to learn representations of encountered objects. Knowledge of\\nego-motion and depth perception enables the agent to effectively associate\\nmultiple object proposals, which serve as training data for learning object\\nrepresentations from unlabelled images. We demonstrate the utility of this\\nrepresentation in two ways. First, we can automatically discover objects by\\nperforming clustering in the learned embedding space. Each resulting cluster\\ncontains examples of one instance seen from various viewpoints and scales.\\nSecond, given a small number of labeled images, we can efficiently learn\\ndetectors for these labels. In the few-shot regime, these detectors have a\\nsubstantially higher mAP of 0.22 compared to 0.12 of off-the-shelf standard\\ndetectors trained on this limited data. Thus, the proposed self-supervision\\nresults in effective environment specific object discovery and detection at no\\nor very small human labeling cost.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03370v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03370v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Etienne Pot', 'Alexander Toshev', 'Jana Kosecka'],\n",
       "  'tasks': ['Object Discovery'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['coco', 'avd'],\n",
       "  'datasets_used_full': ['COCO', 'AVD'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/sarcasmdetection-is-soooo-general-towards-a',\n",
       "  'arxiv_id': '1806.03369',\n",
       "  'title': '#SarcasmDetection is soooo general! Towards a Domain-Independent Approach for Detecting Sarcasm',\n",
       "  'abstract': 'Automatic sarcasm detection methods have traditionally been designed for\\nmaximum performance on a specific domain. This poses challenges for those\\nwishing to transfer those approaches to other existing or novel domains, which\\nmay be typified by very different language characteristics. We develop a\\ngeneral set of features and evaluate it under different training scenarios\\nutilizing in-domain and/or out-of-domain training data. The best-performing\\nscenario, training on both while employing a domain adaptation step, achieves\\nan F1 of 0.780, which is well above baseline F1-measures of 0.515 and 0.345. We\\nalso show that the approach outperforms the best results from prior work on the\\nsame target domain.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03369v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03369v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Natalie Parde', 'Rodney D. Nielsen'],\n",
       "  'tasks': ['Domain Adaptation', 'Sarcasm Detection'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-models-of-interactions-across-sets',\n",
       "  'arxiv_id': '1803.02879',\n",
       "  'title': 'Deep Models of Interactions Across Sets',\n",
       "  'abstract': \"We use deep learning to model interactions across two or more sets of\\nobjects, such as user-movie ratings, protein-drug bindings, or ternary\\nuser-item-tag interactions. The canonical representation of such interactions\\nis a matrix (or a higher-dimensional tensor) with an exchangeability property:\\nthe encoding's meaning is not changed by permuting rows or columns. We argue\\nthat models should hence be Permutation Equivariant (PE): constrained to make\\nthe same predictions across such permutations. We present a parameter-sharing\\nscheme and prove that it could not be made any more expressive without\\nviolating PE. This scheme yields three benefits. First, we demonstrate\\nstate-of-the-art performance on multiple matrix completion benchmarks. Second,\\nour models require a number of parameters independent of the numbers of\\nobjects, and thus scale well to large datasets. Third, models can be queried\\nabout new objects that were not available at training time, but for which\\ninteractions have since been observed. In experiments, our models achieved\\nsurprisingly good generalization performance on this matrix extrapolation task,\\nboth within domains (e.g., new users and new movies drawn from the same\\ndistribution used for training) and even across domains (e.g., predicting music\\nratings after training on movies).\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.02879v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.02879v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Jason Hartford',\n",
       "   'Devon R Graham',\n",
       "   'Kevin Leyton-Brown',\n",
       "   'Siamak Ravanbakhsh'],\n",
       "  'tasks': ['Collaborative Filtering',\n",
       "   'Matrix Completion',\n",
       "   'Recommendation Systems',\n",
       "   'TAG'],\n",
       "  'date': '2018-03-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['movielens', 'douban'],\n",
       "  'datasets_used_full': ['MovieLens', 'Douban'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dynamically-hierarchy-revolution-dirnet-for',\n",
       "  'arxiv_id': '1806.01248',\n",
       "  'title': 'Dynamically Hierarchy Revolution: DirNet for Compressing Recurrent Neural Network on Mobile Devices',\n",
       "  'abstract': 'Recurrent neural networks (RNNs) achieve cutting-edge performance on a\\nvariety of problems. However, due to their high computational and memory\\ndemands, deploying RNNs on resource constrained mobile devices is a challenging\\ntask. To guarantee minimum accuracy loss with higher compression rate and\\ndriven by the mobile resource requirement, we introduce a novel model\\ncompression approach DirNet based on an optimized fast dictionary learning\\nalgorithm, which 1) dynamically mines the dictionary atoms of the projection\\ndictionary matrix within layer to adjust the compression rate 2) adaptively\\nchanges the sparsity of sparse codes cross the hierarchical layers.\\nExperimental results on language model and an ASR model trained with a 1000h\\nspeech dataset demonstrate that our method significantly outperforms prior\\napproaches. Evaluated on off-the-shelf mobile devices, we are able to reduce\\nthe size of original model by eight times with real-time model inference and\\nnegligible accuracy loss.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.01248v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.01248v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jie Zhang', 'Xiaolong Wang', 'Dawei Li', 'Yalin Wang'],\n",
       "  'tasks': ['Dictionary Learning', 'Language Modelling', 'Model Compression'],\n",
       "  'date': '2018-06-04',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['penn-treebank', 'librispeech'],\n",
       "  'datasets_used_full': ['Penn Treebank', 'LibriSpeech'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/a-content-based-late-fusion-approach-applied',\n",
       "  'arxiv_id': '1806.03361',\n",
       "  'title': 'A Content-Based Late Fusion Approach Applied to Pedestrian Detection',\n",
       "  'abstract': 'The variety of pedestrians detectors proposed in recent years has encouraged\\nsome works to fuse pedestrian detectors to achieve a more accurate detection.\\nThe intuition behind is to combine the detectors based on its spatial\\nconsensus. We propose a novel method called Content-Based Spatial Consensus\\n(CSBC), which, in addition to relying on spatial consensus, considers the\\ncontent of the detection windows to learn a weighted-fusion of pedestrian\\ndetectors. The result is a reduction in false alarms and an enhancement in the\\ndetection. In this work, we also demonstrate that there is small influence of\\nthe feature used to learn the contents of the windows of each detector, which\\nenables our method to be efficient even employing simple features. The CSBC\\novercomes state-of-the-art fusion methods in the ETH dataset and in the Caltech\\ndataset. Particularly, our method is more efficient since fewer detectors are\\nnecessary to achieve expressive results.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03361v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03361v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Jessica Sena', 'Artur Jordao', 'William Robson Schwartz'],\n",
       "  'tasks': ['Pedestrian Detection'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['eth', 'inria-person'],\n",
       "  'datasets_used_full': ['ETH', 'INRIA Person'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/situated-mapping-of-sequential-instructions',\n",
       "  'arxiv_id': '1805.10209',\n",
       "  'title': 'Situated Mapping of Sequential Instructions to Actions with Single-step Reward Observation',\n",
       "  'abstract': 'We propose a learning approach for mapping context-dependent sequential\\ninstructions to actions. We address the problem of discourse and state\\ndependencies with an attention-based model that considers both the history of\\nthe interaction and the state of the world. To train from start and goal states\\nwithout access to demonstrations, we propose SESTRA, a learning algorithm that\\ntakes advantage of single-step reward observations and immediate expected\\nreward maximization. We evaluate on the SCONE domains, and show absolute\\naccuracy improvements of 9.8%-25.3% across the domains over approaches that use\\nhigh-level logical representations.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.10209v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.10209v2.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Alane Suhr', 'Yoav Artzi'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-05-25',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['atis'],\n",
       "  'datasets_used_full': ['ATIS'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/measuring-conversational-productivity-in',\n",
       "  'arxiv_id': '1806.03357',\n",
       "  'title': 'Measuring Conversational Productivity in Child Forensic Interviews',\n",
       "  'abstract': 'Child Forensic Interviewing (FI) presents a challenge for effective\\ninformation retrieval and decision making. The high stakes associated with the\\nprocess demand that expert legal interviewers are able to effectively establish\\na channel of communication and elicit substantive knowledge from the\\nchild-client while minimizing potential for experiencing trauma. As a first\\nstep toward computationally modeling and producing quality spoken interviewing\\nstrategies and a generalized understanding of interview dynamics, we propose a\\nnovel methodology to computationally model effectiveness criteria, by applying\\nsummarization and topic modeling techniques to objectively measure and rank the\\nresponsiveness and conversational productivity of a child during FI. We score\\ninformation retrieval by constructing an agenda to represent general topics of\\ninterest and measuring alignment with a given response and leveraging lexical\\nentrainment for responsiveness. For comparison, we present our methods along\\nwith traditional metrics of evaluation and discuss the use of prior information\\nfor generating situational awareness.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03357v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03357v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Victor Ardulov',\n",
       "   'Manoj Kumar',\n",
       "   'Shanna Williams',\n",
       "   'Thomas Lyon',\n",
       "   'Shrikanth Narayanan'],\n",
       "  'tasks': ['Decision Making', 'Information Retrieval'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/semi-amortized-variational-autoencoders',\n",
       "  'arxiv_id': '1802.02550',\n",
       "  'title': 'Semi-Amortized Variational Autoencoders',\n",
       "  'abstract': 'Amortized variational inference (AVI) replaces instance-specific local\\ninference with a global inference network. While AVI has enabled efficient\\ntraining of deep generative models such as variational autoencoders (VAE),\\nrecent empirical work suggests that inference networks can produce suboptimal\\nvariational parameters. We propose a hybrid approach, to use AVI to initialize\\nthe variational parameters and run stochastic variational inference (SVI) to\\nrefine them. Crucially, the local SVI procedure is itself differentiable, so\\nthe inference network and generative model can be trained end-to-end with\\ngradient-based optimization. This semi-amortized approach enables the use of\\nrich generative models without experiencing the posterior-collapse phenomenon\\ncommon in training VAEs for problems like text generation. Experiments show\\nthis approach outperforms strong autoregressive and variational baselines on\\nstandard text and image datasets.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.02550v7',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.02550v7.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Yoon Kim',\n",
       "   'Sam Wiseman',\n",
       "   'Andrew C. Miller',\n",
       "   'David Sontag',\n",
       "   'Alexander M. Rush'],\n",
       "  'tasks': ['Text Generation', 'Variational Inference'],\n",
       "  'date': '2018-02-07',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['omniglot-1'],\n",
       "  'datasets_used_full': ['Omniglot'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/the-effect-of-planning-shape-on-dyna-style',\n",
       "  'arxiv_id': '1806.01825',\n",
       "  'title': 'The Effect of Planning Shape on Dyna-style Planning in High-dimensional State Spaces',\n",
       "  'abstract': 'Dyna is a fundamental approach to model-based reinforcement learning (MBRL)\\nthat interleaves planning, acting, and learning in an online setting. In the\\nmost typical application of Dyna, the dynamics model is used to generate\\none-step transitions from selected start states from the agent\\'s history, which\\nare used to update the agent\\'s value function or policy as if they were real\\nexperiences. In this work, one-step Dyna was applied to several games from the\\nArcade Learning Environment (ALE). We found that the model-based updates\\noffered surprisingly little benefit over simply performing more updates with\\nthe agent\\'s existing experience, even when using a perfect model. We\\nhypothesize that to get the most from planning, the model must be used to\\ngenerate unfamiliar experience. To test this, we experimented with the \"shape\"\\nof planning in multiple different concrete instantiations of Dyna, performing\\nfewer, longer rollouts, rather than many short rollouts. We found that planning\\nshape has a profound impact on the efficacy of Dyna for both perfect and\\nlearned models. In addition to these findings regarding Dyna in general, our\\nresults represent, to our knowledge, the first time that a learned dynamics\\nmodel has been successfully used for planning in the ALE, suggesting that Dyna\\nmay be a viable approach to MBRL in the ALE and other high-dimensional\\nproblems.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.01825v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.01825v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['G. Zacharias Holland', 'Erin J. Talvitie', 'Michael Bowling'],\n",
       "  'tasks': ['Atari Games', 'Model-based Reinforcement Learning'],\n",
       "  'date': '2018-06-05',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['arcade-learning-environment'],\n",
       "  'datasets_used_full': ['Arcade Learning Environment'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/an-optimal-algorithm-for-online-unconstrained',\n",
       "  'arxiv_id': '1806.03349',\n",
       "  'title': 'An Optimal Algorithm for Online Unconstrained Submodular Maximization',\n",
       "  'abstract': \"We consider a basic problem at the interface of two fundamental fields:\\nsubmodular optimization and online learning. In the online unconstrained\\nsubmodular maximization (online USM) problem, there is a universe\\n$[n]=\\\\{1,2,...,n\\\\}$ and a sequence of $T$ nonnegative (not necessarily\\nmonotone) submodular functions arrive over time. The goal is to design a\\ncomputationally efficient online algorithm, which chooses a subset of $[n]$ at\\neach time step as a function only of the past, such that the accumulated value\\nof the chosen subsets is as close as possible to the maximum total value of a\\nfixed subset in hindsight. Our main result is a polynomial-time no-$1/2$-regret\\nalgorithm for this problem, meaning that for every sequence of nonnegative\\nsubmodular functions, the algorithm's expected total value is at least $1/2$\\ntimes that of the best subset in hindsight, up to an error term sublinear in\\n$T$. The factor of $1/2$ cannot be improved upon by any polynomial-time online\\nalgorithm when the submodular functions are presented as value oracles.\\nPrevious work on the offline problem implies that picking a subset uniformly at\\nrandom in each time step achieves zero $1/4$-regret.\\n  A byproduct of our techniques is an explicit subroutine for the two-experts\\nproblem that has an unusually strong regret guarantee: the total value of its\\nchoices is comparable to twice the total value of either expert on rounds it\\ndid not pick that expert. This subroutine may be of independent interest.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03349v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03349v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Tim Roughgarden', 'Joshua R. Wang'],\n",
       "  'tasks': ['online learning'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/dsslic-deep-semantic-segmentation-based',\n",
       "  'arxiv_id': '1806.03348',\n",
       "  'title': 'DSSLIC: Deep Semantic Segmentation-based Layered Image Compression',\n",
       "  'abstract': 'Deep learning has revolutionized many computer vision fields in the last few\\nyears, including learning-based image compression. In this paper, we propose a\\ndeep semantic segmentation-based layered image compression (DSSLIC) framework\\nin which the semantic segmentation map of the input image is obtained and\\nencoded as the base layer of the bit-stream. A compact representation of the\\ninput image is also generated and encoded as the first enhancement layer. The\\nsegmentation map and the compact version of the image are then employed to\\nobtain a coarse reconstruction of the image. The residual between the input and\\nthe coarse reconstruction is additionally encoded as another enhancement layer.\\nExperimental results show that the proposed framework outperforms the\\nH.265/HEVC-based BPG and other codecs in both PSNR and MS-SSIM metrics across a\\nwide range of bit rates in RGB domain. Besides, since semantic segmentation map\\nis included in the bit-stream, the proposed scheme can facilitate many other\\ntasks such as image search and object-based adaptive image compression.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03348v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03348v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mohammad Akbari', 'Jie Liang', 'Jingning Han'],\n",
       "  'tasks': ['Image Compression',\n",
       "   'Image Retrieval',\n",
       "   'MS-SSIM',\n",
       "   'Semantic Segmentation',\n",
       "   'SSIM'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cityscapes', 'ade20k'],\n",
       "  'datasets_used_full': ['Cityscapes', 'ADE20K'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/asp-learning-to-forget-with-adaptive-synaptic',\n",
       "  'arxiv_id': '1703.07655',\n",
       "  'title': 'ASP: Learning to Forget with Adaptive Synaptic Plasticity in Spiking Neural Networks',\n",
       "  'abstract': 'A fundamental feature of learning in animals is the \"ability to forget\" that\\nallows an organism to perceive, model and make decisions from disparate streams\\nof information and adapt to changing environments. Against this backdrop, we\\npresent a novel unsupervised learning mechanism ASP (Adaptive Synaptic\\nPlasticity) for improved recognition with Spiking Neural Networks (SNNs) for\\nreal time on-line learning in a dynamic environment. We incorporate an adaptive\\nweight decay mechanism with the traditional Spike Timing Dependent Plasticity\\n(STDP) learning to model adaptivity in SNNs. The leak rate of the synaptic\\nweights is modulated based on the temporal correlation between the spiking\\npatterns of the pre- and post-synaptic neurons. This mechanism helps in gradual\\nforgetting of insignificant data while retaining significant, yet old,\\ninformation. ASP, thus, maintains a balance between forgetting and immediate\\nlearning to construct a stable-plastic self-adaptive SNN for continuously\\nchanging inputs. We demonstrate that the proposed learning methodology\\naddresses catastrophic forgetting while yielding significantly improved\\naccuracy over the conventional STDP learning method for digit recognition\\napplications. Additionally, we observe that the proposed learning model\\nautomatically encodes selective attention towards relevant features in the\\ninput data while eliminating the influence of background noise (or denoising)\\nfurther improving the robustness of the ASP learning.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1703.07655v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1703.07655v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Priyadarshini Panda',\n",
       "   'Jason M. Allred',\n",
       "   'Shriram Ramanathan',\n",
       "   'Kaushik Roy'],\n",
       "  'tasks': ['Denoising'],\n",
       "  'date': '2017-03-22',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['mnist', 'noisy-mnist'],\n",
       "  'datasets_used_full': ['MNIST', 'n-MNIST'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/discovering-signals-from-web-sources-to',\n",
       "  'arxiv_id': '1806.03342',\n",
       "  'title': 'Discovering Signals from Web Sources to Predict Cyber Attacks',\n",
       "  'abstract': 'Cyber attacks are growing in frequency and severity. Over the past year alone\\nwe have witnessed massive data breaches that stole personal information of\\nmillions of people and wide-scale ransomware attacks that paralyzed critical\\ninfrastructure of several countries. Combating the rising cyber threat calls\\nfor a multi-pronged strategy, which includes predicting when these attacks will\\noccur. The intuition driving our approach is this: during the planning and\\npreparation stages, hackers leave digital traces of their activities on both\\nthe surface web and dark web in the form of discussions on platforms like\\nhacker forums, social media, blogs and the like. These data provide predictive\\nsignals that allow anticipating cyber attacks. In this paper, we describe\\nmachine learning techniques based on deep neural networks and autoregressive\\ntime series models that leverage external signals from publicly available Web\\nsources to forecast cyber attacks. Performance of our framework across ground\\ntruth data over real-world forecasting tasks shows that our methods yield a\\nsignificant lift or increase of F1 for the top signals on predicted cyber\\nattacks. Our results suggest that, when deployed, our system will be able to\\nprovide an effective line of defense against various types of targeted cyber\\nattacks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03342v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03342v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Palash Goyal',\n",
       "   'KSM Tozammel Hossain',\n",
       "   'Ashok Deb',\n",
       "   'Nazgol Tavabi',\n",
       "   'Nathan Bartley',\n",
       "   \"Andr'es Abeliuk\",\n",
       "   'Emilio Ferrara',\n",
       "   'Kristina Lerman'],\n",
       "  'tasks': ['Time Series'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-the-reward-function-for-a',\n",
       "  'arxiv_id': '1801.09624',\n",
       "  'title': 'Learning the Reward Function for a Misspecified Model',\n",
       "  'abstract': \"In model-based reinforcement learning it is typical to decouple the problems\\nof learning the dynamics model and learning the reward function. However, when\\nthe dynamics model is flawed, it may generate erroneous states that would never\\noccur in the true environment. It is not clear a priori what value the reward\\nfunction should assign to such states. This paper presents a novel error bound\\nthat accounts for the reward model's behavior in states sampled from the model.\\nThis bound is used to extend the existing Hallucinated DAgger-MC algorithm,\\nwhich offers theoretical performance guarantees in deterministic MDPs that do\\nnot assume a perfect model can be learned. Empirically, this approach to reward\\nlearning can yield dramatic improvements in control performance when the\\ndynamics model is flawed.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1801.09624v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1801.09624v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Erik Talvitie'],\n",
       "  'tasks': ['Model-based Reinforcement Learning'],\n",
       "  'date': '2018-01-29',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/randomized-prior-functions-for-deep',\n",
       "  'arxiv_id': '1806.03335',\n",
       "  'title': 'Randomized Prior Functions for Deep Reinforcement Learning',\n",
       "  'abstract': \"Dealing with uncertainty is essential for efficient reinforcement learning.\\nThere is a growing literature on uncertainty estimation for deep learning from\\nfixed datasets, but many of the most popular approaches are poorly-suited to\\nsequential decision problems. Other methods, such as bootstrap sampling, have\\nno mechanism for uncertainty that does not come from the observed data. We\\nhighlight why this can be a crucial shortcoming and propose a simple remedy\\nthrough addition of a randomized untrainable `prior' network to each ensemble\\nmember. We prove that this approach is efficient with linear representations,\\nprovide simple illustrations of its efficacy with nonlinear representations and\\nshow that this approach scales to large-scale problems far better than previous\\nattempts.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03335v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03335v2.pdf',\n",
       "  'proceeding': 'NeurIPS 2018 12',\n",
       "  'authors': ['Ian Osband', 'John Aslanides', 'Albin Cassirer'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/securing-distributed-machine-learning-in-high',\n",
       "  'arxiv_id': '1804.10140',\n",
       "  'title': 'Securing Distributed Gradient Descent in High Dimensional Statistical Learning',\n",
       "  'abstract': 'We consider unreliable distributed learning systems wherein the training data is kept confidential by external workers, and the learner has to interact closely with those workers to train a model. In particular, we assume that there exists a system adversary that can adaptively compromise some workers; the compromised workers deviate from their local designed specifications by sending out arbitrarily malicious messages. We assume in each communication round, up to $q$ out of the $m$ workers suffer Byzantine faults. Each worker keeps a local sample of size $n$ and the total sample size is $N=nm$. We propose a secured variant of the gradient descent method that can tolerate up to a constant fraction of Byzantine workers, i.e., $q/m = O(1)$. Moreover, we show the statistical estimation error of the iterates converges in $O(\\\\log N)$ rounds to $O(\\\\sqrt{q/N} + \\\\sqrt{d/N})$, where $d$ is the model dimension. As long as $q=O(d)$, our proposed algorithm achieves the optimal error rate $O(\\\\sqrt{d/N})$. Our results are obtained under some technical assumptions. Specifically, we assume strongly-convex population risk. Nevertheless, the empirical risk (sample version) is allowed to be non-convex. The core of our method is to robustly aggregate the gradients computed by the workers based on the filtering procedure proposed by Steinhardt et al. On the technical front, deviating from the existing literature on robustly estimating a finite-dimensional mean vector, we establish a {\\\\em uniform} concentration of the sample covariance matrix of gradients, and show that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function. To get a near-optimal uniform concentration bound, we develop a new matrix concentration inequality, which might be of independent interest.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1804.10140v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1804.10140v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Lili Su', 'Jiaming Xu'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-04-26',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/provable-defenses-against-adversarial',\n",
       "  'arxiv_id': '1711.00851',\n",
       "  'title': 'Provable defenses against adversarial examples via the convex outer adversarial polytope',\n",
       "  'abstract': 'We propose a method to learn deep ReLU-based classifiers that are provably\\nrobust against norm-bounded adversarial perturbations on the training data. For\\npreviously unseen examples, the approach is guaranteed to detect all\\nadversarial examples, though it may flag some non-adversarial examples as well.\\nThe basic idea is to consider a convex outer approximation of the set of\\nactivations reachable through a norm-bounded perturbation, and we develop a\\nrobust optimization procedure that minimizes the worst case loss over this\\nouter region (via a linear program). Crucially, we show that the dual problem\\nto this linear program can be represented itself as a deep network similar to\\nthe backpropagation network, leading to very efficient optimization approaches\\nthat produce guaranteed bounds on the robust loss. The end result is that by\\nexecuting a few more forward and backward passes through a slightly modified\\nversion of the original network (though possibly with much larger batch sizes),\\nwe can learn a classifier that is provably robust to any norm-bounded\\nadversarial attack. We illustrate the approach on a number of tasks to train\\nclassifiers with robust adversarial guarantees (e.g. for MNIST, we produce a\\nconvolutional classifier that provably has less than 5.8% test error for any\\nadversarial attack with bounded $\\\\ell_\\\\infty$ norm less than $\\\\epsilon = 0.1$),\\nand code for all experiments in the paper is available at\\nhttps://github.com/locuslab/convex_adversarial.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1711.00851v3',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1711.00851v3.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Eric Wong', 'J. Zico Kolter'],\n",
       "  'tasks': ['Adversarial Attack'],\n",
       "  'date': '2017-11-02',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['fashion-mnist', 'har'],\n",
       "  'datasets_used_full': ['Fashion-MNIST', 'HAR'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/learning-to-rank-for-censored-survival-data',\n",
       "  'arxiv_id': '1806.01984',\n",
       "  'title': 'Learning to rank for censored survival data',\n",
       "  'abstract': 'Survival analysis is a type of semi-supervised ranking task where the target\\noutput (the survival time) is often right-censored. Utilizing this information\\nis a challenge because it is not obvious how to correctly incorporate these\\ncensored examples into a model. We study how three categories of loss\\nfunctions, namely partial likelihood methods, rank methods, and our\\nclassification method based on a Wasserstein metric (WM) and the non-parametric\\nKaplan Meier estimate of the probability density to impute the labels of\\ncensored examples, can take advantage of this information. The proposed method\\nallows us to have a model that predict the probability distribution of an\\nevent. If a clinician had access to the detailed probability of an event over\\ntime this would help in treatment planning. For example, determining if the\\nrisk of kidney graft rejection is constant or peaked after some time. Also, we\\ndemonstrate that this approach directly optimizes the expected C-index which is\\nthe most common evaluation metric for ranking survival models.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.01984v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.01984v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Margaux Luck',\n",
       "   'Tristan Sylvain',\n",
       "   'Joseph Paul Cohen',\n",
       "   'Heloise Cardinal',\n",
       "   'Andrea Lodi',\n",
       "   'Yoshua Bengio'],\n",
       "  'tasks': ['Learning-To-Rank', 'Survival Analysis'],\n",
       "  'date': '2018-06-06',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/unsupervised-learning-for-surgical-motion-by',\n",
       "  'arxiv_id': '1806.03318',\n",
       "  'title': 'Unsupervised Learning for Surgical Motion by Learning to Predict the Future',\n",
       "  'abstract': 'We show that it is possible to learn meaningful representations of surgical\\nmotion, without supervision, by learning to predict the future. An architecture\\nthat combines an RNN encoder-decoder and mixture density networks (MDNs) is\\ndeveloped to model the conditional distribution over future motion given past\\nmotion. We show that the learned encodings naturally cluster according to\\nhigh-level activities, and we demonstrate the usefulness of these learned\\nencodings in the context of information retrieval, where a database of surgical\\nmotion is searched for suturing activity using a motion-based query. Future\\nprediction with MDNs is found to significantly outperform simpler baselines as\\nwell as the best previously-published result for this task, advancing\\nstate-of-the-art performance from an F1 score of 0.60 +- 0.14 to 0.77 +- 0.05.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03318v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03318v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Robert DiPietro', 'Gregory D. Hager'],\n",
       "  'tasks': ['Future prediction', 'Information Retrieval'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/adversarial-meta-learning',\n",
       "  'arxiv_id': '1806.03316',\n",
       "  'title': 'Adversarial Meta-Learning',\n",
       "  'abstract': 'Meta-learning enables a model to learn from very limited data to undertake a new task. In this paper, we study the general meta-learning with adversarial samples. We present a meta-learning algorithm, ADML (ADversarial Meta-Learner), which leverages clean and adversarial samples to optimize the initialization of a learning model in an adversarial manner. ADML leads to the following desirable properties: 1) it turns out to be very effective even in the cases with only clean samples; 2) it is robust to adversarial samples, i.e., unlike other meta-learning algorithms, it only leads to a minor performance degradation when there are adversarial samples; 3) it sheds light on tackling the cases with limited and even contaminated samples. It has been shown by extensive experimental results that ADML consistently outperforms three representative meta-learning algorithms in the cases involving adversarial samples, on two widely-used image datasets, MiniImageNet and CIFAR100, in terms of both accuracy and robustness.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03316v3',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03316v3.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Chengxiang Yin', 'Jian Tang', 'Zhiyuan Xu', 'Yanzhi Wang'],\n",
       "  'tasks': ['Meta-Learning'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['cifar-100', 'miniimagenet-1'],\n",
       "  'datasets_used_full': ['CIFAR-100', 'miniImageNet'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/stein-points',\n",
       "  'arxiv_id': '1803.10161',\n",
       "  'title': 'Stein Points',\n",
       "  'abstract': \"An important task in computational statistics and machine learning is to\\napproximate a posterior distribution $p(x)$ with an empirical measure supported\\non a set of representative points $\\\\{x_i\\\\}_{i=1}^n$. This paper focuses on\\nmethods where the selection of points is essentially deterministic, with an\\nemphasis on achieving accurate approximation when $n$ is small. To this end, we\\npresent `Stein Points'. The idea is to exploit either a greedy or a conditional\\ngradient method to iteratively minimise a kernel Stein discrepancy between the\\nempirical measure and $p(x)$. Our empirical results demonstrate that Stein\\nPoints enable accurate approximation of the posterior at modest computational\\ncost. In addition, theoretical results are provided to establish convergence of\\nthe method.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.10161v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.10161v4.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Wilson Ye Chen',\n",
       "   'Lester Mackey',\n",
       "   'Jackson Gorham',\n",
       "   'François-Xavier Briol',\n",
       "   'Chris. J. Oates'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-03-27',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/discriminability-objective-for-training',\n",
       "  'arxiv_id': '1803.04376',\n",
       "  'title': 'Discriminability objective for training descriptive captions',\n",
       "  'abstract': 'One property that remains lacking in image captions generated by contemporary\\nmethods is discriminability: being able to tell two images apart given the\\ncaption for one of them. We propose a way to improve this aspect of caption\\ngeneration. By incorporating into the captioning training objective a loss\\ncomponent directly related to ability (by a machine) to disambiguate\\nimage/caption matches, we obtain systems that produce much more discriminative\\ncaption, according to human evaluation. Remarkably, our approach leads to\\nimprovement in other aspects of generated captions, reflected by a battery of\\nstandard scores such as BLEU, SPICE etc. Our approach is modular and can be\\napplied to a variety of model/loss combinations commonly proposed for image\\ncaptioning.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1803.04376v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1803.04376v2.pdf',\n",
       "  'proceeding': 'CVPR 2018 6',\n",
       "  'authors': ['Ruotian Luo',\n",
       "   'Brian Price',\n",
       "   'Scott Cohen',\n",
       "   'Gregory Shakhnarovich'],\n",
       "  'tasks': ['Image Captioning'],\n",
       "  'date': '2018-03-12',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/curriculum-learning-by-transfer-learning',\n",
       "  'arxiv_id': '1802.03796',\n",
       "  'title': 'Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks',\n",
       "  'abstract': 'We provide theoretical investigation of curriculum learning in the context of\\nstochastic gradient descent when optimizing the convex linear regression loss.\\nWe prove that the rate of convergence of an ideal curriculum learning method is\\nmonotonically increasing with the difficulty of the examples. Moreover, among\\nall equally difficult points, convergence is faster when using points which\\nincur higher loss with respect to the current hypothesis. We then analyze\\ncurriculum learning in the context of training a CNN. We describe a method\\nwhich infers the curriculum by way of transfer learning from another network,\\npre-trained on a different task. While this approach can only approximate the\\nideal curriculum, we observe empirically similar behavior to the one predicted\\nby the theory, namely, a significant boost in convergence speed at the\\nbeginning of training. When the task is made more difficult, improvement in\\ngeneralization performance is also observed. Finally, curriculum learning\\nexhibits robustness against unfavorable conditions such as excessive\\nregularization.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1802.03796v4',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1802.03796v4.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Daphna Weinshall', 'Gad Cohen', 'Dan Amir'],\n",
       "  'tasks': ['Learning Theory', 'Transfer Learning'],\n",
       "  'date': '2018-02-11',\n",
       "  'methods': [{'name': 'Linear Regression',\n",
       "    'full_name': 'Linear Regression',\n",
       "    'description': '**Linear Regression** is a method for modelling a relationship between a dependent variable and independent variables. These models can be fit with numerous approaches. The most common is *least squares*, where we minimize the mean square error between the predicted values $\\\\hat{y} = \\\\textbf{X}\\\\hat{\\\\beta}$ and actual values $y$: $\\\\left(y-\\\\textbf{X}\\\\beta\\\\right)^{2}$.\\r\\n\\r\\nWe can also define the problem in probabilistic terms as a generalized linear model (GLM) where the pdf is a Gaussian distribution, and then perform maximum likelihood estimation to estimate $\\\\hat{\\\\beta}$.\\r\\n\\r\\nImage Source: [Wikipedia](https://en.wikipedia.org/wiki/Linear_regression)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Generalized Linear Models',\n",
       "     'description': '**Generalized Linear Models (GLMs)** are a class of models that generalize upon linear regression by allowing many more distributions to be modeled for the response variable via a link function. Below you can find a continuously updating list of GLMs.',\n",
       "     'parent': None,\n",
       "     'area': 'General'}}],\n",
       "  'datasets_used_lower': ['cifar-100'],\n",
       "  'datasets_used_full': ['CIFAR-100'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/policy-gradient-as-a-proxy-for-dynamic',\n",
       "  'arxiv_id': '1806.03290',\n",
       "  'title': 'Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing',\n",
       "  'abstract': \"Dynamic oracles provide strong supervision for training constituency parsers\\nwith exploration, but must be custom defined for a given parser's transition\\nsystem. We explore using a policy gradient method as a parser-agnostic\\nalternative. In addition to directly optimizing for a tree-level metric such as\\nF1, policy gradient has the potential to reduce exposure bias by allowing\\nexploration during training; moreover, it does not require a dynamic oracle for\\nsupervision. On four constituency parsers in three languages, the method\\nsubstantially outperforms static oracle likelihood training in almost all\\nsettings. For parsers where a dynamic oracle is available (including a novel\\noracle which we define for the transition system of Dyer et al. 2016), policy\\ngradient typically recaptures a substantial fraction of the performance gain\\nafforded by the dynamic oracle.\",\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03290v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03290v1.pdf',\n",
       "  'proceeding': 'ACL 2018 7',\n",
       "  'authors': ['Daniel Fried', 'Dan Klein'],\n",
       "  'tasks': ['Constituency Parsing'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['penn-treebank'],\n",
       "  'datasets_used_full': ['Penn Treebank'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/stabiliser-states-are-efficiently-pac',\n",
       "  'arxiv_id': '1705.00345',\n",
       "  'title': 'Stabiliser states are efficiently PAC-learnable',\n",
       "  'abstract': 'The exponential scaling of the wave function is a fundamental property of\\nquantum systems with far reaching implications in our ability to process\\nquantum information. A problem where these are particularly relevant is quantum\\nstate tomography. State tomography, whose objective is to obtain a full\\ndescription of a quantum system, can be analysed in the framework of\\ncomputational learning theory. In this model, quantum states have been shown to\\nbe Probably Approximately Correct (PAC)-learnable with sample complexity linear\\nin the number of qubits. However, it is conjectured that in general quantum\\nstates require an exponential amount of computation to be learned. Here, using\\nresults from the literature on the efficient classical simulation of quantum\\nsystems, we show that stabiliser states are efficiently PAC-learnable. Our\\nresults solve an open problem formulated by Aaronson [Proc. R. Soc. A, 2088,\\n(2007)] and propose learning theory as a tool for exploring the power of\\nquantum computation.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1705.00345v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1705.00345v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Andrea Rocchetto'],\n",
       "  'tasks': ['Learning Theory', 'Quantum State Tomography'],\n",
       "  'date': '2017-04-30',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/slalom-fast-verifiable-and-private-execution',\n",
       "  'arxiv_id': '1806.03287',\n",
       "  'title': 'Slalom: Fast, Verifiable and Private Execution of Neural Networks in Trusted Hardware',\n",
       "  'abstract': 'As Machine Learning (ML) gets applied to security-critical or sensitive\\ndomains, there is a growing need for integrity and privacy for outsourced ML\\ncomputations. A pragmatic solution comes from Trusted Execution Environments\\n(TEEs), which use hardware and software protections to isolate sensitive\\ncomputations from the untrusted software stack. However, these isolation\\nguarantees come at a price in performance, compared to untrusted alternatives.\\nThis paper initiates the study of high performance execution of Deep Neural\\nNetworks (DNNs) in TEEs by efficiently partitioning DNN computations between\\ntrusted and untrusted devices. Building upon an efficient outsourcing scheme\\nfor matrix multiplication, we propose Slalom, a framework that securely\\ndelegates execution of all linear layers in a DNN from a TEE (e.g., Intel SGX\\nor Sanctum) to a faster, yet untrusted, co-located processor. We evaluate\\nSlalom by running DNNs in an Intel SGX enclave, which selectively delegates\\nwork to an untrusted GPU. For canonical DNNs (VGG16, MobileNet and ResNet\\nvariants) we obtain 6x to 20x increases in throughput for verifiable inference,\\nand 4x to 11x for verifiable and private inference.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03287v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03287v2.pdf',\n",
       "  'proceeding': 'ICLR 2019 5',\n",
       "  'authors': ['Florian Tramèr', 'Dan Boneh'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': ['imagenet'],\n",
       "  'datasets_used_full': ['ImageNet'],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/nonparametric-regression-with-comparisons',\n",
       "  'arxiv_id': '1806.03286',\n",
       "  'title': 'Regression with Comparisons: Escaping the Curse of Dimensionality with Ordinal Information',\n",
       "  'abstract': 'In supervised learning, we typically leverage a fully labeled dataset to design methods for function estimation or prediction. In many practical situations, we are able to obtain alternative feedback, possibly at a low cost. A broad goal is to understand the usefulness of, and to design algorithms to exploit, this alternative feedback. In this paper, we consider a semi-supervised regression setting, where we obtain additional ordinal (or comparison) information for the unlabeled samples. We consider ordinal feedback of varying qualities where we have either a perfect ordering of the samples, a noisy ordering of the samples or noisy pairwise comparisons between the samples. We provide a precise quantification of the usefulness of these types of ordinal feedback in both nonparametric and linear regression, showing that in many cases it is possible to accurately estimate an underlying function with a very small labeled set, effectively \\\\emph{escaping the curse of dimensionality}. We also present lower bounds, that establish fundamental limits for the task and show that our algorithms are optimal in a variety of settings. Finally, we present extensive experiments on new datasets that demonstrate the efficacy and practicality of our algorithms and investigate their robustness to various sources of noise and model misspecification.',\n",
       "  'url_abs': 'https://arxiv.org/abs/1806.03286v2',\n",
       "  'url_pdf': 'https://arxiv.org/pdf/1806.03286v2.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Yichong Xu',\n",
       "   'Sivaraman Balakrishnan',\n",
       "   'Aarti Singh',\n",
       "   'Artur Dubrawski'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/blind-justice-fairness-with-encrypted',\n",
       "  'arxiv_id': '1806.03281',\n",
       "  'title': 'Blind Justice: Fairness with Encrypted Sensitive Attributes',\n",
       "  'abstract': 'Recent work has explored how to train machine learning models which do not\\ndiscriminate against any subgroup of the population as determined by sensitive\\nattributes such as gender or race. To avoid disparate treatment, sensitive\\nattributes should not be considered. On the other hand, in order to avoid\\ndisparate impact, sensitive attributes must be examined, e.g., in order to\\nlearn a fair model, or to check if a given model is fair. We introduce methods\\nfrom secure multi-party computation which allow us to avoid both. By encrypting\\nsensitive attributes, we show how an outcome-based fair model may be learned,\\nchecked, or have its outputs verified and held to account, without users\\nrevealing their sensitive attributes.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03281v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03281v1.pdf',\n",
       "  'proceeding': 'ICML 2018 7',\n",
       "  'authors': ['Niki Kilbertus',\n",
       "   'Adrià Gascón',\n",
       "   'Matt J. Kusner',\n",
       "   'Michael Veale',\n",
       "   'Krishna P. Gummadi',\n",
       "   'Adrian Weller'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/multilingual-neural-machine-translation-with',\n",
       "  'arxiv_id': '1806.03280',\n",
       "  'title': 'Multilingual Neural Machine Translation with Task-Specific Attention',\n",
       "  'abstract': 'Multilingual machine translation addresses the task of translating between\\nmultiple source and target languages. We propose task-specific attention\\nmodels, a simple but effective technique for improving the quality of\\nsequence-to-sequence neural multilingual translation. Our approach seeks to\\nretain as much of the parameter sharing generalization of NMT models as\\npossible, while still allowing for language-specific specialization of the\\nattention model to a particular language-pair or task. Our experiments on four\\nlanguages of the Europarl corpus show that using a target-specific model of\\nattention provides consistent gains in translation quality for all possible\\ntranslation directions, compared to a model in which all parameters are shared.\\nWe observe improved translation quality even in the (extreme) low-resource\\nzero-shot translation directions for which the model never saw explicitly\\npaired parallel data.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03280v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03280v1.pdf',\n",
       "  'proceeding': 'COLING 2018 8',\n",
       "  'authors': ['Graeme Blackwood', 'Miguel Ballesteros', 'Todd Ward'],\n",
       "  'tasks': ['Machine Translation', 'Translation'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/towards-dependability-metrics-for-neural',\n",
       "  'arxiv_id': '1806.02338',\n",
       "  'title': 'Towards Dependability Metrics for Neural Networks',\n",
       "  'abstract': 'Artificial neural networks (NN) are instrumental in realizing\\nhighly-automated driving functionality. An overarching challenge is to identify\\nbest safety engineering practices for NN and other learning-enabled components.\\nIn particular, there is an urgent need for an adequate set of metrics for\\nmeasuring all-important NN dependability attributes. We address this challenge\\nby proposing a number of NN-specific and efficiently computable metrics for\\nmeasuring NN dependability attributes including robustness, interpretability,\\ncompleteness, and correctness.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.02338v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.02338v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Chih-Hong Cheng',\n",
       "   'Georg Nührenberg',\n",
       "   'Chung-Hao Huang',\n",
       "   'Harald Ruess',\n",
       "   'Hirotoshi Yasuoka'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-06',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/cuisinenet-food-attributes-classification',\n",
       "  'arxiv_id': '1805.12081',\n",
       "  'title': 'CuisineNet: Food Attributes Classification using Multi-scale Convolution Network',\n",
       "  'abstract': 'Diversity of food and its attributes represents the culinary habits of\\npeoples from different countries. Thus, this paper addresses the problem of\\nidentifying food culture of people around the world and its flavor by\\nclassifying two main food attributes, cuisine and flavor. A deep learning model\\nbased on multi-scale convotuional networks is proposed for extracting more\\naccurate features from input images. The aggregation of multi-scale convolution\\nlayers with different kernel size is also used for weighting the features\\nresults from different scales. In addition, a joint loss function based on\\nNegative Log Likelihood (NLL) is used to fit the model probability to multi\\nlabeled classes for multi-modal classification task. Furthermore, this work\\nprovides a new dataset for food attributes, so-called Yummly48K, extracted from\\nthe popular food website, Yummly. Our model is assessed on the constructed\\nYummly48K dataset. The experimental results show that our proposed method\\nyields 65% and 62% average F1 score on validation and test set which\\noutperforming the state-of-the-art models.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1805.12081v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1805.12081v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Md. Mostafa Kamal Sarker',\n",
       "   'Mohammed Jabreel',\n",
       "   'Hatem A. Rashwan',\n",
       "   'Syeda Furruka Banu',\n",
       "   'Antonio Moreno',\n",
       "   'Petia Radeva',\n",
       "   'Domenec Puig'],\n",
       "  'tasks': ['Classification', 'General Classification'],\n",
       "  'date': '2018-05-30',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/orbital-petri-nets-a-novel-petri-net-approach',\n",
       "  'arxiv_id': '1806.03267',\n",
       "  'title': 'Orbital Petri Nets: A Novel Petri Net Approach',\n",
       "  'abstract': 'Petri Nets is very interesting tool for studying and simulating different\\nbehaviors of information systems. It can be used in different applications\\nbased on the appropriate class of Petri Nets whereas it is classical, colored\\nor timed Petri Nets. In this paper we introduce a new approach of Petri Nets\\ncalled orbital Petri Nets (OPN) for studying the orbital rotating systems\\nwithin a specific domain. The study investigated and analyzed OPN with\\nhighlighting the problem of space debris collision problem as a case study. The\\nmathematical investigation results of two OPN models proved that space debris\\ncollision problem can be prevented based on the new method of firing sequence\\nin OPN. By this study, new smart algorithms can be implemented and simulated by\\norbital Petri Nets for mitigating the space debris collision problem as a next\\nwork.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03267v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03267v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Mohamed Yorky', 'Aboul Ella Hassanien'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/sheep-identity-recognition-age-and-weight',\n",
       "  'arxiv_id': '1806.04017',\n",
       "  'title': 'Sheep identity recognition, age and weight estimation datasets',\n",
       "  'abstract': 'Increased interest of scientists, producers and consumers in sheep\\nidentification has been stimulated by the dramatic increase in population and\\nthe urge to increase productivity. The world population is expected to exceed\\n9.6 million in 2050. For this reason, awareness is raised towards the necessity\\nof effective livestock production. Sheep is considered as one of the main of\\nfood resources. Most of the research now is directed towards developing real\\ntime applications that facilitate sheep identification for breed management and\\ngathering related information like weight and age. Weight and age are key\\nmatrices in assessing the effectiveness of production. For this reason, visual\\nanalysis proved recently its significant success over other approaches. Visual\\nanalysis techniques need enough images for testing and study completion. For\\nthis reason, collecting sheep images database is a vital step to fulfill such\\nobjective. We provide here datasets for testing and comparing such algorithms\\nwhich are under development. Our collected dataset consists of 416 color images\\nfor different features of sheep in different postures. Images were collected\\nfifty two sheep at a range of year from three months to six years. For each\\nsheep, two images were captured for both sides of the body, two images for both\\nsides of the face, one image from the top view, one image for the hip and one\\nimage for the teeth. The collected images cover different illumination, quality\\nlevels and angle of rotation. The allocated data set can be used to test sheep\\nidentification, weigh estimation, and age detection algorithms. Such algorithms\\nare crucial for disease management, animal assessment and ownership.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.04017v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.04017v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Aya Salama Abdelhady', 'Aboul Ella Hassanenin', 'Aly Fahmy'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/patchfcn-for-intracranial-hemorrhage',\n",
       "  'arxiv_id': '1806.03265',\n",
       "  'title': 'PatchFCN for Intracranial Hemorrhage Detection',\n",
       "  'abstract': 'This paper studies the problem of detecting and segmenting acute intracranial\\nhemorrhage on head computed tomography (CT) scans. We propose to solve both\\ntasks as a semantic segmentation problem using a patch-based fully\\nconvolutional network (PatchFCN). This formulation allows us to accurately\\nlocalize hemorrhages while bypassing the complexity of object detection. Our\\nsystem demonstrates competitive performance with a human expert and the\\nstate-of-the-art on classification tasks (0.976, 0.966 AUC of ROC on\\nretrospective and prospective test sets) and on segmentation tasks (0.785 pixel\\nAP, 0.766 Dice score), while using much less data and a simpler system. In\\naddition, we conduct a series of controlled experiments to understand \"why\"\\nPatchFCN outperforms standard FCN. Our studies show that PatchFCN finds a good\\ntrade-off between batch diversity and the amount of context during training.\\nThese findings may also apply to other medical segmentation tasks.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03265v2',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03265v2.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Wei-cheng Kuo',\n",
       "   'Christian Häne',\n",
       "   'Esther Yuh',\n",
       "   'Pratik Mukherjee',\n",
       "   'Jitendra Malik'],\n",
       "  'tasks': ['Computed Tomography (CT)',\n",
       "   'Object Detection',\n",
       "   'Semantic Segmentation'],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [{'name': 'Max Pooling',\n",
       "    'full_name': 'Max Pooling',\n",
       "    'description': '**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\\r\\n\\r\\nImage Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Pooling Operations',\n",
       "     'description': '**Pooling Operations** are used to pool features together, often downsampling the feature map to a smaller size. They can also induce favourable properties such as translation invariance in image classification, as well as bring together information from different parts of a network in tasks like object detection (e.g. pooling different scales). ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'Convolution',\n",
       "    'full_name': 'Convolution',\n",
       "    'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\\r\\n\\r\\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)',\n",
       "    'introduced_year': 1980,\n",
       "    'source_url': None,\n",
       "    'source_title': None,\n",
       "    'code_snippet_url': None,\n",
       "    'main_collection': {'name': 'Convolutions',\n",
       "     'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.',\n",
       "     'parent': 'Image Feature Extractors',\n",
       "     'area': 'Computer Vision'}},\n",
       "   {'name': 'FCN',\n",
       "    'full_name': 'Fully Convolutional Network',\n",
       "    'description': '**Fully Convolutional Networks**, or **FCNs**, are an architecture used mainly for semantic segmentation. They employ solely locally connected layers, such as [convolution](https://paperswithcode.com/method/convolution), pooling and upsampling. Avoiding the use of dense layers means less parameters (making the networks faster to train). It also means an FCN can work for variable image sizes given all connections are local.\\r\\n\\r\\nThe network consists of a downsampling path, used to extract and interpret the context, and an upsampling path, which allows for localization. \\r\\n\\r\\nFCNs also employ skip connections to recover the fine-grained spatial information lost in the downsampling path.',\n",
       "    'introduced_year': 2000,\n",
       "    'source_url': 'http://arxiv.org/abs/1605.06211v1',\n",
       "    'source_title': 'Fully Convolutional Networks for Semantic Segmentation',\n",
       "    'code_snippet_url': 'https://github.com/pytorch/vision/blob/bf843c664b8ba0ff49d2921237500c77d82f2d04/torchvision/models/segmentation/fcn.py#L9',\n",
       "    'main_collection': {'name': 'Semantic Segmentation Models',\n",
       "     'description': '**Semantic Segmentation Models** are a class of methods that address the task of semantically segmenting an image into different object classes. Below you can find a continuously updating list of semantic segmentation models. ',\n",
       "     'parent': None,\n",
       "     'area': 'Computer Vision'}}],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/information-based-inference-for-singular',\n",
       "  'arxiv_id': '1506.05855',\n",
       "  'title': 'Information-based inference for singular models and finite sample sizes: A frequentist information criterion',\n",
       "  'abstract': 'In the information-based paradigm of inference, model selection is performed\\nby selecting the candidate model with the best estimated predictive\\nperformance. The success of this approach depends on the accuracy of the\\nestimate of the predictive complexity. In the large-sample-size limit of a\\nregular model, the predictive performance is well estimated by the Akaike\\nInformation Criterion (AIC). However, this approximation can either\\nsignificantly under or over-estimating the complexity in a wide range of\\nimportant applications where models are either non-regular or\\nfinite-sample-size corrections are significant. We introduce an improved\\napproximation for the complexity that is used to define a new information\\ncriterion: the Frequentist Information Criterion (QIC). QIC extends the\\napplicability of information-based inference to the finite-sample-size regime\\nof regular models and to singular models. We demonstrate the power and the\\ncomparative advantage of QIC in a number of example analyses.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1506.05855v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1506.05855v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Colin H. LaMont', 'Paul A. Wiggins'],\n",
       "  'tasks': ['Model Selection'],\n",
       "  'date': '2015-06-19',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/deep-learning-with-convolutional-neural',\n",
       "  'arxiv_id': '1703.05051',\n",
       "  'title': 'Deep learning with convolutional neural networks for EEG decoding and visualization',\n",
       "  'abstract': 'PLEASE READ AND CITE THE REVISED VERSION at Human Brain Mapping:\\nhttp://onlinelibrary.wiley.com/doi/10.1002/hbm.23730/full\\n  Code available here: https://github.com/robintibor/braindecode',\n",
       "  'url_abs': 'http://arxiv.org/abs/1703.05051v5',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1703.05051v5.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Robin Tibor Schirrmeister',\n",
       "   'Jost Tobias Springenberg',\n",
       "   'Lukas Dominique Josef Fiederer',\n",
       "   'Martin Glasstetter',\n",
       "   'Katharina Eggensperger',\n",
       "   'Michael Tangermann',\n",
       "   'Frank Hutter',\n",
       "   'Wolfram Burgard',\n",
       "   'Tonio Ball'],\n",
       "  'tasks': ['EEG', 'Eeg Decoding'],\n",
       "  'date': '2017-03-15',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " {'paper_url': 'https://paperswithcode.com/paper/automatic-view-planning-with-multi-scale-deep',\n",
       "  'arxiv_id': '1806.03228',\n",
       "  'title': 'Automatic View Planning with Multi-scale Deep Reinforcement Learning Agents',\n",
       "  'abstract': 'We propose a fully automatic method to find standardized view planes in 3D\\nimage acquisitions. Standard view images are important in clinical practice as\\nthey provide a means to perform biometric measurements from similar anatomical\\nregions. These views are often constrained to the native orientation of a 3D\\nimage acquisition. Navigating through target anatomy to find the required view\\nplane is tedious and operator-dependent. For this task, we employ a multi-scale\\nreinforcement learning (RL) agent framework and extensively evaluate several\\nDeep Q-Network (DQN) based strategies. RL enables a natural learning paradigm\\nby interaction with the environment, which can be used to mimic experienced\\noperators. We evaluate our results using the distance between the anatomical\\nlandmarks and detected planes, and the angles between their normal vector and\\ntarget. The proposed algorithm is assessed on the mid-sagittal and\\nanterior-posterior commissure planes of brain MRI, and the 4-chamber long-axis\\nplane commonly used in cardiac MRI, achieving accuracy of 1.53mm, 1.98mm and\\n4.84mm, respectively.',\n",
       "  'url_abs': 'http://arxiv.org/abs/1806.03228v1',\n",
       "  'url_pdf': 'http://arxiv.org/pdf/1806.03228v1.pdf',\n",
       "  'proceeding': None,\n",
       "  'authors': ['Amir Alansary',\n",
       "   'Loic Le Folgoc',\n",
       "   'Ghislain Vaillant',\n",
       "   'Ozan Oktay',\n",
       "   'Yuanwei Li',\n",
       "   'Wenjia Bai',\n",
       "   'Jonathan Passerat-Palmbach',\n",
       "   'Ricardo Guerrero',\n",
       "   'Konstantinos Kamnitsas',\n",
       "   'Benjamin Hou',\n",
       "   'Steven McDonagh',\n",
       "   'Ben Glocker',\n",
       "   'Bernhard Kainz',\n",
       "   'Daniel Rueckert'],\n",
       "  'tasks': [],\n",
       "  'date': '2018-06-08',\n",
       "  'methods': [],\n",
       "  'datasets_used_lower': [],\n",
       "  'datasets_used_full': [],\n",
       "  'datasets_introduced_lower': [],\n",
       "  'datasets_introduced_full': []},\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p['paper_url'] for p in old_papers]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
